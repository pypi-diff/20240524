# Comparing `tmp/compressed_tensors_nightly-0.3.3.20240522-py3-none-any.whl.zip` & `tmp/compressed_tensors_nightly-0.3.3.20240523-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,45 +1,45 @@
-Zip file size: 62668 bytes, number of entries: 43
--rw-r--r--  2.0 unx      789 b- defN 24-May-22 00:03 compressed_tensors/__init__.py
--rw-r--r--  2.0 unx      755 b- defN 24-May-22 00:03 compressed_tensors/base.py
--rw-r--r--  2.0 unx     1512 b- defN 24-May-22 00:03 compressed_tensors/version.py
--rw-r--r--  2.0 unx      992 b- defN 24-May-22 00:03 compressed_tensors/compressors/__init__.py
--rw-r--r--  2.0 unx     2134 b- defN 24-May-22 00:03 compressed_tensors/compressors/base.py
--rw-r--r--  2.0 unx     1257 b- defN 24-May-22 00:03 compressed_tensors/compressors/dense.py
--rw-r--r--  2.0 unx     5403 b- defN 24-May-22 00:03 compressed_tensors/compressors/helpers.py
--rw-r--r--  2.0 unx     4744 b- defN 24-May-22 00:03 compressed_tensors/compressors/int_quantized.py
--rw-r--r--  2.0 unx    10426 b- defN 24-May-22 00:03 compressed_tensors/compressors/model_compressor.py
--rw-r--r--  2.0 unx     7885 b- defN 24-May-22 00:03 compressed_tensors/compressors/pack_quantized.py
--rw-r--r--  2.0 unx     8637 b- defN 24-May-22 00:03 compressed_tensors/compressors/sparse_bitmask.py
--rw-r--r--  2.0 unx      704 b- defN 24-May-22 00:03 compressed_tensors/config/__init__.py
--rw-r--r--  2.0 unx     1454 b- defN 24-May-22 00:03 compressed_tensors/config/base.py
--rw-r--r--  2.0 unx     1317 b- defN 24-May-22 00:03 compressed_tensors/config/dense.py
--rw-r--r--  2.0 unx     1308 b- defN 24-May-22 00:03 compressed_tensors/config/sparse_bitmask.py
--rw-r--r--  2.0 unx      760 b- defN 24-May-22 00:03 compressed_tensors/quantization/__init__.py
--rw-r--r--  2.0 unx     4360 b- defN 24-May-22 00:03 compressed_tensors/quantization/quant_args.py
--rw-r--r--  2.0 unx     8042 b- defN 24-May-22 00:03 compressed_tensors/quantization/quant_config.py
--rw-r--r--  2.0 unx     1480 b- defN 24-May-22 00:03 compressed_tensors/quantization/quant_scheme.py
--rw-r--r--  2.0 unx      798 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/__init__.py
--rw-r--r--  2.0 unx     7625 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/apply.py
--rw-r--r--  2.0 unx     1776 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/calibration.py
--rw-r--r--  2.0 unx     2247 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/compressed.py
--rw-r--r--  2.0 unx    10520 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/forward.py
--rw-r--r--  2.0 unx     1726 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/frozen.py
--rw-r--r--  2.0 unx     3697 b- defN 24-May-22 00:03 compressed_tensors/quantization/lifecycle/initialize.py
--rw-r--r--  2.0 unx      745 b- defN 24-May-22 00:03 compressed_tensors/quantization/observers/__init__.py
--rw-r--r--  2.0 unx     4900 b- defN 24-May-22 00:03 compressed_tensors/quantization/observers/base.py
--rw-r--r--  2.0 unx     2166 b- defN 24-May-22 00:03 compressed_tensors/quantization/observers/helpers.py
--rw-r--r--  2.0 unx     2045 b- defN 24-May-22 00:03 compressed_tensors/quantization/observers/memoryless.py
--rw-r--r--  2.0 unx     2861 b- defN 24-May-22 00:03 compressed_tensors/quantization/observers/min_max.py
--rw-r--r--  2.0 unx      656 b- defN 24-May-22 00:03 compressed_tensors/quantization/utils/__init__.py
--rw-r--r--  2.0 unx     6017 b- defN 24-May-22 00:03 compressed_tensors/quantization/utils/helpers.py
--rw-r--r--  2.0 unx      658 b- defN 24-May-22 00:03 compressed_tensors/registry/__init__.py
--rw-r--r--  2.0 unx    11890 b- defN 24-May-22 00:03 compressed_tensors/registry/registry.py
--rw-r--r--  2.0 unx      665 b- defN 24-May-22 00:03 compressed_tensors/utils/__init__.py
--rw-r--r--  2.0 unx     1735 b- defN 24-May-22 00:03 compressed_tensors/utils/helpers.py
--rw-r--r--  2.0 unx     8502 b- defN 24-May-22 00:03 compressed_tensors/utils/safetensors_load.py
--rw-r--r--  2.0 unx    11357 b- defN 24-May-22 00:05 compressed_tensors_nightly-0.3.3.20240522.dist-info/LICENSE
--rw-r--r--  2.0 unx     5633 b- defN 24-May-22 00:05 compressed_tensors_nightly-0.3.3.20240522.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-22 00:05 compressed_tensors_nightly-0.3.3.20240522.dist-info/WHEEL
--rw-r--r--  2.0 unx       19 b- defN 24-May-22 00:05 compressed_tensors_nightly-0.3.3.20240522.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4376 b- defN 24-May-22 00:05 compressed_tensors_nightly-0.3.3.20240522.dist-info/RECORD
-43 files, 156665 bytes uncompressed, 55400 bytes compressed:  64.6%
+Zip file size: 63783 bytes, number of entries: 43
+-rw-r--r--  2.0 unx      789 b- defN 24-May-23 00:03 compressed_tensors/__init__.py
+-rw-r--r--  2.0 unx      755 b- defN 24-May-23 00:03 compressed_tensors/base.py
+-rw-r--r--  2.0 unx     1512 b- defN 24-May-23 00:03 compressed_tensors/version.py
+-rw-r--r--  2.0 unx      992 b- defN 24-May-23 00:03 compressed_tensors/compressors/__init__.py
+-rw-r--r--  2.0 unx     2134 b- defN 24-May-23 00:03 compressed_tensors/compressors/base.py
+-rw-r--r--  2.0 unx     1257 b- defN 24-May-23 00:03 compressed_tensors/compressors/dense.py
+-rw-r--r--  2.0 unx     5403 b- defN 24-May-23 00:03 compressed_tensors/compressors/helpers.py
+-rw-r--r--  2.0 unx     4744 b- defN 24-May-23 00:03 compressed_tensors/compressors/int_quantized.py
+-rw-r--r--  2.0 unx    10426 b- defN 24-May-23 00:03 compressed_tensors/compressors/model_compressor.py
+-rw-r--r--  2.0 unx     7885 b- defN 24-May-23 00:03 compressed_tensors/compressors/pack_quantized.py
+-rw-r--r--  2.0 unx     8637 b- defN 24-May-23 00:03 compressed_tensors/compressors/sparse_bitmask.py
+-rw-r--r--  2.0 unx      704 b- defN 24-May-23 00:03 compressed_tensors/config/__init__.py
+-rw-r--r--  2.0 unx     1454 b- defN 24-May-23 00:03 compressed_tensors/config/base.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-May-23 00:03 compressed_tensors/config/dense.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-May-23 00:03 compressed_tensors/config/sparse_bitmask.py
+-rw-r--r--  2.0 unx      760 b- defN 24-May-23 00:03 compressed_tensors/quantization/__init__.py
+-rw-r--r--  2.0 unx     4360 b- defN 24-May-23 00:03 compressed_tensors/quantization/quant_args.py
+-rw-r--r--  2.0 unx     8727 b- defN 24-May-23 00:03 compressed_tensors/quantization/quant_config.py
+-rw-r--r--  2.0 unx     3341 b- defN 24-May-23 00:03 compressed_tensors/quantization/quant_scheme.py
+-rw-r--r--  2.0 unx      798 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/__init__.py
+-rw-r--r--  2.0 unx     7625 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/apply.py
+-rw-r--r--  2.0 unx     1776 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/calibration.py
+-rw-r--r--  2.0 unx     2247 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/compressed.py
+-rw-r--r--  2.0 unx    10520 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/forward.py
+-rw-r--r--  2.0 unx     1726 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/frozen.py
+-rw-r--r--  2.0 unx     3697 b- defN 24-May-23 00:03 compressed_tensors/quantization/lifecycle/initialize.py
+-rw-r--r--  2.0 unx      745 b- defN 24-May-23 00:03 compressed_tensors/quantization/observers/__init__.py
+-rw-r--r--  2.0 unx     5121 b- defN 24-May-23 00:03 compressed_tensors/quantization/observers/base.py
+-rw-r--r--  2.0 unx     2166 b- defN 24-May-23 00:03 compressed_tensors/quantization/observers/helpers.py
+-rw-r--r--  2.0 unx     2165 b- defN 24-May-23 00:03 compressed_tensors/quantization/observers/memoryless.py
+-rw-r--r--  2.0 unx     3669 b- defN 24-May-23 00:03 compressed_tensors/quantization/observers/min_max.py
+-rw-r--r--  2.0 unx      656 b- defN 24-May-23 00:03 compressed_tensors/quantization/utils/__init__.py
+-rw-r--r--  2.0 unx     6017 b- defN 24-May-23 00:03 compressed_tensors/quantization/utils/helpers.py
+-rw-r--r--  2.0 unx      658 b- defN 24-May-23 00:03 compressed_tensors/registry/__init__.py
+-rw-r--r--  2.0 unx    11890 b- defN 24-May-23 00:03 compressed_tensors/registry/registry.py
+-rw-r--r--  2.0 unx      665 b- defN 24-May-23 00:03 compressed_tensors/utils/__init__.py
+-rw-r--r--  2.0 unx     1735 b- defN 24-May-23 00:03 compressed_tensors/utils/helpers.py
+-rw-r--r--  2.0 unx     8502 b- defN 24-May-23 00:03 compressed_tensors/utils/safetensors_load.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-May-23 00:05 compressed_tensors_nightly-0.3.3.20240523.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5633 b- defN 24-May-23 00:05 compressed_tensors_nightly-0.3.3.20240523.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-23 00:05 compressed_tensors_nightly-0.3.3.20240523.dist-info/WHEEL
+-rw-r--r--  2.0 unx       19 b- defN 24-May-23 00:05 compressed_tensors_nightly-0.3.3.20240523.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4376 b- defN 24-May-23 00:05 compressed_tensors_nightly-0.3.3.20240523.dist-info/RECORD
+43 files, 160360 bytes uncompressed, 56515 bytes compressed:  64.8%
```

## zipnote {}

```diff
@@ -108,23 +108,23 @@
 
 Filename: compressed_tensors/utils/helpers.py
 Comment: 
 
 Filename: compressed_tensors/utils/safetensors_load.py
 Comment: 
 
-Filename: compressed_tensors_nightly-0.3.3.20240522.dist-info/LICENSE
+Filename: compressed_tensors_nightly-0.3.3.20240523.dist-info/LICENSE
 Comment: 
 
-Filename: compressed_tensors_nightly-0.3.3.20240522.dist-info/METADATA
+Filename: compressed_tensors_nightly-0.3.3.20240523.dist-info/METADATA
 Comment: 
 
-Filename: compressed_tensors_nightly-0.3.3.20240522.dist-info/WHEEL
+Filename: compressed_tensors_nightly-0.3.3.20240523.dist-info/WHEEL
 Comment: 
 
-Filename: compressed_tensors_nightly-0.3.3.20240522.dist-info/top_level.txt
+Filename: compressed_tensors_nightly-0.3.3.20240523.dist-info/top_level.txt
 Comment: 
 
-Filename: compressed_tensors_nightly-0.3.3.20240522.dist-info/RECORD
+Filename: compressed_tensors_nightly-0.3.3.20240523.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## compressed_tensors/quantization/quant_config.py

```diff
@@ -9,19 +9,22 @@
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from enum import Enum
-from typing import Dict, List, Optional
+from typing import Dict, List, Optional, Union
 
 from compressed_tensors.base import QUANTIZATION_CONFIG_NAME
 from compressed_tensors.config import CompressionFormat
-from compressed_tensors.quantization.quant_scheme import QuantizationScheme
+from compressed_tensors.quantization.quant_scheme import (
+    QuantizationScheme,
+    preset_name_to_scheme,
+)
 from compressed_tensors.quantization.utils import (
     calculate_compression_ratio,
     is_module_quantized,
     iter_named_leaf_modules,
     module_type,
 )
 from pydantic import BaseModel, Field
@@ -101,33 +104,47 @@
 
 class QuantizationConfig(BaseModel):
     """
     Full configuration specifying how a model is quantized. Each quantized layer is
     mapped to a QuantizationScheme in config_groups.
 
     :param config_groups: dict of QuantizationSchemes specifying the quantization
-    settings for each quantized layer
+    settings for each quantized layer. A group could also be a reference to
+    a predefined scheme name, mapped to a list of its target layers/classes
     :param quant_method: a constant used to differentiate sparseML quantization from
     other quantization configs
     :param format: specifies how the quantized model is stored on disk
     :quantization_status: specifies the current status of all quantized layers. It is
     assumed all layers are in the same state.
     :global_compression_ratio: optional informational config to report the model
     compression ratio acheived by the quantization config
     :ignore: optional list of layers to ignore from config_groups. Layers in this list
     are not quantized even if they match up with a target in config_groups
     """
 
-    config_groups: Dict[str, QuantizationScheme]
+    config_groups: Dict[str, Union[QuantizationScheme, List[str]]]
     quant_method: str = "sparseml"
     format: str = "fakequant"
     quantization_status: QuantizationStatus = QuantizationStatus.INITIALIZED
     global_compression_ratio: Optional[float] = None
     ignore: Optional[List[str]] = Field(default_factory=list)
 
+    def model_post_init(self, __context):
+        """
+        updates any quantization schemes defined as presets to be fully loaded
+        schemes
+        """
+        for group_name, targets_or_scheme in self.config_groups.items():
+            if isinstance(targets_or_scheme, QuantizationScheme):
+                continue  # scheme already defined
+            self.config_groups[group_name] = preset_name_to_scheme(
+                name=group_name,
+                targets=targets_or_scheme,
+            )
+
     @staticmethod
     def from_model_config(model_name_or_path) -> "QuantizationConfig":
         """
         Given a path to a model config, extract a quantization config if it exists
 
         :param pretrained_model_name_or_path: path to model config on disk or HF hub
         :return: instantiated QuantizationConfig if config contains a quant config
```

## compressed_tensors/quantization/quant_scheme.py

```diff
@@ -8,21 +8,25 @@
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from copy import deepcopy
 from typing import List, Optional
 
 from compressed_tensors.quantization.quant_args import QuantizationArgs
 from pydantic import BaseModel
 
 
-__all__ = ["QuantizationScheme"]
+__all__ = [
+    "QuantizationScheme",
+    "preset_name_to_scheme",
+]
 
 
 class QuantizationScheme(BaseModel):
     """
     Set of QuantizationArgs defining how the weights, inputs and outputs of target list
     of modules should be quantized
 
@@ -33,7 +37,74 @@
     :param output_activations: quantization config for layer outputs
     """
 
     targets: List[str]
     weights: Optional[QuantizationArgs] = None
     input_activations: Optional[QuantizationArgs] = None
     output_activations: Optional[QuantizationArgs] = None
+
+    @classmethod
+    def default_scheme(
+        cls,
+        targets: Optional[List[str]] = None,
+    ):
+
+        if targets is None:
+            # default to quantizing all Linear layers
+            targets = ["Linear"]
+
+        # default to 8 bit integer symmetric quantization
+        # for weights
+        weights = QuantizationArgs(num_bits=8, symmetric=True)
+
+        # default to 8 bit integer asymmetric quantization
+        input_activations = QuantizationArgs(num_bits=8, symmetric=True)
+
+        # Do not quantize the output activations
+        # by default
+        output_activations = None
+
+        return cls(
+            targets=targets,
+            weights=weights,
+            input_activations=input_activations,
+            output_activations=output_activations,
+        )
+
+
+"""
+Pre-Set Quantization Scheme Args
+"""
+
+
+def preset_name_to_scheme(name: str, targets: List[str]) -> QuantizationScheme:
+    """
+    :param name: preset quantization settings name. must exist in upper case in
+        PRESET_SCHEMES
+    :param targets: list of quantization targets to be passed to the Scheme
+    :return: new QuantizationScheme for a given name with the given targets
+    """
+    name = name.upper()
+
+    if name not in PRESET_SCHEMES:
+        raise KeyError(
+            f"Unknown preset scheme name {name}, "
+            f"available names: {list(PRESET_SCHEMES.keys())}"
+        )
+
+    scheme_args = deepcopy(PRESET_SCHEMES[name])  # deepcopy to avoid args references
+    return QuantizationScheme(
+        targets=targets,
+        **scheme_args,
+    )
+
+
+W8A8 = dict(
+    weights=QuantizationArgs(), input_activations=QuantizationArgs(symmetric=False)
+)
+
+W4A16 = dict(weights=QuantizationArgs(num_bits=4, symmetric=False))
+
+PRESET_SCHEMES = {
+    "W8A8": W8A8,
+    "W4A16": W4A16,
+}
```

## compressed_tensors/quantization/observers/base.py

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Tuple
+from typing import Any, Optional, Tuple
 
 import torch
 from compressed_tensors.quantization.quant_args import (
     QuantizationArgs,
     QuantizationStrategy,
 )
 from compressed_tensors.registry.registry import RegistryMixin
@@ -89,33 +89,40 @@
 
                 # re-calculate scale and zero point, update the stored value
                 self._scale, self._zero_point = self.calculate_qparams(observed)
 
             elif self.quantization_args.strategy == QuantizationStrategy.GROUP:
                 columns = observed.shape[1]
                 scales, zero_points = [], []
-                for i in range(0, columns, self.quantization_args.group_size):
+                group_idxs = range(0, columns, self.quantization_args.group_size)
+                for group_id, group_idx in enumerate(group_idxs):
                     scale, zero_point = self.get_qparams_along_dim(
-                        observed[:, i : (i + group_size)],
+                        observed[:, group_idx : (group_idx + group_size)],
                         0,
+                        tensor_id=group_id,
                     )
                     scales.append(scale)
                     zero_points.append(zero_point)
-                self._scale = torch.stack(scales, dim=1, out=self._scale)
-                self._zero_point = torch.stack(zero_points, dim=1, out=self._zero_point)
+
+                self._scale = torch.cat(scales, dim=1, out=self._scale)
+                self._zero_point = torch.cat(zero_points, dim=1, out=self._zero_point)
 
             elif self.quantization_args.strategy == QuantizationStrategy.CHANNEL:
                 # assume observed is transposed, because its the output, hence use dim 0
                 self._scale, self._zero_point = self.get_qparams_along_dim(observed, 0)
 
             elif self.quantization_args.strategy == QuantizationStrategy.TOKEN:
                 # use dim 1, assume the obsersed.shape = [batch, token, hidden]
                 # should be batch, token
                 self._scale, self._zero_point = self.get_qparams_along_dim(
                     observed, dim=1
                 )
 
         return self._scale, self._zero_point
 
-    def get_qparams_along_dim(self, observed, dim: int):
+    def get_qparams_along_dim(
+        self, observed, dim: int, tensor_id: Optional[Any] = None
+    ):
         reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)
-        return self.calculate_qparams(observed, reduce_dims=reduce_dims)
+        return self.calculate_qparams(
+            observed, reduce_dims=reduce_dims, tensor_id=tensor_id
+        )
```

## compressed_tensors/quantization/observers/memoryless.py

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Tuple
+from typing import Any, Optional, Tuple
 
 import torch
 from compressed_tensors.quantization.observers.base import Observer
 from compressed_tensors.quantization.observers.helpers import calculate_qparams
 from torch import FloatTensor, IntTensor, Tensor
 
 
@@ -29,20 +29,22 @@
     Implements a quantization observer that sets the scale and
     zero point based on the latest observed value without tracking state
     """
 
     def calculate_qparams(
         self,
         observed: Tensor,
+        tensor_id: Optional[Any] = None,
         reduce_dims: Optional[Tuple[int]] = None,
     ) -> Tuple[FloatTensor, IntTensor]:
         """
         Returns the min and max values of observed tensor
 
         :param observed: observed tensor to calculate quantization parameters for
+        :param tensor_id: optional id for tensor; not used for memoryless
         :param reduce_dims: optional tuple of dimensions to reduce along,
             returned scale and zero point will be shaped (1,) along the
             reduced dimensions
         :return: tuple of scale and zero point derived from the observed tensor
         """
 
         if not reduce_dims:
```

## compressed_tensors/quantization/observers/min_max.py

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing,
 # software distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Tuple
+from typing import Any, Optional, Tuple
 
 import torch
 from compressed_tensors.quantization.observers.base import Observer
 from compressed_tensors.quantization.observers.helpers import calculate_qparams
 from compressed_tensors.quantization.quant_args import QuantizationArgs
 from torch import FloatTensor, IntTensor, Tensor
 
@@ -32,45 +32,65 @@
     """
 
     def __init__(
         self, quantization_args: QuantizationArgs, averaging_constant: float = 0.01
     ):
         super().__init__(quantization_args=quantization_args)
 
-        self.min_val = None
-        self.max_val = None
+        self.min_val = {}
+        self.max_val = {}
         self.averaging_constant = averaging_constant
 
     def calculate_qparams(
         self,
         observed: Tensor,
         reduce_dims: Optional[Tuple[int]] = None,
+        tensor_id: Optional[Any] = None,
     ) -> Tuple[FloatTensor, IntTensor]:
         """
         Updates the observed min and max using a moving average smoothed by the
         averaging_constant
 
         :param observed: observed tensor to calculate quantization parameters for
         :param reduce_dims: optional tuple of dimensions to reduce along,
             returned scale and zero point will be shaped (1,) along the
             reduced dimensions
+        :param tensor_id: Optional id if different ranges of observed tensors are
+            passed, useful for sharding tensors by group_size
         :return: tuple of scale and zero point derived from the observed tensor
         """
+        tensor_id = tensor_id or "default"
 
         if not reduce_dims:
             min_val, max_val = torch.aminmax(observed)
         else:
             min_val = torch.amin(observed, dim=reduce_dims, keepdims=True)
             max_val = torch.amax(observed, dim=reduce_dims, keepdims=True)
 
-        if self.min_val is None and self.max_val is None:
-            self.min_val = min_val
-            self.max_val = max_val
+        running_min_val = self.min_val.get(tensor_id, None)
+        running_max_val = self.max_val.get(tensor_id, None)
+
+        if running_min_val is None or running_max_val is None:
+            updated_min_val = min_val
+            updated_max_val = max_val
         else:
-            self.min_val = self.min_val + self.averaging_constant * (
-                min_val - self.min_val
+            updated_min_val = running_min_val + self.averaging_constant * (
+                min_val - running_min_val
             )
-            self.max_val = self.max_val + self.averaging_constant * (
-                max_val - self.max_val
+            updated_max_val = running_max_val + self.averaging_constant * (
+                max_val - running_max_val
             )
 
-        return calculate_qparams(self.min_val, self.max_val, self.quantization_args)
+        self.min_val[tensor_id] = updated_min_val
+        self.max_val[tensor_id] = updated_max_val
+
+        return calculate_qparams(
+            updated_min_val, updated_max_val, self.quantization_args
+        )
+
+    def get_qparams_along_dim(
+        self, observed, dim: int, tensor_id: Optional[Any] = None
+    ):
+        reduce_dims = tuple(idx for idx in range(observed.ndim) if idx != dim)
+        return self.calculate_qparams(
+            observed, reduce_dims=reduce_dims, tensor_id=tensor_id
+        )
```

## Comparing `compressed_tensors_nightly-0.3.3.20240522.dist-info/LICENSE` & `compressed_tensors_nightly-0.3.3.20240523.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `compressed_tensors_nightly-0.3.3.20240522.dist-info/METADATA` & `compressed_tensors_nightly-0.3.3.20240523.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: compressed-tensors-nightly
-Version: 0.3.3.20240522
+Version: 0.3.3.20240523
 Summary: Library for utilization of compressed safetensors of neural network models
 Home-page: https://github.com/neuralmagic/compressed-tensors
 Author: Neuralmagic, Inc.
 Author-email: support@neuralmagic.com
 License: Apache 2.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
```

## Comparing `compressed_tensors_nightly-0.3.3.20240522.dist-info/RECORD` & `compressed_tensors_nightly-0.3.3.20240523.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -11,33 +11,33 @@
 compressed_tensors/compressors/sparse_bitmask.py,sha256=H9oZSTYI1oRCzAMbd4zThUnZd1h2rfs8DmA3tPcvuNE,8637
 compressed_tensors/config/__init__.py,sha256=ZBqWn3r6ku1qfmlHHYp0mQueY0i7Pwhr9rbQk9dDlMc,704
 compressed_tensors/config/base.py,sha256=grf5tDaLep8i2-W_p7H-fW9DOGXDi4Zz7su7zjs1Qqc,1454
 compressed_tensors/config/dense.py,sha256=NgSxnFCnckU9-iunxEaqiFwqgdO7YYxlWKR74jNbjks,1317
 compressed_tensors/config/sparse_bitmask.py,sha256=pZUboRNZTu6NajGOQEFExoPknak5ynVAUeiiYpS1Gt8,1308
 compressed_tensors/quantization/__init__.py,sha256=83J5bPB7PavN2TfCoW7_vEDhfYpm4TDrqYO9vdSQ5bk,760
 compressed_tensors/quantization/quant_args.py,sha256=A6b2V8lhsM8Ho8RjlPBQdxRUDNWhqq-ie5E3RR2_GNg,4360
-compressed_tensors/quantization/quant_config.py,sha256=U6oEzheNK1d-0kHARzwepasnmS7HHqU_zGwoDBJ-lxU,8042
-compressed_tensors/quantization/quant_scheme.py,sha256=X3oqmZPiIKtX5tEKKUj-0N6hB68NeiU2b1GcQEQPadQ,1480
+compressed_tensors/quantization/quant_config.py,sha256=3BcbQ8-Ah7LbTDSSkRu29Yiid33xo0C1ki6NVhxLiaY,8727
+compressed_tensors/quantization/quant_scheme.py,sha256=QwZsCo8QR9ISB_d58WhIngk2gsMM8ooX-LcRPR-JDRw,3341
 compressed_tensors/quantization/lifecycle/__init__.py,sha256=ggRGWRqhCxCaTTDWRcgTVX3axnS2xV6rc5YvdzK7fSg,798
 compressed_tensors/quantization/lifecycle/apply.py,sha256=whKfNGC_EZm0BC23AP7qWfjRe5OJVWmcZOpX7lryZZc,7625
 compressed_tensors/quantization/lifecycle/calibration.py,sha256=mLns4jlaWmBwOW8Jtlm5bMX-JET1AiZYUBO7qa-XuxI,1776
 compressed_tensors/quantization/lifecycle/compressed.py,sha256=VreB10xPwgSLQQlTu20UCrFpRS--cA7-lx5s7nrPPrg,2247
 compressed_tensors/quantization/lifecycle/forward.py,sha256=x9JaIX3TK7cb_-0aCOTTYtA4At9l6v5YOY_70GzIeFU,10520
 compressed_tensors/quantization/lifecycle/frozen.py,sha256=h1XYt89MouBTf3jTYLG_6OdFxIu5q2N8tPjsy6J4E6Y,1726
 compressed_tensors/quantization/lifecycle/initialize.py,sha256=U6g9qifSF6pagQZQZEwd-rwWC6uQ_dZXn1wg6nr1Abg,3697
 compressed_tensors/quantization/observers/__init__.py,sha256=DNH31NQYrIBBcmHsMyFA6whh4pbRsLwuNa6L8AeXaGc,745
-compressed_tensors/quantization/observers/base.py,sha256=yIV2bd9PKPZwodgiBTZEco2ARbD3B0rOKDC0MOFluZs,4900
+compressed_tensors/quantization/observers/base.py,sha256=kywLVwycFvGxuZMU2cy8-KYyNrZCHkinN6YzCL7boLE,5121
 compressed_tensors/quantization/observers/helpers.py,sha256=JwALNfBYY9Eyl8Q180t0lGh8szumQj8TygfNl-isErs,2166
-compressed_tensors/quantization/observers/memoryless.py,sha256=Gach22cZLhDms6ueKF56XOiLhyWVIEYIEXRRXP5Nu8I,2045
-compressed_tensors/quantization/observers/min_max.py,sha256=OGrtyn6_sWuTSx5QgUPVKRIiarfWrK9QqXeRXoJQynw,2861
+compressed_tensors/quantization/observers/memoryless.py,sha256=jH_c6K3gxf4W3VNXQ7tbnP-J_86QTrEfjBn6Kh1C-H8,2165
+compressed_tensors/quantization/observers/min_max.py,sha256=UK7zCMzxv9GGn6BflBxdajV20RiWaCY2RHcvZodCP1w,3669
 compressed_tensors/quantization/utils/__init__.py,sha256=VdtEmP0bvuND_IGQnyqUPc5lnFp-1_yD7StKSX4x80w,656
 compressed_tensors/quantization/utils/helpers.py,sha256=NzAH18Cn_-mTAR87y6IlcQU5gC393XSjgNKC9CRkr78,6017
 compressed_tensors/registry/__init__.py,sha256=FwLSNYqfIrb5JD_6OK_MT4_svvKTN_nEhpgQlQvGbjI,658
 compressed_tensors/registry/registry.py,sha256=fxjOjh2wklCvJhQxwofdy-zV8q7MkQ85SLG77nml2iA,11890
 compressed_tensors/utils/__init__.py,sha256=5DrYjoZbaEvSkJcC-GRSbM_RBHVF4tG9gMd3zsJnjLw,665
 compressed_tensors/utils/helpers.py,sha256=h0jfl9drs5FAx40tCHRcVtJqXixB5hT5yq_IG2aY_-w,1735
 compressed_tensors/utils/safetensors_load.py,sha256=wo9UirGrGlenBqZeqotvpCT7D5MEdjCo2J3HeRaIFoU,8502
-compressed_tensors_nightly-0.3.3.20240522.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-compressed_tensors_nightly-0.3.3.20240522.dist-info/METADATA,sha256=KaoW0FehC_2-kbtxTf2D7fDGC88GCH4aIzM7_blopoM,5633
-compressed_tensors_nightly-0.3.3.20240522.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-compressed_tensors_nightly-0.3.3.20240522.dist-info/top_level.txt,sha256=w2i-GyPs2s1UwVxvutSvN_lM22SXC2hQFBmoMcPnV7Y,19
-compressed_tensors_nightly-0.3.3.20240522.dist-info/RECORD,,
+compressed_tensors_nightly-0.3.3.20240523.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+compressed_tensors_nightly-0.3.3.20240523.dist-info/METADATA,sha256=_c67GXEm0cMZ_AGWhcLqsMZ3hSbFB4KdQ3lL9Dg7M8M,5633
+compressed_tensors_nightly-0.3.3.20240523.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+compressed_tensors_nightly-0.3.3.20240523.dist-info/top_level.txt,sha256=w2i-GyPs2s1UwVxvutSvN_lM22SXC2hQFBmoMcPnV7Y,19
+compressed_tensors_nightly-0.3.3.20240523.dist-info/RECORD,,
```

