# Comparing `tmp/flax-0.8.3.tar.gz` & `tmp/flax-0.8.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flax-0.8.3.tar", last modified: Tue Apr 30 09:57:11 2024, max compression
+gzip compressed data, was "flax-0.8.4.tar", last modified: Fri May 24 17:09:30 2024, max compression
```

## Comparing `flax-0.8.3.tar` & `flax-0.8.4.tar`

### file list

```diff
@@ -1,559 +1,568 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.858113 flax-0.8.3/
--rw-r--r--   0 runner    (1001) docker     (127)       54 2024-04-30 09:57:01.000000 flax-0.8.3/.git-blame-ignore-revs
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.770113 flax-0.8.3/.github/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.770113 flax-0.8.3/.github/ISSUE_TEMPLATE/
--rw-r--r--   0 runner    (1001) docker     (127)      792 2024-04-30 09:57:01.000000 flax-0.8.3/.github/ISSUE_TEMPLATE/bug_report.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/.github/analytics/
--rw-r--r--   0 runner    (1001) docker     (127)      544 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/README.md
--rw-r--r--   0 runner    (1001) docker     (127)    13817 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/get_repo_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     1677 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/issue_activity_since_date.gql
--rw-r--r--   0 runner    (1001) docker     (127)     2016 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/pr_data_query.gql
--rw-r--r--   0 runner    (1001) docker     (127)       34 2024-04-30 09:57:01.000000 flax-0.8.3/.github/analytics/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-04-30 09:57:01.000000 flax-0.8.3/.github/pull_request_template.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/.github/workflows/
--rw-r--r--   0 runner    (1001) docker     (127)     6228 2024-04-30 09:57:01.000000 flax-0.8.3/.github/workflows/build.yml
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-04-30 09:57:01.000000 flax-0.8.3/.github/workflows/pythonpublish.yml
--rw-r--r--   0 runner    (1001) docker     (127)      185 2024-04-30 09:57:01.000000 flax-0.8.3/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-04-30 09:57:01.000000 flax-0.8.3/.pre-commit-config.yaml
--rw-r--r--   0 runner    (1001) docker     (127)      563 2024-04-30 09:57:01.000000 flax-0.8.3/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (127)      293 2024-04-30 09:57:01.000000 flax-0.8.3/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (127)    28201 2024-04-30 09:57:01.000000 flax-0.8.3/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (127)    11309 2024-04-30 09:57:01.000000 flax-0.8.3/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)    10230 2024-04-30 09:57:11.858113 flax-0.8.3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     7910 2024-04-30 09:57:01.000000 flax-0.8.3/README.md
--rw-r--r--   0 runner    (1001) docker     (127)      110 2024-04-30 09:57:01.000000 flax-0.8.3/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/dev/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.774113 flax-0.8.3/dev/.devcontainer/
--rw-r--r--   0 runner    (1001) docker     (127)     2643 2024-04-30 09:57:01.000000 flax-0.8.3/dev/.devcontainer/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (127)     1806 2024-04-30 09:57:01.000000 flax-0.8.3/dev/.devcontainer/devcontainer.json
--rw-r--r--   0 runner    (1001) docker     (127)      814 2024-04-30 09:57:01.000000 flax-0.8.3/dev/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     4581 2024-04-30 09:57:01.000000 flax-0.8.3/dev/update_requirements.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/
--rw-r--r--   0 runner    (1001) docker     (127)       18 2024-04-30 09:57:01.000000 flax-0.8.3/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)      634 2024-04-30 09:57:01.000000 flax-0.8.3/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (127)     5359 2024-04-30 09:57:01.000000 flax-0.8.3/docs/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_ext/
--rw-r--r--   0 runner    (1001) docker     (127)     4255 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/codediff.py
--rw-r--r--   0 runner    (1001) docker     (127)     3392 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/codediff_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2288 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_ext/flax_module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.762112 flax-0.8.3/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (127)      309 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_static/css/flax_theme.css
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.762112 flax-0.8.3/docs/_templates/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/_templates/autosummary/
--rw-r--r--   0 runner    (1001) docker     (127)      674 2024-04-30 09:57:01.000000 flax-0.8.3/docs/_templates/autosummary/flax_module.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.778113 flax-0.8.3/docs/api_reference/
--rw-r--r--   0 runner    (1001) docker     (127)      190 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.config.rst
--rw-r--r--   0 runner    (1001) docker     (127)      321 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.core.frozen_dict.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1679 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.cursor.rst
--rw-r--r--   0 runner    (1001) docker     (127)      158 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.errors.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/
--rw-r--r--   0 runner    (1001) docker     (127)      257 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/helpers.rst
--rw-r--r--   0 runner    (1001) docker     (127)      308 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)      204 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/module.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/
--rw-r--r--   0 runner    (1001) docker     (127)      762 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/activations.rst
--rw-r--r--   0 runner    (1001) docker     (127)      308 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/attention.rst
--rw-r--r--   0 runner    (1001) docker     (127)      268 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)      778 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/initializers.rst
--rw-r--r--   0 runner    (1001) docker     (127)      319 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/linear.rst
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/normalization.rst
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/stochastic.rst
--rw-r--r--   0 runner    (1001) docker     (127)      184 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/rnglib.rst
--rw-r--r--   0 runner    (1001) docker     (127)      263 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/spmd.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.782113 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)      279 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/metrics.rst
--rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/training/optimizer.rst
--rw-r--r--   0 runner    (1001) docker     (127)      398 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/transforms.rst
--rw-r--r--   0 runner    (1001) docker     (127)      411 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/variables.rst
--rw-r--r--   0 runner    (1001) docker     (127)      145 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.experimental.nnx/visualization.rst
--rw-r--r--   0 runner    (1001) docker     (127)      365 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.jax_utils.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/api_reference/flax.linen/
--rw-r--r--   0 runner    (1001) docker     (127)      827 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/activation_functions.rst
--rw-r--r--   0 runner    (1001) docker     (127)      117 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/decorators.rst
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)      141 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/init_apply.rst
--rw-r--r--   0 runner    (1001) docker     (127)      756 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/initializers.rst
--rw-r--r--   0 runner    (1001) docker     (127)       94 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/inspection.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2360 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/layers.rst
--rw-r--r--   0 runner    (1001) docker     (127)      509 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/module.rst
--rw-r--r--   0 runner    (1001) docker     (127)      176 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/profiling.rst
--rw-r--r--   0 runner    (1001) docker     (127)      587 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/spmd.rst
--rw-r--r--   0 runner    (1001) docker     (127)      412 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/transformations.rst
--rw-r--r--   0 runner    (1001) docker     (127)      116 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.linen/variable.rst
--rw-r--r--   0 runner    (1001) docker     (127)      480 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.serialization.rst
--rw-r--r--   0 runner    (1001) docker     (127)      161 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.struct.rst
--rw-r--r--   0 runner    (1001) docker     (127)      275 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.traceback_util.rst
--rw-r--r--   0 runner    (1001) docker     (127)     1212 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.training.rst
--rw-r--r--   0 runner    (1001) docker     (127)      798 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/flax.traverse_util.rst
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-04-30 09:57:01.000000 flax-0.8.3/docs/api_reference/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     5741 2024-04-30 09:57:01.000000 flax-0.8.3/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (127)     6623 2024-04-30 09:57:01.000000 flax-0.8.3/docs/conf_sphinx_patch.py
--rw-r--r--   0 runner    (1001) docker     (127)    11946 2024-04-30 09:57:01.000000 flax-0.8.3/docs/contributing.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/developer_notes/
--rw-r--r--   0 runner    (1001) docker     (127)      153 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    18081 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/lift.md
--rw-r--r--   0 runner    (1001) docker     (127)    22000 2024-04-30 09:57:01.000000 flax-0.8.3/docs/developer_notes/module_lifecycle.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/examples/
--rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/community_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4422 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/core_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)    22579 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/google_research_examples.rst
--rw-r--r--   0 runner    (1001) docker     (127)      148 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     2028 2024-04-30 09:57:01.000000 flax-0.8.3/docs/examples/repositories_that_use_flax.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.786113 flax-0.8.3/docs/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)       70 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/experimental/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     3953 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)   317520 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/mnist_tutorial.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    10359 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/mnist_tutorial.md
--rw-r--r--   0 runner    (1001) docker     (127)   194440 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/nnx_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    13494 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/nnx_basics.md
--rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-04-30 09:57:01.000000 flax-0.8.3/docs/experimental/nnx/transforms.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4134 2024-04-30 09:57:01.000000 flax-0.8.3/docs/faq.rst
--rw-r--r--   0 runner    (1001) docker     (127)    20991 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flax.png
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/flip/
--rw-r--r--   0 runner    (1001) docker     (127)      648 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/0000-template.md
--rw-r--r--   0 runner    (1001) docker     (127)    17256 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/1009-optimizer-api.md
--rw-r--r--   0 runner    (1001) docker     (127)     8189 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/1777-default-dtype.md
--rw-r--r--   0 runner    (1001) docker     (127)    11758 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2396-rnn.md
--rw-r--r--   0 runner    (1001) docker     (127)    10424 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2434-general-metadata.md
--rw-r--r--   0 runner    (1001) docker     (127)     4099 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/2974-kw-only-dataclasses.md
--rw-r--r--   0 runner    (1001) docker     (127)     4068 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/3099-rnnbase-refactor.md
--rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-04-30 09:57:01.000000 flax-0.8.3/docs/flip/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     6679 2024-04-30 09:57:01.000000 flax-0.8.3/docs/glossary.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/guides/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.790113 flax-0.8.3/docs/guides/converting_and_upgrading/
--rw-r--r--   0 runner    (1001) docker     (127)    10603 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst
--rw-r--r--   0 runner    (1001) docker     (127)    26613 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/haiku_migration_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)      255 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    17314 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)    10607 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/optax_update_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     9051 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     4119 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst
--rw-r--r--   0 runner    (1001) docker     (127)     6093 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/data_preprocessing/
--rw-r--r--   0 runner    (1001) docker     (127)     6994 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/full_eval.rst
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8230 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     5149 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/flax_fundamentals/
--rw-r--r--   0 runner    (1001) docker     (127)     4087 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/arguments.md
--rw-r--r--   0 runner    (1001) docker     (127)    38158 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    20797 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.md
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    68258 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    29445 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.md
--rw-r--r--   0 runner    (1001) docker     (127)     3125 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/setup_or_nncompact.rst
--rw-r--r--   0 runner    (1001) docker     (127)     5886 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_fundamentals/state_params.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7226 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_sharp_bits.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     5794 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/flax_sharp_bits.md
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.794113 flax-0.8.3/docs/guides/model_inspection/
--rw-r--r--   0 runner    (1001) docker     (127)    12699 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/extracting_intermediates.rst
--rw-r--r--   0 runner    (1001) docker     (127)      110 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/model_surgery.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/model_inspection/model_surgery.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/docs/guides/parallel_training/
--rw-r--r--   0 runner    (1001) docker     (127)    10480 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/ensembling.rst
--rw-r--r--   0 runner    (1001) docker     (127)    62518 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    24602 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.md
--rw-r--r--   0 runner    (1001) docker     (127)       97 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/parallel_training/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/docs/guides/training_techniques/
--rw-r--r--   0 runner    (1001) docker     (127)     9141 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/batch_norm.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11028 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/dropout.rst
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)     8171 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/lr_schedule.rst
--rw-r--r--   0 runner    (1001) docker     (127)    11290 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/transfer_learning.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     7843 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/transfer_learning.md
--rw-r--r--   0 runner    (1001) docker     (127)    53437 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/use_checkpointing.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    25779 2024-04-30 09:57:01.000000 flax-0.8.3/docs/guides/training_techniques/use_checkpointing.md
--rw-r--r--   0 runner    (1001) docker     (127)     8649 2024-04-30 09:57:01.000000 flax-0.8.3/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (127)    39597 2024-04-30 09:57:01.000000 flax-0.8.3/docs/linen_intro.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    20900 2024-04-30 09:57:01.000000 flax-0.8.3/docs/linen_intro.md
--rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-04-30 09:57:01.000000 flax-0.8.3/docs/philosophy.md
--rw-r--r--   0 runner    (1001) docker     (127)   101054 2024-04-30 09:57:01.000000 flax-0.8.3/docs/quick_start.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    14177 2024-04-30 09:57:01.000000 flax-0.8.3/docs/quick_start.md
--rw-r--r--   0 runner    (1001) docker     (127)      705 2024-04-30 09:57:01.000000 flax-0.8.3/docs/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)      174 2024-04-30 09:57:01.000000 flax-0.8.3/docs/robots.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/examples/
--rw-r--r--   0 runner    (1001) docker     (127)      743 2024-04-30 09:57:01.000000 flax-0.8.3/examples/README.md
--rw-r--r--   0 runner    (1001) docker     (127)      582 2024-04-30 09:57:01.000000 flax-0.8.3/examples/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.798113 flax-0.8.3/examples/cloud/
--rw-r--r--   0 runner    (1001) docker     (127)     4635 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     9222 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/launch_gce.py
--rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-04-30 09:57:01.000000 flax-0.8.3/examples/cloud/startup_script.sh
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/imagenet/
--rw-r--r--   0 runner    (1001) docker     (127)     9944 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/imagenet/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     2192 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1670 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/tpu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/v100_x8.py
--rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/configs/v100_x8_mixed_precision.py
--rw-r--r--   0 runner    (1001) docker     (127)   293668 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/imagenet_fake_data_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     8124 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      341 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    12863 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3049 2024-04-30 09:57:01.000000 flax-0.8.3/examples/imagenet/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.802113 flax-0.8.3/examples/linen_design_test/
--rw-r--r--   0 runner    (1001) docker     (127)     6242 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/attention_simple.py
--rw-r--r--   0 runner    (1001) docker     (127)     3145 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/autoencoder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/dense.py
--rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/linear_regression.py
--rw-r--r--   0 runner    (1001) docker     (127)     2303 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_explicit.py
--rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_inline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1891 2024-04-30 09:57:01.000000 flax-0.8.3/examples/linen_design_test/mlp_lazy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (127)     3320 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     5011 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    12500 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    13273 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4843 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5313 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    20414 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     1994 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5793 2024-04-30 09:57:01.000000 flax-0.8.3/examples/lm1b/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/mnist/
--rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/mnist/configs/
--rw-r--r--   0 runner    (1001) docker     (127)      912 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    98260 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/mnist.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     2366 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/mnist_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)      298 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5231 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2262 2024-04-30 09:57:01.000000 flax-0.8.3/examples/mnist/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.806113 flax-0.8.3/examples/nlp_seq/
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     7884 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4073 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/models.py
--rw-r--r--   0 runner    (1001) docker     (127)       60 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    14100 2024-04-30 09:57:01.000000 flax-0.8.3/examples/nlp_seq/train.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.810113 flax-0.8.3/examples/ogbg_molpcba/
--rw-r--r--   0 runner    (1001) docker     (127)     4486 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.810113 flax-0.8.3/examples/ogbg_molpcba/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1520 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1551 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/default_graph_net.py
--rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/hparam_sweep.py
--rw-r--r--   0 runner    (1001) docker     (127)     1405 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/configs/test.py
--rw-r--r--   0 runner    (1001) docker     (127)     8133 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2198 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     7068 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     5179 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)  1110530 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4769 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)      329 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    13697 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/train.py
--rw-r--r--   0 runner    (1001) docker     (127)    12431 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ogbg_molpcba/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/ppo/
--rw-r--r--   0 runner    (1001) docker     (127)     2501 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/agent.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/ppo/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1955 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/env_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2346 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/models.py
--rw-r--r--   0 runner    (1001) docker     (127)    13152 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_lib.py
--rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_lib_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/ppo_main.py
--rw-r--r--   0 runner    (1001) docker     (127)      192 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8930 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/seed_rl_atari_preprocessing.py
--rw-r--r--   0 runner    (1001) docker     (127)     1897 2024-04-30 09:57:01.000000 flax-0.8.3/examples/ppo/test_episodes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.814113 flax-0.8.3/examples/seq2seq/
--rw-r--r--   0 runner    (1001) docker     (127)      913 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/models.py
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)    24740 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/seq2seq.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     6844 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3201 2024-04-30 09:57:01.000000 flax-0.8.3/examples/seq2seq/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/sst2/
--rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/README.md
--rwxr-xr-x   0 runner    (1001) docker     (127)     2028 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/build_vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/sst2/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/configs/default.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     9834 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3522 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    14403 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     3526 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      156 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     8607 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/sst2.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     9335 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2123 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)   117898 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/vocab.txt
--rwxr-xr-x   0 runner    (1001) docker     (127)     4407 2024-04-30 09:57:01.000000 flax-0.8.3/examples/sst2/vocabulary.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/
--rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/configs/
--rw-r--r--   0 runner    (1001) docker     (127)      883 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)     1458 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/reconstruction.png
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.818113 flax-0.8.3/examples/vae/results/
--rw-r--r--   0 runner    (1001) docker     (127)        6 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/results/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)    43139 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/sample.png
--rw-r--r--   0 runner    (1001) docker     (127)     4596 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-04-30 09:57:01.000000 flax-0.8.3/examples/vae/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/examples/wmt/
--rw-r--r--   0 runner    (1001) docker     (127)     6106 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     7270 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/bleu.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/examples/wmt/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     3482 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    14745 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/decode.py
--rw-r--r--   0 runner    (1001) docker     (127)    12910 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2166 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    18604 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/models.py
--rw-r--r--   0 runner    (1001) docker     (127)      398 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     5314 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    23409 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     1995 2024-04-30 09:57:01.000000 flax-0.8.3/examples/wmt/train_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.822113 flax-0.8.3/flax/
--rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-04-30 09:57:01.000000 flax-0.8.3/flax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5990 2024-04-30 09:57:01.000000 flax-0.8.3/flax/configurations.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/core/
--rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5763 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/axes_scan.py
--rw-r--r--   0 runner    (1001) docker     (127)    10482 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/flax_functional_engine.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     9974 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/frozen_dict.py
--rw-r--r--   0 runner    (1001) docker     (127)    61688 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/lift.py
--rw-r--r--   0 runner    (1001) docker     (127)    11725 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/meta.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/core/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17932 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)    12040 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     6897 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/partial_eval.py
--rw-r--r--   0 runner    (1001) docker     (127)    38092 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/scope.py
--rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/tracers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-04-30 09:57:01.000000 flax-0.8.3/flax/core/variables.py
--rw-r--r--   0 runner    (1001) docker     (127)    26399 2024-04-30 09:57:01.000000 flax-0.8.3/flax/cursor.py
--rw-r--r--   0 runner    (1001) docker     (127)    30713 2024-04-30 09:57:01.000000 flax-0.8.3/flax/errors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      582 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     1831 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/.gitignore
--rw-r--r--   0 runner    (1001) docker     (127)     3678 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     5733 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/docs/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/blog.md
--rw-r--r--   0 runner    (1001) docker     (127)    10253 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/demo.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     4271 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/demo.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.826113 flax-0.8.3/flax/experimental/nnx/docs/images/
--rw-r--r--   0 runner    (1001) docker     (127)   304812 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/images/stateful-transforms.png
--rw-r--r--   0 runner    (1001) docker     (127)    61295 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/quick_start.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    16318 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/tiny_nnx.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    26920 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/why.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)    15238 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/docs/why.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.766113 flax-0.8.3/flax/experimental/nnx/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/lm1b/
--rw-r--r--   0 runner    (1001) docker     (127)     3267 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/
--rw-r--r--   0 runner    (1001) docker     (127)     5096 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    12344 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     3293 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2143 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/main.py
--rw-r--r--   0 runner    (1001) docker     (127)    15055 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/models.py
--rw-r--r--   0 runner    (1001) docker     (127)     9662 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/models_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      347 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1448 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5263 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    20551 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/train.py
--rw-r--r--   0 runner    (1001) docker     (127)     2026 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/train_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5036 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/lm1b/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.830113 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/
--rw-r--r--   0 runner    (1001) docker     (127)     2868 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/01_functional_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     2593 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)     5208 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/05_vae.py
--rw-r--r--   0 runner    (1001) docker     (127)     1865 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (127)     1969 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py
--rw-r--r--   0 runner    (1001) docker     (127)       35 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/examples/toy_examples/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.834113 flax-0.8.3/flax/experimental/nnx/nnx/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3187 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)      626 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/errors.py
--rw-r--r--   0 runner    (1001) docker     (127)     2913 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/filterlib.py
--rw-r--r--   0 runner    (1001) docker     (127)    37769 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2326 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/ids.py
--rw-r--r--   0 runner    (1001) docker     (127)    12260 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/nnx/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/activations.py
--rw-r--r--   0 runner    (1001) docker     (127)    25351 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/dtypes.py
--rw-r--r--   0 runner    (1001) docker     (127)     2596 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/initializers.py
--rw-r--r--   0 runner    (1001) docker     (127)    28389 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    18356 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/nn/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2562 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/proxy_caller.py
--rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/reprlib.py
--rw-r--r--   0 runner    (1001) docker     (127)     8995 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/rnglib.py
--rw-r--r--   0 runner    (1001) docker     (127)     6373 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     7560 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/state.py
--rw-r--r--   0 runner    (1001) docker     (127)     1728 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/tracers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/nnx/training/
--rw-r--r--   0 runner    (1001) docker     (127)     5291 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/training/metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/training/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    55991 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)    22081 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/variables.py
--rw-r--r--   0 runner    (1001) docker     (127)     3508 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/nnx/visualization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/scripts/
--rw-r--r--   0 runner    (1001) docker     (127)       17 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/scripts/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)      285 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/scripts/run-all-examples.bash
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.838113 flax-0.8.3/flax/experimental/nnx/tests/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/experimental/nnx/tests/nn/
--rw-r--r--   0 runner    (1001) docker     (127)     4930 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3069 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_embed.py
--rw-r--r--   0 runner    (1001) docker     (127)     4290 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     7537 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2609 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/nn/test_stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)     1204 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)     1636 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_containers.py
--rw-r--r--   0 runner    (1001) docker     (127)    11196 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_graph_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_helpers.py
--rw-r--r--   0 runner    (1001) docker     (127)      929 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_ids.py
--rw-r--r--   0 runner    (1001) docker     (127)     7462 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_integration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2351 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_metrics.py
--rw-r--r--   0 runner    (1001) docker     (127)    16827 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     4064 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4268 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_partitioning.py
--rw-r--r--   0 runner    (1001) docker     (127)     6492 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_rngs.py
--rw-r--r--   0 runner    (1001) docker     (127)     2780 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     2078 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    28817 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_transforms.py
--rw-r--r--   0 runner    (1001) docker     (127)     1836 2024-04-30 09:57:01.000000 flax-0.8.3/flax/experimental/nnx/tests/test_variable.py
--rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-04-30 09:57:01.000000 flax-0.8.3/flax/ids.py
--rw-r--r--   0 runner    (1001) docker     (127)     5309 2024-04-30 09:57:01.000000 flax-0.8.3/flax/io.py
--rw-r--r--   0 runner    (1001) docker     (127)    11534 2024-04-30 09:57:01.000000 flax-0.8.3/flax/jax_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/linen/
--rw-r--r--   0 runner    (1001) docker     (127)     2191 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/README.md
--rw-r--r--   0 runner    (1001) docker     (127)     5259 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/activation.py
--rw-r--r--   0 runner    (1001) docker     (127)    32287 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     4172 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/batch_apply.py
--rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/combinators.py
--rw-r--r--   0 runner    (1001) docker     (127)     3860 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/dtypes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.842113 flax-0.8.3/flax/linen/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)    11235 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/experimental/layers_with_named_axes.py
--rw-r--r--   0 runner    (1001) docker     (127)    10062 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/fp8_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     2676 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/initializers.py
--rw-r--r--   0 runner    (1001) docker     (127)     8828 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/kw_only_dataclasses.py
--rw-r--r--   0 runner    (1001) docker     (127)    44217 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)   113027 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/module.py
--rw-r--r--   0 runner    (1001) docker     (127)    49760 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)    19421 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/partitioning.py
--rw-r--r--   0 runner    (1001) docker     (127)     5567 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)    45995 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/recurrent.py
--rw-r--r--   0 runner    (1001) docker     (127)    11187 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     3634 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/stochastic.py
--rw-r--r--   0 runner    (1001) docker     (127)    26100 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/summary.py
--rw-r--r--   0 runner    (1001) docker     (127)    79289 2024-04-30 09:57:01.000000 flax-0.8.3/flax/linen/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-04-30 09:57:01.000000 flax-0.8.3/flax/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7792 2024-04-30 09:57:01.000000 flax-0.8.3/flax/metrics/tensorboard.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/oss/
--rw-r--r--   0 runner    (1001) docker     (127)      443 2024-04-30 09:57:01.000000 flax-0.8.3/flax/oss/ .git-blame-ignore-revs
--rw-r--r--   0 runner    (1001) docker     (127)       58 2024-04-30 09:57:01.000000 flax-0.8.3/flax/py.typed
--rw-r--r--   0 runner    (1001) docker     (127)    14315 2024-04-30 09:57:01.000000 flax-0.8.3/flax/serialization.py
--rw-r--r--   0 runner    (1001) docker     (127)     8275 2024-04-30 09:57:01.000000 flax-0.8.3/flax/struct.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      647 2024-04-30 09:57:01.000000 flax-0.8.3/flax/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-04-30 09:57:01.000000 flax-0.8.3/flax/testing/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (127)     1990 2024-04-30 09:57:01.000000 flax-0.8.3/flax/traceback_util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/flax/training/
--rw-r--r--   0 runner    (1001) docker     (127)      613 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    43835 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/checkpoints.py
--rw-r--r--   0 runner    (1001) docker     (127)     3690 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6056 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/dynamic_scale.py
--rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/early_stopping.py
--rw-r--r--   0 runner    (1001) docker     (127)     7419 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/lr_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     3685 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/orbax_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2978 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/prefetch_iterator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4767 2024-04-30 09:57:01.000000 flax-0.8.3/flax/training/train_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    13746 2024-04-30 09:57:01.000000 flax-0.8.3/flax/traverse_util.py
--rw-r--r--   0 runner    (1001) docker     (127)     2819 2024-04-30 09:57:01.000000 flax-0.8.3/flax/typing.py
--rw-r--r--   0 runner    (1001) docker     (127)      650 2024-04-30 09:57:01.000000 flax-0.8.3/flax/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.854113 flax-0.8.3/flax.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    10230 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    16651 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      595 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        5 2024-04-30 09:57:11.000000 flax-0.8.3/flax.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.846113 flax-0.8.3/images/
--rw-r--r--   0 runner    (1001) docker     (127)    80407 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo.png
--rw-r--r--   0 runner    (1001) docker     (127)     3862 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo.svg
--rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo_250px.png
--rw-r--r--   0 runner    (1001) docker     (127)    29095 2024-04-30 09:57:01.000000 flax-0.8.3/images/flax_logo_500px.png
--rw-r--r--   0 runner    (1001) docker     (127)    14116 2024-04-30 09:57:01.000000 flax-0.8.3/pylintrc
--rw-r--r--   0 runner    (1001) docker     (127)     5837 2024-04-30 09:57:01.000000 flax-0.8.3/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-30 09:57:11.858113 flax-0.8.3/setup.cfg
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/
--rw-r--r--   0 runner    (1001) docker     (127)    17456 2024-04-30 09:57:01.000000 flax-0.8.3/tests/checkpoints_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-04-30 09:57:01.000000 flax-0.8.3/tests/colab_tpu_jax_version.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-04-30 09:57:01.000000 flax-0.8.3/tests/configurations_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/core/
--rw-r--r--   0 runner    (1001) docker     (127)     5262 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_frozen_dict_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     8608 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_lift_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6750 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     9633 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/core_scope_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.850113 flax-0.8.3/tests/core/design/
--rw-r--r--   0 runner    (1001) docker     (127)     4795 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_auto_encoder_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_big_resnets_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2500 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_custom_vjp_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4565 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_dense_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2621 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_flow_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4653 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_resnet_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_scan_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2209 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_tied_autoencoder_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_vmap_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-04-30 09:57:01.000000 flax-0.8.3/tests/core/design/core_weight_std_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    25755 2024-04-30 09:57:01.000000 flax-0.8.3/tests/cursor_test.py
--rw-r--r--   0 runner    (1001) docker     (127)      887 2024-04-30 09:57:01.000000 flax-0.8.3/tests/download_dataset_metadata.sh
--rw-r--r--   0 runner    (1001) docker     (127)     2950 2024-04-30 09:57:01.000000 flax-0.8.3/tests/early_stopping_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-04-30 09:57:01.000000 flax-0.8.3/tests/import_test.ipynb
--rw-r--r--   0 runner    (1001) docker     (127)     8291 2024-04-30 09:57:01.000000 flax-0.8.3/tests/io_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     3486 2024-04-30 09:57:01.000000 flax-0.8.3/tests/jax_utils_test.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-30 09:57:11.854113 flax-0.8.3/tests/linen/
--rw-r--r--   0 runner    (1001) docker     (127)     1974 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/initializers_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4357 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/kw_only_dataclasses_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2594 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_activation_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    18253 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_attention_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     2999 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_batch_apply_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_combinators_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_dtypes_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    41550 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_linear_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6137 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_meta_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    86829 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_module_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    16205 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_recurrent_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    46386 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    66422 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/linen_transforms_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    17745 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/partitioning_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    24643 2024-04-30 09:57:01.000000 flax-0.8.3/tests/linen/summary_test.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     4523 2024-04-30 09:57:01.000000 flax-0.8.3/tests/run_all_tests.sh
--rw-r--r--   0 runner    (1001) docker     (127)    16224 2024-04-30 09:57:01.000000 flax-0.8.3/tests/serialization_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-04-30 09:57:01.000000 flax-0.8.3/tests/struct_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    14326 2024-04-30 09:57:01.000000 flax-0.8.3/tests/tensorboard_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6014 2024-04-30 09:57:01.000000 flax-0.8.3/tests/traceback_util_test.py
--rw-r--r--   0 runner    (1001) docker     (127)    10620 2024-04-30 09:57:01.000000 flax-0.8.3/tests/traverse_util_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.284707 flax-0.8.4/
+-rw-r--r--   0 runner    (1001) docker     (127)       54 2024-05-24 17:09:19.000000 flax-0.8.4/.git-blame-ignore-revs
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.192707 flax-0.8.4/.github/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.192707 flax-0.8.4/.github/ISSUE_TEMPLATE/
+-rw-r--r--   0 runner    (1001) docker     (127)      792 2024-05-24 17:09:19.000000 flax-0.8.4/.github/ISSUE_TEMPLATE/bug_report.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.192707 flax-0.8.4/.github/analytics/
+-rw-r--r--   0 runner    (1001) docker     (127)      544 2024-05-24 17:09:19.000000 flax-0.8.4/.github/analytics/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)    13817 2024-05-24 17:09:19.000000 flax-0.8.4/.github/analytics/get_repo_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1677 2024-05-24 17:09:19.000000 flax-0.8.4/.github/analytics/issue_activity_since_date.gql
+-rw-r--r--   0 runner    (1001) docker     (127)     2016 2024-05-24 17:09:19.000000 flax-0.8.4/.github/analytics/pr_data_query.gql
+-rw-r--r--   0 runner    (1001) docker     (127)       34 2024-05-24 17:09:19.000000 flax-0.8.4/.github/analytics/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-24 17:09:19.000000 flax-0.8.4/.github/pull_request_template.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.196707 flax-0.8.4/.github/workflows/
+-rw-r--r--   0 runner    (1001) docker     (127)     6381 2024-05-24 17:09:19.000000 flax-0.8.4/.github/workflows/build.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-24 17:09:19.000000 flax-0.8.4/.github/workflows/pythonpublish.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      185 2024-05-24 17:09:19.000000 flax-0.8.4/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)     1214 2024-05-24 17:09:19.000000 flax-0.8.4/.pre-commit-config.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      563 2024-05-24 17:09:19.000000 flax-0.8.4/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (127)      293 2024-05-24 17:09:19.000000 flax-0.8.4/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (127)    28188 2024-05-24 17:09:19.000000 flax-0.8.4/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (127)    11309 2024-05-24 17:09:19.000000 flax-0.8.4/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)    10328 2024-05-24 17:09:30.280707 flax-0.8.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     8008 2024-05-24 17:09:19.000000 flax-0.8.4/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      110 2024-05-24 17:09:19.000000 flax-0.8.4/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.196707 flax-0.8.4/dev/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.196707 flax-0.8.4/dev/.devcontainer/
+-rw-r--r--   0 runner    (1001) docker     (127)     2643 2024-05-24 17:09:19.000000 flax-0.8.4/dev/.devcontainer/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (127)     1806 2024-05-24 17:09:19.000000 flax-0.8.4/dev/.devcontainer/devcontainer.json
+-rw-r--r--   0 runner    (1001) docker     (127)      814 2024-05-24 17:09:19.000000 flax-0.8.4/dev/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4581 2024-05-24 17:09:19.000000 flax-0.8.4/dev/update_requirements.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.196707 flax-0.8.4/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)       18 2024-05-24 17:09:19.000000 flax-0.8.4/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)      634 2024-05-24 17:09:19.000000 flax-0.8.4/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (127)     5359 2024-05-24 17:09:19.000000 flax-0.8.4/docs/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.200707 flax-0.8.4/docs/_ext/
+-rw-r--r--   0 runner    (1001) docker     (127)     7187 2024-05-24 17:09:19.000000 flax-0.8.4/docs/_ext/codediff.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4133 2024-05-24 17:09:19.000000 flax-0.8.4/docs/_ext/codediff_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2288 2024-05-24 17:09:19.000000 flax-0.8.4/docs/_ext/flax_module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.184707 flax-0.8.4/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.200707 flax-0.8.4/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (127)      309 2024-05-24 17:09:19.000000 flax-0.8.4/docs/_static/css/flax_theme.css
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.184707 flax-0.8.4/docs/_templates/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.200707 flax-0.8.4/docs/_templates/autosummary/
+-rw-r--r--   0 runner    (1001) docker     (127)      674 2024-05-24 17:09:19.000000 flax-0.8.4/docs/_templates/autosummary/flax_module.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.200707 flax-0.8.4/docs/api_reference/
+-rw-r--r--   0 runner    (1001) docker     (127)      190 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.config.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      321 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.core.frozen_dict.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1679 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.cursor.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      158 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.errors.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      365 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.jax_utils.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.204707 flax-0.8.4/docs/api_reference/flax.linen/
+-rw-r--r--   0 runner    (1001) docker     (127)      827 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/activation_functions.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      117 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/decorators.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      141 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/init_apply.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      756 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)       94 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/inspection.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2360 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/layers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      509 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/module.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      176 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/profiling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      587 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/spmd.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      412 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/transformations.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      116 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.linen/variable.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.204707 flax-0.8.4/docs/api_reference/flax.nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)      402 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/graph.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/helpers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      290 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      178 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/module.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.204707 flax-0.8.4/docs/api_reference/flax.nnx/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)      736 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/activations.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/attention.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      255 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      752 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/initializers.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      293 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/linear.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/normalization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      126 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/nn/stochastic.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      158 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/rnglib.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      237 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/spmd.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.204707 flax-0.8.4/docs/api_reference/flax.nnx/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      204 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/training/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      253 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/training/metrics.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/training/optimizer.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/transforms.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      385 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/variables.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      119 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.nnx/visualization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      480 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.serialization.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      161 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.struct.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      275 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.traceback_util.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     1212 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.training.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      798 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/flax.traverse_util.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      283 2024-05-24 17:09:19.000000 flax-0.8.4/docs/api_reference/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5982 2024-05-24 17:09:19.000000 flax-0.8.4/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6623 2024-05-24 17:09:19.000000 flax-0.8.4/docs/conf_sphinx_patch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11946 2024-05-24 17:09:19.000000 flax-0.8.4/docs/contributing.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.208707 flax-0.8.4/docs/developer_notes/
+-rw-r--r--   0 runner    (1001) docker     (127)      153 2024-05-24 17:09:19.000000 flax-0.8.4/docs/developer_notes/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    18081 2024-05-24 17:09:19.000000 flax-0.8.4/docs/developer_notes/lift.md
+-rw-r--r--   0 runner    (1001) docker     (127)    22000 2024-05-24 17:09:19.000000 flax-0.8.4/docs/developer_notes/module_lifecycle.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.208707 flax-0.8.4/docs/examples/
+-rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-05-24 17:09:19.000000 flax-0.8.4/docs/examples/community_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4422 2024-05-24 17:09:19.000000 flax-0.8.4/docs/examples/core_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    22579 2024-05-24 17:09:19.000000 flax-0.8.4/docs/examples/google_research_examples.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-24 17:09:19.000000 flax-0.8.4/docs/examples/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     2028 2024-05-24 17:09:19.000000 flax-0.8.4/docs/examples/repositories_that_use_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4134 2024-05-24 17:09:19.000000 flax-0.8.4/docs/faq.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    20991 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flax.png
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.208707 flax-0.8.4/docs/flip/
+-rw-r--r--   0 runner    (1001) docker     (127)      648 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/0000-template.md
+-rw-r--r--   0 runner    (1001) docker     (127)    17256 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/1009-optimizer-api.md
+-rw-r--r--   0 runner    (1001) docker     (127)     8189 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/1777-default-dtype.md
+-rw-r--r--   0 runner    (1001) docker     (127)    11758 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/2396-rnn.md
+-rw-r--r--   0 runner    (1001) docker     (127)    10424 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/2434-general-metadata.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4099 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/2974-kw-only-dataclasses.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4068 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/3099-rnnbase-refactor.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-05-24 17:09:19.000000 flax-0.8.4/docs/flip/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6679 2024-05-24 17:09:19.000000 flax-0.8.4/docs/glossary.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.208707 flax-0.8.4/docs/guides/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.212707 flax-0.8.4/docs/guides/converting_and_upgrading/
+-rw-r--r--   0 runner    (1001) docker     (127)    10603 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    26283 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/haiku_migration_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      255 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    17336 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    10667 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/optax_update_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     9129 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     4106 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     6109 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.212707 flax-0.8.4/docs/guides/data_preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (127)     6994 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/data_preprocessing/full_eval.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/data_preprocessing/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8230 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/data_preprocessing/loading_datasets.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     5149 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/data_preprocessing/loading_datasets.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.212707 flax-0.8.4/docs/guides/flax_fundamentals/
+-rw-r--r--   0 runner    (1001) docker     (127)     4087 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/arguments.md
+-rw-r--r--   0 runner    (1001) docker     (127)    38145 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/flax_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    20784 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/flax_basics.md
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    68258 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/rng_guide.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    29445 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/rng_guide.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3143 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/setup_or_nncompact.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     5886 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_fundamentals/state_params.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7226 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_sharp_bits.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     5794 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/flax_sharp_bits.md
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.212707 flax-0.8.4/docs/guides/model_inspection/
+-rw-r--r--   0 runner    (1001) docker     (127)    12728 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/model_inspection/extracting_intermediates.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      110 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/model_inspection/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/model_inspection/model_surgery.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/model_inspection/model_surgery.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.216707 flax-0.8.4/docs/guides/parallel_training/
+-rw-r--r--   0 runner    (1001) docker     (127)    10423 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/parallel_training/ensembling.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    62518 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/parallel_training/flax_on_pjit.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    24602 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/parallel_training/flax_on_pjit.md
+-rw-r--r--   0 runner    (1001) docker     (127)       97 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/parallel_training/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.216707 flax-0.8.4/docs/guides/training_techniques/
+-rw-r--r--   0 runner    (1001) docker     (127)     9054 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/batch_norm.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    10945 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/dropout.rst
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     8226 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/lr_schedule.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    11290 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/transfer_learning.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     7843 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/transfer_learning.md
+-rw-r--r--   0 runner    (1001) docker     (127)    53437 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/use_checkpointing.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    25779 2024-05-24 17:09:19.000000 flax-0.8.4/docs/guides/training_techniques/use_checkpointing.md
+-rw-r--r--   0 runner    (1001) docker     (127)     8643 2024-05-24 17:09:19.000000 flax-0.8.4/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)    39597 2024-05-24 17:09:19.000000 flax-0.8.4/docs/linen_intro.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    20900 2024-05-24 17:09:19.000000 flax-0.8.4/docs/linen_intro.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.216707 flax-0.8.4/docs/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     3908 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/index.rst
+-rw-r--r--   0 runner    (1001) docker     (127)   317481 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/mnist_tutorial.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    10320 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/mnist_tutorial.md
+-rw-r--r--   0 runner    (1001) docker     (127)   194427 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/nnx_basics.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    13481 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/nnx_basics.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4406 2024-05-24 17:09:19.000000 flax-0.8.4/docs/nnx/transforms.rst
+-rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-05-24 17:09:19.000000 flax-0.8.4/docs/philosophy.md
+-rw-r--r--   0 runner    (1001) docker     (127)   101054 2024-05-24 17:09:19.000000 flax-0.8.4/docs/quick_start.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    14177 2024-05-24 17:09:19.000000 flax-0.8.4/docs/quick_start.md
+-rw-r--r--   0 runner    (1001) docker     (127)      705 2024-05-24 17:09:19.000000 flax-0.8.4/docs/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      174 2024-05-24 17:09:19.000000 flax-0.8.4/docs/robots.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.216707 flax-0.8.4/examples/
+-rw-r--r--   0 runner    (1001) docker     (127)      743 2024-05-24 17:09:19.000000 flax-0.8.4/examples/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-05-24 17:09:19.000000 flax-0.8.4/examples/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.220707 flax-0.8.4/examples/cloud/
+-rw-r--r--   0 runner    (1001) docker     (127)     4635 2024-05-24 17:09:19.000000 flax-0.8.4/examples/cloud/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     9222 2024-05-24 17:09:19.000000 flax-0.8.4/examples/cloud/launch_gce.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1650 2024-05-24 17:09:19.000000 flax-0.8.4/examples/cloud/startup_script.sh
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.220707 flax-0.8.4/examples/imagenet/
+-rw-r--r--   0 runner    (1001) docker     (127)     9944 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.220707 flax-0.8.4/examples/imagenet/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     2192 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/configs/fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1670 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/configs/tpu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/configs/v100_x8.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1088 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/configs/v100_x8_mixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (127)   293668 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/imagenet.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     3334 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/imagenet_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/imagenet_fake_data_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8124 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4346 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      341 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    12863 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3049 2024-05-24 17:09:19.000000 flax-0.8.4/examples/imagenet/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.224707 flax-0.8.4/examples/linen_design_test/
+-rw-r--r--   0 runner    (1001) docker     (127)     6242 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/attention_simple.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3145 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/autoencoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1246 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/dense.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/linear_regression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2303 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/mlp_explicit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/mlp_inline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1891 2024-05-24 17:09:19.000000 flax-0.8.4/examples/linen_design_test/mlp_lazy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.224707 flax-0.8.4/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (127)     3320 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.224707 flax-0.8.4/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     5011 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12500 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3355 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2190 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13273 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)      347 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     4843 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5313 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20410 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1994 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5793 2024-05-24 17:09:19.000000 flax-0.8.4/examples/lm1b/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.224707 flax-0.8.4/examples/mnist/
+-rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.228707 flax-0.8.4/examples/mnist/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)      912 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2121 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    98260 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/mnist.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     2366 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/mnist_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)      298 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     5231 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2262 2024-05-24 17:09:19.000000 flax-0.8.4/examples/mnist/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.228707 flax-0.8.4/examples/nlp_seq/
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7884 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4073 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6464 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)       60 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    14100 2024-05-24 17:09:19.000000 flax-0.8.4/examples/nlp_seq/train.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.232707 flax-0.8.4/examples/ogbg_molpcba/
+-rw-r--r--   0 runner    (1001) docker     (127)     4486 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.232707 flax-0.8.4/examples/ogbg_molpcba/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1520 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1551 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/configs/default_graph_net.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1946 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/configs/hparam_sweep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1405 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/configs/test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8133 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2571 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2198 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7068 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5179 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)  1110530 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/ogbg_molpcba.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4769 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)      329 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    13697 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12431 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ogbg_molpcba/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.232707 flax-0.8.4/examples/ppo/
+-rw-r--r--   0 runner    (1001) docker     (127)     2501 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2607 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/agent.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.232707 flax-0.8.4/examples/ppo/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1955 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2460 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/env_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2346 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13152 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/ppo_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/ppo_lib_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1529 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/ppo_main.py
+-rw-r--r--   0 runner    (1001) docker     (127)      192 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     8930 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/seed_rl_atari_preprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1897 2024-05-24 17:09:19.000000 flax-0.8.4/examples/ppo/test_episodes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.236707 flax-0.8.4/examples/seq2seq/
+-rw-r--r--   0 runner    (1001) docker     (127)      913 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)    24740 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/seq2seq.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     6844 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3201 2024-05-24 17:09:19.000000 flax-0.8.4/examples/seq2seq/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.236707 flax-0.8.4/examples/sst2/
+-rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/README.md
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2028 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/build_vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.236707 flax-0.8.4/examples/sst2/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     1226 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/configs/default.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9834 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3522 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2118 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14403 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3526 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      156 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     8607 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/sst2.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     9335 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2123 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)   117898 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/vocab.txt
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4407 2024-05-24 17:09:19.000000 flax-0.8.4/examples/sst2/vocabulary.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.240707 flax-0.8.4/examples/vae/
+-rw-r--r--   0 runner    (1001) docker     (127)     1132 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.240707 flax-0.8.4/examples/vae/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)      883 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1458 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1814 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1777 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/reconstruction.png
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.240707 flax-0.8.4/examples/vae/results/
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/results/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)    43139 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/sample.png
+-rw-r--r--   0 runner    (1001) docker     (127)     4596 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-05-24 17:09:19.000000 flax-0.8.4/examples/vae/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.240707 flax-0.8.4/examples/wmt/
+-rw-r--r--   0 runner    (1001) docker     (127)     6106 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7270 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/bleu.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.240707 flax-0.8.4/examples/wmt/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     3482 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14745 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/decode.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12910 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3318 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2166 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18604 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)      398 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     5314 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23409 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1995 2024-05-24 17:09:19.000000 flax-0.8.4/examples/wmt/train_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.244707 flax-0.8.4/flax/
+-rw-r--r--   0 runner    (1001) docker     (127)     1135 2024-05-24 17:09:19.000000 flax-0.8.4/flax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5990 2024-05-24 17:09:19.000000 flax-0.8.4/flax/configurations.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.248707 flax-0.8.4/flax/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1467 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5763 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/axes_scan.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10482 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/flax_functional_engine.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     9974 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/frozen_dict.py
+-rw-r--r--   0 runner    (1001) docker     (127)    61688 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/lift.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11725 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/meta.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.248707 flax-0.8.4/flax/core/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17932 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12040 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6897 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1488 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2546 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/partial_eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38092 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/scope.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/tracers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-24 17:09:19.000000 flax-0.8.4/flax/core/variables.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26399 2024-05-24 17:09:19.000000 flax-0.8.4/flax/cursor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30713 2024-05-24 17:09:19.000000 flax-0.8.4/flax/errors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.248707 flax-0.8.4/flax/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-05-24 17:09:19.000000 flax-0.8.4/flax/experimental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      731 2024-05-24 17:09:19.000000 flax-0.8.4/flax/experimental/nnx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-05-24 17:09:19.000000 flax-0.8.4/flax/ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5309 2024-05-24 17:09:19.000000 flax-0.8.4/flax/io.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11534 2024-05-24 17:09:19.000000 flax-0.8.4/flax/jax_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/linen/
+-rw-r--r--   0 runner    (1001) docker     (127)     2191 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5259 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32708 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4172 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/batch_apply.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3846 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/combinators.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3860 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/dtypes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/linen/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)    11235 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/experimental/layers_with_named_axes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10062 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/fp8_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2676 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8828 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/kw_only_dataclasses.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44217 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)   113805 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/module.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49760 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19421 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5567 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45995 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/recurrent.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11370 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3634 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26100 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/summary.py
+-rw-r--r--   0 runner    (1001) docker     (127)    79325 2024-05-24 17:09:19.000000 flax-0.8.4/flax/linen/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-05-24 17:09:19.000000 flax-0.8.4/flax/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7792 2024-05-24 17:09:19.000000 flax-0.8.4/flax/metrics/tensorboard.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     1831 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (127)     3574 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6220 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/nnx/docs/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/blog.md
+-rw-r--r--   0 runner    (1001) docker     (127)    10240 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/demo.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     4258 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/demo.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.252707 flax-0.8.4/flax/nnx/docs/images/
+-rw-r--r--   0 runner    (1001) docker     (127)   304812 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/images/stateful-transforms.png
+-rw-r--r--   0 runner    (1001) docker     (127)    61282 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/quick_start.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    16318 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/tiny_nnx.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    26880 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/why.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)    15198 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/docs/why.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.188707 flax-0.8.4/flax/nnx/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.256707 flax-0.8.4/flax/nnx/examples/lm1b/
+-rw-r--r--   0 runner    (1001) docker     (127)     3267 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.256707 flax-0.8.4/flax/nnx/examples/lm1b/configs/
+-rw-r--r--   0 runner    (1001) docker     (127)     5096 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/configs/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12344 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/input_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3293 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/input_pipeline_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2143 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15006 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9812 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/models_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      347 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/temperature_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1448 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/temperature_sampler_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5263 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20510 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/train_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4983 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/lm1b/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.256707 flax-0.8.4/flax/nnx/examples/toy_examples/
+-rw-r--r--   0 runner    (1001) docker     (127)     2855 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/01_functional_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2581 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/02_lifted_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5195 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/05_vae.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1852 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/06_scan_over_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1933 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/08_save_load_checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1956 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/09_parameter_surgery.py
+-rw-r--r--   0 runner    (1001) docker     (127)       35 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/examples/toy_examples/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.260707 flax-0.8.4/flax/nnx/nnx/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.260707 flax-0.8.4/flax/nnx/nnx/compat/
+-rw-r--r--   0 runner    (1001) docker     (127)      931 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/compat/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6160 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/compat/module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3120 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/compat/wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      626 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/errors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2938 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/filterlib.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46521 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6526 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2326 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9762 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.264707 flax-0.8.4/flax/nnx/nnx/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1219 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/activations.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25656 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3324 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/dtypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2570 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35746 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6005 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18304 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3494 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/nn/stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5507 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/object.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2951 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/proxy_caller.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3178 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/reprlib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5598 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/rnglib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6347 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8143 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/state.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1715 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/tracers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.264707 flax-0.8.4/flax/nnx/nnx/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      582 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5376 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/training/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4830 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/training/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    56577 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22115 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/variables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3624 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/nnx/visualization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.264707 flax-0.8.4/flax/nnx/scripts/
+-rw-r--r--   0 runner    (1001) docker     (127)       17 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/scripts/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      272 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/scripts/run-all-examples.bash
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.268707 flax-0.8.4/flax/nnx/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)     1163 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.268707 flax-0.8.4/flax/nnx/tests/compat/
+-rw-r--r--   0 runner    (1001) docker     (127)     3296 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/compat/test_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1185 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/compat/test_wrappers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.268707 flax-0.8.4/flax/nnx/tests/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)     4917 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4799 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2152 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_embed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4357 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4230 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7524 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2596 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/nn/test_stochastic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_containers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11539 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_graph_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3086 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_ids.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7449 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_integration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2339 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16173 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4142 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4255 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_partitioning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7229 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_rngs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2839 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2478 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31842 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1849 2024-05-24 17:09:19.000000 flax-0.8.4/flax/nnx/tests/test_variable.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.268707 flax-0.8.4/flax/oss/
+-rw-r--r--   0 runner    (1001) docker     (127)      443 2024-05-24 17:09:19.000000 flax-0.8.4/flax/oss/ .git-blame-ignore-revs
+-rw-r--r--   0 runner    (1001) docker     (127)       58 2024-05-24 17:09:19.000000 flax-0.8.4/flax/py.typed
+-rw-r--r--   0 runner    (1001) docker     (127)    14315 2024-05-24 17:09:19.000000 flax-0.8.4/flax/serialization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8275 2024-05-24 17:09:19.000000 flax-0.8.4/flax/struct.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.268707 flax-0.8.4/flax/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      647 2024-05-24 17:09:19.000000 flax-0.8.4/flax/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9320 2024-05-24 17:09:19.000000 flax-0.8.4/flax/testing/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1990 2024-05-24 17:09:19.000000 flax-0.8.4/flax/traceback_util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.272707 flax-0.8.4/flax/training/
+-rw-r--r--   0 runner    (1001) docker     (127)      613 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43835 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/checkpoints.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3690 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6056 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/dynamic_scale.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3194 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/early_stopping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7419 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/lr_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3685 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/orbax_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2978 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/prefetch_iterator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4767 2024-05-24 17:09:19.000000 flax-0.8.4/flax/training/train_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13746 2024-05-24 17:09:19.000000 flax-0.8.4/flax/traverse_util.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2858 2024-05-24 17:09:19.000000 flax-0.8.4/flax/typing.py
+-rw-r--r--   0 runner    (1001) docker     (127)      650 2024-05-24 17:09:19.000000 flax-0.8.4/flax/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.280707 flax-0.8.4/flax.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    10328 2024-05-24 17:09:30.000000 flax-0.8.4/flax.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    15510 2024-05-24 17:09:30.000000 flax-0.8.4/flax.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-24 17:09:30.000000 flax-0.8.4/flax.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      595 2024-05-24 17:09:30.000000 flax-0.8.4/flax.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        5 2024-05-24 17:09:30.000000 flax-0.8.4/flax.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.272707 flax-0.8.4/images/
+-rw-r--r--   0 runner    (1001) docker     (127)    80407 2024-05-24 17:09:19.000000 flax-0.8.4/images/flax_logo.png
+-rw-r--r--   0 runner    (1001) docker     (127)     3862 2024-05-24 17:09:19.000000 flax-0.8.4/images/flax_logo.svg
+-rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-05-24 17:09:19.000000 flax-0.8.4/images/flax_logo_250px.png
+-rw-r--r--   0 runner    (1001) docker     (127)    29095 2024-05-24 17:09:19.000000 flax-0.8.4/images/flax_logo_500px.png
+-rw-r--r--   0 runner    (1001) docker     (127)    14116 2024-05-24 17:09:19.000000 flax-0.8.4/pylintrc
+-rw-r--r--   0 runner    (1001) docker     (127)     5886 2024-05-24 17:09:19.000000 flax-0.8.4/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-24 17:09:30.284707 flax-0.8.4/setup.cfg
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.272707 flax-0.8.4/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)    17456 2024-05-24 17:09:19.000000 flax-0.8.4/tests/checkpoints_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1733 2024-05-24 17:09:19.000000 flax-0.8.4/tests/colab_tpu_jax_version.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-05-24 17:09:19.000000 flax-0.8.4/tests/configurations_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.276707 flax-0.8.4/tests/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     5262 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/core_frozen_dict_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8608 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/core_lift_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6750 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/core_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9633 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/core_scope_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.276707 flax-0.8.4/tests/core/design/
+-rw-r--r--   0 runner    (1001) docker     (127)     4795 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4537 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_auto_encoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2857 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_big_resnets_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2500 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_custom_vjp_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4565 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_dense_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2621 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_flow_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4653 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_resnet_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2572 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_scan_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2209 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_tied_autoencoder_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2612 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_vmap_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2515 2024-05-24 17:09:19.000000 flax-0.8.4/tests/core/design/core_weight_std_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25755 2024-05-24 17:09:19.000000 flax-0.8.4/tests/cursor_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)      887 2024-05-24 17:09:19.000000 flax-0.8.4/tests/download_dataset_metadata.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     2950 2024-05-24 17:09:19.000000 flax-0.8.4/tests/early_stopping_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1686 2024-05-24 17:09:19.000000 flax-0.8.4/tests/import_test.ipynb
+-rw-r--r--   0 runner    (1001) docker     (127)     8291 2024-05-24 17:09:19.000000 flax-0.8.4/tests/io_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3486 2024-05-24 17:09:19.000000 flax-0.8.4/tests/jax_utils_test.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 17:09:30.280707 flax-0.8.4/tests/linen/
+-rw-r--r--   0 runner    (1001) docker     (127)     1974 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/initializers_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4357 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/kw_only_dataclasses_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2594 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_activation_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19223 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_attention_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2999 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_batch_apply_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_combinators_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_dtypes_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41550 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_linear_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6137 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_meta_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    86829 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_module_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16205 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_recurrent_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46386 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    67610 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/linen_transforms_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18428 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/partitioning_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24643 2024-05-24 17:09:19.000000 flax-0.8.4/tests/linen/summary_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5123 2024-05-24 17:09:19.000000 flax-0.8.4/tests/run_all_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (127)    16224 2024-05-24 17:09:19.000000 flax-0.8.4/tests/serialization_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-24 17:09:19.000000 flax-0.8.4/tests/struct_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14326 2024-05-24 17:09:19.000000 flax-0.8.4/tests/tensorboard_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6014 2024-05-24 17:09:19.000000 flax-0.8.4/tests/traceback_util_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10620 2024-05-24 17:09:19.000000 flax-0.8.4/tests/traverse_util_test.py
```

### Comparing `flax-0.8.3/.github/ISSUE_TEMPLATE/bug_report.md` & `flax-0.8.4/.github/ISSUE_TEMPLATE/bug_report.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/analytics/README.md` & `flax-0.8.4/.github/analytics/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/analytics/get_repo_metrics.py` & `flax-0.8.4/.github/analytics/get_repo_metrics.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/analytics/issue_activity_since_date.gql` & `flax-0.8.4/.github/analytics/issue_activity_since_date.gql`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/analytics/pr_data_query.gql` & `flax-0.8.4/.github/analytics/pr_data_query.gql`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/pull_request_template.md` & `flax-0.8.4/.github/pull_request_template.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.github/workflows/build.yml` & `flax-0.8.4/.github/workflows/build.yml`

 * *Files 6% similar despite different names*

```diff
@@ -82,35 +82,36 @@
     runs-on: ubuntu-20.04-16core
     strategy:
       matrix:
         python-version: ['3.9', '3.10', '3.11']
         test-type: [doctest, pytest, pytype, mypy]
         exclude:
           - test-type: pytype
-            python-version: '3.11'
+            python-version: '3.9'
           - test-type: pytype
             python-version: '3.10'
           - test-type: mypy
             python-version: '3.11'
     steps:
     - uses: actions/checkout@v3
     - name: Set up Python ${{ matrix.python-version }}
       id: setup_python
       uses: actions/setup-python@v4
       with:
         python-version: ${{ matrix.python-version }}
     - name: Get week and year
       id: date_key
       run: echo "DATE=$(date +%j)" >> $GITHUB_OUTPUT
-    - name: Cached virtual environment
-      id: venv_cache
-      uses: actions/cache@v3
-      with:
-        path: venv
-        key: pip-${{ steps.setup_python.outputs.python-version }}-${{ steps.date_key.outputs.DATE }}-${{ hashFiles('**/requirements.txt', 'pyproject.toml') }}
+    # TODO(cgarciae): caching is breaking the install step, disabling for now.
+    # - name: Cached virtual environment
+    #   id: venv_cache
+    #   uses: actions/cache@v3
+    #   with:
+    #     path: venv
+    #     key: pip-${{ steps.setup_python.outputs.python-version }}-${{ steps.date_key.outputs.DATE }}-${{ hashFiles('**/requirements.txt', 'pyproject.toml') }}
     - name: Install Dependencies for cache
       if: steps.venv_cache.outputs.cache-hit != 'true'
       run: |
         if [ -d "venv" ]; then rm -rf venv; fi
         python3 -m venv venv
         venv/bin/python3 -m pip install .[all,testing]
         venv/bin/python3 -m pip install tensorflow_datasets[dev]
@@ -138,15 +139,17 @@
           tests/run_all_tests.sh --no-doctest --no-pytest --no-pytype --use-venv
         else
           echo "Unknown test type: ${{ matrix.test-type }}"
           exit 1
         fi
     - name: Upload coverage to Codecov
       if: matrix.test-type == 'pytest'
-      uses: codecov/codecov-action@v1
+      uses: codecov/codecov-action@v4
+      env:
+        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
       with:
         file: ./coverage.xml
     # The below step just reports the success or failure of tests as a "commit status".
     # This is needed for copybara integration.
     - name: Report success or failure as github status
       if: always()
       shell: bash
```

### Comparing `flax-0.8.3/.github/workflows/pythonpublish.yml` & `flax-0.8.4/.github/workflows/pythonpublish.yml`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.pre-commit-config.yaml` & `flax-0.8.4/.pre-commit-config.yaml`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/.readthedocs.yml` & `flax-0.8.4/.readthedocs.yml`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/CHANGELOG.md` & `flax-0.8.4/CHANGELOG.md`

 * *Files 1% similar despite different names*

```diff
@@ -40,16 +40,16 @@
 - Fix pointless docstring example of nn.checkpoint / nn.remat. by @levskaya in https://github.com/google/flax/pull/3703
 - added default params rng to .apply by @chiamp in https://github.com/google/flax/pull/3698
 - [nnx] add partial_init by @cgarciae in https://github.com/google/flax/pull/3674
 - make make_rng default to 'params' by @chiamp in https://github.com/google/flax/pull/3699
 - Add SimpleCell. by @carlosgmartin in https://github.com/google/flax/pull/3697
 - fix Module.module_paths docstring by @cgarciae in https://github.com/google/flax/pull/3709
 - Guarantee the latest JAX version on CI by @cgarciae in https://github.com/google/flax/pull/3705
-- Replace deprecated API `jax.tree_map` by @copybara-service in https://github.com/google/flax/pull/3715
-- Use `jax.tree_util.tree_map` instead of deprecated `jax.tree_map`. by @copybara-service in https://github.com/google/flax/pull/3714
+- Replace deprecated API `jax.tree.map` by @copybara-service in https://github.com/google/flax/pull/3715
+- Use `jax.tree_util.tree_map` instead of deprecated `jax.tree.map`. by @copybara-service in https://github.com/google/flax/pull/3714
 - [nnx] simplify readme by @cgarciae in https://github.com/google/flax/pull/3707
 - [nnx] add demo.ipynb by @cgarciae in https://github.com/google/flax/pull/3680
 - Fix Tabulate's compute_flops by @cgarciae in https://github.com/google/flax/pull/3721
 - [nnx] simplify TraceState by @cgarciae in https://github.com/google/flax/pull/3724
 - Add broadcast of `strides` and `kernel_dilation` to `nn.ConvTranspose` by @IvyZX in https://github.com/google/flax/pull/3731
 - [nnx] Fix State.__sub__ by @cgarciae in https://github.com/google/flax/pull/3704
 - [nnx] always fold_in on fork + new ForkedKeys return type by @cgarciae in https://github.com/google/flax/pull/3722
@@ -78,15 +78,15 @@
 - Added `nn.compact_name_scope` v3.
 - Add explicit control over frozen/slots setting in `flax.struct.dataclass`.
 - Replacing `jax.tree_util.tree_map` with mapping over leafs.
 - Fixed docs and docstrings.
 
 0.8.0
 -----
-- Added [NNX](https://github.com/google/flax/tree/main/flax/experimental/nnx#nnx), a neural network library for JAX that provides a simple yet powerful module system that adheres to standard Python semantics. Its aim is to combine the robustness of Linen with a simplified, Pythonic API akin to that of PyTorch.
+- Added [NNX](https://github.com/google/flax/tree/main/flax/nnx#nnx), a neural network library for JAX that provides a simple yet powerful module system that adheres to standard Python semantics. Its aim is to combine the robustness of Linen with a simplified, Pythonic API akin to that of PyTorch.
 - Added `nn.compact_name_scope` decorator that enables methods to act as compact name scopes as with regular Haiku methods. This makes porting Haiku code easier.
 - Add copy() method to Module.  This is a user-friendly version of the internal clone() method with better
   defaults for common use cases.
 - Added [`BatchApply`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/layers.html#batchapply) class.
 - Added `sow_weights` option in attention layer.
 - Added [`MultiHeadAttention`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.MultiHeadAttention.html) alias.
 - Added kwargs support for `nn.jit`.
@@ -336,15 +336,15 @@
 
 0.4.2
 -----
 
 New features:
 - Add lifted conditional `nn.cond`.
 - Improved error messages: parameters not found, loading checkpoints.
-- Replace `jax.tree_multimap` (deprecated) with `jax.tree_map`.
+- Replace `jax.tree_multimap` (deprecated) with `jax.tree.map`.
 - Add the "Module Lifecycle" design note.
 - Add support for JAX dynamic stack-based named_call
 
 Bug fixes:
 - Handle rate==1.0 edgecase in Dropout.
 - Fix bug where Linen Module state is reused.
 - Bug fixes and generalizations of nn.partitioning API.
```

### Comparing `flax-0.8.3/LICENSE` & `flax-0.8.4/LICENSE`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/PKG-INFO` & `flax-0.8.4/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.8.3
+Version: 0.8.4
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -63,14 +63,16 @@
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
+** NEW**: Check out the [**NNX**](https://flax.readthedocs.io/en/latest/nnx/index.html) API!
+
 This README is a very short intro. **To learn everything you need to know about Flax, refer to our [full documentation](https://flax.readthedocs.io/).**
 
 Flax was originally started by engineers and researchers within the Brain Team in Google Research (in close collaboration with the JAX team), and is now developed jointly with the open source community.
 
 Flax is being used by a growing
 community of hundreds of folks in various Alphabet research departments
 for their daily work, as well as a [growing community
@@ -248,15 +250,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.2},
+  version = {0.8.4},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.3/README.md` & `flax-0.8.4/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -8,14 +8,16 @@
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
+** NEW**: Check out the [**NNX**](https://flax.readthedocs.io/en/latest/nnx/index.html) API!
+
 This README is a very short intro. **To learn everything you need to know about Flax, refer to our [full documentation](https://flax.readthedocs.io/).**
 
 Flax was originally started by engineers and researchers within the Brain Team in Google Research (in close collaboration with the JAX team), and is now developed jointly with the open source community.
 
 Flax is being used by a growing
 community of hundreds of folks in various Alphabet research departments
 for their daily work, as well as a [growing community
@@ -193,15 +195,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.2},
+  version = {0.8.4},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.3/dev/.devcontainer/Dockerfile` & `flax-0.8.4/dev/.devcontainer/Dockerfile`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/dev/.devcontainer/devcontainer.json` & `flax-0.8.4/dev/.devcontainer/devcontainer.json`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/dev/README.md` & `flax-0.8.4/dev/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/dev/update_requirements.py` & `flax-0.8.4/dev/update_requirements.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/Makefile` & `flax-0.8.4/docs/Makefile`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/README.md` & `flax-0.8.4/docs/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/_ext/codediff.py` & `flax-0.8.4/tests/core/design/core_attention_test.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,134 +8,164 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""Sphinx directive for creating code diff tables.
+from functools import partial
+from typing import Callable, Optional, Sequence
 
-Use directive as follows:
+import jax
+from absl.testing import absltest
+from jax import lax, random
+from jax import numpy as jnp
+
+from flax.core import Array, Scope, init, lift, nn, unfreeze
+
+
+def softmax_attn(scope: Scope, weights: Array):
+  del scope
+  norm_dims = tuple(range(weights.ndim // 2, weights.ndim))
+  log_norms = jax.scipy.special.logsumexp(
+    weights, axis=norm_dims, keepdims=True
+  )
+  return jnp.exp(weights - log_norms)
+
+
+def with_dropout(fn, rate: float, deterministic: bool = False):
+  def attn_fn(scope: Scope, weights: Array):
+    attn_weights = fn(scope, weights)
+    return nn.dropout(
+      scope, attn_weights, deterministic=deterministic, rate=rate
+    )
+
+  return attn_fn
 
-.. codediff::
-  :title_left: <LEFT_CODE_BLOCK_TITLE>
-  :title_right: <RIGHT_CODE_BLOCK_TITLE>
-
-  <CODE_BLOCK_LEFT>
-  ---
-  <CODE_BLOCK_RIGHT>
-
-In order to highlight a line of code, append "#!" to it.
-"""
-from typing import List, Tuple
-
-import sphinx
-from docutils import nodes
-from docutils.parsers.rst import directives
-from docutils.statemachine import ViewList
-from sphinx.util.docutils import SphinxDirective
-
-MISSING = object()
-
-
-class CodeDiffParser:
-  def parse(
-    self,
-    lines,
-    title_left='Base',
-    title_right='Diff',
-    code_sep='---',
-    sync=MISSING,
-  ):
-    sync = sync is not MISSING
-
-    if code_sep not in lines:
-      raise ValueError(
-        'Code separator not found! Code snippets should be '
-        f'separated by {code_sep}.'
-      )
-    idx = lines.index(code_sep)
-    code_left = self._code_block(lines[0:idx])
-    test_code = lines[idx + 1 :]
-    code_right = self._code_block(test_code)
 
-    output = self._tabs(
-      (title_left, code_left), (title_right, code_right), sync=sync
+def _dot_product_attention(
+  scope: Scope,
+  query: Array,
+  key: Array,
+  value: Array,
+  bias: Optional[Array] = None,
+  attn_fn: Callable = softmax_attn,
+  dtype=jnp.float32,
+):
+  assert key.ndim == query.ndim
+  assert key.ndim == value.ndim
+
+  n = query.ndim
+  attn_weights = lax.dot_general(query, key, (((n - 1,), (n - 1,)), ((), ())))
+  if bias is not None:
+    attn_weights += bias
+  attn_weights = attn_fn(scope, attn_weights)
+  attn_weights = attn_weights.astype(dtype)
+
+  contract_dims = (
+    tuple(range(n - 1, attn_weights.ndim)),
+    tuple(range(0, n - 1)),
+  )
+  y = lax.dot_general(attn_weights, value, (contract_dims, ((), ())))
+  return y
+
+
+def dot_product_attention(
+  scope: Scope,
+  inputs_q: Array,
+  inputs_kv: Array,
+  bias: Optional[Array] = None,
+  qkv_features: Optional[int] = None,
+  out_features: Optional[int] = None,
+  attn_fn: Callable = softmax_attn,
+  dtype=jnp.float32,
+):
+  if qkv_features is None:
+    qkv_features = inputs_q.shape[-1]
+  if out_features is None:
+    out_features = inputs_q.shape[-1]
+  dense = partial(nn.dense, features=qkv_features, bias=False, dtype=dtype)
+
+  query = scope.child(dense, 'query')(inputs_q)
+  key = scope.child(dense, 'key')(inputs_kv)
+  value = scope.child(dense, 'value')(inputs_kv)
+
+  y = _dot_product_attention(
+    scope, query, key, value, bias=bias, attn_fn=attn_fn, dtype=dtype
+  )
+
+  return scope.child(nn.dense, 'out')(y, features=out_features, dtype=dtype)
+
+
+def multi_head_dot_product_attention(
+  scope: Scope,
+  inputs_q: Array,
+  inputs_kv: Array,
+  bias: Optional[Array] = None,
+  qkv_features: Optional[int] = None,
+  out_features: Optional[int] = None,
+  attn_fn: Callable = softmax_attn,
+  batch_axes: Sequence[int] = (0,),
+  num_heads: int = 1,
+  dtype=jnp.float32,
+  broadcast_dropout=False,
+):
+  if qkv_features is None:
+    qkv_features = inputs_q.shape[-1]
+  if out_features is None:
+    out_features = inputs_q.shape[-1]
+
+  attn_fn = partial(
+    dot_product_attention,
+    attn_fn=attn_fn,
+    qkv_features=qkv_features // num_heads,
+    out_features=out_features,
+    dtype=dtype,
+  )
+  attn_fn = lift.vmap(
+    attn_fn,
+    in_axes=(None, None, None),
+    out_axes=-2,
+    axis_size=num_heads,
+    variable_axes={'params': 0},
+    split_rngs={'params': True, 'dropout': not broadcast_dropout},
+  )
+  for axis in reversed(sorted(batch_axes)):
+    attn_fn = lift.vmap(
+      attn_fn,
+      in_axes=(axis, axis, axis),
+      out_axes=axis,
+      variable_axes={'params': None},
+      split_rngs={'params': False, 'dropout': not broadcast_dropout},
     )
 
-    return output, test_code
+  y = attn_fn(scope, inputs_q, inputs_kv, bias)
+  return y.mean(axis=-2)
+
 
-  def _code_block(self, lines):
-    """Creates a codeblock."""
-    # Remove right trailing whitespace so we can detect the comments.
-    lines = [x.rstrip() for x in lines]
-    highlight = lambda x: x.endswith('#!')
-    code = map(lambda x: x[:-2].rstrip() if highlight(x) else x, lines)
-    highlights = [i + 1 for i in range(len(lines)) if highlight(lines[i])]
-    highlights = ','.join(str(i) for i in highlights)
-
-    directive = ['.. code-block:: python']
-    if highlights:
-      directive += [f'  :emphasize-lines: {highlights}']
-
-    # Indent code and add empty line so the code is picked up by the directive.
-    return directive + [''] + list(map(lambda x: '  ' + x, code))
-
-  def _tabs(self, *contents: Tuple[str, List[str]], sync):
-    output = ['.. tab-set::'] + ['  ']
-
-    for title, content in contents:
-      output += [f'  .. tab-item:: {title}']
-
-      if sync:
-        key = title.strip()
-        output += [f'    :sync: {key}']
-
-      output += ['    ']
-      output += ['    ' + line for line in content]
-
-    return output
-
-
-class CodeDiffDirective(SphinxDirective):
-  has_content = True
-  option_spec = {
-    'title_left': directives.unchanged,
-    'title_right': directives.unchanged,
-    'code_sep': directives.unchanged,
-    'sync': directives.flag,
-  }
-
-  def run(self):
-    table_code, test_code = CodeDiffParser().parse(
-      list(self.content), **self.options
+class AttentionTest(absltest.TestCase):
+  def test_attention(self):
+    inputs = jnp.ones((2, 7, 16))
+    model = partial(
+      multi_head_dot_product_attention,
+      num_heads=2,
+      batch_axes=(0,),
+      attn_fn=with_dropout(softmax_attn, 0.1, deterministic=False),
     )
 
-    # Create a test node as a comment node so it won't show up in the docs.
-    # We add attribute "testnodetype" so it is be picked up by the doctest
-    # builder. This functionality is not officially documented but can be found
-    # in the source code:
-    # https://github.com/sphinx-doc/sphinx/blob/3.x/sphinx/ext/doctest.py
-    # (search for 'testnodetype').
-    test_code = '\n'.join(test_code)
-    test_node = nodes.comment(test_code, test_code, testnodetype='testcode')
-    # Set the source info so the error message is correct when testing.
-    self.set_source_info(test_node)
-    test_node['options'] = {}
-    test_node['language'] = 'python3'
-
-    # The table node is the side-by-side diff view that will be shown on RTD.
-    table_node = nodes.paragraph()
-    self.content = ViewList(table_code, self.content.parent)
-    self.state.nested_parse(self.content, self.content_offset, table_node)
-
-    return [table_node, test_node]
-
-
-def setup(app):
-  app.add_directive('codediff', CodeDiffDirective)
-
-  return {
-    'version': sphinx.__display_version__,
-    'parallel_read_safe': True,
-    'parallel_write_safe': True,
-  }
+    rngs = {'params': random.key(0), 'dropout': random.key(1)}
+    y, variables = jax.jit(init(model))(rngs, inputs, inputs)
+    variable_shapes = jax.tree_util.tree_map(jnp.shape, variables['params'])
+    self.assertEqual(y.shape, (2, 7, 16))
+    self.assertEqual(
+      unfreeze(variable_shapes),
+      {
+        'key': {'kernel': (2, 16, 8)},
+        'value': {'kernel': (2, 16, 8)},
+        'query': {'kernel': (2, 16, 8)},
+        'out': {'bias': (2, 16), 'kernel': (2, 8, 16)},
+      },
+    )
+
+
+if __name__ == '__main__':
+  absltest.main()
```

### Comparing `flax-0.8.3/docs/_ext/flax_module.py` & `flax-0.8.4/docs/_ext/flax_module.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/_templates/autosummary/flax_module.rst` & `flax-0.8.4/docs/_templates/autosummary/flax_module.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.cursor.rst` & `flax-0.8.4/docs/api_reference/flax.cursor.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/activations.rst` & `flax-0.8.4/docs/api_reference/flax.nnx/nn/activations.rst`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 Activation functions
 ------------------------
 
-.. automodule:: flax.experimental.nnx
-.. currentmodule:: flax.experimental.nnx
+.. automodule:: flax.nnx
+.. currentmodule:: flax.nnx
 
 .. autofunction:: celu
 .. autofunction:: elu
 .. autofunction:: gelu
 .. autofunction:: glu
 .. autofunction:: hard_sigmoid
 .. autofunction:: hard_silu
```

### Comparing `flax-0.8.3/docs/api_reference/flax.experimental.nnx/nn/initializers.rst` & `flax-0.8.4/docs/api_reference/flax.linen/initializers.rst`

 * *Files 11% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 Initializers
 ------------------------
 
-.. automodule:: flax.experimental.nnx.initializers
-.. currentmodule:: flax.experimental.nnx.initializers
+.. automodule:: flax.linen.initializers
+.. currentmodule:: flax.linen.initializers
 
 .. autofunction:: constant
 .. autofunction:: delta_orthogonal
 .. autofunction:: glorot_normal
 .. autofunction:: glorot_uniform
 .. autofunction:: he_normal
 .. autofunction:: he_uniform
```

### Comparing `flax-0.8.3/docs/api_reference/flax.linen/activation_functions.rst` & `flax-0.8.4/docs/api_reference/flax.linen/activation_functions.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.linen/initializers.rst` & `flax-0.8.4/docs/api_reference/flax.nnx/nn/initializers.rst`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 Initializers
 ------------------------
 
-.. automodule:: flax.linen.initializers
-.. currentmodule:: flax.linen.initializers
+.. automodule:: flax.nnx.initializers
+.. currentmodule:: flax.nnx.initializers
 
 .. autofunction:: constant
 .. autofunction:: delta_orthogonal
 .. autofunction:: glorot_normal
 .. autofunction:: glorot_uniform
 .. autofunction:: he_normal
 .. autofunction:: he_uniform
```

### Comparing `flax-0.8.3/docs/api_reference/flax.linen/layers.rst` & `flax-0.8.4/docs/api_reference/flax.linen/layers.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.linen/spmd.rst` & `flax-0.8.4/docs/api_reference/flax.linen/spmd.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.training.rst` & `flax-0.8.4/docs/api_reference/flax.training.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/api_reference/flax.traverse_util.rst` & `flax-0.8.4/docs/api_reference/flax.traverse_util.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/conf.py` & `flax-0.8.4/docs/conf.py`

 * *Files 5% similar despite different names*

```diff
@@ -106,40 +106,51 @@
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named 'default.css' will overwrite the builtin 'default.css'.
 html_static_path = ['_static']
 
 html_extra_path = ['robots.txt']
 
+# href with no underline and white bold text color
+announcement = """
+<a
+  href="https://flax.readthedocs.io/en/latest/nnx/index.html"
+  style="text-decoration: none; color: white;"
+>
+   Check out the new <b>NNX</b> API!
+</a>
+"""
+
 html_theme_options = {
   'repository_url': 'https://github.com/google/flax',
   'use_repository_button': True,  # add a 'link to repository' button
   'use_issues_button': False,  # add an 'Open an Issue' button
   'path_to_docs': (
     'docs'
   ),  # used to compute the path to launch notebooks in colab
   'launch_buttons': {
     'colab_url': 'https://colab.research.google.com/',
   },
   'prev_next_buttons_location': None,
   'show_navbar_depth': 1,
+  'announcement': announcement,
 }
 
 # -- Options for myst ----------------------------------------------
 # uncomment line below to avoid running notebooks during development
 nb_execution_mode = 'off'
 # Notebook cell execution timeout; defaults to 30.
 nb_execution_timeout = 100
 # List of patterns, relative to source directory, that match notebook
 # files that will not be executed.
 myst_enable_extensions = ['dollarmath']
 nb_execution_excludepatterns = [
   'quick_start.ipynb',  # <-- times out
   'transfer_learning.ipynb',  # <-- transformers requires flax<=0.7.0
-  'flax/experimental/nnx',  # exclude nnx
+  'flax/nnx',  # exclude nnx
 ]
 # raise exceptions on execution so CI can catch errors
 nb_execution_allow_errors = False
 nb_execution_raise_on_error = True
 
 # -- Extension configuration -------------------------------------------------
 
@@ -147,15 +158,15 @@
 # types, even if the parameters aren't explicitly documented.
 always_document_param_types = True
 
 # -- doctest configuration -------------------------------------------------
 doctest_global_setup = """
 import jax
 import jax.numpy as jnp
-from flax.experimental import nnx
+from flax import nnx
 
 import logging as slog
 from absl import logging as alog
 
 # Avoid certain absl logging messages to break doctest
 filtered_message = [
   'SaveArgs.aggregate is deprecated',
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `flax-0.8.3/docs/conf_sphinx_patch.py` & `flax-0.8.4/docs/conf_sphinx_patch.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/contributing.md` & `flax-0.8.4/docs/contributing.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/developer_notes/lift.md` & `flax-0.8.4/docs/developer_notes/lift.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/developer_notes/module_lifecycle.rst` & `flax-0.8.4/docs/developer_notes/module_lifecycle.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/examples/community_examples.rst` & `flax-0.8.4/docs/examples/community_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/examples/core_examples.rst` & `flax-0.8.4/docs/examples/core_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/examples/google_research_examples.rst` & `flax-0.8.4/docs/examples/google_research_examples.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/examples/repositories_that_use_flax.rst` & `flax-0.8.4/docs/examples/repositories_that_use_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/experimental/nnx/index.rst` & `flax-0.8.4/docs/nnx/index.rst`

 * *Files 13% similar despite different names*

```diff
@@ -1,19 +1,17 @@
 
 NNX
 ========
 
 
-NNX is a JAX-based neural network library designed for simplicity and power. Its modular
-approach follows standard Python conventions, making it both intuitive and compatible with
-the broader JAX ecosystem.
-
-.. note::
-   NNX is currently in an experimental state and is subject to change. Linen is still the
-   recommended option for large-scale projects. Feedback and contributions are welcome!
+NNX is a **N**\ eural **N**\ etwork library for JA\ **X** that focuses on providing the best
+development experience, so building and experimenting with neural networks is easy and
+intuitive. It achieves this by embracing Pythons object-oriented model and making it
+compatible with JAX transforms, resulting in code that is easy to inspect, debug, and
+analyze.
 
 Features
 ^^^^^^^^^
 
 .. grid::
 
    .. grid-item::
@@ -22,67 +20,67 @@
       .. card:: Pythonic
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
-            Modules are standard Python classes, promoting ease of use and a more familiar
-            development experience.
+            NNX supports the use or regular Python object, providing an intuitive
+            and predictable development experience.
 
    .. grid-item::
       :columns: 12 12 12 6
 
-      .. card:: Compatible
+      .. card:: Simple
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
-            Effortlessly convert between Modules and pytrees using the Functional API for maximum
-            flexibility.
+            NNX relies on Python's object model, this results in simplicity for
+            the user which increases development speed.
 
    .. grid-item::
       :columns: 12 12 12 6
 
-      .. card:: Control
+      .. card:: Streamlined
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
-            Manage a Module's state with precision using typed Variable collections, enabling fine-grained
-            control on JAX transformations.
+            NNX integrates of user feedback and hands-on experience with Linen
+            into a new simplified API.
 
    .. grid-item::
       :columns: 12 12 12 6
 
-      .. card:: User-friendly
+      .. card:: Compatible
          :class-card: sd-border-0
          :shadow: none
          :class-title: sd-fs-5
 
          .. div:: sd-font-normal
 
-            NNX prioritizes simplicity for common use cases, building upon lessons learned from Linen
-            to provide a streamlined experience.
+            NNX makes it very easy integrate objects with regular JAX code
+            via the `Functional API <nnx_basics#the-functional-api>`__.
 
 Basic usage
 ^^^^^^^^^^^^
 
 .. testsetup::
 
    import jax
    import jax.numpy as jnp
 
 .. testcode::
 
-   from flax.experimental import nnx
+   from flax import nnx
    import optax
 
 
    class Model(nnx.Module):
      def __init__(self, din, dmid, dout, rngs: nnx.Rngs):
        self.linear = nnx.Linear(din, dmid, rngs=rngs)
        self.bn = nnx.BatchNorm(dmid, rngs=rngs)
@@ -106,15 +104,22 @@
      optimizer.update(grads)  # inplace updates
 
      return loss
 
 
 Installation
 ^^^^^^^^^^^^
-NNX is under active development, we recommend using the latest version from Flax's GitHub repository:
+
+Install NNX via pip:
+
+.. code-block:: bash
+
+   pip install flax
+
+Or install the latest version from the repository:
 
 .. code-block:: bash
 
    pip install git+https://github.com/google/flax.git
 
 
 ----
@@ -146,15 +151,15 @@
          :link: transforms.html
 
    .. grid-item::
       :columns: 6 6 6 4
 
       .. card:: :material-regular:`menu_book;2em` API reference
          :class-card: sd-text-black sd-bg-light
-         :link: ../../api_reference/index.html
+         :link: ../api_reference/flax.nnx/index.html
 
 
 ----
 
 .. toctree::
    :hidden:
    :maxdepth: 1
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `flax-0.8.3/docs/experimental/nnx/mnist_tutorial.ipynb` & `flax-0.8.4/docs/nnx/mnist_tutorial.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9998392489711934%*

 * *Differences: {"'cells'": "{6: {'source': {insert: [(0, 'from flax import nnx  # NNX API\\n')], delete: [0]}}, "*

 * *            "15: {'source': {insert: [(0, 'The "*

 * *            '[`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) '*

 * *            "decorator traces the `train_step` function for just-in-time compilation with \\n')], "*

 * *            'delete: [0]}}}'}*

```diff
@@ -128,15 +128,15 @@
                         ]
                     },
                     "metadata": {},
                     "output_type": "display_data"
                 }
             ],
             "source": [
-                "from flax.experimental import nnx  # NNX API\n",
+                "from flax import nnx  # NNX API\n",
                 "from functools import partial\n",
                 "\n",
                 "class CNN(nnx.Module):\n",
                 "  \"\"\"A simple CNN model.\"\"\"\n",
                 "\n",
                 "  def __init__(self, *, rngs: nnx.Rngs):\n",
                 "    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
@@ -293,15 +293,15 @@
             ]
         },
         {
             "cell_type": "markdown",
             "id": "17",
             "metadata": {},
             "source": [
-                "The [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.experimental.nnx/transforms.html#flax.experimental.nnx.jit) decorator traces the `train_step` function for just-in-time compilation with \n",
+                "The [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) decorator traces the `train_step` function for just-in-time compilation with \n",
                 "[XLA](https://www.tensorflow.org/xla), optimizing performance on \n",
                 "hardware accelerators. `nnx.jit` is similar to [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit),\n",
                 "except it can transforms functions that contain NNX objects as inputs and outputs.\n",
                 "\n",
                 "## 6. Evaluation step\n",
                 "\n",
                 "Create a separate function to calculate loss and accuracy metrics for the test batch, since this will be outside the `train_step` function. Loss is determined using the `optax.softmax_cross_entropy_with_integer_labels` function, since we're reusing the loss function defined earlier."
```

### Comparing `flax-0.8.3/docs/experimental/nnx/mnist_tutorial.md` & `flax-0.8.4/docs/nnx/mnist_tutorial.md`

 * *Files 1% similar despite different names*

```diff
@@ -73,15 +73,15 @@
 ```
 
 ## 3. Define the Network with NNX
 
 Create a convolutional neural network with NNX by subclassing `nnx.Module`.
 
 ```{code-cell} ipython3
-from flax.experimental import nnx  # NNX API
+from flax import nnx  # NNX API
 from functools import partial
 
 class CNN(nnx.Module):
   """A simple CNN model."""
 
   def __init__(self, *, rngs: nnx.Rngs):
     self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)
@@ -159,15 +159,15 @@
   """Train for a single step."""
   grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)
   (loss, logits), grads = grad_fn(model, batch)
   metrics.update(loss=loss, logits=logits, labels=batch['label'])
   optimizer.update(grads)
 ```
 
-The [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.experimental.nnx/transforms.html#flax.experimental.nnx.jit) decorator traces the `train_step` function for just-in-time compilation with 
+The [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) decorator traces the `train_step` function for just-in-time compilation with 
 [XLA](https://www.tensorflow.org/xla), optimizing performance on 
 hardware accelerators. `nnx.jit` is similar to [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit),
 except it can transforms functions that contain NNX objects as inputs and outputs.
 
 ## 6. Evaluation step
 
 Create a separate function to calculate loss and accuracy metrics for the test batch, since this will be outside the `train_step` function. Loss is determined using the `optax.softmax_cross_entropy_with_integer_labels` function, since we're reusing the loss function defined earlier.
```

### Comparing `flax-0.8.3/docs/experimental/nnx/nnx_basics.ipynb` & `flax-0.8.4/docs/nnx/nnx_basics.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997983870967742%*

 * *Differences: {"'cells'": "{1: {'source': {insert: [(0, 'from flax import nnx\\n')], delete: [0]}}}"}*

```diff
@@ -19,15 +19,15 @@
         },
         {
             "cell_type": "code",
             "execution_count": 1,
             "metadata": {},
             "outputs": [],
             "source": [
-                "from flax.experimental import nnx\n",
+                "from flax import nnx\n",
                 "import jax\n",
                 "import jax.numpy as jnp"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
```

### Comparing `flax-0.8.3/docs/experimental/nnx/nnx_basics.md` & `flax-0.8.4/docs/nnx/nnx_basics.md`

 * *Files 0% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 familiar Python object-oriented code, particularly appealing to users of frameworks
 like PyTorch.
 
 Despite its simplified implementation, NNX supports the same powerful design patterns 
 that have allowed Linen to scale effectively to large codebases.
 
 ```{code-cell} ipython3
-from flax.experimental import nnx
+from flax import nnx
 import jax
 import jax.numpy as jnp
 ```
 
 ## The Module System
 To begin lets see how to create a `Linear` Module using NNX. The main difference between 
 NNX and Module systems like Haiku or Linen is that in NNX everything is **explicit**. This
```

### Comparing `flax-0.8.3/docs/experimental/nnx/transforms.rst` & `flax-0.8.4/docs/nnx/transforms.rst`

 * *Files 4% similar despite different names*

```diff
@@ -3,47 +3,47 @@
 
 In this guide, you will learn the differences using NNX and JAX transformations, and how to
 seamlessly switch between them or use them together. We will be focusing on the ``jit`` and
 ``grad`` function transformations in this guide.
 
 First, let's set up imports and generate some dummy data:
 
-.. testcode::
+.. testcode:: NNX, JAX
 
-  from flax.experimental import nnx
+  from flax import nnx
   import jax
 
   x = jax.random.normal(jax.random.key(0), (1, 2))
   y = jax.random.normal(jax.random.key(1), (1, 3))
 
 Differences between NNX and JAX transformations
 ***********************************************
 
 The primary difference between NNX and JAX transformations is that NNX transformations allow you to
 transform functions that take in NNX graph objects as arguments (`Module`, `Rngs`, `Optimizer`, etc),
 even those whose state will be mutated, whereas they aren't recognized in JAX transformations.
 Therefore NNX transformations can transform functions that are not pure and make mutations and
 side-effects.
 
-NNX's `Functional API <https://flax.readthedocs.io/en/latest/experimental/nnx/nnx_basics.html#the-functional-api>`_
+NNX's `Functional API <https://flax.readthedocs.io/en/latest/nnx/nnx_basics.html#the-functional-api>`_
 provides a way to convert graph structures to pytrees and back, by doing this at every function
 boundary you can effectively use graph structures with any JAX transform and propagate state updates
 in a way consistent with functional purity. NNX custom transforms such as ``nnx.jit`` and ``nnx.grad``
 simply remove the boilerplate, as a result the code looks stateful.
 
 Below is an example of using the ``nnx.jit`` and ``nnx.grad`` transformations compared to using the
 ``jax.jit`` and ``jax.grad`` transformations. Notice the function signature of NNX-transformed
 functions can accept the ``nnx.Linear`` module directly and can make stateful updates to the module,
 whereas the function signature of JAX-transformed functions can only accept the pytree-registered
 ``State`` and ``GraphDef`` objects and must return an updated copy of them to maintain the purity of
 the transformed function.
 
 .. codediff::
-  :title_left: NNX transforms
-  :title_right: JAX transforms
+  :title: NNX transforms, JAX transforms
+  :groups: NNX, JAX
   :sync:
 
   @nnx.jit
   def train_step(model, x, y):
     def loss_fn(model):
       return ((model(x) - y) ** 2).mean()
     grads = nnx.grad(loss_fn)(model)
@@ -79,16 +79,16 @@
 Mixing NNX and JAX transformations
 **********************************
 
 NNX and JAX transformations can be mixed together, so long as the JAX-transformed function is
 pure and has valid argument types that are recognized by JAX.
 
 .. codediff::
-  :title_left: Using ``nnx.jit`` with ``jax.grad``
-  :title_right: Using ``jax.jit`` with ``nnx.grad``
+  :title: Using ``nnx.jit`` with ``jax.grad``, Using ``jax.jit`` with ``nnx.grad``
+  :groups: NNX, JAX
   :sync:
 
   @nnx.jit
   def train_step(model, x, y):
     def loss_fn(graphdef, state): #!
       model = nnx.merge(graphdef, state)
       return ((model(x) - y) ** 2).mean()
```

### Comparing `flax-0.8.3/docs/faq.rst` & `flax-0.8.4/docs/faq.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flax.png` & `flax-0.8.4/docs/flax.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/0000-template.md` & `flax-0.8.4/docs/flip/0000-template.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/1009-optimizer-api.md` & `flax-0.8.4/docs/flip/1009-optimizer-api.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/1777-default-dtype.md` & `flax-0.8.4/docs/flip/1777-default-dtype.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/2396-rnn.md` & `flax-0.8.4/docs/flip/2396-rnn.md`

 * *Files 0% similar despite different names*

```diff
@@ -158,15 +158,15 @@
   # Encode in the reverse order.
   carry_backward, outputs_backward = self.backward_rnn(
     inputs, seq_lengths=seq_lengths,
     return_carry=True, reverse=True, # process in reverse order
     keep_order=True, # but return the sequence in the original order
   )
   # Merge both sequences.
-  outputs = jax.tree_map(self.merge_fn, outputs_forward, outputs_backward)
+  outputs = jax.tree.map(self.merge_fn, outputs_forward, outputs_backward)
 
   return (carry_forward, carry_backward), outputs
 ```
 
 Here `merge_fn` a function that takes both outputs and fuses them (`concat` by default). As showcased in the beginning of this document, usage would look like this:
 
 ```python
```

### Comparing `flax-0.8.3/docs/flip/2434-general-metadata.md` & `flax-0.8.4/docs/flip/2434-general-metadata.md`

 * *Files 0% similar despite different names*

```diff
@@ -119,20 +119,20 @@
 
 We call this type of class wrapping a value and keeping track of some additional data a **box**.
 By defining an abstract base class for this box, the API does not need to be aware of the specifics of the metadata that is tracked.
 This should make the API future proof and modular.
 
 The ``add_axis`` and ``remove_axis`` method return an instance of their own type instead of mutating in-place.
 Typically, an implementation would be a ``flax.struct.PyTreeNode`` because the box should still be a valid JAX value and must therefore be handled by the PyTree API.
-Calling ``jax.tree_map`` on a boxed value will simply map over the value in the box.
-The lifted transforms that need to handle metadata will call ``jax.tree_map(..., is_leaf=lambda x: isinstance(x, AxisMetadata))`` to find the AxisMetadata instances within a PyTree.
+Calling ``jax.tree.map`` on a boxed value will simply map over the value in the box.
+The lifted transforms that need to handle metadata will call ``jax.tree.map(..., is_leaf=lambda x: isinstance(x, AxisMetadata))`` to find the AxisMetadata instances within a PyTree.
 
 Advantages of the boxing approach:
 1. Boxing can be used outside of Flax and metadata is automatically "inherited". For example, the optimizer state will
-   have the same partitioning spec as the parameters, because the state is initialized using a ``jax.tree_map`` over the boxed parameters.
+   have the same partitioning spec as the parameters, because the state is initialized using a ``jax.tree.map`` over the boxed parameters.
 2. Boxes are composable.
 3. Boxing avoids string manipulation and generally avoids having to handle additional auxiliary collections like "param_axes" in the current
    partitioning API.
 4. No need to lift metadata collections separately.
 
 
 Disadvantages:
@@ -180,30 +180,30 @@
 partitioned_dense = nn.Dense(features, kernel_init=with_partitioning(nn.initializers.lecun_normal, (None, "data")))
 ```
 
 Initializing a model that creates partitioned weights would result in the following variable structure:
 
 ```python
 variables = partitioned_dense.init(rng, jnp.ones((4,)))
-jax.tree_map(np.shape, variables)  # => {"params": {"kernel": Partitioned(value=(4, 8), names=(None, "data")), bias: (8,)}}
+jax.tree.map(np.shape, variables)  # => {"params": {"kernel": Partitioned(value=(4, 8), names=(None, "data")), bias: (8,)}}
 ```
 
 The variable tree with metadata can be used to integrate with other libraries and APIs.
 For example, we can turn the ``Partitioned`` metadata into ``jax.pjit`` sharding annotations:
 
 ```python
 def to_sharding_spec(x):
   if isinstance(x, Partitioned):
     return PartitionSpec(*x.names)
   else:
     # fully replicated
     return PartitionSpec()
 
 # Result: {"params": {"kernel": PartitionSpec(None, "data"), bias: PartitionSpec()}}
-variables_pspec = jax.tree_map(to_sharding_spec, variables, is_leaf=lambda x: isinstance(x, Partitioned))
+variables_pspec = jax.tree.map(to_sharding_spec, variables, is_leaf=lambda x: isinstance(x, Partitioned))
 ```
 
 ### Unbox syntax
 
 
 Metadata typically doesn't need to be handled by Modules directly. Therefore, we propose to make Modules agnostic to Metadata boxes by default.
 The ``unbox`` method can be used to unpack a variable such that only the original JAX arrays remain. Users can manually call unbox but to make
```

### Comparing `flax-0.8.3/docs/flip/2974-kw-only-dataclasses.md` & `flax-0.8.4/docs/flip/2974-kw-only-dataclasses.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/3099-rnnbase-refactor.md` & `flax-0.8.4/docs/flip/3099-rnnbase-refactor.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/flip/README.md` & `flax-0.8.4/docs/flip/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/glossary.rst` & `flax-0.8.4/docs/glossary.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/convert_pytorch_to_flax.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/haiku_migration_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/haiku_migration_guide.rst`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 
 Migrating from Haiku to Flax
 ============================
 
 This guide will walk through the process of migrating Haiku models to Flax,
 and highlight the differences between the two libraries.
 
-.. testsetup::
+.. testsetup:: Haiku, Flax
 
   import jax
   import jax.numpy as jnp
   from jax import random
   import optax
   import flax.linen as nn
 
@@ -21,16 +21,15 @@
 whereas Flax classes are ``dataclasses``, meaning you define some class
 attributes that are used to automatically generate a constructor. Also,
 all Flax Modules accept a ``name`` argument without needing to define it,
 whereas in Haiku ``name`` must be explicitly defined in the constructor
 signature and passed to the superclass constructor.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   import haiku as hk
 
   class Block(hk.Module):
     def __init__(self, features: int, name=None):
       super().__init__(name=name)
@@ -85,16 +84,15 @@
 
 Now, a place where Haiku and Flax differ substantially is in how you construct
 the model. In Haiku, you use ``hk.transform`` over a function
 that calls your Module, ``transform`` will return an object with ``init``
 and ``apply`` methods. In Flax, you simply instantiate your Module.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward(x, training: bool):
     return Model(256, 10)(x, training)
 
   model = hk.transform(forward)
 
@@ -108,16 +106,15 @@
 To get the model parameters in both libraries you use the ``init`` method
 with a ``random.key`` plus some inputs to run the model. The main difference here is
 that Flax returns a mapping from collection names to nested array dictionaries,
 ``params`` is just one of these possible collections. In Haiku, you get the ``params``
 structure directly.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   sample_x = jax.numpy.ones((1, 784))
   params = model.init(
     random.key(0),
     sample_x, training=False # <== inputs
   )
@@ -180,16 +177,15 @@
 
 During training in both frameworks you pass the parameters structure to the
 ``apply`` method to run the forward pass. Since we are using dropout, in
 both cases we must provide a ``key`` to ``apply`` in order to generate
 the random dropout masks.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def train_step(key, params, inputs, labels):
     def loss_fn(params):
         logits = model.apply(
           params,
           key,
@@ -214,15 +210,15 @@
         return optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()
 
     grads = jax.grad(loss_fn)(params)
     params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params
 
-.. testcode::
+.. testcode:: Haiku, Flax
   :hide:
 
   train_step(random.key(0), params, sample_x, jnp.ones((1,), dtype=jnp.int32))
 
 The most notable differences is that in Flax you have to
 pass the parameters inside a dictionary with a ``params`` key, and the
 key inside a dictionary with a ``dropout`` key. This is because in Flax
@@ -232,16 +228,15 @@
 Handling State
 -----------------
 
 Now let's see how mutable state is handled in both libraries. We will take
 the same model as before, but now we will replace Dropout with BatchNorm.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   class Block(hk.Module):
     def __init__(self, features: int, name=None):
       super().__init__(name=name)
       self.features = features
 
@@ -274,16 +269,15 @@
 ``use_running_average`` for the same purpose.
 
 To instantiate a stateful model in Haiku you use ``hk.transform_with_state``,
 which changes the signature for ``init`` and ``apply`` to accept and return
 state. As before, in Flax you construct the Module directly.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward(x, training: bool):
     return Model(256, 10)(x, training)
 
   model = hk.transform_with_state(forward)
 
@@ -300,16 +294,15 @@
 in Flax you get a new ``batch_stats`` collection in the ``variables`` dictionary.
 Note that since ``hk.BatchNorm`` only initializes batch statistics when
 ``is_training=True``, we must set ``training=True`` when initializing parameters
 of a Haiku model with an ``hk.BatchNorm`` layer. In Flax, we can set
 ``training=False`` as usual.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   sample_x = jax.numpy.ones((1, 784))
   params, state = model.init(
     random.key(0),
     sample_x, training=True # <== inputs #!
   )
@@ -336,16 +329,15 @@
 ``apply`` method to run the forward pass. In Haiku, now pass the ``state``
 as the second argument to ``apply``, and get the new state as the second
 return value. In Flax, you instead add ``batch_stats`` as a new key to the
 input dictionary, and get the ``updates`` variables dictionary as the second
 return value.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def train_step(params, state, inputs, labels):
     def loss_fn(params):
       logits, new_state = model.apply(
         params, state,
         None, # <== rng
@@ -371,15 +363,15 @@
       return loss, updates["batch_stats"]
 
     grads, batch_stats = jax.grad(loss_fn, has_aux=True)(params)
     params = jax.tree_util.tree_map(lambda p, g: p - 0.1 * g, params, grads)
 
     return params, batch_stats
 
-.. testcode::
+.. testcode:: Flax
   :hide:
 
   train_step(params, batch_stats, sample_x, jnp.ones((1,), dtype=jnp.int32))
 
 One major difference is that in Flax a state collection can be mutable or immutable.
 During ``init`` all collections are mutable by default, however, during ``apply``
 you have to explicitly specify which collections are mutable. In this example,
@@ -402,16 +394,15 @@
 
 In Haiku, we can just define the submodules that ``encode`` and ``decode`` need
 directly in ``__init__``, in this case each will just use a ``Linear`` layer.
 In Flax, we will define an ``encoder`` and a ``decoder`` Module ahead of time
 in ``setup``, and use them in the ``encode`` and ``decode`` respectively.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   class AutoEncoder(hk.Module):
 
 
     def __init__(self, embed_dim: int, output_dim: int, name=None):
       super().__init__(name=name)
@@ -455,16 +446,15 @@
 
 Now, we want to be able to call any method from our ``AutoEncoder`` model. In Haiku we
 can define multiple ``apply`` methods for a module through ``hk.multi_transform``. The
 function passed to ``multi_transform`` defines how to initialize the module and which
 different apply methods to generate.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward():
     module = AutoEncoder(256, 784)
     init = lambda x: module(x)
     return init, (module.encode, module.decode)
 
@@ -481,16 +471,15 @@
 
 
 To initialize the parameters of our model, ``init`` can be used to trigger the
 ``__call__`` method, which uses both the ``encode`` and ``decode``
 method. This will create all the necessary parameters for the model.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   params = model.init(
     random.key(0),
     x=jax.numpy.ones((1, 784)),
   )
   ...
@@ -539,16 +528,15 @@
           },
       })
 
 
 Finally, let's explore how we can employ the ``apply`` function to invoke the ``encode`` method:
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   encode, decode = model.apply
   z = encode(
     params,
     None, # <== rng
     x=jax.numpy.ones((1, 784)),
@@ -589,16 +577,15 @@
 To begin, we will first define a ``RNNCell`` module that will contain the logic for a single
 step of the RNN. We will also define a ``initial_state`` method that will be used to initialize
 the state (a.k.a. ``carry``) of the RNN. Like with ``jax.lax.scan``, the ``RNNCell.__call__``
 method will be a function that takes the carry and input, and returns the new
 carry and output. In this case, the carry and the output are the same.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   class RNNCell(hk.Module):
     def __init__(self, hidden_size: int, name=None):
       super().__init__(name=name)
       self.hidden_size = hidden_size
 
@@ -636,16 +623,15 @@
 ``params`` rng stream (so all steps intialize with the same parameters), and finally
 we will specify that we want scan to run over the second axis of the input and stack
 the outputs along the second axis as well. We will then use this temporary type immediately
 to create an instance of the lifted ``RNNCell`` and use it to create the ``carry`` and
 the run the ``__call__`` method which will ``scan`` over the sequence.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   class RNN(hk.Module):
     def __init__(self, hidden_size: int, name=None):
       super().__init__(name=name)
       self.hidden_size = hidden_size
 
@@ -676,16 +662,15 @@
 transform. In Flax, the lifted transforms can operate over both variable collections and rng
 streams, the user must define how different collections are treated by each transform
 according to the transform's semantics.
 
 Finally, let's quickly view how the ``RNN`` Module would be used in both Haiku and Flax.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward(x):
     return RNN(64)(x)
 
   model = hk.without_apply_rng(hk.transform(forward))
 
@@ -736,16 +721,15 @@
 by setting ``split_rngs={'params': True}`` and ``variable_axes={'params': 0}``
 we are telling ``nn.scan`` create different parameters for each step and slice the
 ``params`` collection along the first axis, effectively implementing a stack of
 ``Block`` Modules as in Haiku.
 
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   class Block(hk.Module):
     def __init__(self, features: int, name=None):
       super().__init__(name=name)
       self.features = features
 
@@ -758,15 +742,15 @@
   class MLP(hk.Module):
     def __init__(self, features: int, num_layers: int, name=None):
         super().__init__(name=name)
         self.features = features
         self.num_layers = num_layers
 
     def __call__(self, x, training: bool):
-     @hk.experimental.layer_stack(self.num_layers)
+      @hk.experimental.layer_stack(self.num_layers)
       def stack_block(x):
         return Block(self.features)(x, training)
 
       stack = hk.experimental.layer_stack(self.num_layers)
       return stack_block(x)
 
   ---
@@ -799,16 +783,15 @@
 its second output. These represent the inputs/outputs per-step but they are ``None``
 because in this case we don't have any.
 
 Initializing each model is the same as in previous examples. In this case,
 we will be specifying that we want to use ``5`` layers each with ``64`` features.
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward(x, training: bool):
     return MLP(64, num_layers=5)(x, training)
 
   model = hk.transform(forward)
 
@@ -872,16 +855,15 @@
 -----------------------------------
 
 In Haiku, it is possible to write the entire model as a single function by using the raw ``hk.{get,set}_{parameter,state}`` to define/access model parameters and states. It very common to write the top-level "Module" as a function instead:
 
 The Flax team recommends a more Module-centric approach that uses `__call__` to define the forward function. The corresponding accessor will be `nn.module.param` and `nn.module.variable` (go to `Handling State <#handling-state>`__ for an explanaion on collections).
 
 .. codediff::
-  :title_left: Haiku
-  :title_right: Flax
+  :title: Haiku, Flax
   :sync:
 
   def forward(x):
 
 
     counter = hk.get_state('counter', shape=[], dtype=jnp.int32, init=jnp.ones)
     multiplier = hk.get_parameter('multiplier', shape=[1,], dtype=x.dtype, init=jnp.ones)
```

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/linen_upgrade_guide.rst`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 Upgrading my codebase to Linen
 ==============================
 
 As of Flax v0.4.0, ``flax.nn`` no longer exists, and is replaced with the new
 Linen API at ``flax.linen``. If your codebase is still using the old API, you
 can use this upgrade guide to upgrade it to Linen.
 
-.. testsetup::
+.. testsetup:: Linen
 
   from flax.training import train_state
   from jax import random
   import optax
   import jax
+  import flax.linen as nn
   from flax.linen import initializers
 
   from jax import lax
   import jax.numpy as jnp
   import numpy as np
   from typing import Any, Callable, Sequence, Tuple
 
@@ -25,16 +26,16 @@
 
   default_kernel_init = initializers.lecun_normal()
 
 Defining simple Flax Modules
 ----------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   from flax import nn
 
   class Dense(base.Module):
     def apply(self,
               inputs,
@@ -47,16 +48,14 @@
         (inputs.shape[-1], features), kernel_init)
       y = jnp.dot(inputs, kernel)
       if use_bias:
         bias = self.param(
           'bias', (features,), bias_init)
         y = y + bias
       return y
-
-    return new_state, metrics
   ---
   from flax import linen as nn  # [1] #!
 
   class Dense(nn.Module):
     features: int  # [2] #!
     use_bias: bool = True
     kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
@@ -91,16 +90,16 @@
    can take arbitrary argument lists).
 
 
 Using Flax Modules inside other Modules
 ---------------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   class Encoder(nn.Module):
 
     def apply(self, x):
       x = nn.Dense(x, 500)
       x = nn.relu(x)
@@ -123,16 +122,16 @@
 
 2. Names can be optionally passed to all module constructors.
 
 Sharing submodules and defining multiple methods
 --------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   class AutoEncoder(nn.Module):
     def _create_submodules(self):
       return Decoder.shared(name="encoder")
 
     def apply(self, x, z_rng, latents=20):
@@ -175,16 +174,16 @@
 
 4. Define additional methods just like in regular Python.
 
 ``Module.partial`` inside other modules
 ---------------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   # no import #!
 
   class ResNet(nn.Module):
     """ResNetV1."""
 
@@ -232,16 +231,16 @@
 Use normal ``functools.partial`` instead of ``Module.partial``. The rest stays
 the same.
 
 Top-level training code patterns
 --------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   def create_model(key):
     _, initial_params = CNN.init_by_shape(
       key, [((1, 28, 28, 1), jnp.float32)])
     model = nn.Model(CNN, initial_params)
     return model
@@ -302,20 +301,20 @@
    is free -- just a wrapper around constructor attributes) and call the
    ``apply`` method (which will call ``__call__`` internally).
 
 Non-trainable variables ("state"): Use within Modules
 -----------------------------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   class BatchNorm(nn.Module):
-    def apply(self, x, ...):
+    def apply(self, x):
       # [...]
       ra_mean = self.state(
         'mean', (x.shape[-1], ), initializers.zeros_init())
       ra_var = self.state(
         'var', (x.shape[-1], ), initializers.ones_init())
       # [...]
   ---
@@ -334,16 +333,16 @@
 for details). Flax also lets you treat each variable collection differently when
 using JAX transformations inside modules.
 
 Non-trainable variables ("state"): Top-level training code patterns
 -------------------------------------------------------------------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   # initial params and state
   def initial_model(key, init_batch):
     with nn.stateful() as initial_state:
       _, initial_params = ResNet.init(key, init_batch)
     model = nn.Model(ResNet, initial_params)
@@ -356,15 +355,15 @@
       logits = model(batch['image'])
     # [...]
 
 
 
   # reads immutable batch statistics during evaluation
   def eval_step(model, model_state, batch):
-  with nn.stateful(model_state, mutable=False):
+    with nn.stateful(model_state, mutable=False):
       logits = model(batch['image'], train=False)
     return compute_metrics(logits, batch['label'])
   ---
   # initial variables ({"param": ..., "batch_stats": ...})
   def initial_variables(key, init_batch):
     return ResNet().init(key, init_batch)  # [1] #!
 
@@ -418,16 +417,16 @@
 
 TODO: Add an example here how to load a new ``TrainState`` object.
 
 Randomness
 ----------
 
 .. codediff::
-  :title_left: Old Flax
-  :title_right: Linen
+  :title: Old Flax, Linen
+  :skip_test: Old Flax
   :sync:
 
   def dropout(inputs, rate, deterministic=False):
     keep_prob = 1. - rate
     if deterministic:
       return inputs
     else:
```

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/optax_update_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/optax_update_guide.rst`

 * *Files 2% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 <https://github.com/google/flax/blob/main/docs/flip/1009-optimizer-api.md>`_ and
 the Flax optimizers have been removed in v0.6.0 - this guide is targeted
 towards :py:mod:`flax.optim` users to help them update their code to Optax.
 
 See also Optax's quick start documentation:
 https://optax.readthedocs.io/en/latest/getting_started.html
 
-.. testsetup::
+.. testsetup:: default, flax.optim, optax
 
   import flax
   import jax
   import jax.numpy as jnp
   import flax.linen as nn
   import optax
 
@@ -38,16 +38,16 @@
 The usage is very similar, with the difference that ``optax`` does not keep a
 copy of the ``params``, so they need to be passed around separately. Flax
 provides the utility :py:class:`~flax.training.train_state.TrainState` to store
 optimizer state, parameters, and other associated data in a single dataclass
 (not used in code below).
 
 .. codediff::
-  :title_left: flax.optim
-  :title_right: optax
+  :title: flax.optim, optax
+  :skip_test: flax.optim
   :sync:
 
   @jax.jit
   def train_step(optimizer, batch):
     grads = jax.grad(loss)(optimizer.target, batch)
 
 
@@ -87,16 +87,16 @@
 
 .. |optax.sgd()| replace:: ``optax.sgd()``
 .. _optax.sgd(): https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd
 .. |optax.chain()| replace:: ``optax.chain()``
 .. _optax.chain(): https://optax.readthedocs.io/en/latest/api/combining_optimizers.html#optax.chain
 
 .. codediff::
-  :title_left: Pre-defined alias
-  :title_right: Combining transformations
+  :title: Pre-defined alias, Combining transformations
+  :groups: default, default
 
   # Note that the aliases follow the convention to use positive
   # values for the learning rate by default.
   tx = optax.sgd(learning_rate, momentum)
 
   ---
 
@@ -122,16 +122,16 @@
 
 .. |optax.adamw()| replace:: ``optax.adamw()``
 .. _optax.adamw(): https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.adamw
 .. |optax.add_decayed_weights()| replace:: ``optax.add_decayed_weights()``
 .. _optax.add_decayed_weights(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.add_decayed_weights
 
 .. codediff::
-  :title_left: flax.optim
-  :title_right: optax
+  :title: flax.optim, optax
+  :skip_test: flax.optim
   :sync:
 
   optimizer_def = flax.optim.Adam(
       learning_rate, weight_decay=weight_decay)
   optimizer = optimizer_def.create(variables['params'])
 
   ---
@@ -154,16 +154,16 @@
 processing the gradients before passing them to the optimizer. With Optax this
 becomes just another gradient transformation |optax.clip_by_global_norm()|_.
 
 .. |optax.clip_by_global_norm()| replace:: ``optax.clip_by_global_norm()``
 .. _optax.clip_by_global_norm(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.clip_by_global_norm
 
 .. codediff::
-  :title_left: flax.optim
-  :title_right: optax
+  :title: flax.optim, optax
+  :skip_test: flax.optim
   :sync:
 
   def train_step(optimizer, batch):
     grads = jax.grad(loss)(optimizer.target, batch)
     grads_flat, _ = jax.tree_util.tree_flatten(grads)
     global_l2 = jnp.sqrt(sum([jnp.vdot(p, p) for p in grads_flat]))
     g_factor = jnp.minimum(1.0, grad_clip_norm / global_l2)
@@ -198,16 +198,16 @@
 
 .. |optax.scale_by_schedule()| replace:: ``optax.scale_by_schedule()``
 .. _optax.scale_by_schedule(): https://optax.readthedocs.io/en/latest/api/transformations.html#optax.scale_by_schedule
 .. |optax.inject_hyperparams()| replace:: ``optax.inject_hyperparams()``
 .. _optax.inject_hyperparams(): https://optax.readthedocs.io/en/latest/api/optimizer_schedules.html#optax.inject_hyperparams
 
 .. codediff::
-  :title_left: flax.optim
-  :title_right: optax
+  :title: flax.optim, optax
+  :skip_test: flax.optim
   :sync:
 
   def train_step(step, optimizer, batch):
     grads = jax.grad(loss)(optimizer.target, batch)
     return step + 1, optimizer.apply_gradient(grads, learning_rate=schedule(step))
 
   ---
@@ -240,16 +240,16 @@
 
 .. |optax.masked()| replace:: ``optax.masked()``
 .. _optax.masked(): https://optax.readthedocs.io/en/latest/api/optimizer_wrappers.html#optax.masked
 .. |optax.multi_transform()| replace:: ``optax.multi_transform()``
 .. _optax.multi_transform(): https://optax.readthedocs.io/en/latest/api/combining_optimizers.html#optax.multi_transform
 
 .. codediff::
-  :title_left: flax.optim
-  :title_right: optax
+  :title: flax.optim, optax
+  :skip_test: flax.optim
   :sync:
 
   kernels = flax.traverse_util.ModelParamTraversal(lambda p, _: 'kernel' in p)
   biases = flax.traverse_util.ModelParamTraversal(lambda p, _: 'bias' in p)
 
   kernel_opt = flax.optim.Momentum(learning_rate, momentum)
   bias_opt = flax.optim.Momentum(learning_rate * 0.1, momentum)
```

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/orbax_upgrade_guide.rst`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 To learn more about Orbax, check out the `quick start introductory Colab notebook <http://colab.research.google.com/github/google/orbax/blob/main/checkpoint/orbax//checkpoint/orbax_checkpoint.ipynb>`__ and `the official Orbax documentation <https://github.com/google/orbax/blob/main/docs/checkpoint.md>`_.
 
 You can click on "Open in Colab" above to run the code from this guide.
 
 Throughout the guide, you will be able to compare code examples with and without the Orbax code.
 
-.. testsetup::
+.. testsetup:: orbax.checkpoint
 
   import flax
   from flax.training import checkpoints, orbax_utils
   import orbax
   import jax
   import jax.numpy as jnp
   import numpy as np
@@ -37,15 +37,15 @@
     shutil.rmtree('/tmp/orbax_upgrade')
   os.makedirs('/tmp/orbax_upgrade')
 
 
 Setup
 *****
 
-.. testcode::
+.. testcode:: orbax.checkpoint
 
   # Create some dummy variables for this example.
   MAX_STEPS = 5
   CKPT_PYTREE = [12, {'foo': 'str', 'bar': np.array((2, 3))}, [1, 4, 10]]
   TARGET_PYTREE = [0, {'foo': '', 'bar': np.array((0))}, [0, 0, 0]]
 
 Most common use case: Saving/loading and managing checkpoints
@@ -67,16 +67,16 @@
 3. Then, call ``orbax.CheckpointManager.restore()`` to restore your data.
 
 4. And, if your checkpoint includes some multi-host/multi-process array, pass the correct ``mesh`` into ``flax.training.orbax_utils.restore_args_from_target()`` to generate the correct ``restore_args`` before restoring.
 
 For example:
 
 .. codediff::
-  :title_left: flax.checkpoints
-  :title_right: orbax.checkpoint
+  :title: flax.checkpoints, orbax.checkpoint
+  :skip_test: flax.checkpoints
   :sync:
 
   CKPT_DIR = '/tmp/orbax_upgrade/'
   flax.config.update('flax_use_orbax_checkpointing', False)
 
   # Inside your training loop
   for step in range(MAX_STEPS):
@@ -115,16 +115,16 @@
 If you prefer to not maintain a top-level checkpoint manager, you can still save and restore any individual checkpoint with an ``orbax.checkpoint.Checkpointer``. Note that this means you cannot use all the Orbax management features.
 
 To migrate to Orbax code, instead of using the ``overwrite`` argument in ``flax.save_checkpoint()`` use the ``force`` argument in ``orbax.checkpoint.Checkpointer.save()``.
 
 For example:
 
 .. codediff::
-  :title_left: flax.checkpoints
-  :title_right: orbax.checkpoint
+  :title: flax.checkpoints, orbax.checkpoint
+  :skip_test: flax.checkpoints
   :sync:
 
   PURE_CKPT_DIR = '/tmp/orbax_upgrade/pure'
   flax.config.update('flax_use_orbax_checkpointing', False)
 
   checkpoints.save_checkpoint(PURE_CKPT_DIR, CKPT_PYTREE, step=0, overwrite=True)
   checkpoints.restore_checkpoint(PURE_CKPT_DIR, target=TARGET_PYTREE)
@@ -145,16 +145,16 @@
 *********************************************
 
 If you need to restore your checkpoints without a target pytree, pass ``item=None`` to ``orbax.checkpoint.Checkpointer`` or ``items=None`` to ``orbax.CheckpointManager``'s ``.restore()`` method, which should trigger the restoration.
 
 For example:
 
 .. codediff::
-  :title_left: flax.checkpoints
-  :title_right: orbax.checkpoint
+  :title: flax.checkpoints, orbax.checkpoint
+  :skip_test: flax.checkpoints
   :sync:
 
   NOTARGET_CKPT_DIR = '/tmp/orbax_upgrade/no_target'
   flax.config.update('flax_use_orbax_checkpointing', False)
 
   checkpoints.save_checkpoint(NOTARGET_CKPT_DIR, CKPT_PYTREE, step=0)
   checkpoints.restore_checkpoint(NOTARGET_CKPT_DIR, target=None)
@@ -184,16 +184,16 @@
 ******************************************
 
 The ``orbax.checkpoint.PyTreeCheckpointHandler`` class, as the name suggests, can only be used for pytrees. Therefore, if you need to save/restore a single pytree leaf (for example, an array), use ``orbax.checkpoint.ArrayCheckpointHandler`` instead.
 
 For example:
 
 .. codediff::
-  :title_left: flax.checkpoints
-  :title_right: orbax.checkpoint
+  :title: flax.checkpoints, orbax.checkpoint
+  :skip_test: flax.checkpoints
   :sync:
 
   ARR_CKPT_DIR = '/tmp/orbax_upgrade/singleton'
   flax.config.update('flax_use_orbax_checkpointing', False)
 
   checkpoints.save_checkpoint(ARR_CKPT_DIR, jnp.arange(10), step=0)
   checkpoints.restore_checkpoint(ARR_CKPT_DIR, target=None)
```

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/regular_dict_upgrade_guide.rst`

 * *Files 12% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 * :meth:`pretty_repr <flax.core.frozen_dict.FrozenDict.pretty_repr>`
 * :meth:`unfreeze <flax.core.frozen_dict.FrozenDict.unfreeze>`
 
 To accommodate the regular dict change, replace usage of ``FrozenDict`` methods with their utility function equivalent from ``flax.core.frozen_dict``.
 These utility functions mimic the behavior of their corresponding ``FrozenDict`` method, and can be called on either ``FrozenDicts`` or regular dicts.
 The following are the utility functions and example upgrade patterns:
 
-.. testsetup::
+.. testsetup:: default, Only ``FrozenDict``, Both ``FrozenDict`` and regular dict
 
   import flax
   import flax.linen as nn
   import jax
   import jax.numpy as jnp
 
   x = jnp.empty((1,3))
@@ -36,58 +36,54 @@
 
   other_variables = jnp.array([1, 1, 1, 1, 1], dtype=jnp.float32)
 
 :meth:`copy <flax.core.frozen_dict.copy>`
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. codediff::
-  :title_left: Only ``FrozenDict``
-  :title_right: Both ``FrozenDict`` and regular dict
+  :title: Only ``FrozenDict``, Both ``FrozenDict`` and regular dict
   :sync:
 
   variables = variables.copy(add_or_replace={'other_variables': other_variables})
 
   ---
 
   variables = flax.core.copy(variables, add_or_replace={'other_variables': other_variables})
 
 :meth:`pop <flax.core.frozen_dict.pop>`
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. codediff::
-  :title_left: Only ``FrozenDict``
-  :title_right: Both ``FrozenDict`` and regular dict
+  :title: Only ``FrozenDict``, Both ``FrozenDict`` and regular dict
   :sync:
 
   state, params = variables.pop('params')
 
   ---
 
   state, params = flax.core.pop(variables, 'params')
 
 :meth:`pretty_repr <flax.core.frozen_dict.pretty_repr>`
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. codediff::
-  :title_left: Only ``FrozenDict``
-  :title_right: Both ``FrozenDict`` and regular dict
+  :title: Only ``FrozenDict``, Both ``FrozenDict`` and regular dict
   :sync:
 
   str_repr = variables.pretty_repr()
 
   ---
 
   str_repr = flax.core.pretty_repr(variables)
 
 :meth:`unfreeze <flax.core.frozen_dict.unfreeze>`
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 
 .. codediff::
-  :title_left: Only ``FrozenDict``
-  :title_right: Both ``FrozenDict`` and regular dict
+  :title: Only ``FrozenDict``, Both ``FrozenDict`` and regular dict
   :sync:
 
   variables = variables.unfreeze()
 
   ---
 
   variables = flax.core.unfreeze(variables)
```

### Comparing `flax-0.8.3/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst` & `flax-0.8.4/docs/guides/converting_and_upgrading/rnncell_upgrade_guide.rst`

 * *Files 7% similar despite different names*

```diff
@@ -7,66 +7,66 @@
 - All necessary metadata is now stored directly within the cell instance, providing a streamlined method signature.
 
 This guide will walk you through these changes, demonstrating how to update your existing code to align with these enhancements.
 
 Basic Usage
 -----------
 
-.. testsetup::
+.. testsetup:: New
 
   import flax.linen as nn
   import jax.numpy as jnp
   import jax
   import functools
 
 Let's begin by defining some variables and a sample input that represents
 a batch of sequences:
 
-.. testcode::
+.. testcode:: New
 
   batch_size = 32
   seq_len = 10
   in_features = 64
   out_features = 128
 
   x = jnp.ones((batch_size, seq_len, in_features))
 
 First and foremost, it's important to note that all metadata, including the number of features,
 carry initializer, and so on, is now stored within the cell instance:
 
 .. codediff::
-  :title_left: Legacy
-  :title_right: New
+  :title: Legacy, New
+  :skip_test: Legacy
   :sync:
 
   cell = nn.LSTMCell()
 
   ---
 
   cell = nn.LSTMCell(features=out_features)
 
 A significant change is that ``initialize_carry`` has been transitioned into an instance method. Given that
 the cell instance now contains all metadata, the ``initialize_carry`` method's
 signature only requires a PRNG key and a sample input:
 
 .. codediff::
-  :title_left: Legacy
-  :title_right: New
+  :title: Legacy, New
+  :skip_test: Legacy
   :sync:
 
   carry = nn.LSTMCell.initialize_carry(jax.random.key(0), (batch_size,), out_features)
 
   ---
 
   carry = cell.initialize_carry(jax.random.key(0), x[:, 0].shape)
 
 Here, ``x[:, 0].shape`` represents the input for the cell (without the time dimension).
 You can also just create the input shape directly when its more convenient:
 
-.. testcode::
+.. testcode:: New
 
   carry = cell.initialize_carry(jax.random.key(0), (batch_size, in_features))
 
 
 Upgrade Patterns
 -----------------
 
@@ -76,16 +76,16 @@
 First, we will show how to upgrade a ``Module`` that wraps
 a cell, applies the scan logic during ``__call__``, and
 has a static ``initialize_carry`` method. Here, we will try
 to make the minimal amount of changes to the code to get
 it working, albeit not in the most idiomatic way:
 
 .. codediff::
-  :title_left: Legacy
-  :title_right: New
+  :title: Legacy, New
+  :skip_test: Legacy
   :sync:
 
   class SimpleLSTM(nn.Module):
 
     @functools.partial(
       nn.transforms.scan,
       variable_broadcast='params',
@@ -125,16 +125,16 @@
 side effects.
 
 Next, we will show a more idiomatic way of writing a similar LSTM module. The main change
 here will be that we will add a ``features`` attribute to the module and use it to initialize
 a ``nn.scan``-ed version of the cell in the ``setup`` method:
 
 .. codediff::
-  :title_left: Legacy
-  :title_right: New
+  :title: Legacy, New
+  :skip_test: Legacy
   :sync:
 
   class SimpleLSTM(nn.Module):
 
     @functools.partial(
       nn.transforms.scan,
       variable_broadcast='params',
```

### Comparing `flax-0.8.3/docs/guides/data_preprocessing/full_eval.rst` & `flax-0.8.4/docs/guides/data_preprocessing/full_eval.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.ipynb` & `flax-0.8.4/docs/guides/data_preprocessing/loading_datasets.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/data_preprocessing/loading_datasets.md` & `flax-0.8.4/docs/guides/data_preprocessing/loading_datasets.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/arguments.md` & `flax-0.8.4/docs/guides/flax_fundamentals/arguments.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.ipynb` & `flax-0.8.4/docs/guides/flax_fundamentals/flax_basics.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9997577519379846%*

 * *Differences: {"'cells'": '{42: {\'source\': {insert: [(2, "JAX released an experimental converter called '*

 * *            '[jax2tf](https://github.com/google/jax/tree/main/jax/jax2tf), which allows converting '*

 * *            "trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF "*

 * *            'Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), '*

 * *            '[TF.js](https://www.tensorflow.org/js), or other downstream applications). The '*

 * *            'repository cont []*

```diff
@@ -947,15 +947,15 @@
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### Exporting to Tensorflow's SavedModel with jax2tf\n",
                 "\n",
-                "JAX released an experimental converter called [jax2tf](https://github.com/google/jax/tree/main/jax/experimental/jax2tf), which allows converting trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), [TF.js](https://www.tensorflow.org/js), or other downstream applications). The repository contains more documentation and has various examples for Flax."
+                "JAX released an experimental converter called [jax2tf](https://github.com/google/jax/tree/main/jax/jax2tf), which allows converting trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), [TF.js](https://www.tensorflow.org/js), or other downstream applications). The repository contains more documentation and has various examples for Flax."
             ]
         }
     ],
     "metadata": {
         "jupytext": {
             "formats": "ipynb,md:myst"
         },
```

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/flax_basics.md` & `flax-0.8.4/docs/guides/flax_fundamentals/flax_basics.md`

 * *Files 1% similar despite different names*

```diff
@@ -465,8 +465,8 @@
 
 Flax provides a handy wrapper - `TrainState` - that simplifies the above code. Check out [`flax.training.train_state.TrainState`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) to learn more.
 
 +++
 
 ### Exporting to Tensorflow's SavedModel with jax2tf
 
-JAX released an experimental converter called [jax2tf](https://github.com/google/jax/tree/main/jax/experimental/jax2tf), which allows converting trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), [TF.js](https://www.tensorflow.org/js), or other downstream applications). The repository contains more documentation and has various examples for Flax.
+JAX released an experimental converter called [jax2tf](https://github.com/google/jax/tree/main/jax/jax2tf), which allows converting trained Flax models into Tensorflow's SavedModel format (so it can be used for [TF Hub](https://www.tensorflow.org/hub), [TF.lite](https://www.tensorflow.org/lite), [TF.js](https://www.tensorflow.org/js), or other downstream applications). The repository contains more documentation and has various examples for Flax.
```

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.ipynb` & `flax-0.8.4/docs/guides/flax_fundamentals/rng_guide.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/rng_guide.md` & `flax-0.8.4/docs/guides/flax_fundamentals/rng_guide.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/setup_or_nncompact.rst` & `flax-0.8.4/docs/guides/flax_fundamentals/setup_or_nncompact.rst`

 * *Files 5% similar despite different names*

```diff
@@ -60,137 +60,138 @@
 000003b0: 6620 466c 6178 2a2a 2e0a 0a48 6572 6520  f Flax**...Here 
 000003c0: 6973 2061 2073 686f 7274 2065 7861 6d70  is a short examp
 000003d0: 6c65 206f 6620 6120 6d6f 6475 6c65 2064  le of a module d
 000003e0: 6566 696e 6564 2069 6e20 626f 7468 2077  efined in both w
 000003f0: 6179 732c 2077 6974 6820 6578 6163 746c  ays, with exactl
 00000400: 790a 7468 6520 7361 6d65 2066 756e 6374  y.the same funct
 00000410: 696f 6e61 6c69 7479 2e0a 0a2e 2e20 7465  ionality..... te
-00000420: 7374 7365 7475 703a 3a0a 0a20 2069 6d70  stsetup::..  imp
-00000430: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
-00000440: 7320 6e6e 0a0a 2e2e 2063 6f64 6564 6966  s nn.... codedif
-00000450: 663a 3a0a 2020 3a74 6974 6c65 5f6c 6566  f::.  :title_lef
-00000460: 743a 2055 7369 6e67 2060 6073 6574 7570  t: Using ``setup
-00000470: 6060 0a20 203a 7469 746c 655f 7269 6768  ``.  :title_righ
-00000480: 743a 2055 7369 6e67 2060 606e 6e2e 636f  t: Using ``nn.co
-00000490: 6d70 6163 7460 600a 0a20 2063 6c61 7373  mpact``..  class
-000004a0: 204d 4c50 286e 6e2e 4d6f 6475 6c65 293a   MLP(nn.Module):
-000004b0: 0a20 2020 2064 6566 2073 6574 7570 2873  .    def setup(s
-000004c0: 656c 6629 3a0a 2020 2020 2020 2320 5375  elf):.      # Su
-000004d0: 626d 6f64 756c 6520 6e61 6d65 7320 6172  bmodule names ar
-000004e0: 6520 6465 7269 7665 6420 6279 2074 6865  e derived by the
-000004f0: 2061 7474 7269 6275 7465 7320 796f 7520   attributes you 
-00000500: 6173 7369 676e 2074 6f2e 2049 6e20 7468  assign to. In th
-00000510: 6973 0a20 2020 2020 2023 2063 6173 652c  is.      # case,
-00000520: 2022 6465 6e73 6531 2220 616e 6420 2264   "dense1" and "d
-00000530: 656e 7365 3222 2e20 5468 6973 2066 6f6c  ense2". This fol
-00000540: 6c6f 7773 2074 6865 206c 6f67 6963 2069  lows the logic i
-00000550: 6e20 5079 546f 7263 682e 0a20 2020 2020  n PyTorch..     
-00000560: 2073 656c 662e 6465 6e73 6531 203d 206e   self.dense1 = n
-00000570: 6e2e 4465 6e73 6528 3332 290a 2020 2020  n.Dense(32).    
-00000580: 2020 7365 6c66 2e64 656e 7365 3220 3d20    self.dense2 = 
-00000590: 6e6e 2e44 656e 7365 2833 3229 0a0a 2020  nn.Dense(32)..  
-000005a0: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-000005b0: 656c 662c 2078 293a 0a20 2020 2020 2078  elf, x):.      x
-000005c0: 203d 2073 656c 662e 6465 6e73 6531 2878   = self.dense1(x
-000005d0: 290a 2020 2020 2020 7820 3d20 6e6e 2e72  ).      x = nn.r
-000005e0: 656c 7528 7829 0a20 2020 2020 2078 203d  elu(x).      x =
-000005f0: 2073 656c 662e 6465 6e73 6532 2878 290a   self.dense2(x).
-00000600: 2020 2020 2020 7265 7475 726e 2078 0a20        return x. 
-00000610: 202d 2d2d 0a20 2063 6c61 7373 204d 4c50   ---.  class MLP
-00000620: 286e 6e2e 4d6f 6475 6c65 293a 0a0a 2020  (nn.Module):..  
-00000630: 2020 406e 6e2e 636f 6d70 6163 7420 2321    @nn.compact #!
-00000640: 0a20 2020 2064 6566 205f 5f63 616c 6c5f  .    def __call_
-00000650: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
-00000660: 2020 7820 3d20 6e6e 2e44 656e 7365 2833    x = nn.Dense(3
-00000670: 322c 206e 616d 653d 2264 656e 7365 3122  2, name="dense1"
-00000680: 2928 7829 2023 210a 2020 2020 2020 7820  )(x) #!.      x 
-00000690: 3d20 6e6e 2e72 656c 7528 7829 0a20 2020  = nn.relu(x).   
-000006a0: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
-000006b0: 3332 2c20 6e61 6d65 3d22 6465 6e73 6532  32, name="dense2
-000006c0: 2229 2878 2920 2321 0a20 2020 2020 2072  ")(x) #!.      r
-000006d0: 6574 7572 6e20 780a 0a53 6f2c 2068 6f77  eturn x..So, how
-000006e0: 2077 6f75 6c64 2079 6f75 2064 6563 6964   would you decid
-000006f0: 6520 7768 6963 6820 7374 796c 6520 746f  e which style to
-00000700: 2075 7365 3f20 4974 2063 616e 2062 6520   use? It can be 
-00000710: 6120 6d61 7474 6572 206f 6620 7461 7374  a matter of tast
-00000720: 652c 2062 7574 2068 6572 6520 6172 6520  e, but here are 
-00000730: 736f 6d65 2070 726f 7320 616e 6420 636f  some pros and co
-00000740: 6e73 3a0a 0a52 6561 736f 6e73 2074 6f20  ns:..Reasons to 
-00000750: 7072 6566 6572 2075 7369 6e67 2060 606e  prefer using ``n
-00000760: 6e2e 636f 6d70 6163 7460 603a 0a5e 5e5e  n.compact``:.^^^
-00000770: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
+00000420: 7374 7365 7475 703a 3a20 5573 696e 6720  stsetup:: Using 
+00000430: 6060 7365 7475 7060 602c 2055 7369 6e67  ``setup``, Using
+00000440: 2060 606e 6e2e 636f 6d70 6163 7460 600a   ``nn.compact``.
+00000450: 0a20 2069 6d70 6f72 7420 666c 6178 2e6c  .  import flax.l
+00000460: 696e 656e 2061 7320 6e6e 0a0a 2e2e 2063  inen as nn.... c
+00000470: 6f64 6564 6966 663a 3a0a 2020 3a74 6974  odediff::.  :tit
+00000480: 6c65 3a20 5573 696e 6720 6060 7365 7475  le: Using ``setu
+00000490: 7060 602c 2055 7369 6e67 2060 606e 6e2e  p``, Using ``nn.
+000004a0: 636f 6d70 6163 7460 600a 0a20 2063 6c61  compact``..  cla
+000004b0: 7373 204d 4c50 286e 6e2e 4d6f 6475 6c65  ss MLP(nn.Module
+000004c0: 293a 0a20 2020 2064 6566 2073 6574 7570  ):.    def setup
+000004d0: 2873 656c 6629 3a0a 2020 2020 2020 2320  (self):.      # 
+000004e0: 5375 626d 6f64 756c 6520 6e61 6d65 7320  Submodule names 
+000004f0: 6172 6520 6465 7269 7665 6420 6279 2074  are derived by t
+00000500: 6865 2061 7474 7269 6275 7465 7320 796f  he attributes yo
+00000510: 7520 6173 7369 676e 2074 6f2e 2049 6e20  u assign to. In 
+00000520: 7468 6973 0a20 2020 2020 2023 2063 6173  this.      # cas
+00000530: 652c 2022 6465 6e73 6531 2220 616e 6420  e, "dense1" and 
+00000540: 2264 656e 7365 3222 2e20 5468 6973 2066  "dense2". This f
+00000550: 6f6c 6c6f 7773 2074 6865 206c 6f67 6963  ollows the logic
+00000560: 2069 6e20 5079 546f 7263 682e 0a20 2020   in PyTorch..   
+00000570: 2020 2073 656c 662e 6465 6e73 6531 203d     self.dense1 =
+00000580: 206e 6e2e 4465 6e73 6528 3332 290a 2020   nn.Dense(32).  
+00000590: 2020 2020 7365 6c66 2e64 656e 7365 3220      self.dense2 
+000005a0: 3d20 6e6e 2e44 656e 7365 2833 3229 0a0a  = nn.Dense(32)..
+000005b0: 2020 2020 6465 6620 5f5f 6361 6c6c 5f5f      def __call__
+000005c0: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
+000005d0: 2078 203d 2073 656c 662e 6465 6e73 6531   x = self.dense1
+000005e0: 2878 290a 2020 2020 2020 7820 3d20 6e6e  (x).      x = nn
+000005f0: 2e72 656c 7528 7829 0a20 2020 2020 2078  .relu(x).      x
+00000600: 203d 2073 656c 662e 6465 6e73 6532 2878   = self.dense2(x
+00000610: 290a 2020 2020 2020 7265 7475 726e 2078  ).      return x
+00000620: 0a20 202d 2d2d 0a20 2063 6c61 7373 204d  .  ---.  class M
+00000630: 4c50 286e 6e2e 4d6f 6475 6c65 293a 0a0a  LP(nn.Module):..
+00000640: 2020 2020 406e 6e2e 636f 6d70 6163 7420      @nn.compact 
+00000650: 2321 0a20 2020 2064 6566 205f 5f63 616c  #!.    def __cal
+00000660: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
+00000670: 2020 2020 7820 3d20 6e6e 2e44 656e 7365      x = nn.Dense
+00000680: 2833 322c 206e 616d 653d 2264 656e 7365  (32, name="dense
+00000690: 3122 2928 7829 2023 210a 2020 2020 2020  1")(x) #!.      
+000006a0: 7820 3d20 6e6e 2e72 656c 7528 7829 0a20  x = nn.relu(x). 
+000006b0: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
+000006c0: 6528 3332 2c20 6e61 6d65 3d22 6465 6e73  e(32, name="dens
+000006d0: 6532 2229 2878 2920 2321 0a20 2020 2020  e2")(x) #!.     
+000006e0: 2072 6574 7572 6e20 780a 0a53 6f2c 2068   return x..So, h
+000006f0: 6f77 2077 6f75 6c64 2079 6f75 2064 6563  ow would you dec
+00000700: 6964 6520 7768 6963 6820 7374 796c 6520  ide which style 
+00000710: 746f 2075 7365 3f20 4974 2063 616e 2062  to use? It can b
+00000720: 6520 6120 6d61 7474 6572 206f 6620 7461  e a matter of ta
+00000730: 7374 652c 2062 7574 2068 6572 6520 6172  ste, but here ar
+00000740: 6520 736f 6d65 2070 726f 7320 616e 6420  e some pros and 
+00000750: 636f 6e73 3a0a 0a52 6561 736f 6e73 2074  cons:..Reasons t
+00000760: 6f20 7072 6566 6572 2075 7369 6e67 2060  o prefer using `
+00000770: 606e 6e2e 636f 6d70 6163 7460 603a 0a5e  `nn.compact``:.^
 00000780: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
-00000790: 5e5e 5e5e 0a0a 312e 2041 6c6c 6f77 7320  ^^^^..1. Allows 
-000007a0: 6465 6669 6e69 6e67 2073 7562 6d6f 6475  defining submodu
-000007b0: 6c65 732c 2070 6172 616d 6574 6572 7320  les, parameters 
-000007c0: 616e 6420 6f74 6865 7220 7661 7269 6162  and other variab
-000007d0: 6c65 7320 6e65 7874 2074 6f20 7768 6572  les next to wher
-000007e0: 6520 7468 6579 2061 7265 2075 7365 643a  e they are used:
-000007f0: 206c 6573 730a 2020 2073 6372 6f6c 6c69   less.   scrolli
-00000800: 6e67 2075 702f 646f 776e 2074 6f20 7365  ng up/down to se
-00000810: 6520 686f 7720 6576 6572 7974 6869 6e67  e how everything
-00000820: 2069 7320 6465 6669 6e65 642e 0a32 2e20   is defined..2. 
-00000830: 5265 6475 6365 7320 636f 6465 2064 7570  Reduces code dup
-00000840: 6c69 6361 7469 6f6e 2077 6865 6e20 7468  lication when th
-00000850: 6572 6520 6172 6520 636f 6e64 6974 696f  ere are conditio
-00000860: 6e61 6c73 206f 7220 666f 7220 6c6f 6f70  nals or for loop
-00000870: 7320 7468 6174 2063 6f6e 6469 7469 6f6e  s that condition
-00000880: 616c 6c79 2064 6566 696e 650a 2020 2073  ally define.   s
-00000890: 7562 6d6f 6475 6c65 732c 2070 6172 616d  ubmodules, param
-000008a0: 6574 6572 7320 6f72 2076 6172 6961 626c  eters or variabl
-000008b0: 6573 2e0a 332e 2043 6f64 6520 7479 7069  es..3. Code typi
-000008c0: 6361 6c6c 7920 6c6f 6f6b 7320 6d6f 7265  cally looks more
-000008d0: 206c 696b 6520 6d61 7468 656d 6174 6963   like mathematic
-000008e0: 616c 206e 6f74 6174 696f 6e3a 2060 6079  al notation: ``y
-000008f0: 203d 2073 656c 662e 7061 7261 6d28 2757   = self.param('W
-00000900: 272c 202e 2e2e 2920 4020 7820 2b20 7365  ', ...) @ x + se
-00000910: 6c66 2e70 6172 616d 2827 6227 2c20 2e2e  lf.param('b', ..
-00000920: 2e29 6060 0a20 2020 6c6f 6f6b 7320 7369  .)``.   looks si
-00000930: 6d69 6c61 7220 746f 203a 6d61 7468 3a60  milar to :math:`
-00000940: 793d 5778 2b62 6060 290a 342e 2049 6620  y=Wx+b``).4. If 
-00000950: 796f 7520 6172 6520 7573 696e 6720 7368  you are using sh
-00000960: 6170 6520 696e 6665 7265 6e63 652c 2069  ape inference, i
-00000970: 2e65 2e20 7573 696e 6720 7061 7261 6d65  .e. using parame
-00000980: 7465 7273 2077 686f 7365 2073 6861 7065  ters whose shape
-00000990: 2f76 616c 7565 2064 6570 656e 6420 6f6e  /value depend on
-000009a0: 2073 6861 7065 7320 6f66 0a20 2020 7468   shapes of.   th
-000009b0: 6520 696e 7075 7473 2028 7768 6963 6820  e inputs (which 
-000009c0: 6172 6520 756e 6b6e 6f77 6e20 6174 2069  are unknown at i
-000009d0: 6e69 7469 616c 697a 6174 696f 6e29 2c20  nitialization), 
-000009e0: 7468 6973 2069 7320 6e6f 7420 706f 7373  this is not poss
-000009f0: 6962 6c65 2075 7369 6e67 2060 6073 6574  ible using ``set
-00000a00: 7570 6060 2e0a 0a52 6561 736f 6e73 2074  up``...Reasons t
-00000a10: 6f20 7072 6566 6572 2075 7369 6e67 2060  o prefer using `
-00000a20: 6073 6574 7570 6060 3a0a 5e5e 5e5e 5e5e  `setup``:.^^^^^^
-00000a30: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
-00000a40: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 0a0a 312e  ^^^^^^^^^^^^..1.
-00000a50: 2043 6c6f 7365 7220 746f 2074 6865 2050   Closer to the P
-00000a60: 7954 6f72 6368 2063 6f6e 7665 6e74 696f  yTorch conventio
-00000a70: 6e2c 2074 6875 7320 6561 7369 6572 2077  n, thus easier w
-00000a80: 6865 6e20 706f 7274 696e 6720 6d6f 6465  hen porting mode
-00000a90: 6c73 0a20 2020 6672 6f6d 2050 7954 6f72  ls.   from PyTor
-00000aa0: 6368 0a32 2e20 536f 6d65 2070 656f 706c  ch.2. Some peopl
-00000ab0: 6520 6669 6e64 2069 7420 6d6f 7265 206e  e find it more n
-00000ac0: 6174 7572 616c 2074 6f20 6578 706c 6963  atural to explic
-00000ad0: 6974 6c79 2073 6570 6172 6174 6520 7468  itly separate th
-00000ae0: 6520 6465 6669 6e69 7469 6f6e 0a20 2020  e definition.   
-00000af0: 6f66 2073 7562 6d6f 6475 6c65 7320 616e  of submodules an
-00000b00: 6420 7661 7269 6162 6c65 7320 6672 6f6d  d variables from
-00000b10: 2077 6865 7265 2074 6865 7920 6172 6520   where they are 
-00000b20: 7573 6564 0a33 2e20 416c 6c6f 7773 2064  used.3. Allows d
-00000b30: 6566 696e 696e 6720 6d6f 7265 2074 6861  efining more tha
-00000b40: 6e20 6f6e 6520 2266 6f72 7761 7264 2070  n one "forward p
-00000b50: 6173 7322 206d 6574 686f 640a 2020 2028  ass" method.   (
-00000b60: 7365 6520 3a63 6c61 7373 3a60 4d75 6c74  see :class:`Mult
-00000b70: 6970 6c65 4d65 7468 6f64 7343 6f6d 7061  ipleMethodsCompa
-00000b80: 6374 4572 726f 7220 3c66 6c61 782e 6572  ctError <flax.er
-00000b90: 726f 7273 2e4d 756c 7469 706c 654d 6574  rors.MultipleMet
-00000ba0: 686f 6473 436f 6d70 6163 7445 7272 6f72  hodsCompactError
-00000bb0: 3e60 290a 0a0a 0a0a 2e2e 205f 604c 696e  >`)....... _`Lin
-00000bc0: 656e 603a 2068 7474 7073 3a2f 2f6a 6178  en`: https://jax
-00000bd0: 2e72 6561 6474 6865 646f 6373 2e69 6f2f  .readthedocs.io/
-00000be0: 656e 2f6c 6174 6573 742f 6e6f 7465 626f  en/latest/notebo
-00000bf0: 6f6b 732f 7468 696e 6b69 6e67 5f69 6e5f  oks/thinking_in_
-00000c00: 6a61 782e 6874 6d6c 234a 4954 2d6d 6563  jax.html#JIT-mec
-00000c10: 6861 6e69 6373 3a2d 7472 6163 696e 672d  hanics:-tracing-
-00000c20: 616e 642d 7374 6174 6963 2d76 6172 6961  and-static-varia
-00000c30: 626c 6573 0a                             bles.
+00000790: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
+000007a0: 5e5e 5e5e 5e5e 0a0a 312e 2041 6c6c 6f77  ^^^^^^..1. Allow
+000007b0: 7320 6465 6669 6e69 6e67 2073 7562 6d6f  s defining submo
+000007c0: 6475 6c65 732c 2070 6172 616d 6574 6572  dules, parameter
+000007d0: 7320 616e 6420 6f74 6865 7220 7661 7269  s and other vari
+000007e0: 6162 6c65 7320 6e65 7874 2074 6f20 7768  ables next to wh
+000007f0: 6572 6520 7468 6579 2061 7265 2075 7365  ere they are use
+00000800: 643a 206c 6573 730a 2020 2073 6372 6f6c  d: less.   scrol
+00000810: 6c69 6e67 2075 702f 646f 776e 2074 6f20  ling up/down to 
+00000820: 7365 6520 686f 7720 6576 6572 7974 6869  see how everythi
+00000830: 6e67 2069 7320 6465 6669 6e65 642e 0a32  ng is defined..2
+00000840: 2e20 5265 6475 6365 7320 636f 6465 2064  . Reduces code d
+00000850: 7570 6c69 6361 7469 6f6e 2077 6865 6e20  uplication when 
+00000860: 7468 6572 6520 6172 6520 636f 6e64 6974  there are condit
+00000870: 696f 6e61 6c73 206f 7220 666f 7220 6c6f  ionals or for lo
+00000880: 6f70 7320 7468 6174 2063 6f6e 6469 7469  ops that conditi
+00000890: 6f6e 616c 6c79 2064 6566 696e 650a 2020  onally define.  
+000008a0: 2073 7562 6d6f 6475 6c65 732c 2070 6172   submodules, par
+000008b0: 616d 6574 6572 7320 6f72 2076 6172 6961  ameters or varia
+000008c0: 626c 6573 2e0a 332e 2043 6f64 6520 7479  bles..3. Code ty
+000008d0: 7069 6361 6c6c 7920 6c6f 6f6b 7320 6d6f  pically looks mo
+000008e0: 7265 206c 696b 6520 6d61 7468 656d 6174  re like mathemat
+000008f0: 6963 616c 206e 6f74 6174 696f 6e3a 2060  ical notation: `
+00000900: 6079 203d 2073 656c 662e 7061 7261 6d28  `y = self.param(
+00000910: 2757 272c 202e 2e2e 2920 4020 7820 2b20  'W', ...) @ x + 
+00000920: 7365 6c66 2e70 6172 616d 2827 6227 2c20  self.param('b', 
+00000930: 2e2e 2e29 6060 0a20 2020 6c6f 6f6b 7320  ...)``.   looks 
+00000940: 7369 6d69 6c61 7220 746f 203a 6d61 7468  similar to :math
+00000950: 3a60 793d 5778 2b62 6060 290a 342e 2049  :`y=Wx+b``).4. I
+00000960: 6620 796f 7520 6172 6520 7573 696e 6720  f you are using 
+00000970: 7368 6170 6520 696e 6665 7265 6e63 652c  shape inference,
+00000980: 2069 2e65 2e20 7573 696e 6720 7061 7261   i.e. using para
+00000990: 6d65 7465 7273 2077 686f 7365 2073 6861  meters whose sha
+000009a0: 7065 2f76 616c 7565 2064 6570 656e 6420  pe/value depend 
+000009b0: 6f6e 2073 6861 7065 7320 6f66 0a20 2020  on shapes of.   
+000009c0: 7468 6520 696e 7075 7473 2028 7768 6963  the inputs (whic
+000009d0: 6820 6172 6520 756e 6b6e 6f77 6e20 6174  h are unknown at
+000009e0: 2069 6e69 7469 616c 697a 6174 696f 6e29   initialization)
+000009f0: 2c20 7468 6973 2069 7320 6e6f 7420 706f  , this is not po
+00000a00: 7373 6962 6c65 2075 7369 6e67 2060 6073  ssible using ``s
+00000a10: 6574 7570 6060 2e0a 0a52 6561 736f 6e73  etup``...Reasons
+00000a20: 2074 6f20 7072 6566 6572 2075 7369 6e67   to prefer using
+00000a30: 2060 6073 6574 7570 6060 3a0a 5e5e 5e5e   ``setup``:.^^^^
+00000a40: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  ^^^^^^^^^^^^^^^^
+00000a50: 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 0a0a  ^^^^^^^^^^^^^^..
+00000a60: 312e 2043 6c6f 7365 7220 746f 2074 6865  1. Closer to the
+00000a70: 2050 7954 6f72 6368 2063 6f6e 7665 6e74   PyTorch convent
+00000a80: 696f 6e2c 2074 6875 7320 6561 7369 6572  ion, thus easier
+00000a90: 2077 6865 6e20 706f 7274 696e 6720 6d6f   when porting mo
+00000aa0: 6465 6c73 0a20 2020 6672 6f6d 2050 7954  dels.   from PyT
+00000ab0: 6f72 6368 0a32 2e20 536f 6d65 2070 656f  orch.2. Some peo
+00000ac0: 706c 6520 6669 6e64 2069 7420 6d6f 7265  ple find it more
+00000ad0: 206e 6174 7572 616c 2074 6f20 6578 706c   natural to expl
+00000ae0: 6963 6974 6c79 2073 6570 6172 6174 6520  icitly separate 
+00000af0: 7468 6520 6465 6669 6e69 7469 6f6e 0a20  the definition. 
+00000b00: 2020 6f66 2073 7562 6d6f 6475 6c65 7320    of submodules 
+00000b10: 616e 6420 7661 7269 6162 6c65 7320 6672  and variables fr
+00000b20: 6f6d 2077 6865 7265 2074 6865 7920 6172  om where they ar
+00000b30: 6520 7573 6564 0a33 2e20 416c 6c6f 7773  e used.3. Allows
+00000b40: 2064 6566 696e 696e 6720 6d6f 7265 2074   defining more t
+00000b50: 6861 6e20 6f6e 6520 2266 6f72 7761 7264  han one "forward
+00000b60: 2070 6173 7322 206d 6574 686f 640a 2020   pass" method.  
+00000b70: 2028 7365 6520 3a63 6c61 7373 3a60 4d75   (see :class:`Mu
+00000b80: 6c74 6970 6c65 4d65 7468 6f64 7343 6f6d  ltipleMethodsCom
+00000b90: 7061 6374 4572 726f 7220 3c66 6c61 782e  pactError <flax.
+00000ba0: 6572 726f 7273 2e4d 756c 7469 706c 654d  errors.MultipleM
+00000bb0: 6574 686f 6473 436f 6d70 6163 7445 7272  ethodsCompactErr
+00000bc0: 6f72 3e60 290a 0a0a 0a0a 2e2e 205f 604c  or>`)....... _`L
+00000bd0: 696e 656e 603a 2068 7474 7073 3a2f 2f6a  inen`: https://j
+00000be0: 6178 2e72 6561 6474 6865 646f 6373 2e69  ax.readthedocs.i
+00000bf0: 6f2f 656e 2f6c 6174 6573 742f 6e6f 7465  o/en/latest/note
+00000c00: 626f 6f6b 732f 7468 696e 6b69 6e67 5f69  books/thinking_i
+00000c10: 6e5f 6a61 782e 6874 6d6c 234a 4954 2d6d  n_jax.html#JIT-m
+00000c20: 6563 6861 6e69 6373 3a2d 7472 6163 696e  echanics:-tracin
+00000c30: 672d 616e 642d 7374 6174 6963 2d76 6172  g-and-static-var
+00000c40: 6961 626c 6573 0a                        iables.
```

### Comparing `flax-0.8.3/docs/guides/flax_fundamentals/state_params.rst` & `flax-0.8.4/docs/guides/flax_fundamentals/state_params.rst`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_sharp_bits.ipynb` & `flax-0.8.4/docs/guides/flax_sharp_bits.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/flax_sharp_bits.md` & `flax-0.8.4/docs/guides/flax_sharp_bits.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/model_inspection/extracting_intermediates.rst` & `flax-0.8.4/docs/guides/model_inspection/extracting_intermediates.rst`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 Extracting intermediate values
 ==============================
 
 This guide will show you how to extract intermediate values from a module.
 Let's start with this simple CNN that uses :code:`nn.compact`.
 
-.. testsetup::
+.. testsetup:: default, sow
 
   import flax
   import flax.linen as nn
   import jax
   import jax.numpy as jnp
   from flax.core import FrozenDict
   from typing import Sequence
@@ -46,16 +46,16 @@
 Store intermediate values in a new variable collection
 ------------------------------------------------------
 
 The CNN can be augmented with calls to ``sow`` to store intermediates as following:
 
 
 .. codediff::
-  :title_left: Default CNN
-  :title_right: CNN using sow API
+  :title: Default CNN, CNN using sow API
+  :groups: default, sow
 
   class CNN(nn.Module):
     @nn.compact
     def __call__(self, x):
       x = nn.Conv(features=32, kernel_size=(3, 3))(x)
       x = nn.relu(x)
       x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
@@ -97,15 +97,15 @@
   times in its parent module, and we want to catch all the sowed values.
 * Therefore you want to make sure that you **do not** feed intermediate values back
   into ``variables``. Otherwise every call will increase the length of that tuple
   and trigger a recompile.
 * To override the default append behavior, specify ``init_fn`` and ``reduce_fn``
   - see :meth:`Module.sow() <flax.linen.Module.sow>`.
 
-.. testcode::
+.. testcode:: sow
 
   class SowCNN2(nn.Module):
     @nn.compact
     def __call__(self, x):
       mod = SowCNN(name='SowCNN')
       return mod(x) + mod(x)  # Calling same module instance twice.
 
@@ -225,16 +225,16 @@
   y, state = CNN().apply(variables, batch, capture_intermediates=filter_Dense, mutable=["intermediates"])
   dense_intermediates = state['intermediates']
 
 Note that ``capture_intermediates`` will only apply to layers. You can use ``self.sow`` to manually store
 non-layer intermediates, but the filter function won't be applied to it.
 
 .. codediff::
-  :title_left: Capturing all layer intermediates
-  :title_right: Using filter function and ``self.sow()``
+  :title: Capturing all layer intermediates, Using filter function and ``self.sow()``
+  :groups: default, sow
 
   class Model(nn.Module):
     @nn.compact
     def __call__(self, x):
       a = nn.Dense(4)(x) # Dense_0
       b = nn.Dense(4)(x) # Dense_1
       c = a + b # not a Flax layer, so won't be stored as an intermediate
@@ -281,15 +281,15 @@
   preds, feats = predict(params, batch)
   feats # intermediate c in Model is stored and isn't filtered out by the filter function #!
 
 To separate the intermediates extracted from ``self.sow`` from the intermediates extracted from ``capture_intermediates``,
 we can either define a separate collection like ``self.sow('sow_intermediates', 'c', c)``, or manually filter out
 the intermediates after calling ``.apply()``. For example:
 
-.. testcode::
+.. testcode:: sow
 
   flattened_dict = flax.traverse_util.flatten_dict(feats['intermediates'], sep='/')
   flattened_dict['c']
 
 In terms of efficiency, as long as everything is jitted, then any intermediates you don't end up using
 should be optimized away by XLA.
```

### Comparing `flax-0.8.3/docs/guides/model_inspection/model_surgery.ipynb` & `flax-0.8.4/docs/guides/model_inspection/model_surgery.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/model_inspection/model_surgery.md` & `flax-0.8.4/docs/guides/model_inspection/model_surgery.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/parallel_training/ensembling.rst` & `flax-0.8.4/docs/guides/parallel_training/ensembling.rst`

 * *Files 3% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 * split the random seed to obtain different parameter initialization,
 * replicate the inputs and unreplicate the outputs where necessary,
 * average probabilities across devices to compute the predictions.
 
 In this HOWTO we omit some of the code such as imports, the CNN module, and
 metrics computation, but they can be found in the `MNIST example`_.
 
-.. testsetup::
+.. testsetup:: Single-model, Ensemble
 
   import functools
   from flax import jax_utils
 
   # Copied from examples/mnist/train.py
   from absl import logging
   from flax import linen as nn
@@ -65,16 +65,15 @@
 We start by creating a parallel version of ``create_train_state()``, which
 retrieves the initial parameters of the models. We do this using |jax.pmap()|_.
 The effect of "pmapping" a function is that it will compile the function with
 XLA (similar to |jax.jit()|_), but execute it in parallel on XLA devices (e.g.,
 GPUs/TPUs).
 
 .. codediff::
-  :title_left: Single-model
-  :title_right: Ensemble
+  :title: Single-model, Ensemble
   :sync:
 
   #!
   def create_train_state(rng, learning_rate, momentum):
     cnn = CNN()
     params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
     tx = optax.sgd(learning_rate, momentum)
@@ -104,16 +103,15 @@
 Next we simply do the same for the functions ``apply_model()`` and
 ``update_model()``. To compute the predictions from the ensemble, we take the
 average of the individual probabilities. We use |jax.lax.pmean()|_ to compute
 the average *across devices*. This also requires us to specify the
 ``axis_name`` to both |jax.pmap()|_ and |jax.lax.pmean()|_.
 
 .. codediff::
-  :title_left: Single-model
-  :title_right: Ensemble
+  :title: Single-model, Ensemble
   :sync:
 
   @jax.jit  #!
   def apply_model(state, images, labels):
     def loss_fn(params):
       logits = CNN().apply({'params': params}, images)
       one_hot = jax.nn.one_hot(labels, 10)
@@ -152,16 +150,15 @@
 ---------------------
 
 Next we transform the ``train_epoch()`` function. When calling the pmapped
 functions from above, we mainly need to take care of duplicating the arguments
 for all devices where necessary, and de-duplicating the return values.
 
 .. codediff::
-  :title_left: Single-model
-  :title_right: Ensemble
+  :title: Single-model, Ensemble
   :sync:
 
   def train_epoch(state, train_ds, batch_size, rng):
     train_ds_size = len(train_ds['image'])
     steps_per_epoch = train_ds_size // batch_size
 
     perms = jax.random.permutation(rng, len(train_ds['image']))
@@ -214,16 +211,15 @@
 We can now rewrite the actual training logic. This consists of two simple
 changes: making sure the RNGs are replicated when we pass them to
 ``create_train_state()``, and replicating the test dataset, which is much
 smaller than the train dataset so we can do this for the entire dataset
 directly.
 
 .. codediff::
-  :title_left: Single-model
-  :title_right: Ensemble
+  :title: Single-model, Ensemble
   :sync:
 
   train_ds, test_ds = get_datasets()
   #!
   rng = jax.random.key(0)
 
   rng, init_rng = jax.random.split(rng)
```

### Comparing `flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.ipynb` & `flax-0.8.4/docs/guides/parallel_training/flax_on_pjit.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/parallel_training/flax_on_pjit.md` & `flax-0.8.4/docs/guides/parallel_training/flax_on_pjit.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/training_techniques/batch_norm.rst` & `flax-0.8.4/docs/guides/training_techniques/batch_norm.rst`

 * *Files 6% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 
 Batch normalization is a regularization technique used to speed up training and improve convergence.
 During training, it computes running averages over feature dimensions. This adds a new form
 of non-differentiable state that must be handled appropriately.
 
 Throughout the guide, you will be able to compare code examples with and without Flax ``BatchNorm``.
 
-.. testsetup::
+.. testsetup:: No BatchNorm, With BatchNorm
 
   import flax.linen as nn
   import jax.numpy as jnp
   import jax
   import optax
   from typing import Any
   from flax.core import FrozenDict
@@ -32,16 +32,15 @@
 Note: In other machine learning frameworks, like PyTorch or
 TensorFlow (Keras), this is specified via a mutable state or a call flag (for example, in
 `torch.nn.Module.eval <https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval>`__
 or ``tf.keras.Model`` by setting the
 `training <https://www.tensorflow.org/api_docs/python/tf/keras/Model#call>`__ flag).
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   class MLP(nn.Module):
     @nn.compact
     def __call__(self, x):
       x = nn.Dense(features=4)(x)
 
@@ -71,16 +70,15 @@
 
 Note: You can learn more in the ``flax.linen`` `variables <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`__
 API documentation.
 
 The ``batch_stats`` collection must be extracted from the ``variables`` for later use.
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   mlp = MLP()
   x = jnp.ones((1, 3))
   variables = mlp.init(jax.random.key(0), x)
   params = variables['params']
 
@@ -97,16 +95,15 @@
 
 
 Flax ``BatchNorm`` adds a total of 4 variables: ``mean`` and ``var`` that live in the
 ``batch_stats`` collection, and ``scale`` and ``bias`` that live in the ``params``
 collection.
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   FrozenDict({
     'params': {
       'Dense_0': {
           'bias': (4,),
           'kernel': (3, 4),
@@ -150,16 +147,15 @@
 
 * ``batch_stats`` must be passed as an input variable.
 * The ``batch_stats`` collection needs to be marked as mutable by setting ``mutable=['batch_stats']``.
 * The mutated variables are returned as a second output.
   The updated ``batch_stats`` must be extracted from here.
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   y = mlp.apply(
     {'params': params},
     x,
   )
   ...
@@ -178,16 +174,15 @@
 When integrating models that use ``BatchNorm`` into a training loop, the main challenge
 is handling the additional ``batch_stats`` state. To do this, you need to:
 
 * Add a ``batch_stats`` field to a custom :meth:`flax.training.train_state.TrainState <flax.training.train_state.TrainState>` class.
 * Pass the ``batch_stats`` values to the :meth:`train_state.TrainState.create <train_state.TrainState.create>` method.
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   from flax.training import train_state
 
 
   state = train_state.TrainState.create(
     apply_fn=mlp.apply,
@@ -211,20 +206,19 @@
 In addition, update your ``train_step`` function to reflect these changes:
 
 * Pass all new parameters to ``flax.linen.apply`` (as previously discussed).
 * The ``updates`` to the ``batch_stats`` must be propagated out of the ``loss_fn``.
 * The ``batch_stats`` from the ``TrainState`` must be updated.
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   @jax.jit
-  def train_step(state: TrainState, batch):
+  def train_step(state: train_state.TrainState, batch):
     """Train for a single step."""
     def loss_fn(params):
       logits = state.apply_fn(
         {'params': params},
         x=batch['image'])
       loss = optax.softmax_cross_entropy_with_integer_labels(
         logits=logits, labels=batch['label'])
@@ -261,20 +255,19 @@
 
 The ``eval_step`` is much simpler. Because ``batch_stats`` is not mutable, no
 updates
 need to be propagated. Make sure you pass the ``batch_stats`` to ``flax.linen.apply``,
 and the ``train`` argument is set to ``False``:
 
 .. codediff::
-  :title_left: No BatchNorm
-  :title_right: With BatchNorm
+  :title: No BatchNorm, With BatchNorm
   :sync:
 
   @jax.jit
-  def eval_step(state: TrainState, batch):
+  def eval_step(state: train_state.TrainState, batch):
     """Train for a single step."""
     logits = state.apply_fn(
       {'params': params},
       x=batch['image'])
     loss = optax.softmax_cross_entropy_with_integer_labels(
       logits=logits, labels=batch['label'])
     metrics = {
```

### Comparing `flax-0.8.3/docs/guides/training_techniques/dropout.rst` & `flax-0.8.4/docs/guides/training_techniques/dropout.rst`

 * *Files 3% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 Dropout is a stochastic regularization technique that randomly removes hidden
 and visible units in a network.
 
 Throughout the guide, you will be able to compare code examples with and without
 Flax ``Dropout``.
 
-.. testsetup::
+.. testsetup:: No Dropout, With Dropout
 
   import flax.linen as nn
   import jax.numpy as jnp
   import jax
   import optax
 
 Split the PRNG key
@@ -33,16 +33,15 @@
 ` JAX - The Sharp Bits  Randomness and PRNG keys <https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#jax-prng>`__.
 
 Begin by splitting the PRNG key using
 `jax.random.split() <https://jax.readthedocs.io/en/latest/_autosummary/jax.random.split.html>`__
 into three keys, including one for Flax Linen ``Dropout``.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   root_key = jax.random.key(seed=0)
   main_key, params_key = jax.random.split(key=root_key)
   ---
   root_key = jax.random.key(seed=0)
   main_key, params_key, dropout_key = jax.random.split(key=root_key, num=3) #!
@@ -94,16 +93,15 @@
 unique key each time you call it. Internally, :meth:`flax.linen.Dropout` makes
 use of :meth:`flax.linen.Module.make_rng` to create a key for dropout. You can
 check out the
 `source code <https://github.com/google/flax/blob/5714e57a0dc8146eb58a7a06ed768ed3a17672f9/flax/linen/stochastic.py#L72>`__.
 In short, :meth:`flax.linen.Module.make_rng` *guarantees full reproducibility*.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   class MyModel(nn.Module):
     num_neurons: int
 
     @nn.compact
     def __call__(self, x):
@@ -133,16 +131,15 @@
   `variable dictionary <https://flax.readthedocs.io/en/latest/api_reference/flax.linen/variable.html>`__.
 
 Here, the main difference between the code without Flax ``Dropout``
 and with ``Dropout`` is that the ``training`` (or ``train``) argument must be
 provided if you need dropout enabled.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   my_model = MyModel(num_neurons=3)
   x = jnp.empty((3, 4, 4))
 
   variables = my_model.init(params_key, x)
   params = variables['params']
@@ -159,16 +156,15 @@
 When using :meth:`flax.linen.apply()` to run your model:
 
 * Pass ``training=True`` to :meth:`flax.linen.apply()`.
 * Then, to draw PRNG keys during the forward pass (with dropout), provide a PRNG key
   to seed the ``'dropout'`` stream when you call :meth:`flax.linen.apply()`.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   # No need to pass the `training` and `rngs` flags.
   y = my_model.apply({'params': params}, x)
   ---
   # Dropout is enabled with `training=True` (that is, `deterministic=False`). #!
   y = my_model.apply({'params': params}, x, training=True, rngs={'dropout': dropout_key}) #!
@@ -192,16 +188,15 @@
 the training step function. Refer to the
 :meth:`flax.training.train_state.TrainState` API docs to learn more.
 
 * First, add a ``key`` field to a custom :meth:`flax.training.train_state.TrainState` class.
 * Then, pass the ``key`` valuein this case, the ``dropout_key``to the :meth:`train_state.TrainState.create` method.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   from flax.training import train_state
 
   state = train_state.TrainState.create(
     apply_fn=my_model.apply,
     params=params,
@@ -232,20 +227,19 @@
   afterwards. However, using ``jax.random.fold_in()`` makes sure to 1) fold in
   unique data; and 2) can result in longer sequences of PRNG streams.
 
 * Finally, when performing the forward pass, pass the new PRNG key to ``state.apply_fn()``
   as an extra parameter.
 
 .. codediff::
-  :title_left: No Dropout
-  :title_right: With Dropout
+  :title: No Dropout, With Dropout
   :sync:
 
   @jax.jit
-  def train_step(state: TrainState, batch):
+  def train_step(state: train_state.TrainState, batch):
 
     def loss_fn(params):
       logits = state.apply_fn(
         {'params': params},
         x=batch['image'],
```

### Comparing `flax-0.8.3/docs/guides/training_techniques/lr_schedule.rst` & `flax-0.8.4/docs/guides/training_techniques/lr_schedule.rst`

 * *Files 6% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 
 We will show you how to...
 
 * define a learning rate schedule
 * train a simple model using that schedule
 
 
-.. testsetup::
+.. testsetup:: Default learning rate, Learning rate schedule
 
   import jax
   import jax.numpy as jnp
   import flax.linen as nn
   from flax.training import train_state
   import optax
   import numpy as np
@@ -71,15 +71,15 @@
     metrics = {
         'loss': loss,
         'accuracy': accuracy,
     }
     return metrics
 
 
-.. testcode::
+.. testcode:: Default learning rate, Learning rate schedule
 
   def create_learning_rate_fn(config, base_learning_rate, steps_per_epoch):
     """Creates learning rate schedule."""
     warmup_fn = optax.linear_schedule(
         init_value=0., end_value=base_learning_rate,
         transition_steps=config.warmup_epochs * steps_per_epoch)
     cosine_epochs = max(config.num_epochs - config.warmup_epochs, 1)
@@ -95,16 +95,15 @@
 ``create_learning_rate_fn`` function and then pass the function to your |Optax|_ optimizer.
 For example using this schedule on MNIST would require changing the ``train_step`` function:
 
 .. |Optax| replace:: ``Optax``
 .. _Optax: https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules
 
 .. codediff::
-  :title_left: Default learning rate
-  :title_right: Learning rate schedule
+  :title: Default learning rate, Learning rate schedule
   :sync:
 
   @jax.jit
   def train_step(state, batch):
     def loss_fn(params):
       logits = CNN().apply({'params': params}, batch['image'])
       one_hot = jax.nn.one_hot(batch['label'], 10)
@@ -132,16 +131,15 @@
     lr = learning_rate_fn(state.step) #!
     metrics['learning_rate'] = lr #!
     return new_state, metrics
 
 And the ``train_epoch`` function:
 
 .. codediff::
-  :title_left: Default learning rate
-  :title_right: Learning rate schedule
+  :title: Default learning rate, Learning rate schedule
   :sync:
 
   def train_epoch(state, train_ds, batch_size, epoch, rng):
     """Trains for a single epoch."""
     train_ds_size = len(train_ds['image'])
     steps_per_epoch = train_ds_size // batch_size
     perms = jax.random.permutation(rng, len(train_ds['image']))
@@ -189,16 +187,15 @@
     return state, epoch_metrics
 
 
 And the ``create_train_state`` function:
 
 
 .. codediff::
-  :title_left: Default learning rate
-  :title_right: Learning rate schedule
+  :title: Default learning rate, Learning rate schedule
   :sync:
 
   def create_train_state(rng, config):
     """Creates initial `TrainState`."""
     cnn = CNN()
     params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
     tx = optax.sgd(config.learning_rate, config.momentum)
@@ -210,15 +207,15 @@
     cnn = CNN()
     params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']
     tx = optax.sgd(learning_rate_fn, config.momentum) #!
     return train_state.TrainState.create(
         apply_fn=cnn.apply, params=params, tx=tx)
 
 
-.. testcleanup::
+.. testcleanup:: Learning rate schedule
 
   config = get_config()
 
   train_ds_size = config.train_ds_size
   steps_per_epoch = train_ds_size // config.batch_size
   learning_rate_fn = create_learning_rate_fn(config, config.learning_rate, steps_per_epoch)
```

### Comparing `flax-0.8.3/docs/guides/training_techniques/transfer_learning.ipynb` & `flax-0.8.4/docs/guides/training_techniques/transfer_learning.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/training_techniques/transfer_learning.md` & `flax-0.8.4/docs/guides/training_techniques/transfer_learning.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/training_techniques/use_checkpointing.ipynb` & `flax-0.8.4/docs/guides/training_techniques/use_checkpointing.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/guides/training_techniques/use_checkpointing.md` & `flax-0.8.4/docs/guides/training_techniques/use_checkpointing.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/index.rst` & `flax-0.8.4/docs/index.rst`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 
 Flax is used by
 `hundreds of projects (and growing) <https://github.com/google/flax/network/dependents?package_id=UGFja2FnZS01MjEyMjA2MA%3D%3D>`__,
 both in the open source community
 (like `Hugging Face <https://huggingface.co/flax-community>`__)
 and at Google
 (like
-`PaLM <https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html>`__,
+`Gemini <https://deepmind.google/technologies/gemini>`__,
 `Imagen <https://imagen.research.google>`__,
 `Scenic <https://github.com/google-research/scenic/>`__,
 and `Big Vision <https://github.com/google-research/big_vision>`__).
 
 
 Features
 ^^^^^^^^^
@@ -305,14 +305,16 @@
 
          .. div:: sd-text-center sd-font-italic
 
             On-device differentiable reinforcement learning environments
 
 
 
+.. role:: bold
+  :class: bold
 
 .. toctree::
    :hidden:
    :maxdepth: 2
 
    Quick start <quick_start>
    guides/flax_fundamentals/flax_basics
@@ -321,8 +323,8 @@
    glossary
    faq
    developer_notes/index
    philosophy
    contributing
    experimental
    api_reference/index
-   experimental/index
+   NNX <nnx/index>
```

### Comparing `flax-0.8.3/docs/linen_intro.ipynb` & `flax-0.8.4/docs/linen_intro.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/linen_intro.md` & `flax-0.8.4/docs/linen_intro.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/philosophy.md` & `flax-0.8.4/docs/philosophy.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/quick_start.ipynb` & `flax-0.8.4/docs/quick_start.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/quick_start.md` & `flax-0.8.4/docs/quick_start.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/docs/requirements.txt` & `flax-0.8.4/docs/requirements.txt`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/README.md` & `flax-0.8.4/examples/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/__init__.py` & `flax-0.8.4/examples/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/cloud/README.md` & `flax-0.8.4/examples/cloud/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/cloud/launch_gce.py` & `flax-0.8.4/examples/cloud/launch_gce.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/cloud/startup_script.sh` & `flax-0.8.4/examples/cloud/startup_script.sh`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/README.md` & `flax-0.8.4/examples/imagenet/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/configs/default.py` & `flax-0.8.4/examples/imagenet/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/configs/fake_data_benchmark.py` & `flax-0.8.4/examples/imagenet/configs/fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/configs/tpu.py` & `flax-0.8.4/examples/imagenet/configs/tpu.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/configs/v100_x8.py` & `flax-0.8.4/examples/imagenet/configs/v100_x8.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/configs/v100_x8_mixed_precision.py` & `flax-0.8.4/examples/imagenet/configs/v100_x8_mixed_precision.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/imagenet.ipynb` & `flax-0.8.4/examples/imagenet/imagenet.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/imagenet_benchmark.py` & `flax-0.8.4/examples/imagenet/imagenet_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/imagenet_fake_data_benchmark.py` & `flax-0.8.4/examples/imagenet/imagenet_fake_data_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/input_pipeline.py` & `flax-0.8.4/examples/imagenet/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/main.py` & `flax-0.8.4/examples/imagenet/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/models.py` & `flax-0.8.4/examples/imagenet/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/models_test.py` & `flax-0.8.4/examples/imagenet/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/train.py` & `flax-0.8.4/examples/imagenet/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/imagenet/train_test.py` & `flax-0.8.4/examples/imagenet/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/attention_simple.py` & `flax-0.8.4/examples/linen_design_test/attention_simple.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/autoencoder.py` & `flax-0.8.4/examples/linen_design_test/autoencoder.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/dense.py` & `flax-0.8.4/examples/linen_design_test/dense.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/linear_regression.py` & `flax-0.8.4/examples/linen_design_test/linear_regression.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/mlp_explicit.py` & `flax-0.8.4/examples/linen_design_test/mlp_explicit.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/mlp_inline.py` & `flax-0.8.4/examples/linen_design_test/mlp_inline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/linen_design_test/mlp_lazy.py` & `flax-0.8.4/examples/linen_design_test/mlp_lazy.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/README.md` & `flax-0.8.4/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/configs/default.py` & `flax-0.8.4/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/input_pipeline.py` & `flax-0.8.4/examples/lm1b/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/input_pipeline_test.py` & `flax-0.8.4/examples/lm1b/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/main.py` & `flax-0.8.4/examples/lm1b/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/models.py` & `flax-0.8.4/examples/lm1b/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/temperature_sampler.py` & `flax-0.8.4/examples/lm1b/temperature_sampler.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/temperature_sampler_test.py` & `flax-0.8.4/examples/lm1b/temperature_sampler_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/tokenizer.py` & `flax-0.8.4/examples/lm1b/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/train.py` & `flax-0.8.4/examples/lm1b/train.py`

 * *Files 0% similar despite different names*

```diff
@@ -578,15 +578,15 @@
           metrics_sums = jax.tree_util.tree_map(jnp.sum, train_metrics)
           denominator = metrics_sums.pop("denominator")
           summary = jax.tree_util.tree_map(
               lambda x: x / denominator, metrics_sums
           )  # pylint: disable=cell-var-from-loop
           summary["learning_rate"] = lr
           summary["perplexity"] = jnp.clip(
-              jnp.exp(summary["loss"]), a_max=1.0e4
+              jnp.exp(summary["loss"]), max=1.0e4
           )
           summary = {"train_" + k: v for k, v in summary.items()}
           writer.write_scalars(step, summary)
           train_metrics = []
 
         with report_progress.timed("eval"):
           eval_results = evaluate(
@@ -594,15 +594,15 @@
               params=state.params,
               eval_ds=eval_ds,
               num_eval_steps=config.num_eval_steps,
               config=eval_config,
           )
           # (clipped) perplexity after averaging log-perplexitie
           eval_results["perplexity"] = jnp.clip(
-              jnp.exp(eval_results["loss"]), a_max=1.0e4
+              jnp.exp(eval_results["loss"]), max=1.0e4
           )
           writer.write_scalars(
               step, {"eval_" + k: v for k, v in eval_results.items()}
           )
 
         with report_progress.timed("generate_text"):
           exemplars = generate_prediction(
```

### Comparing `flax-0.8.3/examples/lm1b/train_test.py` & `flax-0.8.4/examples/lm1b/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/lm1b/utils.py` & `flax-0.8.4/examples/lm1b/utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/README.md` & `flax-0.8.4/examples/mnist/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/configs/default.py` & `flax-0.8.4/examples/mnist/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/main.py` & `flax-0.8.4/examples/mnist/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/mnist.ipynb` & `flax-0.8.4/examples/mnist/mnist.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/mnist_benchmark.py` & `flax-0.8.4/examples/mnist/mnist_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/train.py` & `flax-0.8.4/examples/mnist/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/mnist/train_test.py` & `flax-0.8.4/examples/mnist/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/nlp_seq/README.md` & `flax-0.8.4/examples/nlp_seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/nlp_seq/input_pipeline.py` & `flax-0.8.4/examples/nlp_seq/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/nlp_seq/input_pipeline_test.py` & `flax-0.8.4/examples/nlp_seq/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/nlp_seq/models.py` & `flax-0.8.4/examples/nlp_seq/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/nlp_seq/train.py` & `flax-0.8.4/examples/nlp_seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/README.md` & `flax-0.8.4/examples/ogbg_molpcba/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/configs/default.py` & `flax-0.8.4/examples/ogbg_molpcba/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/configs/default_graph_net.py` & `flax-0.8.4/examples/ogbg_molpcba/configs/default_graph_net.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/configs/hparam_sweep.py` & `flax-0.8.4/examples/ogbg_molpcba/configs/hparam_sweep.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/configs/test.py` & `flax-0.8.4/examples/ogbg_molpcba/configs/test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/input_pipeline.py` & `flax-0.8.4/examples/ogbg_molpcba/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/input_pipeline_test.py` & `flax-0.8.4/examples/ogbg_molpcba/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/main.py` & `flax-0.8.4/examples/ogbg_molpcba/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/models.py` & `flax-0.8.4/examples/ogbg_molpcba/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/models_test.py` & `flax-0.8.4/examples/ogbg_molpcba/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba.ipynb` & `flax-0.8.4/examples/ogbg_molpcba/ogbg_molpcba.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py` & `flax-0.8.4/examples/ogbg_molpcba/ogbg_molpcba_benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/train.py` & `flax-0.8.4/examples/ogbg_molpcba/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ogbg_molpcba/train_test.py` & `flax-0.8.4/examples/ogbg_molpcba/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/README.md` & `flax-0.8.4/examples/ppo/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/agent.py` & `flax-0.8.4/examples/ppo/agent.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/configs/default.py` & `flax-0.8.4/examples/ppo/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/env_utils.py` & `flax-0.8.4/examples/ppo/env_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/models.py` & `flax-0.8.4/examples/ppo/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/ppo_lib.py` & `flax-0.8.4/examples/ppo/ppo_lib.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/ppo_lib_test.py` & `flax-0.8.4/examples/ppo/ppo_lib_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/ppo_main.py` & `flax-0.8.4/examples/ppo/ppo_main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/seed_rl_atari_preprocessing.py` & `flax-0.8.4/examples/ppo/seed_rl_atari_preprocessing.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/ppo/test_episodes.py` & `flax-0.8.4/examples/ppo/test_episodes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/README.md` & `flax-0.8.4/examples/seq2seq/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/input_pipeline.py` & `flax-0.8.4/examples/seq2seq/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/models.py` & `flax-0.8.4/examples/seq2seq/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/seq2seq.ipynb` & `flax-0.8.4/examples/seq2seq/seq2seq.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/train.py` & `flax-0.8.4/examples/seq2seq/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/seq2seq/train_test.py` & `flax-0.8.4/examples/seq2seq/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/README.md` & `flax-0.8.4/examples/sst2/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/build_vocabulary.py` & `flax-0.8.4/examples/sst2/build_vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/configs/default.py` & `flax-0.8.4/examples/sst2/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/input_pipeline.py` & `flax-0.8.4/examples/sst2/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/input_pipeline_test.py` & `flax-0.8.4/examples/sst2/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/main.py` & `flax-0.8.4/examples/sst2/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/models.py` & `flax-0.8.4/examples/sst2/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/models_test.py` & `flax-0.8.4/examples/sst2/models_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/sst2.ipynb` & `flax-0.8.4/examples/sst2/sst2.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/train.py` & `flax-0.8.4/examples/sst2/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/train_test.py` & `flax-0.8.4/examples/sst2/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/vocab.txt` & `flax-0.8.4/examples/sst2/vocab.txt`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/sst2/vocabulary.py` & `flax-0.8.4/examples/sst2/vocabulary.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/README.md` & `flax-0.8.4/examples/vae/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/configs/default.py` & `flax-0.8.4/examples/vae/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/input_pipeline.py` & `flax-0.8.4/examples/vae/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/main.py` & `flax-0.8.4/examples/vae/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/models.py` & `flax-0.8.4/examples/vae/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/reconstruction.png` & `flax-0.8.4/examples/vae/reconstruction.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/sample.png` & `flax-0.8.4/examples/vae/sample.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/train.py` & `flax-0.8.4/examples/vae/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/vae/utils.py` & `flax-0.8.4/examples/vae/utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/README.md` & `flax-0.8.4/examples/wmt/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/bleu.py` & `flax-0.8.4/examples/wmt/bleu.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/configs/default.py` & `flax-0.8.4/examples/wmt/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/decode.py` & `flax-0.8.4/examples/wmt/decode.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/input_pipeline.py` & `flax-0.8.4/examples/wmt/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/input_pipeline_test.py` & `flax-0.8.4/examples/wmt/input_pipeline_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/main.py` & `flax-0.8.4/examples/wmt/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/models.py` & `flax-0.8.4/examples/wmt/models.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/tokenizer.py` & `flax-0.8.4/examples/wmt/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/train.py` & `flax-0.8.4/examples/wmt/train.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/examples/wmt/train_test.py` & `flax-0.8.4/examples/wmt/train_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/__init__.py` & `flax-0.8.4/flax/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/configurations.py` & `flax-0.8.4/flax/configurations.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/__init__.py` & `flax-0.8.4/flax/core/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/axes_scan.py` & `flax-0.8.4/flax/core/axes_scan.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/flax_functional_engine.ipynb` & `flax-0.8.4/flax/core/flax_functional_engine.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/frozen_dict.py` & `flax-0.8.4/flax/core/frozen_dict.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/lift.py` & `flax-0.8.4/flax/core/lift.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/meta.py` & `flax-0.8.4/flax/core/meta.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/nn/__init__.py` & `flax-0.8.4/flax/core/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/nn/attention.py` & `flax-0.8.4/flax/core/nn/attention.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/nn/linear.py` & `flax-0.8.4/flax/core/nn/linear.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/nn/normalization.py` & `flax-0.8.4/flax/core/nn/normalization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/nn/stochastic.py` & `flax-0.8.4/flax/core/nn/stochastic.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/partial_eval.py` & `flax-0.8.4/flax/core/partial_eval.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/scope.py` & `flax-0.8.4/flax/core/scope.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/tracers.py` & `flax-0.8.4/flax/core/tracers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/core/variables.py` & `flax-0.8.4/flax/core/variables.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/cursor.py` & `flax-0.8.4/flax/cursor.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/errors.py` & `flax-0.8.4/flax/errors.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/__init__.py` & `flax-0.8.4/flax/experimental/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/.gitignore` & `flax-0.8.4/flax/nnx/.gitignore`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/README.md` & `flax-0.8.4/flax/nnx/README.md`

 * *Files 17% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 [![codecov](https://codecov.io/gh/cgarciae/nnx/branch/main/graph/badge.svg?token=VqJjL474Z7)](https://codecov.io/gh/cgarciae/nnx)
 
 # NNX
 
-_**N**eural **N**etworks for JA**X**_ - | [docs](https://flax.readthedocs.io/en/latest/experimental/nnx/index.html) |
+_**N**eural **N**etworks for JA**X**_ - | [docs](https://flax.readthedocs.io/en/latest/nnx/index.html) |
 
 NNX is a JAX-based neural network library that focuses on providing the best development experience to make
 building and experimenting with neural networks as easy and intuitive as possible.
 
 * **Pythonic**: Modules are standard Python classes, promoting ease of use and a more familiar
   development experience.
 * **Easy-to-use**: NNX provides a set of transforms that take care of state management, allowing
@@ -24,15 +24,15 @@
 ## What does NNX look like?
 
 NNX removes most of the friction from building and training neural networks in JAX. It provides
 a Module system that uses standard Python classes, and a set of transforms that extend
 JAX to handle objects.
 
 ```python
-from flax.experimental import nnx
+from flax import nnx
 import optax
 
 class Model(nnx.Module):
   def __init__(self, din, dmid, dout, rngs: nnx.Rngs):
     self.linear = nnx.Linear(din, dmid, rngs=rngs)
     self.bn = nnx.BatchNorm(dmid, rngs=rngs)
     self.dropout = nnx.Dropout(0.2, rngs=rngs)
@@ -54,25 +54,25 @@
 
   loss, grads = nnx.value_and_grad(loss_fn)(model)
   optimizer.update(grads)  # inplace updates
 
   return loss
 ```
 
-To learn more about the `Module` abstraction, check out our [NNX Basics](https://flax.readthedocs.io/en/latest/experimental/nnx/nnx_basics.html#) guide.
+To learn more about the `Module` abstraction, check out our [NNX Basics](https://flax.readthedocs.io/en/latest/nnx/nnx_basics.html#) guide.
 
 ## Installation
 
 To get started with `nnx`, install Flax from GitHub:
 ```
 pip install git+https://github.com/google/flax.git
 ```
 
 ### Examples
 
-* [LM1B](https://github.com/google/flax/tree/main/flax/experimental/nnx/examples/lm1b): A language model trained on the 1 Billion Word Benchmark dataset.
+* [LM1B](https://github.com/google/flax/tree/main/flax/nnx/examples/lm1b): A language model trained on the 1 Billion Word Benchmark dataset.
 
 #### Toy Examples
-* [Basic Example](https://github.com/google/flax/tree/main/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py): Shows how to train a simple model using NNX.
-* [Using the Functional API](https://github.com/google/flax/tree/main/flax/experimental/nnx/examples/toy_examples/01_functional_api.py): Shows how to train a simple model using the functional API.
-* [Training a VAE](https://github.com/google/flax/tree/main/flax/experimental/nnx/examples/toy_examples/05_vae.py): Shows how to train a VAE on the binarized MNIST dataset.
-* [Scan over layers](https://github.com/google/flax/tree/main/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py): An contrived example that implements scan over layers with dropout and a share BatcNorm layer to showcase how lifted transforms can be implemented. It uses the functional API along with `jax.vmap` and `jax.lax.scan`.
+* [Basic Example](https://github.com/google/flax/tree/main/flax/nnx/examples/toy_examples/02_lifted_transforms.py): Shows how to train a simple model using NNX.
+* [Using the Functional API](https://github.com/google/flax/tree/main/flax/nnx/examples/toy_examples/01_functional_api.py): Shows how to train a simple model using the functional API.
+* [Training a VAE](https://github.com/google/flax/tree/main/flax/nnx/examples/toy_examples/05_vae.py): Shows how to train a VAE on the binarized MNIST dataset.
+* [Scan over layers](https://github.com/google/flax/tree/main/flax/nnx/examples/toy_examples/06_scan_over_layers.py): An contrived example that implements scan over layers with dropout and a share BatcNorm layer to showcase how lifted transforms can be implemented. It uses the functional API along with `jax.vmap` and `jax.lax.scan`.
```

### Comparing `flax-0.8.3/flax/experimental/nnx/__init__.py` & `flax-0.8.4/flax/nnx/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,36 +14,40 @@
 
 from flax.linen.pooling import avg_pool as avg_pool
 from flax.linen.pooling import max_pool as max_pool
 from flax.linen.pooling import min_pool as min_pool
 from flax.linen.pooling import pool as pool
 from flax.typing import Initializer as Initializer
 
-from .nnx import compatibility as compatibility
+from .nnx.compat import wrappers as wrappers
 from .nnx import graph as graph
 from .nnx import errors as errors
-from .nnx import errors as helpers
+from .nnx import helpers as helpers
+from .nnx import compat as compat
 from .nnx.filterlib import All as All
 from .nnx.filterlib import Not as Not
 from .nnx.graph import GraphDef as GraphDef
-from .nnx.graph import GraphNode as GraphNode
+from .nnx.object import Object as Object
 from .nnx.helpers import Dict as Dict
 from .nnx.helpers import List as List
 from .nnx.helpers import Sequential as Sequential
 from .nnx.helpers import TrainState as TrainState
 from .nnx.module import M as M
 from .nnx.module import Module as Module
 from .nnx.graph import merge as merge
 from .nnx.graph import UpdateContext as UpdateContext
+from .nnx.graph import update_context as update_context
+from .nnx.graph import current_update_context as current_update_context
 from .nnx.graph import split as split
 from .nnx.graph import update as update
 from .nnx.graph import clone as clone
 from .nnx.graph import pop as pop
 from .nnx.graph import state as state
 from .nnx.graph import graphdef as graphdef
+from .nnx.graph import iter_graph as iter_graph
 from .nnx.nn import initializers as initializers
 from .nnx.nn.activations import celu as celu
 from .nnx.nn.activations import elu as elu
 from .nnx.nn.activations import gelu as gelu
 from .nnx.nn.activations import glu as glu
 from .nnx.nn.activations import hard_sigmoid as hard_sigmoid
 from .nnx.nn.activations import hard_silu as hard_silu
@@ -67,27 +71,32 @@
 from .nnx.nn.activations import tanh as tanh
 from .nnx.nn.attention import MultiHeadAttention as MultiHeadAttention
 from .nnx.nn.attention import combine_masks as combine_masks
 from .nnx.nn.attention import dot_product_attention as dot_product_attention
 from .nnx.nn.attention import make_attention_mask as make_attention_mask
 from .nnx.nn.attention import make_causal_mask as make_causal_mask
 from .nnx.nn.linear import Conv as Conv
+from .nnx.nn.linear import ConvTranspose as ConvTranspose
 from .nnx.nn.linear import Embed as Embed
 from .nnx.nn.linear import Linear as Linear
 from .nnx.nn.linear import LinearGeneral as LinearGeneral
 from .nnx.nn.linear import Einsum as Einsum
+from .nnx.nn.lora import LoRA as LoRA
+from .nnx.nn.lora import LoRALinear as LoRALinear
+from .nnx.nn.lora import LoRAParam as LoRAParam
 from .nnx.nn.normalization import BatchNorm as BatchNorm
 from .nnx.nn.normalization import LayerNorm as LayerNorm
 from .nnx.nn.normalization import RMSNorm as RMSNorm
 from .nnx.nn.stochastic import Dropout as Dropout
 from .nnx.rnglib import Rngs as Rngs
 from .nnx.rnglib import RngStream as RngStream
 from .nnx.rnglib import RngState as RngState
 from .nnx.rnglib import RngKey as RngKey
 from .nnx.rnglib import RngCount as RngCount
+from .nnx.rnglib import ForkStates as ForkStates
 from .nnx.rnglib import fork as fork
 from .nnx.spmd import PARTITION_NAME as PARTITION_NAME
 from .nnx.spmd import get_partition_spec as get_partition_spec
 from .nnx.spmd import get_named_sharding as get_named_sharding
 from .nnx.spmd import with_partitioning as with_partitioning
 from .nnx.spmd import with_sharding_constraint as with_sharding_constraint
 from .nnx.state import State as State
@@ -103,14 +112,15 @@
 from .nnx.transforms import grad as grad
 from .nnx.transforms import jit as jit
 from .nnx.transforms import remat as remat
 from .nnx.transforms import scan as scan
 from .nnx.transforms import value_and_grad as value_and_grad
 from .nnx.transforms import vmap as vmap
 from .nnx.transforms import eval_shape as eval_shape
+from .nnx.transforms import cond as cond
 from .nnx.variables import EMPTY as EMPTY
 from .nnx.variables import A as A
 from .nnx.variables import BatchStat as BatchStat
 from .nnx.variables import Cache as Cache
 from .nnx.variables import Empty as Empty
 from .nnx.variables import Intermediate as Intermediate
 from .nnx.variables import Param as Param
```

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/demo.ipynb` & `flax-0.8.4/flax/nnx/docs/demo.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9996279761904762%*

 * *Differences: {"'cells'": "{1: {'source': {insert: [(2, 'from flax import nnx')], delete: [2]}}}"}*

```diff
@@ -13,15 +13,15 @@
             "execution_count": 1,
             "id": "e8099a6f",
             "metadata": {},
             "outputs": [],
             "source": [
                 "import jax\n",
                 "from jax import numpy as jnp\n",
-                "from flax.experimental import nnx"
+                "from flax import nnx"
             ]
         },
         {
             "cell_type": "markdown",
             "id": "bcc5cffe",
             "metadata": {},
             "source": [
```

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/demo.md` & `flax-0.8.4/flax/nnx/docs/demo.md`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 ---
 
 # NNX Demo
 
 ```{code-cell} ipython3
 import jax
 from jax import numpy as jnp
-from flax.experimental import nnx
+from flax import nnx
 ```
 
 ### [1] NNX is Pythonic
 
 ```{code-cell} ipython3
 :outputId: d8ef66d5-6866-4d5c-94c2-d22512bfe718
```

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/images/stateful-transforms.png` & `flax-0.8.4/flax/nnx/docs/images/stateful-transforms.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/quick_start.ipynb` & `flax-0.8.4/flax/nnx/docs/quick_start.ipynb`

 * *Files 0% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9999717194570136%*

 * *Differences: {"'cells'": "{8: {'source': {insert: [(2, 'from flax import nnx\\n')], delete: [2]}}}"}*

```diff
@@ -142,15 +142,15 @@
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "import jax\n",
                 "import jax.numpy as jnp\n",
-                "from flax.experimental import nnx\n",
+                "from flax import nnx\n",
                 "\n",
                 "\n",
                 "class CNN(nnx.Module):\n",
                 "\n",
                 "  def __init__(self, *, rngs: nnx.Rngs):\n",
                 "    self.conv1 = nnx.Conv(1, 32, kernel_size=(3, 3), rngs=rngs)\n",
                 "    self.conv2 = nnx.Conv(32, 64, kernel_size=(3, 3), rngs=rngs)\n",
```

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/tiny_nnx.ipynb` & `flax-0.8.4/flax/nnx/docs/tiny_nnx.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/why.ipynb` & `flax-0.8.4/flax/nnx/docs/why.ipynb`

 * *Files 1% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9995760959470636%*

 * *Differences: {"'cells'": "{0: {'source': {insert: [(3, '[![Open In "*

 * *            "Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/nnx/docs/why.ipynb)\\n'), "*

 * *            "(21, '[[nnx on github](https://github.com/google/flax/tree/main/flax/nnx)]\\n'), (22, "*

 * *            "'[[this doc on "*

 * *            "github](https://github.com/google/flax/blob/main/flax/nnx/docs/why.ipynb)]')], "*

 * *            "delete: [22, 21, 3]}}, 1: {'source': {inser []*

```diff
@@ -3,15 +3,15 @@
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "# Why NNX?\n",
                 "\n",
                 "<!-- open in colab button -->\n",
-                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)\n",
+                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/nnx/docs/why.ipynb)\n",
                 "\n",
                 "Four years ago we developed the Flax \"Linen\" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.\n",
                 "\n",
                 "We introduced some ideas that have proven to be good:\n",
                 " - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.\n",
                 " - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)\n",
                 " - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.\n",
@@ -21,29 +21,29 @@
                 "NNX is an attempt to keep the features that made Linen useful while introducing some new principles:\n",
                 "\n",
                 "- Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.\n",
                 "- A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks.\n",
                 "\n",
                 "We'd love to hear from any of our users about their thoughts on these ideas.\n",
                 "\n",
-                "[[nnx on github](https://github.com/google/flax/tree/main/flax/experimental/nnx)]\n",
-                "[[this doc on github](https://github.com/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)]"
+                "[[nnx on github](https://github.com/google/flax/tree/main/flax/nnx)]\n",
+                "[[this doc on github](https://github.com/google/flax/blob/main/flax/nnx/docs/why.ipynb)]"
             ]
         },
         {
             "cell_type": "code",
             "execution_count": 108,
             "metadata": {},
             "outputs": [],
             "source": [
                 "! pip install -U git+https://github.com/google/flax.git\n",
                 "from functools import partial\n",
                 "import jax\n",
                 "from jax import random, numpy as jnp\n",
-                "from flax.experimental import nnx"
+                "from flax import nnx"
             ]
         },
         {
             "cell_type": "markdown",
             "metadata": {},
             "source": [
                 "### NNX is Pythonic\n",
@@ -488,15 +488,15 @@
                     "execution_count": 112,
                     "metadata": {},
                     "output_type": "execute_result"
                 }
             ],
             "source": [
                 "# class transform:\n",
-                "ScannedLinear = nnx.Scan(nnx.Linear, variable_axes={nnx.Param: 0}, length=4)\n",
+                "ScannedLinear = nnx.Scan.constructor(nnx.Linear, variable_axes={nnx.Param: 0}, length=4)\n",
                 "\n",
                 "scanned = ScannedLinear(2, 2, rngs=nnx.Rngs(0))\n",
                 "scanned.get_state()"
             ]
         },
         {
             "cell_type": "code",
```

### Comparing `flax-0.8.3/flax/experimental/nnx/docs/why.md` & `flax-0.8.4/flax/nnx/docs/why.md`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
     format_version: 0.13
     jupytext_version: 1.13.8
 ---
 
 # Why NNX?
 
 <!-- open in colab button -->
-[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/flax/blob/main/flax/nnx/docs/why.ipynb)
 
 Four years ago we developed the Flax "Linen" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.
 
 We introduced some ideas that have proven to be good:
  - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.
  - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)
  - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.
@@ -25,23 +25,23 @@
 NNX is an attempt to keep the features that made Linen useful while introducing some new principles:
 
 - Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.
 - A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks.
 
 We'd love to hear from any of our users about their thoughts on these ideas.
 
-[[nnx on github](https://github.com/google/flax/tree/main/flax/experimental/nnx)]
-[[this doc on github](https://github.com/google/flax/blob/main/flax/experimental/nnx/docs/why.ipynb)]
+[[nnx on github](https://github.com/google/flax/tree/main/flax/nnx)]
+[[this doc on github](https://github.com/google/flax/blob/main/flax/nnx/docs/why.ipynb)]
 
 ```{code-cell}
 ! pip install -U git+https://github.com/google/flax.git
 from functools import partial
 import jax
 from jax import random, numpy as jnp
-from flax.experimental import nnx
+from flax import nnx
 ```
 
 ### NNX is Pythonic
 The main feature of NNX Module is that it adheres to Python semantics. This means that:
 
 * fields are mutable so you can perform inplace updates
 * Module references can be shared between multiple Modules
@@ -252,15 +252,15 @@
 
 Like linen, for convenience we still provide simple lifted transforms for standard JAX transforms, usable as class transforms and decorators.  We've endeavored to simplify the API for scan and vmap compared to the flax specifications.
 
 ```{code-cell}
 :outputId: c4800a49-efd1-4ee5-e703-6e63e18da4cb
 
 # class transform:
-ScannedLinear = nnx.Scan(nnx.Linear, variable_axes={nnx.Param: 0}, length=4)
+ScannedLinear = nnx.Scan.constructor(nnx.Linear, variable_axes={nnx.Param: 0}, length=4)
 
 scanned = ScannedLinear(2, 2, rngs=nnx.Rngs(0))
 scanned.get_state()
 ```
 
 ```{code-cell}
 :outputId: 9efd6e71-d180-4674-ade0-2b02057a400b
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/README.md` & `flax-0.8.4/flax/nnx/examples/lm1b/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/configs/default.py` & `flax-0.8.4/flax/nnx/examples/lm1b/configs/default.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline.py` & `flax-0.8.4/flax/nnx/examples/lm1b/input_pipeline.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/input_pipeline_test.py` & `flax-0.8.4/flax/nnx/examples/lm1b/input_pipeline_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,15 @@
     config.max_target_length = _TARGET_LENGTH
     config.max_eval_target_length = _EVAL_TARGET_LENGTH
     config.max_predict_length = _PREDICT_TARGET_LENGTH
 
     vocab_path = os.path.join(tempfile.mkdtemp(), 'sentencepiece_model')
 
     # Go two directories up to the root of the flax directory.
-    flax_root_dir = pathlib.Path(__file__).parents[5]
+    flax_root_dir = pathlib.Path(__file__).parents[4]
     data_dir = str(flax_root_dir) + '/.tfds/metadata'  # pylint: disable=unused-variable
 
     with tfds.testing.mock_data(num_examples=128, data_dir=data_dir):
       train_ds, eval_ds, predict_ds, _ = input_pipeline.get_datasets(
         n_devices=2, config=config, vocab_path=vocab_path
       )
     return train_ds, eval_ds, predict_ds
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/main.py` & `flax-0.8.4/flax/nnx/examples/lm1b/main.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/models.py` & `flax-0.8.4/flax/nnx/examples/lm1b/models.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,16 +28,16 @@
 from typing import Any, Optional
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax import lax
 
-from flax.experimental import nnx
-from flax.experimental.nnx.examples.lm1b.configs import default
+from flax import nnx
+from configs import default
 
 Shape = tuple[int, ...]
 Dtype = Any
 
 
 @dataclasses.dataclass(unsafe_hash=True)
 class TransformerConfig:
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/models_test.py` & `flax-0.8.4/flax/nnx/examples/lm1b/models_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,28 +23,25 @@
 
 import jax
 import jax.numpy as jnp
 from absl.testing import absltest
 from jax import random
 
 from flax import traverse_util
-from flax.experimental import nnx
-from flax.experimental.nnx.examples.lm1b.configs import default
-from flax.experimental.nnx.examples.lm1b.models import (
-  TransformerConfig,
-  TransformerLM,
-)
-from flax.experimental.nnx.examples.lm1b.utils import HasCache
+from flax import nnx
+from configs import default
+from models import TransformerConfig, TransformerLM
+from utils import HasCache
 
 jax.config.update('jax_disable_most_optimizations', True)
 
 # add project_root to import lm1b Linen model
-project_root = str(Path(__file__).absolute().parents[5])
+project_root = str(Path(__file__).absolute().parents[4])
 sys.path.append(project_root)
-from examples.lm1b.models import TransformerLM as TransformerLinen
+from examples.lm1b.models import TransformerLM as TransformerLinen  # type: ignore[import-error]
 
 sys.path.pop()
 
 
 @dataclasses.dataclass(unsafe_hash=True)
 class CompatTransformerConfig(TransformerConfig):
   decode: bool | None = None
@@ -204,14 +201,17 @@
         mlp='data',
         kv=None,
         vocab=None,
       ),
       deterministic=True,
       decode=False,
     )
+    # Set dropout rates to avoid create dropout states
+    config.dropout_rate = 0.0
+    config.attention_dropout_rate = 0.0
 
     model_nnx = nnx.eval_shape(lambda: TransformerLM(config, rngs=nnx.Rngs(0)))
     _, params_nnx = nnx.split(model_nnx, nnx.Param)
 
     model_linen = TransformerLinen(config)
 
     sample_inputs = random.randint(random.PRNGKey(0), (1, 3), 0, 20)
@@ -238,14 +238,17 @@
         mlp='data',
         kv=None,
         vocab=None,
       ),
       deterministic=True,
       decode=True,
     )
+    # Set dropout rates to avoid create dropout states
+    config.dropout_rate = 0.0
+    config.attention_dropout_rate = 0.0
 
     model_nnx = nnx.eval_shape(lambda: TransformerLM(config, rngs=nnx.Rngs(0)))
     for _path, m in model_nnx.iter_modules():
       if isinstance(m, HasCache):
         input_shape = (batch_size, config.max_len, config.emb_dim)
         m.init_cache(input_shape, dtype=config.dtype)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler.py` & `flax-0.8.4/flax/nnx/examples/lm1b/temperature_sampler.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py` & `flax-0.8.4/flax/nnx/examples/lm1b/temperature_sampler_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/tokenizer.py` & `flax-0.8.4/flax/nnx/examples/lm1b/tokenizer.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/train.py` & `flax-0.8.4/flax/nnx/examples/lm1b/train.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,15 +38,15 @@
 from configs import default
 from jax import random
 from jax.sharding import Mesh, NamedSharding
 from jax.sharding import PartitionSpec as P
 from utils import HasCache, TrainState
 
 from flax import linen as nn
-from flax.experimental import nnx
+from flax import nnx
 from flax.training import checkpoints, common_utils
 
 
 def rsqrt_schedule(
   init_value: float,
   shift: int = 0,
 ):
@@ -601,31 +601,29 @@
           lr = train_metrics.pop('learning_rate').mean()
           metrics_sums = jax.tree_util.tree_map(jnp.sum, train_metrics)
           denominator = metrics_sums.pop('denominator')
           summary = jax.tree_util.tree_map(
             lambda x: x / denominator, metrics_sums
           )  # pylint: disable=cell-var-from-loop
           summary['learning_rate'] = lr
-          summary['perplexity'] = jnp.clip(
-            jnp.exp(summary['loss']), a_max=1.0e4
-          )
+          summary['perplexity'] = jnp.clip(jnp.exp(summary['loss']), max=1.0e4)
           summary = {'train_' + k: v for k, v in summary.items()}
           writer.write_scalars(step, summary)
           train_metrics = []
 
         with report_progress.timed('eval'):
           eval_results = evaluate(
             jit_eval_step=jit_eval_step,
             state=state,
             eval_ds=eval_ds,
             num_eval_steps=config.num_eval_steps,
           )
           # (clipped) perplexity after averaging log-perplexitie
           eval_results['perplexity'] = jnp.clip(
-            jnp.exp(eval_results['loss']), a_max=1.0e4
+            jnp.exp(eval_results['loss']), max=1.0e4
           )
           writer.write_scalars(
             step, {'eval_' + k: v for k, v in eval_results.items()}
           )
 
         with report_progress.timed('generate_text'):
           exemplars = generate_prediction(
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/train_test.py` & `flax-0.8.4/flax/nnx/examples/lm1b/train_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -48,18 +48,22 @@
     config.mlp_dim = 512
     config.num_heads = 4
 
     config.max_target_length = 32
     config.max_eval_target_length = 32
     config.max_predict_length = 32
 
+    # Set dropout rates to avoid create dropout states
+    config.dropout_rate = 0.0
+    config.attention_dropout_rate = 0.0
+
     workdir = tempfile.mkdtemp()
 
     # Go two directories up to the root of the flax directory.
-    flax_root_dir = pathlib.Path(__file__).parents[5]
+    flax_root_dir = pathlib.Path(__file__).parents[4]
     data_dir = str(flax_root_dir) + '/.tfds/metadata'  # pylint: disable=unused-variable
     print('data_dir: ', data_dir)
 
     with tfds.testing.mock_data(num_examples=128, data_dir=data_dir):
       train.train_and_evaluate(config, workdir)
     logging.info('workdir content: %s', tf.io.gfile.listdir(workdir))
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/lm1b/utils.py` & `flax-0.8.4/flax/nnx/examples/lm1b/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,32 +18,31 @@
 from typing import Any, Callable
 from typing_extensions import Protocol, runtime_checkable
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax.experimental import mesh_utils
-from flax.experimental.nnx.examples.lm1b.configs import default
+from configs import default
 from models import TransformerConfig, TransformerLM
 
-from flax.experimental import nnx
+from flax import nnx
 from flax.training import train_state
 
 Dtype = Any
 Shape = tuple[int, ...]
 
 
 class TrainState(train_state.TrainState):
   graphdef: nnx.GraphDef[TransformerLM]
 
 
 @runtime_checkable
 class HasCache(Protocol):
-  def init_cache(self, input_shape: Shape, dtype: Dtype = jnp.float32):
-    ...
+  def init_cache(self, input_shape: Shape, dtype: Dtype = jnp.float32): ...
 
 
 # Mesh utils.
 # -----------------------------------------------------------------------------
 
 
 def create_device_mesh(config: default.Config):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/01_functional_api.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/01_functional_api.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 # %%
 import jax
 import jax.numpy as jnp
 import matplotlib.pyplot as plt
 import numpy as np
 
-from flax.experimental import nnx
+from flax import nnx
 
 X = np.linspace(0, 1, 100)[:, None]
 Y = 0.8 * X**2 + 0.1 + np.random.normal(0, 0.1, size=X.shape)
 
 
 def dataset(batch_size):
   while True:
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/02_lifted_transforms.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 # %%
 import jax
 import jax.numpy as jnp
 import matplotlib.pyplot as plt
 import numpy as np
 import optax
 
-from flax.experimental import nnx
+from flax import nnx
 
 X = np.linspace(0, 1, 100)[:, None]
 Y = 0.8 * X**2 + 0.1 + np.random.normal(0, 0.1, size=X.shape)
 
 
 def dataset(batch_size):
   while True:
@@ -58,14 +58,15 @@
     return x
 
 
 model = MLP(din=1, dhidden=32, dout=1, rngs=nnx.Rngs(0))
 tx = optax.sgd(1e-3)
 optimizer = nnx.Optimizer(model, tx)
 
+
 @nnx.jit
 def train_step(model: MLP, optimizer: nnx.Optimizer, batch):
   x, y = batch
 
   def loss_fn(model: MLP):
     y_pred = model(x)
     return jnp.mean((y - y_pred) ** 2)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/05_vae.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/05_vae.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 import jax
 import jax.numpy as jnp
 import matplotlib.pyplot as plt
 import numpy as np
 import optax
 from datasets import load_dataset
 
-from flax.experimental import nnx
+from flax import nnx
 
 np.random.seed(42)
 latent_size = 32
 image_shape: tp.Sequence[int] = (28, 28)
 steps_per_epoch: int = 200
 batch_size: int = 64
 epochs: int = 20
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/06_scan_over_layers.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 from functools import partial
 
 import jax
 import jax.numpy as jnp
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class Block(nnx.Module):
   def __init__(self, dim: int, *, rngs: nnx.Rngs):
     self.linear = nnx.Linear(dim, dim, rngs=rngs)
     self.bn = nnx.BatchNorm(dim, rngs=rngs)
     self.dropout = nnx.Dropout(0.5, rngs=rngs)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/08_save_load_checkpoints.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 from tempfile import TemporaryDirectory
 
 import jax
 import jax.numpy as jnp
 import orbax.checkpoint as orbax
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class MLP(nnx.Module):
   def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):
     self.dense1 = nnx.Linear(din, dmid, rngs=rngs)
     self.dense2 = nnx.Linear(dmid, dout, rngs=rngs)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py` & `flax-0.8.4/flax/nnx/examples/toy_examples/09_parameter_surgery.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import jax
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 # lets pretend this function loads a pretrained model from a checkpoint
 def load_pretrained():
   return nnx.Linear(784, 128, rngs=nnx.Rngs(0))
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/__init__.py` & `flax-0.8.4/flax/metrics/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/compatibility.py` & `flax-0.8.4/flax/nnx/nnx/compat/wrappers.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,20 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import dataclasses
 import typing as tp
 from typing import Any
 
-from flax.experimental import nnx
+from flax import nnx
 from flax import linen
-from flax.experimental.nnx.nnx import variables as variableslib
-from flax.experimental.nnx.nnx.module import GraphDef, Module
-from flax.experimental.nnx.nnx.rnglib import Rngs
-from flax.experimental.nnx.nnx.state import State
+from flax.nnx.nnx import variables as variableslib
+from flax.nnx.nnx.module import GraphDef, Module
+from flax.nnx.nnx.rnglib import Rngs
+from flax.nnx.nnx.state import State
 
 M = tp.TypeVar('M', bound=Module)
 
 
 # Flax-like style is NNX
 @dataclasses.dataclass
 class Functional(tp.Generic[M]):
@@ -103,9 +103,8 @@
           self.states[collection] = variableslib.variable_type(collection)(
             value
           )
 
     return out
 
 
-class NNXWrapper(linen.Module):
-  ...
+class NNXWrapper(linen.Module): ...
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/errors.py` & `flax-0.8.4/flax/nnx/nnx/errors.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/filterlib.py` & `flax-0.8.4/flax/nnx/nnx/filterlib.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,18 @@
 
 def to_predicate(filter: Filter) -> Predicate:
   if isinstance(filter, str):
     return WithTag(filter)
   elif isinstance(filter, type):
     return OfType(filter)
   elif isinstance(filter, bool):
-    return Everything() if filter else Nothing()
+    if filter:
+      return Everything()
+    else:
+      return Nothing()
   elif filter is Ellipsis:
     return Everything()
   elif filter is None:
     return Nothing()
   elif callable(filter):
     return filter
   elif isinstance(filter, (list, tuple)):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/graph.py` & `flax-0.8.4/flax/nnx/nnx/graph.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,50 +10,49 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from __future__ import annotations
 
+from collections import defaultdict
 import dataclasses
 import enum
+import functools
 import threading
 import typing as tp
-from abc import ABCMeta
 from copy import deepcopy
 
-
 import jax
 import numpy as np
 import typing_extensions as tpe
 
-from flax.experimental.nnx.nnx import (
-  errors,
+from flax.nnx.nnx import (
   filterlib,
   reprlib,
-  tracers,
 )
-from flax.experimental.nnx.nnx.proxy_caller import (
+from flax.nnx.nnx.proxy_caller import (
   ApplyCaller,
   CallableProxy,
   DelayedAccessor,
 )
-from flax.experimental.nnx.nnx.state import (
+from flax.nnx.nnx.state import (
   FlatState,
   State,
   StateLeaf,
   is_state_leaf,
 )
-from flax.experimental.nnx.nnx.variables import Variable, VariableState
-from flax.typing import PathParts, Key
+from flax.nnx.nnx.variables import Variable, VariableState
+from flax.typing import Key, PathParts
 
 A = tp.TypeVar('A')
 B = tp.TypeVar('B')
 C = tp.TypeVar('C')
-G = tp.TypeVar('G', bound='GraphNode')
+F = tp.TypeVar('F', bound=tp.Callable)
+
 HA = tp.TypeVar('HA', bound=tp.Hashable)
 HB = tp.TypeVar('HB', bound=tp.Hashable)
 
 Index = int
 Names = tp.Sequence[int]
 Node = tp.TypeVar('Node')
 Leaf = tp.TypeVar('Leaf')
@@ -67,27 +66,26 @@
   State,
   tuple[State, ...],
 ]
 
 NodeLeaf = tp.Union[Variable[tp.Any], np.ndarray, jax.Array]
 
 
-def is_node_leaf(x: tp.Any) -> tpe.TypeGuard[NodeLeaf]:
-  return isinstance(x, (Variable, np.ndarray, jax.Array))
+@dataclasses.dataclass
+class GraphContext(threading.local):
+  update_context_stacks: defaultdict[str, list[UpdateContext]] = (
+    dataclasses.field(default_factory=lambda: defaultdict(list))
+  )
 
 
-@dataclasses.dataclass
-class GraphUtilsContext(threading.local):
-  node_types: dict[
-    type, 'NodeImpl[tp.Any, tp.Any, tp.Any]'
-  ] = dataclasses.field(default_factory=dict)
-  seen_modules_repr: set[int] | None = None
+GRAPH_CONTEXT = GraphContext()
 
 
-CONTEXT = GraphUtilsContext()
+def is_node_leaf(x: tp.Any) -> tpe.TypeGuard[NodeLeaf]:
+  return isinstance(x, (Variable, np.ndarray, jax.Array))
 
 
 class _HashById(tp.Hashable, tp.Generic[A]):
   """A wrapper around a value that uses its id for hashing and equality.
   This is used by RefMap to explicitly use object id as the hash for the keys.
   """
 
@@ -149,15 +147,15 @@
 
 
 @dataclasses.dataclass(frozen=True)
 class GraphNodeImpl(NodeImplBase[Node, Leaf, AuxData]):
   set_key: tp.Callable[[Node, Key, Leaf], None]
   pop_key: tp.Callable[[Node, Key], Leaf]
   create_empty: tp.Callable[[AuxData], Node]
-  clear: tp.Callable[[Node, AuxData], None]
+  clear: tp.Callable[[Node], None]
 
   def init(self, node: Node, items: tuple[tuple[Key, Leaf], ...]):
     for key, value in items:
       self.set_key(node, key, value)
 
 
 @dataclasses.dataclass(frozen=True)
@@ -166,65 +164,68 @@
 
 
 NodeImpl = tp.Union[
   GraphNodeImpl[Node, Leaf, AuxData], PytreeNodeImpl[Node, Leaf, AuxData]
 ]
 
 
+_node_impl_for_type: dict[type, 'NodeImpl[tp.Any, tp.Any, tp.Any]'] = {}
+
+
 def register_graph_node_type(
   type: type,
   flatten: tp.Callable[[Node], tuple[tp.Sequence[tuple[Key, Leaf]], AuxData]],
   set_key: tp.Callable[[Node, Key, Leaf], None],
   pop_key: tp.Callable[[Node, Key], Leaf],
   create_empty: tp.Callable[[AuxData], Node],
-  clear: tp.Callable[[Node, AuxData], None],
+  clear: tp.Callable[[Node], None],
 ):
-  CONTEXT.node_types[type] = GraphNodeImpl(
+  _node_impl_for_type[type] = GraphNodeImpl(
     type=type,
     flatten=flatten,
     set_key=set_key,
     pop_key=pop_key,
     create_empty=create_empty,
     clear=clear,
   )
 
 
 def is_node(x: tp.Any) -> bool:
-  if type(x) in CONTEXT.node_types:
+  if type(x) in _node_impl_for_type:
     return True
   return is_pytree_node(x)
 
 
 def is_graph_node(x: tp.Any) -> bool:
-  return type(x) in CONTEXT.node_types
+  return type(x) in _node_impl_for_type
 
 
 def is_node_type(x: type[tp.Any]) -> bool:
-  return x in CONTEXT.node_types or x is PytreeType
+  return x in _node_impl_for_type or x is PytreeType
 
 
 def get_node_impl(x: Node) -> NodeImpl[Node, tp.Any, tp.Any]:
   if isinstance(x, Variable):
     raise ValueError(f'Variable is not a node: {x}')
 
   node_type = type(x)
 
-  if node_type not in CONTEXT.node_types:
+  if node_type not in _node_impl_for_type:
     if is_pytree_node(x):
       return PYTREE_NODE_IMPL
     else:
       raise ValueError(f'Unknown node type: {x}')
 
-  return CONTEXT.node_types[node_type]
+  return _node_impl_for_type[node_type]
 
 
 def get_node_impl_for_type(x: type[Node]) -> NodeImpl[Node, tp.Any, tp.Any]:
   if x is PytreeType:
     return PYTREE_NODE_IMPL
-  return CONTEXT.node_types[x]
+  return _node_impl_for_type[x]
 
 
 class _HashableMapping(tp.Mapping[HA, HB], tp.Hashable):
   def __init__(self, mapping: tp.Mapping[HA, HB] | tp.Iterable[tuple[HA, HB]]):
     self._mapping = dict(mapping)
 
   def __contains__(self, key: object) -> bool:
@@ -254,49 +255,49 @@
 @dataclasses.dataclass(frozen=True, repr=False)
 class NodeDef(tp.Generic[Node], reprlib.Representable):
   type: tp.Type[Node]
   index: int
   attributes: tuple[Key, ...]
   subgraphs: _HashableMapping[Key, tp.Union['NodeDef[tp.Any]', Index]]
   static_fields: _HashableMapping[Key, tp.Any]
-  variables: _HashableMapping[Key, Index]
+  leaves: _HashableMapping[Key, Index | None]
   metadata: tp.Any
 
   @classmethod
   def create(
     cls,
     type: tp.Type[Node],
     index: int,
     attributes: tuple[Key, ...],
-    subgraphs: tp.Iterable[tuple[Key, tp.Union['GraphDef[tp.Any]', Index]]],
+    subgraphs: tp.Iterable[tuple[Key, tp.Union['NodeDef[tp.Any]', Index]]],
     static_fields: tp.Iterable[tuple[Key, tp.Any]],
-    variables: tp.Iterable[tuple[Key, Index]],
+    leaves: tp.Iterable[tuple[Key, Index | None]],
     metadata: tp.Any,
   ):
     return cls(
       type=type,
       index=index,
       attributes=attributes,
       subgraphs=_HashableMapping(subgraphs),
       static_fields=_HashableMapping(static_fields),
-      variables=_HashableMapping(variables),
+      leaves=_HashableMapping(leaves),
       metadata=metadata,
     )
 
   def __nnx_repr__(self):
     yield reprlib.Object(type=type(self))
 
     yield reprlib.Attr('type', self.type.__name__)
     yield reprlib.Attr('index', self.index)
     yield reprlib.Attr('attributes', self.attributes)
     yield reprlib.Attr('subgraphs', reprlib.PrettyMapping(self.subgraphs))
     yield reprlib.Attr(
       'static_fields', reprlib.PrettyMapping(self.static_fields)
     )
-    yield reprlib.Attr('variables', reprlib.PrettyMapping(self.variables))
+    yield reprlib.Attr('variables', reprlib.PrettyMapping(self.leaves))
     yield reprlib.Attr('metadata', self.metadata)
 
 
 @dataclasses.dataclass(frozen=True, repr=False)
 class GraphDef(tp.Generic[Node], reprlib.Representable):
   nodedef: NodeDef[Node]
   index_mapping: dict[Index, Index] | None
@@ -392,42 +393,43 @@
   # only cache graph nodes
   if isinstance(node_impl, GraphNodeImpl):
     index = len(refmap)
     refmap[node] = index
   else:
     index = -1
 
-  subgraphs: list[tuple[Key, tp.Union[NodeDef[Node], int]]] = []
+  subgraphs: list[tuple[Key, tp.Union[NodeDef[Node], Index]]] = []
   static_fields: list[tuple[Key, tp.Any]] = []
-  variables: list[tuple[Key, int]] = []
+  leaves: list[tuple[Key, Index | None]] = []
 
   values, metadata = node_impl.flatten(node)
   for key, value in values:
     if is_node(value):
       nodedef = _graph_flatten((*path, key), refmap, flat_state, value)
       subgraphs.append((key, nodedef))
     elif isinstance(value, Variable):
       if value in refmap:
-        variables.append((key, refmap[value]))
+        leaves.append((key, refmap[value]))
       else:
         flat_state[(*path, key)] = value.to_state()
         variable_index = refmap[value] = len(refmap)
-        variables.append((key, variable_index))
+        leaves.append((key, variable_index))
     elif is_state_leaf(value):
       flat_state[(*path, key)] = value
+      leaves.append((key, None))
     else:
       static_fields.append((key, value))
 
   nodedef = NodeDef.create(
     type=node_impl.type,
     index=index,
     attributes=tuple(key for key, _ in values),
     subgraphs=subgraphs,
     static_fields=static_fields,
-    variables=variables,
+    leaves=leaves,
     metadata=metadata,
   )
   return nodedef
 
 
 def unflatten(
   graphdef: GraphDef[Node],
@@ -452,15 +454,15 @@
     graphdef.nodedef, state.raw_mapping, index_to_ref, idxmap
   )
   return node, index_to_ref
 
 
 def _graph_unflatten(
   nodedef: tp.Union[NodeDef[Node], int],
-  state: dict[Key, StateLeaf | dict[Key, tp.Any]],
+  state: tp.Mapping[Key, StateLeaf | tp.Mapping[Key, tp.Any]],
   index_to_ref: dict[Index, tp.Any],
   idxmap: dict[Index, tp.Any] | None,
 ) -> Node:
   """Recursive helper for graph_unflatten.
 
   Args:
     nodedef: A NodeDef instance or an index to a node in the cache.
@@ -483,106 +485,129 @@
     raise RuntimeError(f'NodeDef index {nodedef.index} already used.')
 
   node_impl = get_node_impl_for_type(nodedef.type)
 
   def _get_children():
     children: dict[Key, NodeLeaf | Node] = {}
 
+    # NOTE: we could allw adding new StateLeafs here
     if unkown_keys := set(state) - set(nodedef.attributes):
       raise ValueError(f'Unknown keys: {unkown_keys}')
 
+    # for every key in attributes there are 6 possible cases:
+    #  - (2) the key can either be present in the state or not
+    #  - (3) the key can be a subgraph, a leaf, or a static attribute
     for key in nodedef.attributes:
-      if key in nodedef.static_fields:
-        children[key] = nodedef.static_fields[key]
-      elif key not in state:
+      if key not in state:
         # TODO(cgarcia): maybe we shouldn't support unflattening with missing keys?
         # if key is not present create an empty types
-        if key in nodedef.subgraphs:
+        if key in nodedef.static_fields:
+          children[key] = nodedef.static_fields[key]
+        elif key in nodedef.subgraphs:
           # if the key is a subgraph we create an empty node
           subgraphdef = nodedef.subgraphs[key]
           if isinstance(subgraphdef, int):
             # subgraph exists, take it from the cache
             children[key] = index_to_ref[subgraphdef]
           else:
-            # create an empty node
+            # create a node from an empty state, reasoning:
+            # * its a node with no state
+            # * its a node with state but only through references of already
+            #   created nodes
             substate = {}
             children[key] = _graph_unflatten(
               subgraphdef, substate, index_to_ref, idxmap
             )
-        elif key in nodedef.variables:
-          variable_index = nodedef.variables[key]
-          if variable_index in index_to_ref:
+        elif key in nodedef.leaves:
+          leaf_index = nodedef.leaves[key]
+          if leaf_index is not None and leaf_index in index_to_ref:
             # variable exists, take it from the cache
-            children[key] = index_to_ref[variable_index]
+            children[key] = index_to_ref[leaf_index]
           else:
             # key for a variable is missing, raise an error
             raise ValueError(
-              f'Expected key for Variable but was not found in state: {key!r}'
+              f'Expected key {key!r} in state while building node of type '
+              f'{nodedef.type.__name__}.'
             )
         else:
           raise RuntimeError(f'Unknown static field: {key!r}')
       else:
         value = state[key]
+        if key in nodedef.static_fields:
+          raise ValueError(
+            f'Got state for static field {key!r}, this is not supported.'
+          )
         if key in nodedef.subgraphs:
           if is_state_leaf(value):
             raise ValueError(
               f'Expected value of type {nodedef.subgraphs[key]} for '
               f'{key!r}, but got {value!r}'
             )
           assert isinstance(value, dict)
           subgraphdef = nodedef.subgraphs[key]
 
           if isinstance(subgraphdef, int):
-            node = index_to_ref[subgraphdef]
+            children[key] = index_to_ref[subgraphdef]
           else:
-            node = children[key] = _graph_unflatten(
+            children[key] = _graph_unflatten(
               subgraphdef, value, index_to_ref, idxmap
             )
 
-        elif key in nodedef.variables:
-          variable_index = nodedef.variables[key]
-          if variable_index in index_to_ref:
-            children[key] = index_to_ref[variable_index]
+        elif key in nodedef.leaves:
+          if not is_state_leaf(value):
+            raise ValueError(f'Expected a leaf for {key!r}, but got {value!r}')
+
+          leaf_index = nodedef.leaves[key]
+
+          if leaf_index is None:
+            # if the leaf is None, it means that the value was originally
+            # a non-VariableState leaf, however we allow providing a
+            # VariableState presumbly created by modifying the State
+            if isinstance(value, VariableState):
+              value = value.to_variable()
+            children[key] = value
+          elif leaf_index in index_to_ref:
+            # add an existing variable
+            children[key] = index_to_ref[leaf_index]
           else:
+            # its a unseen variable, create a new one
             if not isinstance(value, VariableState):
               raise ValueError(
                 f'Expected a Variable type for {key!r}, but got {type(value)}.'
               )
-            if idxmap is not None and variable_index in idxmap:
-              variable = idxmap[variable_index]
+            # when idxmap is present, check if the Varable exists there
+            # and update existing variables if it does
+            if idxmap is not None and leaf_index in idxmap:
+              variable = idxmap[leaf_index]
               if not isinstance(variable, Variable):
                 raise ValueError(
                   f'Expected a Variable type for {key!r}, but got {type(variable)}.'
                 )
               variable.copy_from_state(value)
-            else:
+            else:  # if it doesn't, create a new variable
               assert isinstance(value, VariableState)
               variable = value.to_variable()
             children[key] = variable
-            index_to_ref[variable_index] = variable
-        elif is_state_leaf(value):
-          if isinstance(value, VariableState):
-            value = value.to_variable()
-          children[key] = value
+            index_to_ref[leaf_index] = variable
         else:
-          raise RuntimeError
+          raise RuntimeError(f'Unknown key: {key!r}, this is a bug.')
 
     return children
 
   if isinstance(node_impl, GraphNodeImpl):
     # we create an empty node first and add it to the index
     # this avoids infinite recursion when there is a reference cycle
     if idxmap is not None and nodedef.index in idxmap:
       node = idxmap[nodedef.index]
       if type(node) != nodedef.type:
         raise ValueError(
           f'Expected a node of type {nodedef.type} for index '
           f'{nodedef.index}, but got a node of type {type(node)}.'
         )
-      node_impl.clear(node, nodedef.metadata)
+      node_impl.clear(node)
     else:
       node = node_impl.create_empty(nodedef.metadata)
     index_to_ref[nodedef.index] = node
     children = _get_children()
     node_impl.init(node, tuple(children.items()))
   else:
     # if the node type does not support the creation of an empty object it means
@@ -645,22 +670,22 @@
           raise ValueError(
             f'Cannot pop key {name!r} from node of type {type(node).__name__}'
           )
         id_to_index[id(value)] = len(id_to_index)
         node_impl.pop_key(node, name)
         if isinstance(value, Variable):
           value = value.to_state()
-        state[node_path] = value
+        state[node_path] = value  # type: ignore[index] # mypy is wrong here?
         break
     else:
       # NOTE: should we raise an error here?
       pass
 
 
-def _graph_update_dynamic(node: tp.Any, state: dict[Key, tp.Any]):
+def _graph_update_dynamic(node: tp.Any, state: tp.Mapping[Key, tp.Any]):
   if not is_node(node):
     raise RuntimeError(f'Unsupported type: {type(node)}')
 
   node_impl = get_node_impl(node)
   node_dict = node_impl.node_dict(node)
   for key, value in state.items():
     # case 1: new state is being added
@@ -737,15 +762,15 @@
       f'but got {type(updates).__name__!r}'
     )
   if not is_node(node):
     raise ValueError(f'Unsupported node type: {type(node)}')
 
   if id(updates) in cache:
     if cache[id(updates)] != status:
-      str_path = '/'.join(path)
+      str_path = '/'.join(str(p) for p in path)
       if status is _StaticModuleStatus.NEW:
         raise ValueError(
           f'Trying to add a new node at path {str_path!r} but a'
           ' node with the same reference has been updated'
         )
       else:
         raise ValueError(
@@ -804,144 +829,410 @@
           f'type {type(node).__name__}. Current value is {node_dict[name]!r}, '
           f'new value is {value_updates!r}.'
         )
 
       node_impl.set_key(node, name, value_updates)
 
 
+# --------------------------------------------------------
+# UpdateContext
+# --------------------------------------------------------
+
+
+# --------------------------------------------------------
+# UpdateContext
+# --------------------------------------------------------
+
+
 @dataclasses.dataclass
 class UpdateContext:
-  refmap: RefMap[tp.Any, Index] | None = None
-  idxmap: dict[Index, tp.Any] | None = None
-
-  # define context manager to clean up refmap and idxmap
-  # on exit
-  def __enter__(self):
-    return self
+  """A context manager for handling complex state updates."""
 
-  def __exit__(self, *args, **kwargs):
-    self.refmap = None
-    self.idxmap = None
+  tag: str
+  refmap: RefMap[tp.Any, Index] | None
+  idxmap: dict[Index, tp.Any] | None
 
   # define hash and eq to make this an opaque object
   def __hash__(self):
     return 0
 
   def __eq__(self, other):
     return isinstance(other, UpdateContext)
 
   @tp.overload
-  def split(self, graph_node: A, /) -> tuple[GraphDef[A], State]:
-    ...
+  def split(self, graph_node: A, /) -> tuple[GraphDef[A], State]: ...
 
   @tp.overload
   def split(
     self, graph_node: A, first: filterlib.Filter, /
-  ) -> tuple[GraphDef[A], State]:
-    ...
+  ) -> tuple[GraphDef[A], State]: ...
 
   @tp.overload
   def split(
     self,
     graph_node: A,
     first: filterlib.Filter,
     second: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]:
-    ...
+  ) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]: ...
 
   def split(
     self, node: A, *filters: filterlib.Filter
   ) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]:
+    """Split a graph node into a :class:`GraphDef` and one or more :class:`State`s. State is
+    a ``Mapping`` from strings or integers to ``Variables``, Arrays or nested States. GraphDef
+    contains all the static information needed to reconstruct a ``Module`` graph, it is analogous
+    to JAXs ``PyTreeDef``. :func:`split` is used in conjunction with :func:`merge` to switch
+    seamlessly between stateful and stateless representations of the graph.
+
+    Example usage::
+
+      >>> from flax.experimental import nnx
+      >>> import jax, jax.numpy as jnp
+      ...
+      >>> class Foo(nnx.Module):
+      ...   def __init__(self, rngs):
+      ...     self.batch_norm = nnx.BatchNorm(2, rngs=rngs)
+      ...     self.linear = nnx.Linear(2, 3, rngs=rngs)
+      ...
+      >>> node = Foo(nnx.Rngs(0))
+      >>> graphdef, params, batch_stats = nnx.split(node, nnx.Param, nnx.BatchStat)
+      ...
+      >>> jax.tree.map(jnp.shape, params)
+      State({
+        'batch_norm': {
+          'bias': VariableState(
+            type=Param,
+            value=(2,)
+          ),
+          'scale': VariableState(
+            type=Param,
+            value=(2,)
+          )
+        },
+        'linear': {
+          'bias': VariableState(
+            type=Param,
+            value=(3,)
+          ),
+          'kernel': VariableState(
+            type=Param,
+            value=(2, 3)
+          )
+        }
+      })
+      >>> jax.tree.map(jnp.shape, batch_stats)
+      State({
+        'batch_norm': {
+          'mean': VariableState(
+            type=BatchStat,
+            value=(2,)
+          ),
+          'var': VariableState(
+            type=BatchStat,
+            value=(2,)
+          )
+        }
+      })
+
+    Arguments:
+      node: graph node to split.
+      filters: some optional filters to group the state into mutually exclusive substates.
+    Returns:
+      ``GraphDef`` and one or more ``States`` equal to the number of filters passed. If no
+      filters are passed, a single ``State`` is returned.
+    """
     if self.refmap is not None and self.idxmap is None:
       raise ValueError(
         "'merge' was not called in-between the first and second call to 'split'"
       )
     graphdef, state, refmap = flatten(node, idxmap=self.idxmap)
 
+    states: State | tuple[State, ...]
     if len(filters) == 0:
       states = (state,)
     elif len(filters) == 1:
       states = (state.split(filters[0]),)
     else:
       states = state.split(filters[0], filters[1], *filters[2:])
 
     if self.refmap is None:
       self.refmap = refmap
 
+    if graphdef.index_mapping is not None:
+      # clear idxmap to remove any references to tracers
+      self.idxmap = None
+
     return graphdef, states[0], *states[1:]
 
   def merge(
     self,
     graphdef: GraphDef[A],
     state: State,
     *states: State,
   ) -> A:
-    # TODO: add docstring of example usage
-    if states:
-      state = State.merge(state, *states)
-
-    node, self.idxmap = unflatten(graphdef, state)
-    return node
-
-  def update(
-    self,
-    new_graphdef: GraphDef[A],
-    state: State,
-    /,
-    *states: State,
-  ):
+    """merge"""
     if self.refmap is None:
       raise ValueError('Cannot update a graphdef without refmap.')
-    if new_graphdef.index_mapping is None:
-      raise ValueError('Cannot update a graphdef without index_mapping.')
 
     if states:
       state = State.merge(state, *states)
 
-    index_to_ref = compose_mapping_reversed(
-      self.refmap, new_graphdef.index_mapping
-    )
-    return unflatten(new_graphdef, state, idxmap=index_to_ref)[0]
+    if graphdef.index_mapping is None:
+      node, self.idxmap = unflatten(graphdef, state)
+    else:
+      index_to_ref = compose_mapping_reversed(
+        self.refmap, graphdef.index_mapping
+      )
+      node, _idxmap = unflatten(graphdef, state, idxmap=index_to_ref)
+      # clear references
+      self.refmap = None
+      self.idxmap = None
+
+    return node
 
 
 jax.tree_util.register_static(UpdateContext)
 
 
+@dataclasses.dataclass
+class UpdateContextManager:
+  tag: str
+
+  def __enter__(self):
+    ctx = UpdateContext(self.tag, None, None)
+    GRAPH_CONTEXT.update_context_stacks[self.tag].append(ctx)
+    return ctx
+
+  def __exit__(self, *args):
+    stack = GRAPH_CONTEXT.update_context_stacks[self.tag]
+    if not stack:
+      raise RuntimeError(
+          f'No update context found for tag {self.tag!r}, this is a bug.'
+      )
+
+    ctx = GRAPH_CONTEXT.update_context_stacks[self.tag].pop()
+    # clear references
+    ctx.refmap = None
+    ctx.idxmap = None
+
+  def __call__(self, f: F) -> F:
+    @functools.wraps(f)
+    def update_context_manager_wrapper(*args, **kwargs):
+      with self:
+        return f(*args, **kwargs)
+
+    return update_context_manager_wrapper  # type: ignore
+
+
+def update_context(tag: str):
+  """Creates an :class:`UpdateContext` context manager which can be used to handle
+  more complex state updates beyond what ``nnx.update`` can handle, including
+  updates to static properties and graph structure.
+
+  UpdateContext exposes a ``split`` and ``merge`` API with the same
+  signature as ``nnx.split`` / ``nnx.merge`` but performs some bookkeeping
+  to have the necessary information in order to perfectly update the input
+  objects based on the changes made inside the transform. The UpdateContext
+  must call split and merge a total of 4 times, the first
+  and last calls happen outside the transform and the second and third calls
+  happen inside the transform as shown in the diagram below::
+
+
+                          idxmap
+    (2) merge  split (3)
+                                              
+                         inside               
+          . . . . . . . . . . . . . . . . . .  index_mapping
+                         outside              
+                                              
+    (1) split merge (4)
+                          refmap
+
+
+  The first call to split ``(1)`` creates a ``refmap`` which keeps track of the
+  outer references, and the first call to merge ``(2)`` creates an ``idxmap`` which
+  keeps track of the inner references. The second call to split ``(3)`` combines
+  the refmap and idxmap to produce the ``index_mapping`` which indicates
+  how the outer references map to the inner references. Finally, the last call to
+  merge ``(4)`` uses the index_mapping and the refmap to reconstruct the
+  output of the transform while reusing/updating the inner references. To avoid
+  memory leaks, the idxmap is cleared after ``(3)`` and the refmap is
+  cleared after ``(4)``, and both are cleared after the context manager exits.
+
+  Here is a simple example showing the use of ``update_context``::
+
+    >>> from flax import nnx
+    ...
+    >>> m1 = nnx.Dict({})
+    >>> with nnx.update_context('example') as ctx:
+    ...   graphdef, state = ctx.split(m1)
+    ...   @jax.jit
+    ...   def f(graphdef, state):
+    ...     m2 = ctx.merge(graphdef, state)
+    ...     m2.a = 1
+    ...     m2.ref = m2  # create a reference cycle
+    ...     return ctx.split(m2)
+    ...   graphdef_out, state_out = f(graphdef, state)
+    ...   m3 = ctx.merge(graphdef_out, state_out)
+    ...
+    >>> assert m1 is m3
+    >>> assert m1.a == 1
+    >>> assert m1.ref is m1
+
+  Note that ``update_context`` takes in a ``tag`` argument which is used
+  primarily as a safety mechanism reduce the risk of accidentally using the
+  wrong UpdateContext when using :func:`current_update_context` to access the
+  current active context. current_update_context can be used as a way of
+  accessing the current active context without having to pass it as a capture::
+
+    >>> from flax import nnx
+    ...
+    >>> m1 = nnx.Dict({})
+    >>> @jax.jit
+    ... def f(graphdef, state):
+    ...   ctx = nnx.current_update_context('example')
+    ...   m2 = ctx.merge(graphdef, state)
+    ...   m2.a = 1     # insert static attribute
+    ...   m2.ref = m2  # create a reference cycle
+    ...   return ctx.split(m2)
+    ...
+    >>> @nnx.update_context('example')
+    ... def g(m1):
+    ...   ctx = nnx.current_update_context('example')
+    ...   graphdef, state = ctx.split(m1)
+    ...   graphdef_out, state_out = f(graphdef, state)
+    ...   return ctx.merge(graphdef_out, state_out)
+    ...
+    >>> m3 = g(m1)
+    >>> assert m1 is m3
+    >>> assert m1.a == 1
+    >>> assert m1.ref is m1
+
+  As shown in the code above, ``update_context`` can also be used as a
+  decorator that creates/activates an UpdateContext context for the
+  duration of the function. The context can be accessed using
+  :func:`current_update_context`.
+
+  Args:
+    tag: A string tag to identify the context.
+  """
+  return UpdateContextManager(tag)
+
+
+def current_update_context(tag: str) -> UpdateContext:
+  """Returns the current active :class:`UpdateContext` for the given tag."""
+  stack = GRAPH_CONTEXT.update_context_stacks[tag]
+  if not stack:
+    raise ValueError(f'No update context found for tag {tag!r}.')
+  return stack[-1]
+
+
+# --------------------------------------------------------
+# Functional API
+# --------------------------------------------------------
+
+
 @tp.overload
-def split(graph_node: A, /) -> tuple[GraphDef[A], State]:
-  ...
+def split(graph_node: A, /) -> tuple[GraphDef[A], State]: ...
 
 
 @tp.overload
 def split(
   graph_node: A,
   first: filterlib.Filter,
   /,
-) -> tuple[GraphDef[A], State]:
-  ...
+) -> tuple[GraphDef[A], State]: ...
 
 
 @tp.overload
 def split(
   graph_node: A,
   first: filterlib.Filter,
   second: filterlib.Filter,
   /,
   *filters: filterlib.Filter,
-) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]:
-  ...
+) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]: ...
 
 
 def split(
   node: A, *filters: filterlib.Filter
 ) -> tuple[GraphDef[A], State, tpe.Unpack[tuple[State, ...]]]:
+  """Split a graph node into a :class:`GraphDef` and one or more :class:`State`s. State is
+  a ``Mapping`` from strings or integers to ``Variables``, Arrays or nested States. GraphDef
+  contains all the static information needed to reconstruct a ``Module`` graph, it is analogous
+  to JAXs ``PyTreeDef``. :func:`split` is used in conjunction with :func:`merge` to switch
+  seamlessly between stateful and stateless representations of the graph.
+
+  Example usage::
+
+    >>> from flax.experimental import nnx
+    >>> import jax, jax.numpy as jnp
+    ...
+    >>> class Foo(nnx.Module):
+    ...   def __init__(self, rngs):
+    ...     self.batch_norm = nnx.BatchNorm(2, rngs=rngs)
+    ...     self.linear = nnx.Linear(2, 3, rngs=rngs)
+    ...
+    >>> node = Foo(nnx.Rngs(0))
+    >>> graphdef, params, batch_stats = nnx.split(node, nnx.Param, nnx.BatchStat)
+    ...
+    >>> jax.tree.map(jnp.shape, params)
+    State({
+      'batch_norm': {
+        'bias': VariableState(
+          type=Param,
+          value=(2,)
+        ),
+        'scale': VariableState(
+          type=Param,
+          value=(2,)
+        )
+      },
+      'linear': {
+        'bias': VariableState(
+          type=Param,
+          value=(3,)
+        ),
+        'kernel': VariableState(
+          type=Param,
+          value=(2, 3)
+        )
+      }
+    })
+    >>> jax.tree.map(jnp.shape, batch_stats)
+    State({
+      'batch_norm': {
+        'mean': VariableState(
+          type=BatchStat,
+          value=(2,)
+        ),
+        'var': VariableState(
+          type=BatchStat,
+          value=(2,)
+        )
+      }
+    })
+
+  :func:`split` and :func:`merge` are primarily used to interact directly with JAX
+  transformations, see
+  `Functional API <https://flax.readthedocs.io/en/latest/nnx/nnx_basics.html#the-functional-api>`__
+  for more information.
+
+  Arguments:
+    node: graph node to split.
+    filters: some optional filters to group the state into mutually exclusive substates.
+  Returns:
+    ``GraphDef`` and one or more ``States`` equal to the number of filters passed. If no
+    filters are passed, a single ``State`` is returned.
+  """
   graphdef, state, _ = flatten(node)
 
+  states: State | tuple[State, ...]
   if len(filters) == 0:
     states = (state,)
   elif len(filters) == 1:
     states = (state.split(filters[0]),)
   else:
     states = state.split(filters[0], filters[1], *filters[2:])
 
@@ -950,14 +1241,47 @@
 
 def merge(
   graphdef: GraphDef[A],
   state: State,
   /,
   *states: State,
 ) -> A:
+  """The inverse of :func:`split`.
+
+  ``merge`` takes a :class:`GraphDef` and one or more :class:`State`s and creates
+  a new node with the same structure as the original node.
+
+  Example usage::
+
+    >>> from flax.experimental import nnx
+    >>> import jax, jax.numpy as jnp
+    ...
+    >>> class Foo(nnx.Module):
+    ...   def __init__(self, rngs):
+    ...     self.batch_norm = nnx.BatchNorm(2, rngs=rngs)
+    ...     self.linear = nnx.Linear(2, 3, rngs=rngs)
+    ...
+    >>> node = Foo(nnx.Rngs(0))
+    >>> graphdef, params, batch_stats = nnx.split(node, nnx.Param, nnx.BatchStat)
+    ...
+    >>> new_node = nnx.merge(graphdef, params, batch_stats)
+    >>> assert isinstance(new_node, Foo)
+    >>> assert isinstance(new_node.batch_norm, nnx.BatchNorm)
+    >>> assert isinstance(new_node.linear, nnx.Linear)
+
+  :func:`split` and :func:`merge` are primarily used to interact directly with JAX
+  transformations, see
+  `Functional API <https://flax.readthedocs.io/en/latest/nnx/nnx_basics.html#the-functional-api>`__
+  for more information.
+
+  Args:
+    graphdef: A :class:`GraphDef` object.
+    state: A :class:`State` object.
+    states: Additional :class:`State` objects.
+  """
   if states:
     state = State.merge(state, *states)
 
   node, _ = unflatten(graphdef, state)
   return node
 
 
@@ -965,73 +1289,69 @@
   if states:
     state = State.merge(state, *states)
 
   _graph_update_dynamic(node, state.raw_mapping)
 
 
 @tp.overload
-def state(node, /) -> State:
-  ...
+def state(node, /) -> State: ...
 
 
 @tp.overload
-def state(node, first: filterlib.Filter, /) -> State:
-  ...
+def state(node, first: filterlib.Filter, /) -> State: ...
 
 
 @tp.overload
 def state(
   node,
   first: filterlib.Filter,
   second: filterlib.Filter,
   /,
   *filters: filterlib.Filter,
-) -> tuple[State, ...]:
-  ...
+) -> tuple[State, ...]: ...
 
 
 def state(
   node,
   *filters: filterlib.Filter,
 ) -> tp.Union[State, tuple[State, ...]]:
   state = flatten(node)[1]
 
+  states: State | tuple[State, ...]
   if len(filters) == 0:
     states = state
   elif len(filters) == 1:
     states = state.filter(filters[0])
   else:
-    states = state.filter(filters[0], filters[1], *filters[1:])
+    states = state.filter(filters[0], filters[1], *filters[2:])
 
   return states
 
 
 def graphdef(node: tp.Any, /) -> GraphDef[tp.Any]:
   graphdef, _, _ = flatten(node)
   return graphdef
 
 
 @tp.overload
 def pop(
   node,
   filter: filterlib.Filter,
   /,
-) -> State:
-  ...
+) -> State: ...
 
 
 @tp.overload
 def pop(
   node,
   filter: filterlib.Filter,
   filter2: filterlib.Filter,
   /,
   *filters: filterlib.Filter,
-) -> tuple[State, ...]:
-  ...
+) -> tuple[State, ...]: ...
 
 
 def pop(node, *filters: filterlib.Filter) -> tp.Union[State, tuple[State, ...]]:
   if len(filters) == 0:
     raise ValueError('Expected at least one filter')
 
   id_to_index: dict[int, Index] = {}
@@ -1054,33 +1374,61 @@
 
 
 def clone(node: Node) -> Node:
   graphdef, state = split(node)
   return merge(graphdef, state)
 
 
-def iter_nodes(node: tp.Any, /) -> tp.Iterator[tuple[PathParts, tp.Any]]:
+def iter_graph(node: tp.Any, /) -> tp.Iterator[tuple[PathParts, tp.Any]]:
+  """Iterates over all nested nodes and leaves of a graph node, including the current node.
+
+  ``iter_graph`` creates a generator that yields path and value pairs, where
+  the path is a tuple of strings or integers representing the path to the value from the
+  root. Repeated nodes are visited only once. Leaves include static values.
+
+  Example::
+    >>> from flax import nnx
+    >>> import jax.numpy as jnp
+    ...
+    >>> class Linear(nnx.Module):
+    ...   def __init__(self, din, dout, *, rngs: nnx.Rngs):
+    ...     self.din, self.dout = din, dout
+    ...     self.w = nnx.Param(jax.random.uniform(rngs.next(), (din, dout)))
+    ...     self.b = nnx.Param(jnp.zeros((dout,)))
+    ...
+    >>> module = Linear(3, 4, rngs=nnx.Rngs(0))
+    >>> graph = [module, module]
+    ...
+    >>> for path, value in nnx.iter_graph(graph):
+    ...   print(path, type(value).__name__)
+    ...
+    (0, 'b') Param
+    (0, 'din') int
+    (0, 'dout') int
+    (0, 'w') Param
+    (0,) Linear
+    () list
+  """
   visited: set[int] = set()
   path_parts: PathParts = ()
-  yield from _iter_nodes(node, visited, path_parts)
+  yield from _iter_graph(node, visited, path_parts)
 
 
-def _iter_nodes(
+def _iter_graph(
   node: tp.Any, visited: set[int], path_parts: PathParts
 ) -> tp.Iterator[tuple[PathParts, tp.Any]]:
-  if not is_node(node):
-    return
-  if id(node) in visited:
-    return
-  visited.add(id(node))
+  if is_node(node):
+    if id(node) in visited:
+      return
+    visited.add(id(node))
+    node_dict = get_node_impl(node).node_dict(node)
+    for key, value in node_dict.items():
+      yield from _iter_graph(value, visited, (*path_parts, key))
+
   yield path_parts, node
-  node_impl = get_node_impl(node)
-  node_dict = node_impl.node_dict(node)
-  for key, value in node_dict.items():
-    yield from _iter_nodes(value, visited, (*path_parts, key))
 
 
 def compose_mapping(
   map_ab: tp.Mapping[A, B], map_bc: tp.Mapping[B, C], /
 ) -> dict[A, C]:
   return {a: map_bc[b] for a, b in map_ab.items() if b in map_bc}
 
@@ -1143,188 +1491,33 @@
 
   return jax.tree_util.tree_map(
     _maybe_insert, pytree, is_leaf=lambda x: isinstance(x, GraphNodeIndex)
   )
 
 
 # ---------------------------------------------------------
-# GraphNode
-# ---------------------------------------------------------
-
-
-class ModuleState(reprlib.Representable):
-  __slots__ = ('_trace_state', '_initializing')
-
-  def __init__(self, initializing: bool = False):
-    self._trace_state = tracers.TraceState()
-    self._initializing = initializing
-
-  @property
-  def trace_state(self) -> tracers.TraceState:
-    return self._trace_state
-
-  @property
-  def initializing(self) -> bool:
-    return self._initializing
-
-  def __nnx_repr__(self):
-    yield reprlib.Object(type(self))
-    yield reprlib.Attr('trace_state', self._trace_state)
-
-
-class GraphNodeMeta(ABCMeta):
-  if not tp.TYPE_CHECKING:
-
-    def __call__(cls, *args: Any, **kwargs: Any) -> Any:
-      return _graph_node_meta_call(cls, *args, **kwargs)
-
-
-def _graph_node_meta_call(cls: tp.Type[G], *args, **kwargs) -> G:
-  node = cls.__new__(cls, *args, **kwargs)
-  vars(node)['_graph_node__state'] = ModuleState()
-  node.__init__(*args, **kwargs)
-
-  return node
-
-@dataclasses.dataclass(frozen=True, repr=False)
-class Array:
-  shape: tp.Tuple[int, ...]
-  dtype: tp.Any
-
-  def __repr__(self):
-    return f'Array(shape={self.shape}, dtype={self.dtype.name})'
-
-
-class GraphNode(reprlib.Representable, metaclass=GraphNodeMeta):
-  if tp.TYPE_CHECKING:
-    _graph_node__state: ModuleState
-
-  def __init_subclass__(cls) -> None:
-    super().__init_subclass__()
-
-    register_graph_node_type(
-      type=cls,
-      flatten=cls._graph_node_flatten,
-      set_key=cls._graph_node_set_key,
-      pop_key=cls._graph_node_pop_key,
-      create_empty=cls._graph_node_create_empty,
-      clear=cls._graph_node_clear,
-    )
-
-  if not tp.TYPE_CHECKING:
-
-    def __setattr__(self, name: str, value: Any) -> None:
-      self._setattr(name, value)
-
-  def _setattr(self, name: str, value: tp.Any) -> None:
-    self.check_valid_context(
-      f"Cannot mutate '{type(self).__name__}' from different trace level"
-    )
-    object.__setattr__(self, name, value)
-
-  def check_valid_context(self, error_msg: str) -> None:
-    if not self._graph_node__state.trace_state.is_valid():
-      raise errors.TraceContextError(error_msg)
-
-  def __deepcopy__(self: G, memo=None) -> G:
-    graphdef, state = split(self)
-    graphdef = deepcopy(graphdef)
-    state = deepcopy(state)
-    return merge(graphdef, state)
-
-  def __nnx_repr__(self):
-    if CONTEXT.seen_modules_repr is None:
-      CONTEXT.seen_modules_repr = set()
-      clear_seen = True
-    else:
-      clear_seen = False
-
-    if id(self) in CONTEXT.seen_modules_repr:
-      yield reprlib.Object(type=type(self), empty_repr='...')
-      return
-
-    yield reprlib.Object(type=type(self))
-    CONTEXT.seen_modules_repr.add(id(self))
-
-    try:
-      for name, value in vars(self).items():
-        if name.startswith('_'):
-          continue
-
-        def to_shape_dtype(value):
-          if isinstance(value, Variable):
-            return value.replace(
-              raw_value=jax.tree.map(to_shape_dtype, value.raw_value)
-            )
-          elif isinstance(value, (np.ndarray, jax.Array)):
-            return Array(value.shape, value.dtype)
-          return value
-
-        value = jax.tree.map(to_shape_dtype, value)
-        yield reprlib.Attr(name, repr(value))
-    finally:
-      if clear_seen:
-        CONTEXT.seen_modules_repr = None
-
-  # Graph Definition
-  def _graph_node_flatten(self):
-    nodes = sorted(
-      (key, value)
-      for key, value in vars(self).items()
-      if key != '_graph_node__state'
-    )
-    return nodes, type(self)
-
-  def _graph_node_set_key(self, key: Key, value: tp.Any):
-    if not isinstance(key, str):
-      raise KeyError(f'Invalid key: {key!r}')
-    elif (
-      hasattr(self, key)
-      and isinstance(variable := getattr(self, key), Variable)
-      and isinstance(value, VariableState)
-    ):
-      variable.copy_from_state(value)
-    else:
-      setattr(self, key, value)
-
-  def _graph_node_pop_key(self, key: Key):
-    if not isinstance(key, str):
-      raise KeyError(f'Invalid key: {key!r}')
-    return vars(self).pop(key)
-
-  @staticmethod
-  def _graph_node_create_empty(node_type: tp.Type[G]) -> G:
-    node = object.__new__(node_type)
-    vars(node).update(_graph_node__state=ModuleState())
-    return node
-
-  def _graph_node_clear(self, cls: tp.Type[G]):
-    module_state = self._graph_node__state
-    module_vars = vars(self)
-    module_vars.clear()
-    module_vars['_graph_node__state'] = module_state
-
-
-# ---------------------------------------------------------
 # Pytree
 # ---------------------------------------------------------
-class PytreeType:
-  ...
+class PytreeType: ...
 
 
 def is_pytree_node(x: tp.Any) -> bool:
   return not jax.tree_util.all_leaves([x])
 
 
 def _key_path_to_key(key: tp.Any) -> Key:
   if isinstance(key, jax.tree_util.SequenceKey):
     return key.idx
   elif isinstance(
     key, (jax.tree_util.DictKey, jax.tree_util.FlattenedIndexKey)
   ):
+    if not isinstance(key.key, Key):
+      raise ValueError(
+        f'Invalid key: {key.key}. May be due to its type not being hashable or comparable.'
+      )
     return key.key
   elif isinstance(key, jax.tree_util.GetAttrKey):
     return key.name
   else:
     return str(key)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/helpers.py` & `flax-0.8.4/flax/nnx/nnx/helpers.py`

 * *Files 3% similar despite different names*

```diff
@@ -30,36 +30,34 @@
 import inspect
 import typing as tp
 
 import jax
 import jax.numpy as jnp
 import optax
 
-from flax.experimental.nnx.nnx.graph import Key
-from flax.experimental.nnx.nnx.module import GraphDef, Module
-from flax.experimental.nnx.nnx.proxy_caller import ApplyCaller
-from flax.experimental.nnx.nnx.rnglib import Rngs
-from flax.experimental.nnx.nnx.state import State
+from flax.nnx.nnx.graph import Key
+from flax.nnx.nnx.module import GraphDef, Module
+from flax.nnx.nnx.proxy_caller import ApplyCaller
+from flax.nnx.nnx.rnglib import Rngs
+from flax.nnx.nnx.state import State
 from flax.training.train_state import struct
 
 A = tp.TypeVar('A')
 M = tp.TypeVar('M', bound=Module)
 TS = tp.TypeVar('TS', bound='TrainState')
 
 
 class Dict(Module, tp.Mapping[str, A]):
   @tp.overload
-  def __init__(self, iterable: tp.Iterable[tp.Tuple[str, A]], /):
-    ...
+  def __init__(self, iterable: tp.Iterable[tp.Tuple[str, A]], /): ...
 
   @tp.overload
   def __init__(
     self, mapping: tp.Optional[tp.Mapping[str, A]] = None, /, **kwargs: A
-  ):
-    ...
+  ): ...
 
   def __init__(self, *args, **kwargs):
     for name, value in dict(*args, **kwargs).items():
       setattr(self, name, value)
 
   def __getitem__(self, key) -> A:
     return getattr(self, key)
@@ -70,15 +68,15 @@
   def __getattr__(self, key) -> A:
     return super().__getattribute__(key)
 
   def __setattr__(self, key, value):
     super().__setattr__(key, value)
 
   def __iter__(self) -> tp.Iterator[str]:
-    return (k for k in vars(self) if k != '_graph_node__state')
+    return (k for k in vars(self) if k != '_object__state')
 
   def __len__(self) -> int:
     return len(vars(self))
 
 
 class List(Module, tp.Generic[A]):
   def __init__(self, elems: tp.Iterable[A], /):
@@ -106,15 +104,15 @@
   def __len__(self) -> int:
     return self._length
 
   def _graph_node_flatten(self):
     nodes: list[tuple[Key, tp.Any]] = sorted(
       (int(key), value)
       for key, value in vars(self).items()
-      if key not in ('_graph_node__state', '_length')
+      if key not in ('_object__state', '_length')
     )
     nodes.append(('_length', self._length))
     return nodes, type(self)
 
   def _graph_node_set_key(self, key: Key, value: tp.Any):
     if isinstance(key, int):
       key = str(key)
@@ -122,23 +120,26 @@
 
   def _graph_node_pop_key(self, key: Key):
     if isinstance(key, int):
       key = str(key)
     return super()._graph_node_pop_key(key)
 
 
-class Sequential(List):
+class Sequential(Module):
+  def __init__(self, *fns: tp.Callable[..., tp.Any]):
+    self.layers = list(fns)
+
   def __call__(self, *args, rngs: tp.Optional[Rngs] = None, **kwargs) -> tp.Any:
     output: tp.Any = None
 
-    for i, f in enumerate(self):
+    for i, f in enumerate(self.layers):
       if not callable(f):
         raise TypeError(f'Sequence[{i}] is not callable: {f}')
       if i > 0:
-        if isinstance(output, tp.Tuple):
+        if isinstance(output, tuple):
           args = output
           kwargs = {}
         elif isinstance(output, dict):
           args = ()
           kwargs = output
         else:
           args = (output,)
@@ -150,48 +151,46 @@
 
     return output
 
 
 class ModuleDefApply(tp.Protocol, tp.Generic[M]):
   def __call__(
     self, state: State, *states: State
-  ) -> ApplyCaller[tuple[State, GraphDef[M]]]:
-    ...
+  ) -> ApplyCaller[tuple[State, GraphDef[M]]]: ...
 
 
 class TrainState(tp.Generic[M], struct.PyTreeNode):
   graphdef: GraphDef[M]
   params: State
-  tx: optax.GradientTransformation = struct.field(pytree_node=False)
   opt_state: optax.OptState
   step: jax.Array
+  tx: optax.GradientTransformation = struct.field(pytree_node=False)
 
   @classmethod
   def create(
     cls,
     graphdef: GraphDef[M],
     *,
     params: State,
     tx: optax.GradientTransformation,
     step: int = 0,
     **kwargs,
   ):
     return cls(
       graphdef=graphdef,
       params=params,
-      tx=tx,
       opt_state=tx.init(params),
       step=jnp.asarray(step),
+      tx=tx,
       **kwargs,
     )
 
   if tp.TYPE_CHECKING:
 
-    def __getattr__(self, key: str) -> tp.Any:
-      ...
+    def __getattr__(self, key: str) -> tp.Any: ...
 
   def apply(
     self, state: tp.Union[State, str], *states: tp.Union[State, str]
   ) -> ApplyCaller[tuple[GraphDef[M], State]]:
     states = (state, *states)
 
     _states: list[State] = []
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/ids.py` & `flax-0.8.4/flax/nnx/nnx/ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/module.py` & `flax-0.8.4/flax/nnx/nnx/module.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,68 +10,48 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from __future__ import annotations
 
-import dataclasses
 import typing as tp
 from functools import partial
 
 import jax.tree_util as jtu
 
-from flax.experimental.nnx.nnx import (
+from flax.nnx.nnx import (
   filterlib,
   graph,
 )
-from flax.experimental.nnx.nnx import variables as variableslib
-from flax.experimental.nnx.nnx.graph import GraphDef, GraphNode, GraphNodeMeta
-from flax.experimental.nnx.nnx.proxy_caller import (
-  CallableProxy,
-  DelayedAccessor,
-)
-from flax.experimental.nnx.nnx.state import State
-from flax.experimental.nnx.nnx.variables import Variable
+from flax.nnx.nnx import variables as variableslib
+from flax.nnx.nnx.graph import GraphDef
+from flax.nnx.nnx.object import Object, ObjectMeta
+from flax.nnx.nnx.state import State, StateLeaf
 from flax.typing import Path, PathParts
 
 A = tp.TypeVar('A')
 B = tp.TypeVar('B')
 M = tp.TypeVar('M', bound='Module')
 S = tp.TypeVar('S', bound=tp.Union[State, tuple[State, ...]])
 V = tp.TypeVar('V', bound=variableslib.Variable[tp.Any])
+F = tp.TypeVar('F', bound=tp.Callable[..., tp.Any])
 
 StateMapping = tp.Mapping[Path, tp.Any]
 tuple_reduce = lambda xs, x: xs + (x,)
 tuple_init = lambda: ()
 
-@tp.runtime_checkable
-class _HasSetup(tp.Protocol):
-  def setup(self) -> None:
-    ...
-
-
-class ModuleMeta(GraphNodeMeta):
-  if not tp.TYPE_CHECKING:
-
-    def __call__(cls, *args: Any, **kwargs: Any) -> Any:
-      return _module_meta_call(cls, *args, **kwargs)
-
-
-def _module_meta_call(cls: tp.Type[M], *args, **kwargs) -> M:
-  module: M = GraphNodeMeta.__call__(cls, *args, **kwargs)
-
-  if dataclasses.is_dataclass(module):
-    if isinstance(module, _HasSetup):
-      module.setup()
 
-  return module
+class ModuleMeta(ObjectMeta):
+  # we keep a trivial derived class just in case we need to
+  # add more functionality in the future
+  pass
 
 
-class Module(graph.GraphNode, metaclass=ModuleMeta):
+class Module(Object, metaclass=ModuleMeta):
   """"""
 
   def sow(
     self,
     variable_type: tp.Type[variableslib.Variable[tp.Any]],
     name: str,
     value: A,
@@ -90,124 +70,57 @@
           f"got '{type(variable).__name__}'"
         )
       variable.raw_value = reduce_fn(variable.raw_value, value)
     else:
       reduced_value = reduce_fn(init_fn(), value)
       setattr(self, name, variable_type(reduced_value))
 
-  @property
-  def init(self: M) -> M:
-    """Calls a method in initialization mode.
-
-    When a method is called using ``init``, the ``is_initializing`` method
-    will return ``True``. This is useful to implement Modules that support
-    lazy initialization.
-
-    Example::
-
-      >>> from flax.experimental import nnx
-      >>> import jax
-      >>> import jax.numpy as jnp
-      ...
-      >>> class Linear(nnx.Module):
-      ...   def __init__(self, dout, rngs: nnx.Rngs):
-      ...     self.dout = dout
-      ...     self.rngs = rngs
-      ...
-      ...   def __call__(self, x):
-      ...     if self.is_initializing():
-      ...       din = x.shape[-1]
-      ...       if not hasattr(self, 'w'):
-      ...         key = self.rngs.params()
-      ...         self.w = nnx.Param(jax.random.uniform(key, (din, self.dout)))
-      ...       if not hasattr(self, 'b'):
-      ...         self.b = nnx.Param(jnp.zeros((self.dout,)))
-      ...
-      ...     return x @ self.w + self.b
-      ...
-      >>> linear = Linear(3, nnx.Rngs(0))
-      >>> x = jnp.ones((5, 2))
-      >>> y = linear.init(x)
-      >>> linear.w.value.shape
-      (2, 3)
-      >>> linear.b.value.shape
-      (3,)
-      >>> y.shape
-      (5, 3)
-    """
-
-    def _init_context(accessor: DelayedAccessor, *args, **kwargs):
-      for _, value in graph.iter_nodes(self):
-        if isinstance(value, GraphNode):
-          value._graph_node__state._initializing = True
-
-      method = accessor(self)
-      try:
-        out = method(*args, **kwargs)
-      finally:
-        for _, value in graph.iter_nodes(self):
-          if isinstance(value, GraphNode):
-            value._graph_node__state._initializing = False
-
-      return out
-
-    return CallableProxy(_init_context)  # type: ignore
-
-  def is_initializing(self) -> bool:
-    """Returns whether the Module is initializing.
-
-    ``is_initializing`` returns ``True`` if the Module is currently being run
-    under ``init``.
-    """
-
-    return self._graph_node__state._initializing
-
   def iter_modules(self) -> tp.Iterator[tuple[PathParts, Module]]:
     """Iterates over all nested Modules of the current Module, including the current Module.
 
     ``iter_modules`` creates a generator that yields the path and the Module instance, where
     the path is a tuple of strings or integers representing the path to the Module from the
     root Module.
 
     Example::
 
-      >>> from flax.experimental import nnx
+      >>> from flax import nnx
       ...
       >>> class Block(nnx.Module):
       ...   def __init__(self, din, dout, *, rngs: nnx.Rngs):
       ...     self.linear = nnx.Linear(din, dout, rngs=rngs)
       ...     self.dropout = nnx.Dropout(0.5)
       ...     self.batch_norm = nnx.BatchNorm(10, rngs=rngs)
       ...
       ...
       >>> model = Block(2, 5, rngs=nnx.Rngs(0))
       >>> for path, module in model.iter_modules():
       ...   print(path, type(module).__name__)
       ...
-      () Block
       ('batch_norm',) BatchNorm
       ('dropout',) Dropout
       ('linear',) Linear
+      () Block
     """
-    for path, value in graph.iter_nodes(self):
+    for path, value in graph.iter_graph(self):
       if isinstance(value, Module):
         yield path, value
 
   def set_attributes(
     self,
     *filters: filterlib.Filter,
     raise_if_not_found: bool = True,
     **attributes: tp.Any,
   ) -> None:
     """Sets the attributes of nested Modules including the current Module.
     If the attribute is not found in the Module, it is ignored.
 
     Example::
 
-      >>> from flax.experimental import nnx
+      >>> from flax import nnx
       ...
       >>> class Block(nnx.Module):
       ...   def __init__(self, din, dout, *, rngs: nnx.Rngs):
       ...     self.linear = nnx.Linear(din, dout, rngs=rngs)
       ...     self.dropout = nnx.Dropout(0.5, deterministic=False)
       ...     self.batch_norm = nnx.BatchNorm(10, use_running_average=False, rngs=rngs)
       ...
@@ -257,15 +170,15 @@
     ``train`` uses ``set_attributes`` to recursively set attributes ``deterministic=False``
     and ``use_running_average=False`` of all nested Modules that have these attributes.
     Its primarily used to control the runtime behavior of the ``Dropout`` and ``BatchNorm``
     Modules.
 
     Example::
 
-      >>> from flax.experimental import nnx
+      >>> from flax import nnx
       ...
       >>> class Block(nnx.Module):
       ...   def __init__(self, din, dout, *, rngs: nnx.Rngs):
       ...     self.linear = nnx.Linear(din, dout, rngs=rngs)
       ...     # initialize Dropout and BatchNorm in eval mode
       ...     self.dropout = nnx.Dropout(0.5, deterministic=True)
       ...     self.batch_norm = nnx.BatchNorm(10, use_running_average=True, rngs=rngs)
@@ -293,15 +206,15 @@
     ``eval`` uses ``set_attributes`` to recursively set attributes ``deterministic=True``
     and ``use_running_average=True`` of all nested Modules that have these attributes.
     Its primarily used to control the runtime behavior of the ``Dropout`` and ``BatchNorm``
     Modules.
 
     Example::
 
-      >>> from flax.experimental import nnx
+      >>> from flax import nnx
       ...
       >>> class Block(nnx.Module):
       ...   def __init__(self, din, dout, *, rngs: nnx.Rngs):
       ...     self.linear = nnx.Linear(din, dout, rngs=rngs)
       ...     self.dropout = nnx.Dropout(0.5)
       ...     self.batch_norm = nnx.BatchNorm(10, rngs=rngs)
       ...
@@ -325,38 +238,39 @@
   def __init_subclass__(cls, experimental_pytree: bool = False) -> None:
     super().__init_subclass__()
 
     if experimental_pytree:
       jtu.register_pytree_with_keys(
         cls,
         partial(_module_flatten, with_keys=True),
-        _module_unflatten,
+        _module_unflatten,  # type: ignore[arg-type]
         flatten_func=partial(_module_flatten, with_keys=False),
       )
 
 
 # -------------------------
 # Pytree Definition
 # -------------------------
 def _module_flatten(module: Module, *, with_keys: bool):
   graphdef, state = graph.split(module)
   key_values = sorted(state.raw_mapping.items())
   keys = tuple(key for key, _ in key_values)
 
+  children: tuple[tp.Any, ...]
   if with_keys:
     children = tuple((jtu.DictKey(key), value) for key, value in key_values)
   else:
     children = tuple(value for _, value in key_values)
 
   return children, (keys, graphdef)
 
 
 def _module_unflatten(
   paths_moduledef: tuple[tuple[Path, ...], GraphDef[M]],
-  variables: tuple[Variable[tp.Any], ...],
+  variables: tuple[StateLeaf, ...],
 ) -> M:
   paths, graphdef = paths_moduledef
   return graph.merge(graphdef, State(zip(paths, variables)))
 
 
 def first_from(*args: tp.Optional[A], error_msg: str) -> A:
   """Return the first non-None argument.
@@ -369,9 +283,7 @@
   Returns:
     The first non-None argument.
   """
   for arg in args:
     if arg is not None:
       return arg
   raise ValueError(error_msg)
-
-
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/__init__.py` & `flax-0.8.4/flax/nnx/nnx/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/activations.py` & `flax-0.8.4/flax/nnx/nnx/nn/activations.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/attention.py` & `flax-0.8.4/flax/nnx/nnx/nn/attention.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,39 +13,40 @@
 # limitations under the License.
 
 """Attention core modules for Flax."""
 
 from __future__ import annotations
 
 import functools
-from typing import Any, Callable, Optional, overload
+from typing import Any, Callable, Optional
 
 import jax
 import jax.numpy as jnp
 from jax import lax, random
 
-from flax.experimental import nnx
-from flax.experimental.nnx.nnx import rnglib
-from flax.experimental.nnx.nnx.module import Module, first_from
-from flax.experimental.nnx.nnx.nn import initializers
-from flax.experimental.nnx.nnx.nn.dtypes import promote_dtype
-from flax.experimental.nnx.nnx.nn.linear import (
+from flax import nnx
+from flax.nnx.nnx import rnglib
+from flax.nnx.nnx.module import Module, first_from
+from flax.nnx.nnx.nn import initializers
+from flax.nnx.nnx.nn.dtypes import promote_dtype
+from flax.nnx.nnx.nn.linear import (
   LinearGeneral,
   default_kernel_init,
 )
-from flax.experimental.nnx.nnx.nn.normalization import LayerNorm
+from flax.nnx.nnx.nn.normalization import LayerNorm
 from flax.typing import (
-  Array,
   Dtype,
   Shape,
   Initializer,
   PrecisionLike,
   DotGeneralT,
 )
 
+Array = jax.Array
+
 
 def dot_product_attention_weights(
   query: Array,
   key: Array,
   bias: Optional[Array] = None,
   mask: Optional[Array] = None,
   broadcast_dropout: bool = True,
@@ -84,15 +85,15 @@
     module: the Module that will sow the attention weights into the
       ``nnx.Intermediate`` collection. If ``module`` is None, the attention
       weights will not be sowed.
 
   Returns:
     Output of shape `[batch..., num_heads, q_length, kv_length]`.
   """
-  query, key = promote_dtype(query, key, dtype=dtype)
+  query, key = promote_dtype((query, key), dtype=dtype)  # type: ignore[bad-unpacking]
   dtype = query.dtype
 
   assert query.ndim == key.ndim, 'q, k must have same rank.'
   assert query.shape[:-3] == key.shape[:-3], 'q, k batch dims must match.'
   assert query.shape[-2] == key.shape[-2], 'q, k num_heads must match.'
   assert query.shape[-1] == key.shape[-1], 'q, k depths must match.'
 
@@ -180,15 +181,15 @@
     module: the Module that will sow the attention weights into the
       ``nnx.Intermediate`` collection. If ``module`` is None, the attention
       weights will not be sowed.
 
   Returns:
     Output of shape `[batch..., q_length, num_heads, v_depth_per_head]`.
   """
-  query, key, value = promote_dtype(query, key, value, dtype=dtype)
+  query, key, value = promote_dtype((query, key, value), dtype=dtype)  # type: ignore[bad-unpacking]
   dtype = query.dtype
   assert key.ndim == query.ndim == value.ndim, 'q, k, v must have same rank.'
   assert (
     query.shape[:-3] == key.shape[:-3] == value.shape[:-3]
   ), 'q, k, v batch dims must match.'
   assert (
     query.shape[-2] == key.shape[-2] == value.shape[-2]
@@ -277,15 +278,19 @@
     broadcast_dropout: bool: use a broadcasted dropout along batch dims.
     dropout_rate: dropout rate
     deterministic: if false, the attention weight is masked randomly using
       dropout, whereas if true, the attention weights are deterministic.
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer for the kernel of the Dense layers.
+    out_kernel_init: optional initializer for the kernel of the output Dense layer,
+      if None, the kernel_init is used.
     bias_init: initializer for the bias of the Dense layers.
+    out_bias_init: optional initializer for the bias of the output Dense layer,
+      if None, the bias_init is used.
     use_bias: bool: whether pointwise QKVO dense transforms use bias.
     attention_fn: dot_product_attention or compatible function. Accepts query,
       key, value, and returns output of shape `[bs, dim1, dim2, ..., dimN,,
       num_heads, value_channels]``
     decode: whether to prepare and use an autoregressive cache.
     normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).
   """
@@ -300,15 +305,17 @@
     dtype: Dtype | None = None,
     param_dtype: Dtype = jnp.float32,
     broadcast_dropout: bool = True,
     dropout_rate: float = 0.0,
     deterministic: bool | None = None,
     precision: PrecisionLike = None,
     kernel_init: Initializer = default_kernel_init,
+    out_kernel_init: Initializer | None = None,
     bias_init: Initializer = initializers.zeros_init(),
+    out_bias_init: Initializer | None = None,
     use_bias: bool = True,
     attention_fn: Callable[..., Array] = dot_product_attention,
     decode: bool | None = None,
     normalize_qk: bool = False,
     # Deprecated, will be removed.
     qkv_dot_general: DotGeneralT | None = None,
     out_dot_general: DotGeneralT | None = None,
@@ -327,15 +334,17 @@
     self.dtype = dtype
     self.param_dtype = param_dtype
     self.broadcast_dropout = broadcast_dropout
     self.dropout_rate = dropout_rate
     self.deterministic = deterministic
     self.precision = precision
     self.kernel_init = kernel_init
+    self.out_kernel_init = out_kernel_init
     self.bias_init = bias_init
+    self.out_bias_init = out_bias_init
     self.use_bias = use_bias
     self.attention_fn = attention_fn
     self.decode = decode
     self.normalize_qk = normalize_qk
     self.qkv_dot_general = qkv_dot_general
     self.out_dot_general = out_dot_general
     self.qkv_dot_general_cls = qkv_dot_general_cls
@@ -364,14 +373,16 @@
     )
     # project inputs_q to multi-headed q/k/v
     # dimensions are then [batch..., length, n_heads, n_features_per_head]
     self.query = linear_general(rngs=rngs)
     self.key = linear_general(rngs=rngs)
     self.value = linear_general(rngs=rngs)
 
+    self.query_ln: LayerNorm | None
+    self.key_ln: LayerNorm | None
     if self.normalize_qk:
       # Normalizing query and key projections stabilizes training with higher
       # LR. See ViT-22B paper http://arxiv.org/abs/2302.05442 for analysis.
       self.query_ln = LayerNorm(
         self.head_dim,
         use_bias=False,
         dtype=self.dtype,
@@ -389,55 +400,29 @@
       self.query_ln = None
       self.key_ln = None
 
     self.out = LinearGeneral(
       in_features=(self.num_heads, self.head_dim),
       out_features=self.out_features,
       axis=(-2, -1),
-      kernel_init=self.kernel_init,
-      bias_init=self.bias_init,
+      kernel_init=self.out_kernel_init or self.kernel_init,
+      bias_init=self.out_bias_init or self.bias_init,
       use_bias=self.use_bias,
       dtype=self.dtype,
       param_dtype=self.param_dtype,
       precision=self.precision,
       dot_general=self.out_dot_general,
       dot_general_cls=self.out_dot_general_cls,
       rngs=rngs,
     )
+    self.rngs = rngs if dropout_rate > 0.0 else None
 
-  @overload
-  def __call__(
-    self,
-    inputs_q: Array,
-    inputs_k: Optional[Array] = None,
-    inputs_v: Optional[Array] = None,
-    *,
-    mask: Optional[Array] = None,
-    deterministic: Optional[bool] = None,
-    dropout_rng: Optional[Array] = None,
-    rngs: rnglib.Rngs | None = None,
-    sow_weights: bool = False,
-    decode: bool | None = None,
-  ):
-    ...
-
-  @overload
-  def __call__(
-    self,
-    inputs_q: Array,
-    *,
-    inputs_kv: Array | None = None,
-    mask: Array | None = None,
-    deterministic: bool | None = None,
-    dropout_rng: Array | None = None,
-    rngs: rnglib.Rngs | None = None,
-    sow_weights: bool = False,
-    decode: bool | None = None,
-  ):
-    ...
+    self.cached_key: nnx.Cache[Array] | None = None
+    self.cached_value: nnx.Cache[Array] | None = None
+    self.cache_index: nnx.Cache[Array] | None = None
 
   def __call__(
     self,
     inputs_q: Array,
     inputs_k: Array | None = None,
     inputs_v: Array | None = None,
     *,
@@ -472,14 +457,16 @@
         `dropout` key.
       sow_weights: if ``True``, the attention weights are sowed into the
         'intermediates' collection.
 
     Returns:
       output of shape `[batch_sizes..., length, features]`.
     """
+    if rngs is None:
+      rngs = self.rngs
 
     if inputs_k is None:
       if inputs_v is not None:
         raise ValueError(
           '`inputs_k` cannot be None if `inputs_v` is not None. '
           'To have both `inputs_k` and `inputs_v` be the same value, pass in the '
           'value to `inputs_k` and leave `inputs_v` as None.'
@@ -511,14 +498,22 @@
       decode,
       self.decode,
       error_msg="""No `decode` argument was provided to MultiHeadAttention
         as either a __call__ argument, class attribute, or nnx.flag.""",
     )
 
     if decode:
+      if (
+        self.cached_key is None
+        or self.cached_value is None
+        or self.cache_index is None
+      ):
+        raise ValueError(
+          'Autoregressive cache not initialized, call ``init_cache`` first.'
+        )
       (
         *batch_dims,
         max_length,
         num_heads,
         depth_per_head,
       ) = self.cached_key.value.shape
       # shape check of cached keys against query input
@@ -592,15 +587,15 @@
   def init_cache(self, input_shape: Shape, dtype: Dtype = jnp.float32):
     """Initializes cache for fast autoregressive decoding. When
     ``decode=True``, this method must be called first before performing
     forward inference.
 
     Example usage::
 
-      >>> from flax.experimental import nnx
+      >>> from flax import nnx
       >>> import jax.numpy as jnp
       ...
       >>> rngs = nnx.Rngs(42)
       ...
       >>> x = jnp.ones((1, 3))
       >>> model_nnx = nnx.MultiHeadAttention(
       ...   num_heads=2,
@@ -681,15 +676,17 @@
     idxs,
     jnp.greater_equal,
     extra_batch_dims=extra_batch_dims,
     dtype=dtype,
   )
 
 
-def combine_masks(*masks: Optional[Array], dtype: Dtype = jnp.float32) -> Array:
+def combine_masks(
+  *masks: Optional[Array], dtype: Dtype = jnp.float32
+) -> Array | None:
   """Combine attention masks.
 
   Args:
     *masks: set of attention mask arguments to combine, some can be None.
     dtype: dtype for the returned mask.
 
   Returns:
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/dtypes.py` & `flax-0.8.4/flax/nnx/nnx/nn/dtypes.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,20 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import typing as tp
 from typing import Optional
 from flax.typing import Dtype
 from jax import numpy as jnp
-from typing_extensions import TypeVarTuple, Unpack
 
-T = TypeVarTuple('T')
+T = tp.TypeVar('T', bound=tuple)
 
 
 def canonicalize_dtype(
   *args, dtype: Optional[Dtype] = None, inexact: bool = True
 ) -> Dtype:
   """Canonicalize an optional dtype to the definitive dtype.
 
@@ -48,17 +48,15 @@
     if inexact and not jnp.issubdtype(dtype, jnp.inexact):
       dtype = jnp.promote_types(jnp.float32, dtype)
   if inexact and not jnp.issubdtype(dtype, jnp.inexact):
     raise ValueError(f'Dtype must be inexact: {dtype}')
   return dtype
 
 
-def promote_dtype(
-  *args: Unpack[T], dtype=None, inexact=True
-) -> tuple[Unpack[T]]:
+def promote_dtype(args: T, /, *, dtype=None, inexact=True) -> T:
   """ "Promotes input arguments to a specified or inferred dtype.
 
   All args are cast to the same dtype. See ``canonicalize_dtype`` for how
   this dtype is determined.
 
   The behavior of promote_dtype is mostly a convinience wrapper around
   ``jax.numpy.promote_types``. The differences being that it automatically casts
@@ -75,8 +73,9 @@
     of `jnp.inexact`. Inexact dtypes are real or complex floating points. This
     is useful when you want to apply operations that don't work directly on
     integers like taking a mean for example.
   Returns:
     The arguments cast to arrays of the same dtype.
   """
   dtype = canonicalize_dtype(*args, dtype=dtype, inexact=inexact)
-  return tuple(jnp.asarray(x, dtype) if x is not None else None for x in args)
+  arrays = tuple(jnp.asarray(x, dtype) if x is not None else None for x in args)
+  return arrays  # type: ignore[return-value]
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/initializers.py` & `flax-0.8.4/flax/nnx/nnx/nn/initializers.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,28 +38,28 @@
 DtypeLikeInexact = tp.Any
 
 
 def zeros_init() -> Initializer:
   """Builds an initializer that returns a constant array full of zeros.
 
   >>> import jax, jax.numpy as jnp
-  >>> from flax.experimental.nnx import initializers
+  >>> from flax.nnx import initializers
   >>> zeros_initializer = initializers.zeros_init()
   >>> zeros_initializer(jax.random.key(42), (2, 3), jnp.float32)
   Array([[0., 0., 0.],
          [0., 0., 0.]], dtype=float32)
   """
   return zeros
 
 
 def ones_init() -> Initializer:
   """Builds an initializer that returns a constant array full of ones.
 
   >>> import jax, jax.numpy as jnp
-  >>> from flax.experimental.nnx import initializers
+  >>> from flax.nnx import initializers
   >>> ones_initializer = initializers.ones_init()
   >>> ones_initializer(jax.random.key(42), (3, 2), jnp.float32)
   Array([[1., 1.],
          [1., 1.],
          [1., 1.]], dtype=float32)
   """
   return ones
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/linear.py` & `flax-0.8.4/flax/nnx/nnx/nn/linear.py`

 * *Files 10% similar despite different names*

```diff
@@ -32,35 +32,36 @@
 import jax
 import jax.numpy as jnp
 import numpy as np
 from jax import lax
 import opt_einsum
 
 from flax.core.frozen_dict import FrozenDict
-from flax.experimental import nnx
-from flax.experimental.nnx.nnx import rnglib, variables
-from flax.experimental.nnx.nnx.module import Module, first_from
-from flax.experimental.nnx.nnx.nn import dtypes, initializers
+from flax import nnx
+from flax.nnx.nnx import rnglib, variables
+from flax.nnx.nnx.module import Module, first_from
+from flax.nnx.nnx.nn import dtypes, initializers
 from flax.typing import (
-  Array,
   Dtype,
   Shape,
   Initializer,
   PrecisionLike,
   DotGeneralT,
   ConvGeneralDilatedT,
   PaddingLike,
   LaxPadding,
 )
 
+Array = jax.Array
 Axis = int
 Size = int
 
 
 default_kernel_init = initializers.lecun_normal()
+default_bias_init = initializers.zeros_init()
 
 
 def canonicalize_padding(padding: PaddingLike, rank: int) -> LaxPadding:
   """ "Canonicalizes conv padding to a jax.lax supported format."""
   if isinstance(padding, str):
     return padding
   if isinstance(padding, int):
@@ -105,15 +106,15 @@
 
 
 class LinearGeneral(Module):
   """A linear transformation with flexible axes.
 
   Example usage::
 
-    >>> from flax.experimental import nnx
+    >>> from flax import nnx
     >>> import jax, jax.numpy as jnp
     ...
     >>> # equivalent to `nnx.Linear(2, 4)`
     >>> layer = nnx.LinearGeneral(2, 4, rngs=nnx.Rngs(0))
     >>> layer.kernel.value.shape
     (2, 4)
     >>> # output features (4, 5)
@@ -154,25 +155,25 @@
     *,
     axis: Axis | tp.Sequence[Axis] = -1,
     batch_axis: tp.Mapping[Axis, Size] = FrozenDict({}),
     use_bias: bool = True,
     dtype: Dtype | None = None,
     param_dtype: Dtype = jnp.float32,
     kernel_init: Initializer = default_kernel_init,
-    bias_init: Initializer = initializers.zeros_init(),
+    bias_init: Initializer = default_bias_init,
     precision: PrecisionLike = None,
     # Deprecated. Will be removed.
     dot_general: DotGeneralT | None = None,
     dot_general_cls: tp.Any = None,
     rngs: rnglib.Rngs,
   ):
     self.in_features = _canonicalize_tuple(in_features)
     self.out_features = _canonicalize_tuple(out_features)
     self.axis = _canonicalize_tuple(axis)
-    self.batch_axis = FrozenDict(batch_axis)
+    self.batch_axis = FrozenDict[Axis, Size](batch_axis)
     self.use_bias = use_bias
     self.dtype = dtype
     self.param_dtype = param_dtype
     self.kernel_init = kernel_init
     self.bias_init = bias_init
     self.precision = precision
     self.dot_general = dot_general
@@ -265,15 +266,15 @@
     kernel = self.kernel.value
     bias = self.bias.value
 
     batch_ind = tuple(range(n_batch_dims))
     contract_ind = tuple(range(n_batch_dims, n_axis + n_batch_dims))
 
     inputs, kernel, bias = dtypes.promote_dtype(
-      inputs, kernel, bias, dtype=self.dtype
+      (inputs, kernel, bias), dtype=self.dtype
     )
 
     if self.dot_general_cls is not None:
       dot_general = self.dot_general_cls()
     elif self.dot_general is not None:
       dot_general = self.dot_general
     else:
@@ -292,15 +293,16 @@
     return out
 
 
 class Linear(Module):
   """A linear transformation applied over the last dimension of the input.
 
   Attributes:
-    features: the number of output features.
+    in_features: the number of input features.
+    out_features: the number of output features.
     use_bias: whether to add a bias to the output (default: True).
     dtype: the dtype of the computation (default: infer from input and params).
     param_dtype: the dtype passed to parameter initializers (default: float32).
     precision: numerical precision of the computation see `jax.lax.Precision`
       for details.
     kernel_init: initializer function for the weight matrix.
     bias_init: initializer function for the bias.
@@ -312,15 +314,15 @@
     out_features: int,
     *,
     use_bias: bool = True,
     dtype: tp.Optional[Dtype] = None,
     param_dtype: Dtype = jnp.float32,
     precision: PrecisionLike = None,
     kernel_init: Initializer = default_kernel_init,
-    bias_init: Initializer = initializers.zeros_init(),
+    bias_init: Initializer = default_bias_init,
     dot_general: DotGeneralT = lax.dot_general,
     rngs: rnglib.Rngs,
   ):
     kernel_key = rngs.params()
     self.kernel = nnx.Param(
       kernel_init(kernel_key, (in_features, out_features), param_dtype)
     )
@@ -349,15 +351,15 @@
     Returns:
       The transformed input.
     """
     kernel = self.kernel.value
     bias = self.bias.value
 
     inputs, kernel, bias = dtypes.promote_dtype(
-      inputs, kernel, bias, dtype=self.dtype
+      (inputs, kernel, bias), dtype=self.dtype
     )
     y = self.dot_general(
       inputs,
       kernel,
       (((inputs.ndim - 1,), (0,)), ((), ())),
       precision=self.precision,
     )
@@ -367,15 +369,15 @@
 
 
 class Einsum(Module):
   """An einsum transformation with learnable kernel and bias.
 
   Example usage::
 
-    >>> from flax.experimental import nnx
+    >>> from flax import nnx
     >>> import jax.numpy as jnp
     ...
     >>> layer = nnx.Einsum('nta,hab->nthb', (8, 2, 4), (8, 4), rngs=nnx.Rngs(0))
     >>> layer.kernel.value.shape
     (8, 2, 4)
     >>> layer.bias.value.shape
     (8, 4)
@@ -406,23 +408,24 @@
     kernel_shape: Shape,
     bias_shape: tp.Optional[Shape] = None,
     *,
     dtype: tp.Optional[Dtype] = None,
     param_dtype: Dtype = jnp.float32,
     precision: PrecisionLike = None,
     kernel_init: Initializer = default_kernel_init,
-    bias_init: Initializer = initializers.zeros_init(),
+    bias_init: Initializer = default_bias_init,
     rngs: rnglib.Rngs,
   ):
     einsum_str = einsum_str.replace(' ', '')
     self._einsum_str_check(einsum_str)
 
     kernel_key = rngs.params()
     self.kernel = nnx.Param(kernel_init(kernel_key, kernel_shape, param_dtype))
 
+    self.bias: nnx.Param | None
     if bias_shape is not None:
       bias_key = rngs.params()
       self.bias = nnx.Param(bias_init(bias_key, bias_shape, param_dtype))
     else:
       self.bias = None
 
     self.einsum_str = einsum_str
@@ -456,17 +459,19 @@
       error_msg="""No `einsum_str` argument was provided to Einsum
         as either a __call__ argument, or class attribute.""",
     )
     einsum_str = einsum_str.replace(' ', '')
     self._einsum_str_check(einsum_str)
 
     inputs, kernel, bias = dtypes.promote_dtype(
-      inputs,
-      self.kernel.value,
-      self.bias.value if self.bias is not None else self.bias,
+      (
+        inputs,
+        self.kernel.value,
+        self.bias.value if self.bias is not None else self.bias,
+      ),
       dtype=self.dtype,
     )
 
     y = jnp.einsum(einsum_str, inputs, kernel, precision=self.precision)
 
     if bias is not None:
       broadcasted_bias_shape = self._infer_broadcasted_bias_shape(
@@ -572,15 +577,15 @@
     feature_group_count: int = 1,
     use_bias: bool = True,
     mask_fn: tp.Optional[tp.Callable[[Array], Array]] = None,
     dtype: tp.Optional[Dtype] = None,
     param_dtype: Dtype = jnp.float32,
     precision: PrecisionLike = None,
     kernel_init: Initializer = default_kernel_init,
-    bias_init: Initializer = initializers.zeros_init(),
+    bias_init: Initializer = default_bias_init,
     conv_general_dilated: ConvGeneralDilatedT = lax.conv_general_dilated,
     rngs: rnglib.Rngs,
   ):
     if isinstance(kernel_size, int):
       kernel_size = (kernel_size,)
     else:
       kernel_size = tuple(kernel_size)
@@ -637,15 +642,15 @@
     """
 
     assert isinstance(self.kernel_size, tuple)
     kernel_size = self.kernel_size
 
     def maybe_broadcast(
       x: tp.Optional[tp.Union[int, tp.Sequence[int]]],
-    ) -> tp.Tuple[int, ...]:
+    ) -> tuple[int, ...]:
       if x is None:
         # backward compatibility with using None as sentinel for
         # broadcast 1
         x = 1
       if isinstance(x, int):
         return (x,) * len(kernel_size)
       return tuple(x)
@@ -666,15 +671,15 @@
     kernel_dilation = maybe_broadcast(self.kernel_dilation)
 
     padding_lax = canonicalize_padding(self.padding, len(kernel_size))
     if padding_lax == 'CIRCULAR':
       kernel_size_dilated = [
         (k - 1) * d + 1 for k, d in zip(kernel_size, kernel_dilation)
       ]
-      zero_pad: tp.List[tp.Tuple[int, int]] = [(0, 0)]
+      zero_pad: tp.List[tuple[int, int]] = [(0, 0)]
       pads = (
         zero_pad
         + [((k - 1) // 2, k // 2) for k in kernel_size_dilated]
         + [(0, 0)]
       )
       inputs = jnp.pad(inputs, pads, mode='wrap')
       padding_lax = 'VALID'
@@ -697,15 +702,15 @@
 
     if self.mask_fn is not None:
       kernel = self.mask_fn(kernel)
 
     bias = self.bias.value
 
     inputs, kernel, bias = dtypes.promote_dtype(
-      inputs, kernel, bias, dtype=self.dtype
+      (inputs, kernel, bias), dtype=self.dtype
     )
 
     y = self.conv_general_dilated(
       inputs,
       kernel,
       strides,
       padding_lax,
@@ -722,14 +727,220 @@
 
     if num_batch_dimensions != 1:
       output_shape = input_batch_shape + y.shape[1:]
       y = jnp.reshape(y, output_shape)
     return y
 
 
+class ConvTranspose(Module):
+  # features: int
+  # kernel_size: Union[int, Sequence[int]]
+  # strides: Optional[Sequence[int]] = None
+  # padding: PaddingLike = 'SAME'
+  # kernel_dilation: Optional[Sequence[int]] = None
+  # use_bias: bool = True
+  # mask: Optional[Array] = None
+  # dtype: Optional[Dtype] = None
+  # param_dtype: Dtype = jnp.float32
+  # precision: PrecisionLike = None
+  # kernel_init: Initializer = default_kernel_init
+  # bias_init: Initializer = initializers.zeros_init()
+  # transpose_kernel: bool = False
+
+  def __init__(
+    self,
+    in_features: int,
+    out_features: int,
+    kernel_size: int | tp.Sequence[int],
+    strides: int | tp.Sequence[int] | None = None,
+    *,
+    padding: PaddingLike = 'SAME',
+    kernel_dilation: int | tp.Sequence[int] | None = None,
+    use_bias: bool = True,
+    mask: Array | None = None,
+    dtype: Dtype | None = None,
+    param_dtype: Dtype = jnp.float32,
+    precision: PrecisionLike | None = None,
+    kernel_init: Initializer = default_kernel_init,
+    bias_init: Initializer = default_bias_init,
+    transpose_kernel: bool = False,
+    rngs: rnglib.Rngs,
+  ):
+    if isinstance(kernel_size, int):
+      kernel_size = (kernel_size,)
+    else:
+      kernel_size = tuple(kernel_size)
+
+    self.kernel_size = kernel_size
+    self.in_features = in_features
+    self.out_features = out_features
+    self.strides = strides
+    self.padding = padding
+    self.kernel_dilation = kernel_dilation
+    self.use_bias = use_bias
+    self.mask = mask
+    self.dtype = dtype
+    self.param_dtype = param_dtype
+    self.precision = precision
+    self.kernel_init = kernel_init
+    self.bias_init = bias_init
+    self.transpose_kernel = transpose_kernel
+
+    if self.transpose_kernel:
+      kernel_shape = kernel_size + (self.out_features, in_features)
+    else:
+      kernel_shape = kernel_size + (in_features, self.out_features)
+
+    self.kernel_shape = kernel_shape
+    self.kernel = nnx.Param(
+      self.kernel_init(rngs.params(), kernel_shape, self.param_dtype)
+    )
+
+    self.bias: nnx.Param | None
+    if self.use_bias:
+      self.bias = nnx.Param(
+        self.bias_init(rngs.params(), (self.out_features,), self.param_dtype)
+      )
+    else:
+      self.bias = None
+
+  def __call__(self, inputs: Array) -> Array:
+    """Applies a transposed convolution to the inputs.
+
+    Behaviour mirrors of ``jax.lax.conv_transpose``.
+
+    Args:
+      inputs: input data with dimensions (*batch_dims, spatial_dims...,
+        features). This is the channels-last convention, i.e. NHWC for a 2d
+        convolution and NDHWC for a 3D convolution. Note: this is different from
+        the input convention used by ``lax.conv_general_dilated``, which puts the
+        spatial dimensions last.
+        Note: If the input has more than 1 batch dimension, all batch dimensions
+        are flattened into a single dimension for the convolution and restored
+        before returning.  In some cases directly vmap'ing the layer may yield
+        better performance than this default flattening approach.  If the input
+        lacks a batch dimension it will be added for the convolution and removed
+        n return, an allowance made to enable writing single-example code.
+
+    Returns:
+      The convolved data.
+    """
+    kernel_size = self.kernel_size
+
+    def maybe_broadcast(
+      x: tp.Optional[tp.Union[int, tp.Sequence[int]]],
+    ) -> tuple[int, ...]:
+      if x is None:
+        # backward compatibility with using None as sentinel for
+        # broadcast 1
+        x = 1
+      if isinstance(x, int):
+        return (x,) * len(kernel_size)
+      return tuple(x)
+
+    # Combine all input batch dimensions into a single leading batch axis.
+    num_batch_dimensions = inputs.ndim - (len(kernel_size) + 1)
+    if num_batch_dimensions != 1:
+      input_batch_shape = inputs.shape[:num_batch_dimensions]
+      total_batch_size = int(np.prod(input_batch_shape))
+      flat_input_shape = (total_batch_size,) + inputs.shape[
+        num_batch_dimensions:
+      ]
+      inputs = jnp.reshape(inputs, flat_input_shape)
+
+    strides = maybe_broadcast(self.strides)
+    kernel_dilation = maybe_broadcast(self.kernel_dilation)
+
+    kernel_shape = self.kernel_shape
+
+    if self.mask is not None and self.mask.shape != kernel_shape:
+      raise ValueError(
+        'Mask needs to have the same shape as weights. '
+        f'Shapes are: {self.mask.shape}, {kernel_shape}'
+      )
+
+    kernel = self.kernel.value
+
+    if self.mask is not None:
+      kernel *= self.mask
+
+    padding_lax = canonicalize_padding(self.padding, len(kernel_size))
+    if padding_lax == 'CIRCULAR':
+      padding_lax = 'VALID'
+
+    bias = self.bias.value if self.bias is not None else None
+
+    inputs, kernel, bias = dtypes.promote_dtype(
+      (inputs, kernel, bias), dtype=self.dtype
+    )
+
+    y = lax.conv_transpose(
+      inputs,
+      kernel,
+      strides,
+      padding_lax,
+      rhs_dilation=kernel_dilation,
+      transpose_kernel=self.transpose_kernel,
+      precision=self.precision,
+    )
+
+    if self.padding == 'CIRCULAR':
+      # For circular padding, we need to identify the size of the final output
+      # ("period") along each spatial dimension, pad each dimension to an
+      # integer number of periods, and wrap the array periodically around each
+      # dimension. Padding should be done in such a way that the start of the
+      # original input data inside the padded array is located at integer
+      # number of periods - otherwise the result would be circularly shifted.
+
+      # Compute period along each spatial dimension - it's input size scaled
+      # by the stride.
+      scaled_x_dims = [
+        x_dim * stride
+        for x_dim, stride in zip(jnp.shape(inputs)[1:-1], strides)
+      ]
+      # Compute difference between the current size of y and the final output
+      # size, and complement this difference to 2 * period - that gives how
+      # much we need to pad.
+      size_diffs = [
+        -(y_dim - x_dim) % (2 * x_dim)
+        for y_dim, x_dim in zip(y.shape[1:-1], scaled_x_dims)
+      ]
+      if self.transpose_kernel:
+        # If the kernel is transposed, the "+1" is put on the right to
+        # mirror the regular convolution. If the same kernel parameters are used
+        # as for Conv, this layer then computes the proper transpose convolution.
+        total_pad = [
+          (size_diff // 2, (size_diff + 1) // 2) for size_diff in size_diffs
+        ]
+      else:
+        # Divide the padding equally between left and right. The choice to put
+        # "+1" on the left (and not on the right) represents a convention for
+        # aligning even-sized kernels.
+        total_pad = [
+          ((size_diff + 1) // 2, size_diff // 2) for size_diff in size_diffs
+        ]
+      y = jnp.pad(y, [(0, 0)] + total_pad + [(0, 0)])
+      # Wrap the result periodically around each spatial dimension,
+      # one by one.
+      for i in range(1, y.ndim - 1):
+        y = y.reshape(
+          y.shape[:i] + (-1, scaled_x_dims[i - 1]) + y.shape[i + 1 :]
+        )
+        y = y.sum(axis=i)
+
+    if self.use_bias:
+      y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))  # type: ignore
+
+    if num_batch_dimensions != 1:
+      output_shape = input_batch_shape + y.shape[1:]
+      y = jnp.reshape(y, output_shape)
+
+    return y
+
+
 default_embed_init = initializers.variance_scaling(
   1.0, 'fan_in', 'normal', out_axis=0
 )
 
 
 class Embed(Module):
   """Embedding Module.
@@ -783,15 +994,15 @@
       with an additional `features` dimension appended.
     """
     if not jnp.issubdtype(inputs.dtype, jnp.integer):
       raise ValueError('Input type must be an integer or unsigned integer.')
     # Use take because fancy indexing numpy arrays with JAX indices does not
     # work correctly.
     (embedding,) = dtypes.promote_dtype(
-      self.embedding.value, dtype=self.dtype, inexact=False
+      (self.embedding.value,), dtype=self.dtype, inexact=False
     )
     if self.num_embeddings == 1:
       return jnp.where(
         jnp.broadcast_to(inputs[..., None], inputs.shape + (self.features,))
         == 0,
         embedding,
         jnp.nan,
@@ -808,10 +1019,10 @@
     Returns:
       An array with final dim `num_embeddings` corresponding to the batched
       inner-product of the array of query vectors against each embedding.
       Commonly used for weight-sharing between embeddings and logit transform
       in NLP models.
     """
     query, embedding = dtypes.promote_dtype(
-      query, self.embedding.value, dtype=self.dtype
+      (query, self.embedding.value), dtype=self.dtype
     )
     return jnp.dot(query, embedding.T)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/normalization.py` & `flax-0.8.4/flax/nnx/nnx/nn/normalization.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,18 +14,18 @@
 
 import typing as tp
 
 import jax
 import jax.numpy as jnp
 from jax import lax
 
-from flax.experimental import nnx
-from flax.experimental.nnx.nnx import rnglib
-from flax.experimental.nnx.nnx.module import Module, first_from
-from flax.experimental.nnx.nnx.nn import dtypes, initializers
+from flax import nnx
+from flax.nnx.nnx import rnglib
+from flax.nnx.nnx.module import Module, first_from
+from flax.nnx.nnx.nn import dtypes, initializers
 from flax.typing import (
   Array,
   Dtype,
   Initializer,
   Axes,
 )
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/nn/stochastic.py` & `flax-0.8.4/flax/nnx/nnx/nn/stochastic.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,16 +30,16 @@
 import dataclasses
 from typing import Sequence
 
 import jax
 import jax.numpy as jnp
 from jax import lax, random
 
-from flax.experimental.nnx.nnx import rnglib
-from flax.experimental.nnx.nnx.module import Module, first_from
+from flax.nnx.nnx import rnglib
+from flax.nnx.nnx.module import Module, first_from
 
 
 @dataclasses.dataclass
 class Dropout(Module):
   """Create a dropout layer.
 
   Attributes:
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/reprlib.py` & `flax-0.8.4/flax/nnx/nnx/reprlib.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,16 +89,15 @@
     value = elem.value if isinstance(elem.value, str) else repr(elem.value)
 
     value = value.replace('\n', '\n' + config.elem_indent)
 
     return f'{config.elem_indent}{elem.start}{elem.key}{config.value_sep}{value}{elem.end}'
 
   with add_indent(config.elem_indent):
-    elems = list(map(_repr_elem, iterator))
-  elems = ',\n'.join(elems)
+    elems = ',\n'.join(map(_repr_elem, iterator))
 
   if elems:
     elems = '\n' + elems + '\n'
   else:
     elems = config.empty_repr
 
   type_repr = (
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/rnglib.py` & `flax-0.8.4/flax/nnx/nnx/rnglib.py`

 * *Files 23% similar despite different names*

```diff
@@ -24,25 +24,25 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import dataclasses
-import functools
 import typing as tp
 
 import jax
 import jax.numpy as jnp
 
-from flax.experimental.nnx.nnx import graph
-from flax.experimental.nnx.nnx.state import State
-from flax.experimental.nnx.nnx.variables import Variable
-from flax.experimental.nnx.nnx import filterlib
-from flax.experimental.nnx.nnx.graph import GraphNode
+from flax.nnx.nnx import graph
+from flax.nnx.nnx.state import State
+from flax.nnx.nnx.variables import Variable
+from flax.nnx.nnx import filterlib
+from flax.nnx.nnx.filterlib import All
+from flax.nnx.nnx.object import Object
 
 Counts = list[int]
 AxesValue = tp.Union[int, None]
 SplitPattern = tp.Union[AxesValue, tuple[AxesValue, ...]]
 
 
 class Missing:
@@ -53,74 +53,62 @@
 
 
 class RngState(Variable[jax.Array]):
   pass
 
 
 class RngCount(RngState):
-  pass
+  tag: str
 
 
 class RngKey(RngState):
   tag: str
 
+
 class RngKeyBackup(RngState):
   pass
 
 
 NotKey = filterlib.All(RngState, filterlib.Not(RngKey))
 
 
 @dataclasses.dataclass(repr=False)
-class RngStream(GraphNode):
+class RngStream(Object):
   def __init__(
     self,
     tag: str,
     key: jax.Array,
     count: jax.Array,
   ):
     self.key = RngKey(key, tag=tag)
-    self.count = RngCount(count)
+    self.count = RngCount(count, tag=tag)
     self.key_backups: list[RngKeyBackup] = []
 
   def __post_init__(self):
     if not isinstance(self.key, jax.Array):
       raise TypeError(f'key must be a jax.Array, got {type(self.key)}')
 
   def __call__(self) -> jax.Array:
     self.check_valid_context(
       'Cannot call RngStream from a different trace level'
     )
     key = jax.random.fold_in(self.key.value, self.count.value)
     self.count.value += 1
     return key
 
-  def fork(self, pattern: SplitPattern) -> jax.Array:
-    if pattern is None:
-      # broadcast key
-      key = self()
-    else:
-      if isinstance(pattern, int):
-        num_splits = pattern
-      else:
-        num_splits = tuple(x if x is not None else 1 for x in pattern)
-      key = jax.random.split(self.key.value, num_splits)
-      self.count.value += 1
-    return key
-
 
 RngValue = tp.Union[int, jax.Array]
 RngDict = tp.Union[
   tp.Mapping[str, int],
   tp.Mapping[str, jax.Array],
   tp.Mapping[str, RngValue],
 ]
 
 
-class Rngs(GraphNode, tp.Mapping[str, tp.Callable[[], jax.Array]]):
+class Rngs(Object, tp.Mapping[str, tp.Callable[[], jax.Array]]):
   def __init__(
     self,
     default: RngValue | RngDict | None = None,
     /,
     **rngs: RngValue,
   ):
     if default is not None:
@@ -133,15 +121,15 @@
       stream = RngStream(
         tag=name,
         key=jax.random.key(value) if isinstance(value, int) else value,
         count=jnp.array(0, dtype=jnp.uint32),
       )
       setattr(self, name, stream)
 
-  def _get_stream(self, name: str, error_type: Exception) -> RngStream:
+  def _get_stream(self, name: str, error_type: type[Exception]) -> RngStream:
     rngs_vars = vars(self)
     if name not in rngs_vars:
       if 'default' not in rngs_vars:
         raise error_type(f"No RNG named {name!r} or 'default' found in Rngs.")
       stream = rngs_vars['default']
     else:
       stream = rngs_vars[name]
@@ -155,167 +143,66 @@
     return self._get_stream(name, AttributeError)
 
   def __call__(self):
     return self.default()
 
   def __iter__(self) -> tp.Iterator[str]:
     for name in vars(self):
-      if name != '_graph_node__state':
+      if name != '_object__state':
         yield name
 
   def __len__(self) -> int:
     return len(vars(self)) - 1
 
   def __contains__(self, name: tp.Any) -> bool:
     return name in vars(self)
 
 
-  def fork(
-    self,
-    _default: SplitPattern
-    | dict[filterlib.Filter, SplitPattern]
-    | Missing = MISSING,
-    /,
-    **patterns: SplitPattern,
-  ) -> ForkedKeys:
-    filter_patterns: list[tuple[filterlib.Filter, SplitPattern]]
-    if isinstance(_default, dict):
-      # merge default and patterns
-      filter_patterns = [
-        *_default.items(),
-        *patterns.items(),
-        (..., None),  # broadcast all remaining
-      ]
-    else:
-      default = None if isinstance(_default, Missing) else _default
-      filter_patterns = [
-        *patterns.items(),
-        (..., default),  # split all remaining with default
-      ]
-
-    predicate_pattern = [
-      (filterlib.to_predicate(filter_), pattern)
-      for filter_, pattern in filter_patterns
-    ]
-
-    splits: dict[str, jax.Array] = {}
-    broadcasts: dict[str, jax.Array] = {}
-
-    for name, stream in self.items():
-      for predicate, pattern in predicate_pattern:
-        stream_path = (name,)
-        # here we check if the stream's RngKey tag matches the predicate
-        # the stream_path is no longer needed, but we keep it for consistency
-        if predicate(stream_path, stream.key):
-          fork = stream.fork(pattern)
-          if pattern is None:
-            broadcasts[name] = fork
-          else:
-            splits[name] = fork
-          break
-      else:
-        raise RuntimeError(
-          f'Strea {name!r} did not match any predicate, this is a bug.'
-        )
-
-    return ForkedKeys(broadcasts, splits)
-
-
-class ForkedKeys(tp.Mapping[str, jax.Array]):
-  def __init__(
-    self,
-    broadcast_rngs: dict[str, jax.Array],
-    split_rngs: dict[str, jax.Array],
-  ):
-    self.broadcasts = broadcast_rngs
-    self.splits = split_rngs
-
-  def __getitem__(self, key: str) -> jax.Array:
-    if key in self.broadcasts:
-      return self.broadcasts[key]
-    elif key in self.splits:
-      return self.splits[key]
-    else:
-      raise KeyError(f'Key "{key}" not found in SplitRng.')
-
-  def __iter__(self) -> tp.Iterator[str]:
-    yield from self.broadcasts
-    yield from self.splits
-
-  def __len__(self) -> int:
-    return len(self.broadcasts) + len(self.splits)
-
-
-def _split_rng_flatten(rngs: ForkedKeys, *, with_keys: bool):
-  broadcast_names = sorted(rngs.broadcasts.keys())
-  split_names = sorted(rngs.splits.keys())
-
-  items = [(name, rngs.broadcasts[name]) for name in broadcast_names]
-  items += [(name, rngs.splits[name]) for name in split_names]
-
-  if with_keys:
-    nodes = tuple((jax.tree_util.DictKey(name), value) for name, value in items)
-  else:
-    nodes = tuple(value for _, value in items)
-
-  metadata = (broadcast_names, split_names)
-
-  return nodes, metadata
+class ForkStates(tp.NamedTuple):
+  split_keys: State
+  split_counts: State
+  broadcast_keys: State
+  broadcast_counts: State
 
 
-def _split_rng_unflatten(
-  metadata: tuple[tuple[str, ...], tuple[str, ...]],
-  nodes: tuple[jax.Array, ...],
-):
-  broadcast_names, split_names = metadata
-  num_broadcasts = len(broadcast_names)
-  rngs = ForkedKeys(
-    dict(zip(broadcast_names, nodes[:num_broadcasts])),
-    dict(zip(split_names, nodes[num_broadcasts:])),
-  )
-  return rngs
-
-
-jax.tree_util.register_pytree_with_keys(
-  ForkedKeys,
-  functools.partial(_split_rng_flatten, with_keys=True),
-  _split_rng_unflatten,
-  flatten_func=functools.partial(_split_rng_flatten, with_keys=False),
-)
-
 def fork(
   state: State,
   split_filter: filterlib.Filter,
   split_pattern: SplitPattern,
-) -> tuple[State, State]:
+) -> ForkStates:
   if split_pattern is None:
     raise RuntimeError('Split pattern cannot be None, this is a bug.')
+
+  num_splits: int | tuple[int, ...]
   if isinstance(split_pattern, int):
     num_splits = split_pattern
   else:
     num_splits = tuple(x if x is not None else 1 for x in split_pattern)
 
-  not_keys, split_state, broadcast_state = state.split(
-    NotKey, split_filter, ...
+  split_keys, split_counts, broadcast_keys, broadcast_counts = state.split(
+    All(split_filter, RngKey),
+    All(split_filter, RngCount),
+    [RngKey, RngKeyBackup],  # Any
+    RngCount,
   )
-  broadcast_state = State.merge(not_keys, broadcast_state)
 
   def split_key(key: tp.Any) -> jax.Array:
     if not isinstance(key, jax.Array):
       raise TypeError(f'key must be a jax.Array, got {type(key)}')
 
     return jax.random.split(key, num_splits)
 
-  split_state = jax.tree.map(split_key, split_state)
+  split_keys = jax.tree.map(split_key, split_keys)
+
+  return ForkStates(split_keys, split_counts, broadcast_keys, broadcast_counts)
 
-  return split_state, broadcast_state
 
 def backup_keys(node: tp.Any, /):
   streams: list[RngStream] = []
-  for _, stream in graph.iter_nodes(node):
+  for _, stream in graph.iter_graph(node):
     if isinstance(stream, RngStream):
       stream.key_backups.append(RngKeyBackup(stream.key.value))
       streams.append(stream)
   return streams
 
 
 def restore_keys(streams: list[RngStream], /):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/spmd.py` & `flax-0.8.4/flax/nnx/nnx/spmd.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,16 +15,16 @@
 import functools
 import typing as tp
 
 import jax
 from jax.interpreters import pxla
 from jax.sharding import Mesh, PartitionSpec
 
-from flax.experimental.nnx.nnx import variables
-from flax.experimental.nnx.nnx.state import State
+from flax.nnx.nnx import variables
+from flax.nnx.nnx.state import State
 from flax.typing import (
   Array,
   ArrayPytree,  # pylint: disable=invalid-name
   PartitionSpecPytree,  # pylint: disable=invalid-name
   Sharding,
 )
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/state.py` & `flax-0.8.4/flax/nnx/nnx/state.py`

 * *Files 7% similar despite different names*

```diff
@@ -31,16 +31,16 @@
 import typing_extensions as tpe
 
 import jax
 import jax.tree_util as jtu
 import numpy as np
 
 from flax import traverse_util
-from flax.experimental.nnx.nnx import filterlib, reprlib
-from flax.experimental.nnx.nnx.variables import VariableState
+from flax.nnx.nnx import filterlib, reprlib
+from flax.nnx.nnx.variables import VariableState
 from flax.typing import Key, PathParts
 
 A = tp.TypeVar('A')
 
 StateLeaf = tp.Union[VariableState[tp.Any], np.ndarray, jax.Array]
 FlatState = dict[PathParts, StateLeaf]
 
@@ -66,31 +66,43 @@
   def __init__(
     self,
     mapping: tp.Union[
       tp.Mapping[Key, tp.Mapping | StateLeaf],
       tp.Iterator[tuple[Key, tp.Mapping | StateLeaf]],
     ],
     /,
+    *,
+    _copy: bool = True,
   ):
+    if _copy:
+      _mapping = dict(mapping)
+    else:
+      if not isinstance(mapping, dict):
+        raise ValueError(
+          'Expected a dictionary when `_copy=False`, '
+          f'got {type(mapping)} instead.'
+        )
+      _mapping = mapping
+
     if tp.TYPE_CHECKING:
-      self._mapping = dict(mapping)
+      self._mapping = _mapping
     else:
-      super().__setattr__('_mapping', dict(mapping))
+      super().__setattr__('_mapping', _mapping)
 
   @property
-  def raw_mapping(self) -> dict[Key, dict[str, tp.Any] | tp.Any]:
-    return self._mapping
+  def raw_mapping(self) -> tp.Mapping[Key, tp.Mapping[Key, tp.Any] | StateLeaf]:
+    return self._mapping  # type: ignore
 
-  def __contains__(self, key: Key) -> bool:
+  def __contains__(self, key) -> bool:
     return key in self._mapping
 
   def __getitem__(self, key: Key) -> State | StateLeaf:
     value = self._mapping[key]
     if isinstance(value, tp.Mapping):
-      return State(value)
+      return State(value, _copy=False)
     return value
 
   def __getattr__(self, key: Key) -> State | StateLeaf:
     if '_mapping' not in vars(self) or key not in self._mapping:
       raise AttributeError(f"No attribute '{key}' in State")
     return self[key]
 
@@ -126,91 +138,89 @@
   def from_flat_path(
     cls, flat_state: tp.Mapping[PathParts, StateLeaf], /
   ) -> State:
     nested_state = traverse_util.unflatten_dict(flat_state)
     return cls(nested_state)
 
   @tp.overload
-  def split(self, first: filterlib.Filter, /) -> 'State':
-    ...
+  def split(self, first: filterlib.Filter, /) -> 'State': ...
 
   @tp.overload
   def split(
     self,
     first: filterlib.Filter,
     second: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tuple['State', ...]:
-    ...
+  ) -> tuple['State', ...]: ...
 
   def split(
     self, first: filterlib.Filter, /, *filters: filterlib.Filter
   ) -> tp.Union['State', tuple['State', ...]]:
     filters = (first, *filters)
-    *states, rest = _split_state(self, *filters)
+    *states_, rest = _split_state(self, *filters)
 
     if rest:
       raise ValueError(
         'Non-exhaustive filters, got a non-empty remainder: '
-        f'{list(rest.keys())}.\nUse `...` to match all remaining elements.'
+        f'{rest}.\nUse `...` to match all remaining elements.'
       )
 
-    if len(states) == 1:
-      states = states[0]
+    states: State | tuple[State, ...]
+    if len(states_) == 1:
+      states = states_[0]
     else:
-      states = tuple(states)
-    return states
+      states = tuple(states_)
+    return states  # type: ignore[bad-return-type]
 
   @tp.overload
   def filter(
     self,
     first: filterlib.Filter,
     /,
-  ) -> 'State':
-    ...
+  ) -> 'State': ...
 
   @tp.overload
   def filter(
     self,
     first: filterlib.Filter,
     second: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
-  ) -> tuple['State', ...]:
-    ...
+  ) -> tuple['State', ...]: ...
 
   def filter(
     self,
     first: filterlib.Filter,
     /,
     *filters: filterlib.Filter,
   ) -> tp.Union['State', tuple['State', ...]]:
-    *states, _rest = _split_state(self, first, *filters)
+    *states_, _rest = _split_state(self, first, *filters)
 
-    assert len(states) == len(filters) + 1
+    assert len(states_) == len(filters) + 1
 
-    if len(states) == 1:
-      states = states[0]
+    states: State | tuple[State, ...]
+    if len(states_) == 1:
+      states = states_[0]
     else:
-      states = tuple(states)
+      states = tuple(states_)
 
-    return states
+    return states  # type: ignore[bad-return-type]
 
   @staticmethod
   def merge(state: 'State', /, *states: 'State') -> 'State':
     states = (state, *states)
 
     if len(states) == 1:
       return states[0]
 
     new_state: FlatState = {}
 
     for state in states:
-      new_state.update(state.flat_state())
+      new_state.update(state.flat_state())  # type: ignore[attribute-error] # pytype is wrong here
 
     return State.from_flat_path(new_state)
 
   def __or__(self, other: 'State') -> 'State':
     if not other:
       return self
     return State.merge(self, other)
@@ -221,14 +231,15 @@
 
     self_flat = self.flat_state()
     other_flat = other.flat_state()
     diff = {k: v for k, v in self_flat.items() if k not in other_flat}
 
     return State.from_flat_path(diff)
 
+
 def _state_flatten_with_keys(x: State):
   items = sorted(x._mapping.items())
   children = tuple((jtu.DictKey(key), value) for key, value in items)
   return children, tuple(key for key, _ in items)
 
 
 def _state_unflatten(
@@ -237,15 +248,15 @@
 ):
   return State(zip(static, leaves))
 
 
 jax.tree_util.register_pytree_with_keys(
   State,
   _state_flatten_with_keys,
-  _state_unflatten,
+  _state_unflatten,  # type: ignore[arg-type]
 )
 
 
 def _split_state(
   state: State,
   *filters: filterlib.Filter,
 ) -> tuple[State, ...]:
@@ -266,14 +277,14 @@
   flat_states: tuple[FlatState, ...] = tuple(
     {} for _ in range(len(predicates) + 1)
   )
 
   for path, value in flat_state.items():
     for i, predicate in enumerate(predicates):
       if predicate(path, value):
-        flat_states[i][path] = value
+        flat_states[i][path] = value  # type: ignore[index] # mypy is wrong here?
         break
     else:
       # if we didn't break, set leaf to last state
-      flat_states[-1][path] = value
+      flat_states[-1][path] = value  # type: ignore[index] # mypy is wrong here?
 
   return tuple(State.from_flat_path(flat_state) for flat_state in flat_states)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/tracers.py` & `flax-0.8.4/flax/nnx/nnx/tracers.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 import typing as tp
 
 import jax
 import jax.core
 from jax.core import MainTrace
 
-from flax.experimental.nnx.nnx import reprlib
+from flax.nnx.nnx import reprlib
 
 
 @tp.runtime_checkable
 class Tracer(tp.Protocol):
   _trace: jax.core.Trace
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/training/metrics.py` & `flax-0.8.4/flax/nnx/nnx/training/metrics.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,72 +24,86 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import jax, jax.numpy as jnp
-from flax.experimental.nnx.nnx.variables import Variable
-from flax.experimental.nnx.nnx import filterlib, graph
+from flax.nnx.nnx.object import Object
+from flax.nnx.nnx.variables import Variable
+from flax.nnx.nnx import filterlib, graph
 
 import typing as tp
 
-#TODO: add tests and docstrings
+# TODO: add tests and docstrings
+
 
 class MetricState(Variable):
   """Wrapper class for Metric Variables."""
+
   pass
 
-class Metric(graph.GraphNode):
+
+class Metric(Object):
   def __init__(self):
     raise NotImplementedError('Must override `__init__()` method.')
+
   def reset(self):
     raise NotImplementedError('Must override `reset()` method.')
-  def update(self):
+
+  def update(self, **kwargs) -> None:
     raise NotImplementedError('Must override `update()` method.')
+
   def compute(self):
     raise NotImplementedError('Must override `compute()` method.')
+
   def split(self, *filters: filterlib.Filter):
     return graph.split(self, *filters)
 
 
 class Average(Metric):
   def __init__(self, argname: str = 'values'):
     self.argname = argname
     self.total = MetricState(jnp.array(0, dtype=jnp.float32))
     self.count = MetricState(jnp.array(0, dtype=jnp.int32))
+
   def reset(self):
     self.total.value = jnp.array(0, dtype=jnp.float32)
     self.count.value = jnp.array(0, dtype=jnp.int32)
 
   def update(self, **kwargs):
     if self.argname not in kwargs:
       raise TypeError(f"Expected keyword argument '{self.argname}'")
     values: tp.Union[int, float, jax.Array] = kwargs[self.argname]
-    self.total.value += values if isinstance(values, (int, float)) else values.sum()
+    self.total.value += (
+      values if isinstance(values, (int, float)) else values.sum()
+    )
     self.count.value += 1 if isinstance(values, (int, float)) else values.size
+
   def compute(self):
     return self.total.value / self.count.value
 
+
 class Accuracy(Average):
-  def update(self, *, logits: jax.Array, labels: jax.Array, **_):
+  def update(self, *, logits: jax.Array, labels: jax.Array, **_):  # type: ignore[override]
     if logits.ndim != labels.ndim + 1 or labels.dtype != jnp.int32:
       raise ValueError(
-        f"Expected labels.dtype==jnp.int32 and logits.ndim={logits.ndim}=="
-        f"labels.ndim+1={labels.ndim + 1}"
+        f'Expected labels.dtype==jnp.int32 and logits.ndim={logits.ndim}=='
+        f'labels.ndim+1={labels.ndim + 1}'
       )
-    super().update(values=(logits.argmax(axis=-1)==labels))
+    super().update(values=(logits.argmax(axis=-1) == labels))
+
 
 class MultiMetric(Metric):
   """MultiMetric class to store multiple metrics and update them in a single call.
 
   Example usage::
 
     >>> import jax, jax.numpy as jnp
-    >>> from flax.experimental import nnx
+    >>> from flax import nnx
     ...
     >>> logits = jax.random.normal(jax.random.key(0), (5, 2))
     >>> labels = jnp.array([1, 1, 0, 1, 0])
     >>> logits2 = jax.random.normal(jax.random.key(1), (5, 2))
     >>> labels2 = jnp.array([0, 1, 1, 1, 1])
     ...
     >>> batch_loss = jnp.array([1, 2, 3, 4])
@@ -106,23 +120,30 @@
     >>> metrics.update(logits=logits2, labels=labels2, values=batch_loss2)
     >>> metrics.compute()
     {'accuracy': Array(0.7, dtype=float32), 'loss': Array(2., dtype=float32)}
     >>> metrics.reset()
     >>> metrics.compute()
     {'accuracy': Array(nan, dtype=float32), 'loss': Array(nan, dtype=float32)}
   """
+
   def __init__(self, **metrics):
     # TODO: raise error if a kwarg is passed that is in ('reset', 'update', 'compute'), since these names are reserved for methods
     self._metric_names = []
     for metric_name, metric in metrics.items():
       self._metric_names.append(metric_name)
       vars(self)[metric_name] = metric
+
   def reset(self):
     for metric_name in self._metric_names:
       getattr(self, metric_name).reset()
+
   def update(self, **updates):
     # TODO: should we give the option of updating only some of the metrics and not all? e.g. if for some kwargs==None, don't do update
     # TODO: should we raise an error if a kwarg is passed into **updates that has no match with any underlying metric? e.g. user typo
     for metric_name in self._metric_names:
       getattr(self, metric_name).update(**updates)
+
   def compute(self):
-    return {f'{metric_name}': getattr(self, metric_name).compute() for metric_name in self._metric_names}
+    return {
+      f'{metric_name}': getattr(self, metric_name).compute()
+      for metric_name in self._metric_names
+    }
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/training/optimizer.py` & `flax-0.8.4/flax/nnx/nnx/training/optimizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -26,31 +26,35 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import jax.numpy as jnp
 import optax
 
-from flax.experimental import nnx
-from flax.experimental.nnx.nnx import filterlib, graph
-from flax.experimental.nnx.nnx.variables import Variable
+from flax import nnx
+from flax.nnx.nnx import filterlib, graph
+from flax.nnx.nnx.object import Object
+from flax.nnx.nnx.variables import Variable
+
+# TODO: add tests and docstrings
 
-#TODO: add tests and docstrings
 
 class OptState(Variable):
   """Wrapper class for Optimizer Variables."""
+
   pass
 
-class Optimizer(graph.GraphNode):
+
+class Optimizer(Object):
   """Simple train state for the common case with a single Optax optimizer.
 
   Example usage::
 
     >>> import jax, jax.numpy as jnp
-    >>> from flax.experimental import nnx
+    >>> from flax import nnx
     >>> import optax
     ...
     >>> class Model(nnx.Module):
     ...   def __init__(self, rngs):
     ...     self.linear1 = nnx.Linear(2, 3, rngs=rngs)
     ...     self.linear2 = nnx.Linear(3, 4, rngs=rngs)
     ...   def __call__(self, x):
@@ -129,17 +133,14 @@
     Returns:
       An updated instance of ``self`` with ``step`` incremented by one, ``params``
       and ``opt_state`` updated by applying ``grads``, and additional attributes
       replaced as specified by ``kwargs``.
     """
     params = nnx.state(self.model, nnx.Param)
 
-    updates, new_opt_state = self.tx.update(
-      grads, self.opt_state, params
-    )
+    updates, new_opt_state = self.tx.update(grads, self.opt_state, params)
     new_params = optax.apply_updates(params, updates)
     assert isinstance(new_params, nnx.State)
 
     self.step.value += 1
     nnx.update(self.model, new_params)
     self.opt_state = new_opt_state
-
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/transforms.py` & `flax-0.8.4/flax/nnx/nnx/transforms.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,383 +21,159 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+# pytype: skip-file
 from __future__ import annotations
 
+from abc import abstractmethod
 import dataclasses
 import functools
 import typing as tp
-from abc import abstractmethod
 
+from flax import struct
 from flax.core.frozen_dict import FrozenDict
-
-import jax
-import jax.core
-import jax.numpy as jnp
-import jax.stages
-
-from jax._src.tree_util import broadcast_prefix
-from flax.experimental.nnx.nnx import (
+from flax.nnx.nnx import (
   filterlib,
   graph,
   rnglib,
   spmd,
   variables,
 )
-from flax.experimental.nnx.nnx.module import GraphDef, Module, ModuleMeta
-from flax.experimental.nnx.nnx.proxy_caller import (
+from flax.nnx.nnx.module import GraphDef, Module
+from flax.nnx.nnx.proxy_caller import (
   CallableProxy,
   DelayedAccessor,
 )
-from flax.experimental.nnx.nnx.state import State
+from flax.nnx.nnx.state import State
 from flax.typing import Leaf
+import jax
+from jax._src.tree_util import broadcast_prefix
+import jax.core
+import jax.numpy as jnp
+import jax.stages
 
 A = tp.TypeVar('A')
 C = tp.TypeVar('C')
 B = tp.TypeVar('B')
 F = tp.TypeVar('F', bound=tp.Callable[..., tp.Any])
 G = tp.TypeVar('G', bound=tp.Callable[..., tp.Any])
 M = tp.TypeVar('M', bound=Module)
+MA = tp.TypeVar('MA', bound=Module)
 N = tp.TypeVar('N', bound=Module)
 StrInt = tp.TypeVar('StrInt', str, int)
 AxisName = tp.Hashable
 Leaves = tp.List[Leaf]
 Index = int
 
+
 def _normalize_sequence(
   x: StrInt | tp.Iterable[StrInt] | None, /
 ) -> tuple[StrInt, ...]:
   if x is None:
     return ()
   elif isinstance(x, (str, int)):
     return (x,)  # type: ignore
   else:
     return tuple(x)
 
 
-class LiftedModule(Module, tp.Generic[M]):
+class LiftedModule(tp.Generic[M], Module):  # type: ignore[ignored-abstractmethod]
   @abstractmethod
   def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
-    ...
+    pass
 
   @property
   @abstractmethod
   def _submodule(self) -> M:
-    ...
+    pass  # type: ignore[bad-return-type] # why pytype?
 
   def __call__(self, *args, **kwargs) -> tp.Any:
     return self.call(*args, **kwargs)  # type: ignore
 
   @property
   def call(self) -> tp.Any:
     module = self
 
     def check_and_call(accessor: DelayedAccessor, *args, **kwargs):
       return self._call(accessor, *args, **kwargs)
 
-    proxy = CallableProxy(check_and_call)
+    proxy = CallableProxy(check_and_call)  # type: ignore[arg-type]
 
     while isinstance(module._submodule, LiftedModule):
       module = module._submodule
       proxy = proxy.call
 
     return proxy  # type: ignore
 
 
 # -------------------------------
 # jit
 # -------------------------------
 
 UNSPECIFIED = object()
 
+
+def _default_constrain_state(state: State) -> State:
+  state_spec = spmd.get_partition_spec(state)
+  state = jax.lax.with_sharding_constraint(state, state_spec)
+  return state
+
+
 @dataclasses.dataclass(frozen=True)
 class JitStaticInputs:
   graphdef: GraphDef[tuple[tp.Any, ...]]
-  ctx: graph.UpdateContext
+  constrain_state: tp.Callable[[State], State] | None
+  f: tp.Callable[..., tp.Any]
 
 
 jax.tree_util.register_static(JitStaticInputs)
 
 
 @dataclasses.dataclass(frozen=True)
 class JitStaticOutputs:
   graphdef: GraphDef[tuple[tp.Any, ...]]
   index_mapping: dict[Index, Index]
 
 
 jax.tree_util.register_static(JitStaticOutputs)
 
-def _default_constrain_object_state(state: State) -> State:
-  state_spec = spmd.get_partition_spec(state)
-  state = jax.lax.with_sharding_constraint(state, state_spec)
-  return state
-
-
-@dataclasses.dataclass
-class JITOptions:
-  in_shardings: tp.Any
-  out_shardings: tp.Any
-  static_argnums: tuple[int, ...]
-  static_argnames: tuple[str, ...]
-  donate_argnums: tuple[int, ...]
-  donate_argnames: tuple[str, ...]
-  keep_unused: bool
-  device: tp.Optional[jax.Device]
-  backend: tp.Optional[str]
-  inline: bool
-  abstracted_axes: tp.Optional[tp.Any]
-  # nnx specific
-  donate_state: bool
-  constrain_state: tp.Callable[[State], State] | None
-
-  @classmethod
-  def from_jit_kwargs(
-    cls,
-    in_shardings: tp.Any,
-    out_shardings: tp.Any,
-    static_argnums: int | tp.Sequence[int] | None,
-    static_argnames: str | tp.Iterable[str] | None,
-    donate_argnums: int | tp.Sequence[int] | None,
-    donate_argnames: str | tp.Iterable[str] | None,
-    keep_unused: bool,
-    device: tp.Optional[jax.Device],
-    backend: tp.Optional[str],
-    inline: bool,
-    abstracted_axes: tp.Optional[tp.Any],
-    donate_state: bool,
-    constrain_state: bool | tp.Callable[[State], State],
-  ):
-    _static_argnums = _normalize_sequence(static_argnums)
-    _static_argnames = _normalize_sequence(static_argnames)
-    _donate_argnums = _normalize_sequence(donate_argnums)
-    _donate_argnames = _normalize_sequence(donate_argnames)
-
-    if donate_state:
-      _donate_argnames = (*_donate_argnames, '_nnx_jit_state')
-
-    if callable(constrain_state):
-      _constrain_object_state = constrain_state
-    elif constrain_state:
-      _constrain_object_state = _default_constrain_object_state
-    else:
-      _constrain_object_state = None
-
-    return cls(
-      in_shardings=in_shardings,
-      out_shardings=out_shardings,
-      static_argnums=_static_argnums,
-      static_argnames=_static_argnames,
-      donate_argnums=_donate_argnums,
-      donate_argnames=_donate_argnames,
-      keep_unused=keep_unused,
-      device=device,
-      backend=backend,
-      inline=inline,
-      abstracted_axes=abstracted_axes,
-      donate_state=donate_state,
-      constrain_state=_constrain_object_state,
-    )
-
-  def get_jit_kwargs(self) -> dict[str, tp.Any]:
-    kwargs = vars(self).copy()
-    del kwargs['donate_state']
-    del kwargs['constrain_state']
-    if kwargs['in_shardings'] is UNSPECIFIED:
-      kwargs.pop('in_shardings')
-    if kwargs['out_shardings'] is UNSPECIFIED:
-      kwargs.pop('out_shardings')
-    return kwargs
-
-
-class JITMeta(ModuleMeta):
-  def __call__(
-    self,
-    module_constructor: tp.Callable[..., M],
-    *,
-    in_shardings: tp.Any = UNSPECIFIED,
-    out_shardings: tp.Any = UNSPECIFIED,
-    static_argnums: int | tp.Sequence[int] | None = None,
-    static_argnames: str | tp.Iterable[str] | None = None,
-    donate_argnums: int | tp.Sequence[int] | None = None,
-    donate_argnames: str | tp.Iterable[str] | None = None,
-    keep_unused: bool = False,
-    device: tp.Optional[jax.Device] = None,
-    backend: tp.Optional[str] = None,
-    inline: bool = False,
-    abstracted_axes: tp.Optional[tp.Any] = None,
-    # nnx specific
-    donate_state: bool = False,
-    constrain_state: bool | tp.Callable[[State], State] = False,
-  ) -> tp.Callable[..., 'Jit[M]']:
-    super_call = super().__call__
-
-    def _create_jit(*args, **kwargs) -> Jit[M]:
-      return super_call(
-        module_constructor=module_constructor,
-        in_shardings=in_shardings,
-        out_shardings=out_shardings,
-        static_argnums=static_argnums,
-        static_argnames=static_argnames,
-        donate_argnums=donate_argnums,
-        donate_argnames=donate_argnames,
-        keep_unused=keep_unused,
-        device=device,
-        backend=backend,
-        inline=inline,
-        abstracted_axes=abstracted_axes,
-        # nnx specific
-        donate_state=donate_state,
-        constrain_state=constrain_state,
-        # submodule args
-        module_init_args=args,
-        module_init_kwargs=kwargs,
-      )
-
-    return _create_jit
-
-
-class JittedFn(tp.Protocol):
-  def __call__(
-    self,
-    *args: tp.Any,
-    _nnx_jit_static: JitStaticInputs,
-    _nnx_jit_state: State,
-    **kwargs: tp.Any,
-  ) -> tuple[
-    tp.Any, State, GraphDef[tuple[tuple[tp.Any, ...], tuple[tp.Any, ...]]]
-  ]:
-    ...
-
-
-def get_jitted_fn(f, options: JITOptions) -> JittedFn:
-  jit_kwargs = options.get_jit_kwargs()
-
-  @functools.partial(jax.jit, **jit_kwargs)
-  def jitted_fn(
-    *args: tp.Any,
-    _nnx_jit_static: JitStaticInputs,
-    _nnx_jit_state: State,
-    **kwargs: tp.Any,
-  ) -> tuple[tp.Any, State, GraphDef[tuple[tp.Any, ...]]]:
-    ctx = _nnx_jit_static.ctx
-    graphdef = _nnx_jit_static.graphdef
-    state: State = _nnx_jit_state
-
-    if options.constrain_state is not None:
-      state = options.constrain_state(state)
-
-    input_graph_nodes = ctx.merge(graphdef, state)
-
-    (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
-
-    out = f(*args, **kwargs)
-
-    out, output_graph_nodes = graph.extract_graph_nodes(out)
 
-    graphdef, state = ctx.split((input_graph_nodes, output_graph_nodes))
-
-    if options.constrain_state is not None:
-      state = options.constrain_state(state)
-
-    return out, state, graphdef
+def _jitted_fn(
+  *args: tp.Any,
+  _nnx_jit_static: JitStaticInputs,
+  _nnx_jit_state: State,
+  **kwargs: tp.Any,
+) -> tuple[tp.Any, State, GraphDef[tuple[tp.Any, ...]]]:
+  ctx = graph.current_update_context('jit')
+  graphdef = _nnx_jit_static.graphdef
+  constrain_state = _nnx_jit_static.constrain_state
+  f = _nnx_jit_static.f
+  state: State = _nnx_jit_state
 
-  return jitted_fn
+  if constrain_state is not None:
+    state = constrain_state(state)
 
+  input_graph_nodes = ctx.merge(graphdef, state)
 
-def jit_apply(
-  options: JITOptions,
-  jitted_fn: JittedFn,
-  args: tuple[tp.Any, ...],
-  kwargs: dict[str, tp.Any],
-) -> tp.Any:
-  ctx = graph.UpdateContext()
-  (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
-  graphdef, state = ctx.split(input_graph_nodes)
+  (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
 
-  out, output_state, output_graphdef = jitted_fn(
-    *args,
-    _nnx_jit_static=JitStaticInputs(graphdef, ctx),
-    _nnx_jit_state=state,
-    **kwargs,
-  )
-  input_graph_nodes, output_graph_nodes = ctx.update(
-    output_graphdef, output_state
-  )
-  out = graph.insert_graph_nodes(out, output_graph_nodes)
-  return out
-
-
-class Jit(LiftedModule[M], metaclass=JITMeta):
-  def __init__(
-    self,
-    module_constructor: tp.Callable[..., M],
-    *,
-    in_shardings: tp.Any = UNSPECIFIED,
-    out_shardings: tp.Any = UNSPECIFIED,
-    static_argnums: int | tp.Sequence[int] | None = None,
-    static_argnames: str | tp.Iterable[str] | None = None,
-    donate_argnums: int | tp.Sequence[int] | None = None,
-    donate_argnames: str | tp.Iterable[str] | None = None,
-    keep_unused: bool = False,
-    device: tp.Optional[jax.Device] = None,
-    backend: tp.Optional[str] = None,
-    inline: bool = False,
-    abstracted_axes: tp.Optional[tp.Any] = None,
-    # nnx specific
-    donate_state: bool = False,
-    constrain_state: bool | tp.Callable[[State], State] = False,
-    # submodule args
-    module_init_args: tuple[tp.Any, ...],
-    module_init_kwargs: dict[str, tp.Any],
-  ):
-    self.options = JITOptions.from_jit_kwargs(
-      in_shardings=in_shardings,
-      out_shardings=out_shardings,
-      static_argnums=static_argnums,
-      static_argnames=static_argnames,
-      donate_argnums=donate_argnums,
-      donate_argnames=donate_argnames,
-      keep_unused=keep_unused,
-      device=device,
-      backend=backend,
-      inline=inline,
-      abstracted_axes=abstracted_axes,
-      donate_state=donate_state,
-      constrain_state=constrain_state,
-    )
-    self.accessor: tp.Optional[DelayedAccessor] = None
+  out = f(*args, **kwargs)
 
-    def jit_call_module(module, *args, **kwargs):
-      assert self.accessor is not None
-      method = self.accessor(module)
-      return method(*args, **kwargs)
+  out, output_graph_nodes = graph.extract_graph_nodes(out)
 
-    self.jitted_fn: JittedFn[M] = get_jitted_fn(jit_call_module, self.options)
-    self.module_constructor = module_constructor
-    self.jit_module = self.module_constructor(
-      *module_init_args, **module_init_kwargs
-    )
+  graphdef, state = ctx.split((input_graph_nodes, output_graph_nodes))
 
-  @property
-  def _submodule(self) -> M:
-    return self.jit_module
+  if constrain_state is not None:
+    state = constrain_state(state)
 
-  def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
-    self.accessor = accessor
-    try:
-      out = jit_apply(
-        self.options, self.jitted_fn, (self.jit_module, *args), kwargs
-      )
-    finally:
-      self.accessor = None
-    return out
+  return out, state, graphdef
 
 
 def jit(
   fun: F,
   *,
   in_shardings: tp.Any = UNSPECIFIED,
   out_shardings: tp.Any = UNSPECIFIED,
@@ -537,199 +313,294 @@
       :func:`nnx.spmd.get_partition_spec`. If a callable, the object State will
       passed to the callable which must return the constrained object State. If
       False, the object state will not be constrained. Default False.
 
   Returns:
     A wrapped version of ``fun``, set up for just-in-time compilation.
   """
-  options = JITOptions.from_jit_kwargs(
-    in_shardings=in_shardings,
-    out_shardings=out_shardings,
-    static_argnums=static_argnums,
-    static_argnames=static_argnames,
-    donate_argnums=donate_argnums,
-    donate_argnames=donate_argnames,
+
+  _static_argnums = _normalize_sequence(static_argnums)
+  _static_argnames = _normalize_sequence(static_argnames)
+  _donate_argnums = _normalize_sequence(donate_argnums)
+  _donate_argnames = _normalize_sequence(donate_argnames)
+
+  if donate_state:
+    _donate_argnames = (*_donate_argnames, '_nnx_jit_state')
+
+  if callable(constrain_state):
+    _constrain_state = constrain_state
+  elif constrain_state:
+    _constrain_state = _default_constrain_state
+  else:
+    _constrain_state = None
+
+  jit_kwargs = {}
+  if in_shardings is not UNSPECIFIED:
+    jit_kwargs['in_shardings'] = in_shardings
+  if out_shardings is not UNSPECIFIED:
+    jit_kwargs['out_shardings'] = out_shardings
+
+  jitted_fn = jax.jit(
+    _jitted_fn,
+    static_argnums=_static_argnums,
+    static_argnames=_static_argnames,
+    donate_argnums=_donate_argnums,
+    donate_argnames=_donate_argnames,
     keep_unused=keep_unused,
     device=device,
     backend=backend,
     inline=inline,
     abstracted_axes=abstracted_axes,
-    donate_state=donate_state,
-    constrain_state=constrain_state,
+    **jit_kwargs,
   )
-  jitted_fn = get_jitted_fn(fun, options)
 
   @functools.wraps(fun)
-  def jit_apply_wrapper(*args, **kwargs):
-    return jit_apply(options, jitted_fn, args, kwargs)
-
-  wrapper = jit_apply_wrapper
-  wrapper.inner = jitted_fn
-
-  return wrapper  # type: ignore
-
-
-# -------------------------------
-# grad
-# -------------------------------
+  @graph.update_context('jit')
+  def jit_wrapper(*args, **kwargs):
+    ctx = graph.current_update_context('jit')
+    (args, kwargs), input_graph_nodes = graph.extract_graph_nodes(
+      (args, kwargs)
+    )
+    graphdef, state = ctx.split(input_graph_nodes)
+    out, output_state, output_graphdef = jitted_fn(
+      *args,
+      _nnx_jit_static=JitStaticInputs(graphdef, _constrain_state, fun),
+      _nnx_jit_state=state,
+      **kwargs,
+    )
+    input_graph_nodes, output_graph_nodes = ctx.merge(
+      output_graphdef, output_state
+    )
+    out = graph.insert_graph_nodes(out, output_graph_nodes)
+    return out
 
+  jit_wrapper.inner = jitted_fn  # type: ignore
 
-@dataclasses.dataclass
-class GradOptions:
-  argnums: tuple[int, ...]
-  has_aux: bool
-  holomorphic: bool
-  allow_int: bool
-  reduce_axes: tp.Sequence[AxisName]
-  return_value: bool
-  wrt: filterlib.Filter
+  return jit_wrapper  # type: ignore
 
 
-class GradMeta(ModuleMeta):
-  def __call__(
-    self,
-    module_constructor: tp.Callable[..., M],
-    has_aux: bool = False,
-    holomorphic: bool = False,
-    allow_int: bool = False,
-    reduce_axes: tp.Sequence[AxisName] = (),
-    return_value: bool = False,
+class Jit(tp.Generic[M], LiftedModule[M]):
+  @staticmethod
+  def constructor(
+    module_constructor: tp.Callable[..., MA],
     *,
-    wrt: filterlib.Filter = variables.Param,
-  ) -> tp.Callable[..., 'Grad[M]']:
-    super_call = super().__call__
-
-    def _create_grad(*args, **kwargs) -> Grad[M]:
-      return super_call(
+    in_shardings: tp.Any = UNSPECIFIED,
+    out_shardings: tp.Any = UNSPECIFIED,
+    static_argnums: int | tp.Sequence[int] | None = None,
+    static_argnames: str | tp.Iterable[str] | None = None,
+    donate_argnums: int | tp.Sequence[int] | None = None,
+    donate_argnames: str | tp.Iterable[str] | None = None,
+    keep_unused: bool = False,
+    device: tp.Optional[jax.Device] = None,
+    backend: tp.Optional[str] = None,
+    inline: bool = False,
+    abstracted_axes: tp.Optional[tp.Any] = None,
+    # nnx specific
+    donate_state: bool = False,
+    constrain_state: bool | tp.Callable[[State], State] = False,
+  ) -> tp.Callable[..., 'Jit[MA]']:
+    def _create_jit(*args, **kwargs):
+      return Jit(
         module_constructor=module_constructor,
-        wrt=wrt,
-        has_aux=has_aux,
-        holomorphic=holomorphic,
-        allow_int=allow_int,
-        reduce_axes=reduce_axes,
-        return_value=return_value,
+        in_shardings=in_shardings,
+        out_shardings=out_shardings,
+        static_argnums=static_argnums,
+        static_argnames=static_argnames,
+        donate_argnums=donate_argnums,
+        donate_argnames=donate_argnames,
+        keep_unused=keep_unused,
+        device=device,
+        backend=backend,
+        inline=inline,
+        abstracted_axes=abstracted_axes,
+        # nnx specific
+        donate_state=donate_state,
+        constrain_state=constrain_state,
         # submodule args
         module_init_args=args,
         module_init_kwargs=kwargs,
       )
 
-    return _create_grad
-
+    return _create_jit
 
-class Grad(LiftedModule[M], metaclass=GradMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
-    argnums: int | tp.Sequence[int] = 0,
-    has_aux: bool = False,
-    holomorphic: bool = False,
-    allow_int: bool = False,
-    reduce_axes: tp.Sequence[AxisName] = (),
-    return_value: bool = False,
     *,
-    wrt: filterlib.Filter = variables.Param,
+    in_shardings: tp.Any = UNSPECIFIED,
+    out_shardings: tp.Any = UNSPECIFIED,
+    static_argnums: int | tp.Sequence[int] | None = None,
+    static_argnames: str | tp.Iterable[str] | None = None,
+    donate_argnums: int | tp.Sequence[int] | None = None,
+    donate_argnames: str | tp.Iterable[str] | None = None,
+    keep_unused: bool = False,
+    device: tp.Optional[jax.Device] = None,
+    backend: tp.Optional[str] = None,
+    inline: bool = False,
+    abstracted_axes: tp.Optional[tp.Any] = None,
+    # nnx specific
+    donate_state: bool = False,
+    constrain_state: bool | tp.Callable[[State], State] = False,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
-    _argnums = _normalize_sequence(argnums)
-    self.options = GradOptions(
-      argnums=_argnums,
-      has_aux=has_aux,
-      holomorphic=holomorphic,
-      allow_int=allow_int,
-      reduce_axes=reduce_axes,
-      return_value=return_value,
-      wrt=wrt,
+    @functools.partial(
+      jit,
+      in_shardings=in_shardings,
+      out_shardings=out_shardings,
+      static_argnums=static_argnums,
+      static_argnames=static_argnames,
+      donate_argnums=donate_argnums,
+      donate_argnames=donate_argnames,
+      keep_unused=keep_unused,
+      device=device,
+      backend=backend,
+      inline=inline,
+      abstracted_axes=abstracted_axes,
+      donate_state=donate_state,
+      constrain_state=constrain_state,
     )
+    def jit_call_module(
+      module, *args, _nnx_jit_accessor: DelayedAccessor, **kwargs
+    ):
+      method = _nnx_jit_accessor(module)
+      return method(*args, **kwargs)
+
+    self.jitted_fn = jit_call_module
     self.module_constructor = module_constructor
-    self.grad_module = self.module_constructor(
+    self.jit_module = self.module_constructor(
       *module_init_args, **module_init_kwargs
     )
 
   @property
   def _submodule(self) -> M:
-    return self.grad_module
+    return self.jit_module
 
   def _call(self, accessor: DelayedAccessor, *args, **kwargs) -> tp.Any:
-    def grad_call_apply(module, *args, **kwargs):
-      method = accessor(module)
-      return method(*args, **kwargs)
+    out = self.jitted_fn(
+      self.jit_module, *args, _nnx_jit_accessor=accessor, **kwargs
+    )
+    return out
+
+
+# -------------------------------
+# grad
+# -------------------------------
 
-    return grad_apply(self.options, grad_call_apply, (self.grad_module, *args))
 
+def grad_fn(*args):
+  f: tp.Callable[..., tp.Any]
+  graphdef: GraphDef[tuple[dict[int, tp.Any], tuple[tp.Any, ...]]]
+  non_diff_state: State
+  has_aux: bool
+  diff_args: list[int]
+  ctx = graph.current_update_context('grad')
+  *_args, f, graphdef, non_diff_state, has_aux, diff_args = args
 
-def grad_apply(options: GradOptions, f, args: tuple[tp.Any, ...]):
-  _, input_nodes = graph.extract_graph_nodes(args)
+  # rebuild diff_state from substates in args
+  diff_state = State({})
+  for i in diff_args:
+    diff_state[i] = _args[i]
+  diff_state = State({0: diff_state.raw_mapping})
 
-  _args = list(args)
-  diff_graph_nodes: dict[int, tp.Any] = {
-    i: arg
-    for i, arg in enumerate(args)
-    if i in options.argnums and graph.is_node(arg)
-  }
+  diff_graph_nodes, input_nodes = ctx.merge(
+    graphdef, diff_state, non_diff_state
+  )
 
-  _, diff_state, _ = graph.split(diff_graph_nodes, options.wrt, ...)
-  for i in diff_graph_nodes:
-    _args[i] = diff_state[i]
+  # add nodes to the args
+  for i, arg in diff_graph_nodes.items():
+    _args[i] = arg
 
-  transform = jax.value_and_grad if options.return_value else jax.grad
-  out_nodes = None
+  out = f(*_args)
 
-  argnums = options.argnums[0] if len(options.argnums) == 1 else options.argnums
+  out, out_nodes = graph.extract_graph_nodes(out)
 
-  @functools.partial(
-    transform,
-    argnums=argnums,
-    has_aux=True,
-    holomorphic=options.holomorphic,
-    allow_int=options.allow_int,
-    reduce_axes=options.reduce_axes,
-  )
-  def grad_fn(*args):
-    nonlocal out_nodes
+  graphdef_out, state_out = ctx.split((input_nodes, out_nodes))
+
+  if has_aux:
+    loss, aux = out
+    out = (loss, (graphdef_out, state_out, aux))
+  else:
+    out = (out, (graphdef_out, state_out))
+
+  return out
+
+
+def _grad_general(
+  f: tp.Callable[..., tp.Any],
+  argnums: int | tp.Sequence[int],
+  has_aux: bool,
+  holomorphic: bool,
+  allow_int: bool,
+  reduce_axes: tp.Sequence[AxisName],
+  wrt: filterlib.Filter,
+  return_value: bool,
+) -> tp.Callable[..., tp.Any]:
+  @graph.update_context('grad')
+  def grad_wrapper(*args):
+    ctx: graph.UpdateContext = graph.current_update_context('grad')
+    _argnums = _normalize_sequence(argnums)
+    _, input_nodes = graph.extract_graph_nodes(args)
 
     _args = list(args)
-    for i, graph_node in diff_graph_nodes.items():
-      diff_state: State = _args[i]
-      graph.update(graph_node, diff_state)
-      _args[i] = graph_node
-
-    out = f(*_args)
-    out, out_nodes = graph.extract_graph_nodes(out)
-
-    _, updates, _ = graph.flatten((input_nodes, out_nodes))
-
-    if options.has_aux:
-      loss, aux = out
-      out = (loss, (updates, aux))
-    else:
-      out = (out, updates)
+    diff_graph_nodes: dict[int, tp.Any] = {
+      i: arg
+      for i, arg in enumerate(args)
+      if i in _argnums and graph.is_node(arg)
+    }
+
+    def only_diff(path: tuple, value: tp.Any) -> bool:
+      # diff_graph_nodes is the first element in the tuple
+      return path[0] == 0
+
+    graphdef, diff_state, non_diff_state = ctx.split(
+      (diff_graph_nodes, input_nodes), filterlib.All(wrt, only_diff), ...
+    )  # type: ignore[misc]
+
+    # extract diff_state substates into the args
+    diff_args: list[int] = []
+    if 0 in diff_state:
+      for i, diff_substate in diff_state[0].items():  # type: ignore
+        assert isinstance(i, int)
+        _args[i] = diff_substate
+        diff_args.append(i)
+    transform = jax.value_and_grad if return_value else jax.grad
 
-    return out
+    _argnums = _argnums[0] if len(_argnums) == 1 else _argnums
 
-  out = grad_fn(*_args)
+    out = transform(
+      grad_fn,
+      argnums=_argnums,
+      has_aux=True,
+      holomorphic=holomorphic,
+      allow_int=allow_int,
+      reduce_axes=reduce_axes,
+    )(*_args, f, graphdef, non_diff_state, has_aux, diff_args)
 
-  updates: State
-  if options.return_value:
-    if options.has_aux:
-      (loss, (updates, aux)), grads = out
-      out = (loss, aux), grads
-    else:
-      (loss, updates), grads = out
-      out = loss, grads
-  else:
-    if options.has_aux:
-      grads, (updates, aux) = out
-      out = grads, aux
+    if return_value:
+      if has_aux:
+        (loss, (graphdef_out, state_out, aux)), grads = out
+        out = (loss, aux), grads
+      else:
+        (loss, (graphdef_out, state_out)), grads = out
+        out = loss, grads
     else:
-      out, updates = out
+      if has_aux:
+        grads, (graphdef_out, state_out, aux) = out
+        out = grads, aux
+      else:
+        out, (graphdef_out, state_out) = out
 
-  graph.update((input_nodes, out_nodes), updates)
-  return out
+    input_nodes, out_nodes = ctx.merge(graphdef_out, state_out)
+
+    out = graph.insert_graph_nodes(out, out_nodes)
+    return out
+
+  return grad_wrapper
 
 
 def grad(
   f: tp.Callable[..., tp.Any],
   argnums: int | tp.Sequence[int] = 0,
   has_aux: bool = False,
   holomorphic: bool = False,
@@ -745,15 +616,15 @@
   which by default is set to `nnx.Param`. Internally the ``State`` of
   graph nodes is extracted, filtered according to `wrt` filter, and
   passed to the underlying ``jax.grad`` function. The gradients
   of graph nodes are of type ``State``.
 
   Example::
 
-    >>> from flax.experimental import nnx
+    >>> from flax import nnx
     ...
     >>> m = nnx.Linear(2, 3, rngs=nnx.Rngs(0))
     >>> x = jnp.ones((1, 2))
     >>> y = jnp.ones((1, 3))
     ...
     >>> loss_fn = lambda m, x, y: jnp.mean((m(x) - y) ** 2)
     >>> grad_fn = nnx.grad(loss_fn, wrt=nnx.Param)
@@ -796,437 +667,143 @@
       function that computes the total gradient while ``grad(f)`` will create
       one that computes the per-example gradient.
     wrt: Optional, filterlib.Filter. Filter to extract the differentiable state
       of each graph node. Default is `nnx.Param`.
 
   """
 
-  if f.__name__ == '__init__':
-    raise ValueError('Cannot use `grad` with `__init__`')
-
-  _argnums = _normalize_sequence(argnums)
-  options = GradOptions(
-    argnums=_argnums,
-    wrt=wrt,
-    has_aux=has_aux,
-    holomorphic=holomorphic,
-    allow_int=allow_int,
-    reduce_axes=reduce_axes,
+  return _grad_general(
+    f,
+    argnums,
+    has_aux,
+    holomorphic,
+    allow_int,
+    reduce_axes,
+    wrt,
     return_value=False,
   )
 
-  @functools.wraps(f)
-  def grad_wrapper(*args):
-    return grad_apply(options, f, args)
-
-  return grad_wrapper  # type: ignore
-
-
-
 
 def value_and_grad(
   f: tp.Callable[..., tp.Any],
   argnums: int | tp.Sequence[int] = 0,
   has_aux: bool = False,
   holomorphic: bool = False,
   allow_int: bool = False,
   reduce_axes: tp.Sequence[AxisName] = (),
   *,
   wrt: filterlib.Filter = variables.Param,
 ) -> tp.Callable[..., tp.Any]:
-  if f.__name__ == '__init__':
-    raise ValueError('Cannot use `value_and_grad` with `__init__`')
-
-  _argnums = _normalize_sequence(argnums)
-  options = GradOptions(
-    argnums=_argnums,
-    has_aux=has_aux,
-    holomorphic=holomorphic,
-    allow_int=allow_int,
-    reduce_axes=reduce_axes,
+  return _grad_general(
+    f,
+    argnums,
+    has_aux,
+    holomorphic,
+    allow_int,
+    reduce_axes,
+    wrt,
     return_value=True,
-    wrt=wrt,
   )
 
-  @functools.wraps(f)
-  def value_and_grad_wrapper(*args):
-    return grad_apply(options, f, args)
 
-  return value_and_grad_wrapper  # type: ignore
-
-
-# -------------------------------
-# scan
-# -------------------------------
-
-@dataclasses.dataclass
-class ScanOptions:
-  length: int | None
-  reverse: bool
-  unroll: int | bool
-  _split_transpose: bool
-  # extended api
-  in_axes: tp.Any
-  in_axes_kwargs: tp.Any
-  out_axes: tp.Any
-  carry_argnum: int
-  # nnx specific
-  state_axes: tp.Mapping[filterlib.Filter, int]
-  split_rngs: filterlib.Filter
-  transform_metadata: tp.Mapping[str, tp.Any]
-  scan_output: bool
-
-
-class ScanMeta(ModuleMeta):
-  def __call__(
-    self,
-    module_constructor: tp.Callable[..., M],
+class Grad(tp.Generic[M], LiftedModule[M]):
+  @staticmethod
+  def constructor(
+    module_constructor: tp.Callable[..., MA],
+    has_aux: bool = False,
+    holomorphic: bool = False,
+    allow_int: bool = False,
+    reduce_axes: tp.Sequence[AxisName] = (),
+    return_value: bool = False,
     *,
-    length: int | None = None,
-    reverse: bool = False,
-    unroll: int | bool = 1,
-    _split_transpose: bool = False,
-    # extended api
-    in_axes: int | None | tp.Sequence[tp.Any] = 0,
-    in_axes_kwargs: tp.Any = 0,
-    out_axes: tp.Any = 0,
-    carry_argnum: int = 1,
-    # nnx specific
-    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
-    split_rngs: filterlib.Filter = ...,
-    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
-    scan_output: bool = True,
-  ) -> tp.Callable[..., 'Scan[M]']:
-    super_call = super().__call__
-
-    def _create_scan(*args, **kwargs) -> Scan[M]:
-      return super_call(
+    wrt: filterlib.Filter = variables.Param,
+  ) -> tp.Callable[..., 'Grad[MA]']:
+    def _create_grad(*args, **kwargs):
+      return Grad(
         module_constructor=module_constructor,
+        wrt=wrt,
+        has_aux=has_aux,
+        holomorphic=holomorphic,
+        allow_int=allow_int,
+        reduce_axes=reduce_axes,
+        return_value=return_value,
+        # submodule args
         module_init_args=args,
         module_init_kwargs=kwargs,
-        # base api
-        length=length,
-        reverse=reverse,
-        unroll=unroll,
-        _split_transpose=_split_transpose,
-        # extended api
-        in_axes=in_axes,
-        in_axes_kwargs=in_axes_kwargs,
-        out_axes=out_axes,
-        carry_argnum=carry_argnum,
-        # nnx specific
-        state_axes=state_axes,
-        split_rngs=split_rngs,
-        transform_metadata=transform_metadata,
-        scan_output=scan_output,
       )
 
-    return _create_scan
-
+    return _create_grad
 
-class Scan(LiftedModule[M], metaclass=ScanMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
+    argnums: int | tp.Sequence[int] = 0,
+    has_aux: bool = False,
+    holomorphic: bool = False,
+    allow_int: bool = False,
+    reduce_axes: tp.Sequence[AxisName] = (),
     *,
-    length: int | None = None,
-    reverse: bool = False,
-    unroll: int | bool = 1,
-    _split_transpose: bool = False,
-    # extended api
-    in_axes: int | None | tp.Sequence[tp.Any] = 0,
-    in_axes_kwargs: tp.Any = 0,
-    out_axes: tp.Any = 0,
-    carry_argnum: int = 1,
-    # nnx specific
-    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
-    split_rngs: filterlib.Filter = ...,
-    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
-    scan_output: bool = True,
+    wrt: filterlib.Filter = variables.Param,
     # submodule args
     module_init_args: tuple[tp.Any, ...],
     module_init_kwargs: dict[str, tp.Any],
   ):
     self.module_constructor = module_constructor
-    self.options = ScanOptions(
-      length=length,
-      reverse=reverse,
-      unroll=unroll,
-      _split_transpose=_split_transpose,
-      # extended api
-      in_axes=in_axes,
-      in_axes_kwargs=in_axes_kwargs,
-      out_axes=out_axes,
-      carry_argnum=carry_argnum,
-      # nnx specific
-      state_axes=state_axes,
-      split_rngs=split_rngs,
-      transform_metadata=transform_metadata,
-      scan_output=scan_output,
-    )
-    # use Vmap to handle initialisation
-    vmapped_module = Vmap(
-      module_constructor,
-      in_axes=in_axes,
-      out_axes=None,
-      axis_name=None,
-      axis_size=length,
-      spmd_axis_name=None,
-      state_axes=state_axes,
-      split_rngs=split_rngs,
-      in_axes_kwargs=in_axes_kwargs,
-      transform_metadata=transform_metadata,
-    )(*module_init_args, **module_init_kwargs)
-
-    self.scan_module = vmapped_module.vmap_module
-
-  @property
-  def _submodule(self) -> M:
-    return self.scan_module
-
-  def _call(
-    self, accessor: DelayedAccessor, *args, **kwargs
-  ) -> tuple[tp.Any, tp.Any]:
-    def scan_call_apply(module, *args, **kwargs):
-      method = accessor(module)
-      return method(*args, **kwargs)
-
-    return scan_apply(
-      self.options,
-      scan_call_apply,
-      (self._submodule, *args),
-      kwargs,
-    )
-
-
-def scan_apply(
-  options: ScanOptions,
-  f: tp.Callable[..., tuple[C, B] | C],
-  args: tuple[tp.Any, ...],
-  kwargs: dict[str, tp.Any],
-) -> tuple[C, B] | C:
-  # extract nodes
-  (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
-  input_rng_streams = rnglib.backup_keys(input_graph_nodes)
-
-  # extract carry arg
-  carry_arg, args = _extract_carry_arg(args, options.carry_argnum)
-
-  ctx = graph.UpdateContext()
-  # split module state
-  filters = (*options.state_axes.keys(), ...)
-  graphdef, rng_state, *scan_states, carry_state = ctx.split(
-    input_graph_nodes, rnglib.RngState, *filters
-  )
-
-  # transpose axes arg
-  flatdef, flat_scan, flat_carry = _transpose_and_split(
-    (args, kwargs, scan_states),
-    (
-      options.in_axes,
-      options.in_axes_kwargs,
-      list(options.state_axes.values()),
-    ),
-  )
-
-  # infer length
-  lengths: set[int] = set(
-    x.shape[axis]  # type: ignore
-    for x, axis in zip(flat_scan, flatdef.flat_axes)
-    if axis is not None
-  )
-
-  if len(lengths) > 1:
-    raise ValueError(
-      'Inconsistent lengths between state_axes states and '
-      f'arguments: {lengths}'
-    )
-  elif len(lengths) == 0:
-    if options.length is None:
-      raise ValueError(
-        'Cannot infer length from state_axes states or axes_arg, '
-        'please specify `length`'
-      )
-    length = options.length
-  else:
-    length = lengths.pop()
-    if options.length is not None and options.length != length:
-      raise ValueError(
-        f'Specified length {options.length} is not the same as the inferred '
-        f'length {length}'
-      )
-
-  # split rng state
-  split_keys, carry_keys = rnglib.fork(
-    rng_state,
-    options.split_rngs,
-    length,
-  )
-
-  def scan_fn(
-    carry: tuple[
-      State,  # carry_keys
-      State,  # carry_state
-      tp.Any,  # carry_arg
-    ],
-    scan: tuple[
-      State,  # split_keys
-      list[jax.Array | None],  # flat_scan
-    ],
-  ):
-    carry_keys, carry_state, carry_arg = carry
-    split_keys, flat_scan = scan
-
-    # merge args and kwargs
-    args, kwargs, scan_states = _unflatten_splits(
-      flatdef, flat_scan, flat_carry
-    )
-    # remove metadata axis name from Variable.sharding
-    if spmd.PARTITION_NAME in options.transform_metadata:
-      scan_states = [
-        spmd.remove_axis(state, index, options.transform_metadata)
-        for state, index in zip(scan_states, options.state_axes.values())
-      ]
-
-    # insert carry arg
-    args = _insert_carry_arg(args, options.carry_argnum, carry_arg)
-
-    # merge module state
-    input_graph_nodes = ctx.merge(
-      graphdef, *scan_states, carry_state, split_keys, carry_keys
-    )
-    (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
-
-    out = f(*args, **kwargs)
-
-    if options.scan_output:
-      if not isinstance(out, tuple) or len(out) != 2:
-        raise ValueError(
-          'Expected a tuple of length 2 as the output of the scan function, '
-          f'got {out}'
-        )
-      out = tp.cast(tuple[C, B], out)
-      carry_arg_out, scan_args_out = out
-    else:
-      out = tp.cast(C, out)
-      carry_arg_out = out
-      scan_args_out = None
-
-    (
-      (carry_arg_out, scan_args_out),
-      output_graph_nodes,
-    ) = graph.extract_graph_nodes((carry_arg_out, scan_args_out))
-
-    # split module state
-    (
-      graphdef_out,
-      rng_state_out,
-      *scan_states_out,
-      carry_state_out,
-    ) = ctx.split(
-      (input_graph_nodes, output_graph_nodes),
-      rnglib.RngState,
-      *filters,
+    self.grad_module = self.module_constructor(
+      *module_init_args, **module_init_kwargs
     )
 
-    not_keys_out, split_keys_out, carry_keys_out = rng_state_out.split(
-      rnglib.NotKey, options.split_rngs, ...
+    @functools.partial(
+      grad,
+      argnums=argnums,
+      has_aux=has_aux,
+      holomorphic=holomorphic,
+      allow_int=allow_int,
+      reduce_axes=reduce_axes,
+      wrt=wrt,
     )
-    carry_keys_out = State.merge(not_keys_out, carry_keys_out)
-
-    if 1 in carry_state_out:
-      raise ValueError(
-        f'Cannot add new carry state during scan, got {carry_state_out[1]}'
-      )
-    if 0 in carry_state_out:
-      carry_state_out = carry_state_out[0]
-      assert isinstance(carry_state_out, State)
-    if 1 in carry_keys_out:
-      raise ValueError(
-        f'Cannot add new carry keys during scan, got {carry_keys_out[1]}'
-      )
-    if 0 in carry_keys_out:
-      carry_keys_out = carry_keys_out[0]
-      assert isinstance(carry_keys_out, State)
-
-    # add metadata axis name to Variable.sharding
-    if spmd.PARTITION_NAME in options.transform_metadata:
-      scan_states_out = [
-        spmd.add_axis(state, index, options.transform_metadata)
-        for state, index in zip(scan_states_out, options.state_axes.values())
-      ]
-
-    carry_out = (carry_keys_out, carry_state_out, carry_arg_out)
-    scan_out = (graphdef_out, scan_args_out, scan_states_out, split_keys_out)
-
-    return carry_out, scan_out
+    def grad_call_apply(module, *args):
+      *args, accessor = args
+      method = accessor(module)
+      return method(*args)
 
-  carry = (carry_keys, carry_state, carry_arg)
-  scan = (split_keys, flat_scan)
+    self.grad_apply = grad_call_apply
 
-  carry_out, scan_out = jax.lax.scan(
-    scan_fn,
-    carry,
-    scan,
-    length=length,
-    reverse=options.reverse,
-    unroll=options.unroll,
-    _split_transpose=options._split_transpose,
-  )
-  carry_keys_out, carry_state_out, carry_arg_out = carry_out
-  graphdef_out, scan_args_out, scan_states_out, split_keys_out = scan_out
-
-  scan_args_out, scan_states_out = _transpose_tree(
-    (scan_args_out, scan_states_out),
-    (options.out_axes, list(options.state_axes.values())),
-    axis_is_source=False,
-  )
-
-  if carry_state_out:
-    carry_state_out = State({0: carry_state_out._mapping})
-  if carry_keys_out:
-    carry_keys_out = State({0: carry_keys_out._mapping})
-  _, output_graph_nodes = ctx.update(
-    graphdef_out,
-    *scan_states_out,
-    carry_state_out,
-    carry_keys_out,
-    split_keys_out,
-  )
+  @property
+  def _submodule(self) -> M:
+    return self.grad_module
 
-  carry_arg_out, scan_args_out = graph.insert_graph_nodes(
-    (carry_arg_out, scan_args_out), output_graph_nodes
-  )
+  def _call(self, accessor: DelayedAccessor, *args) -> tp.Any:
+    return self.grad_apply(self.grad_module, *args, accessor)
 
-  rnglib.restore_keys(input_rng_streams)
 
-  if options.scan_output:
-    scan_args_out = tp.cast(B, scan_args_out)
-    return carry_arg_out, scan_args_out
-  else:
-    return carry_arg_out
+# -------------------------------
+# scan
+# -------------------------------
 
 
 @dataclasses.dataclass(frozen=True)
 class FlatDef(tp.Generic[A]):
   type: type[A]
   treedef: jax.tree_util.PyTreeDef
   flat_axes: list[int | None]
 
+
 jax.tree_util.register_static(FlatDef)
 
-def _transpose_tree(tree: A, axes, /, *, axis_is_source: bool) -> A:
+
+def _transpose_tree(tree: A, axes, /, *, move_front: bool) -> A:
   flatdef, flat_transposes, _ = _transpose_and_split(
-    tree, axes, allow_none=False, axis_is_source=axis_is_source
+    tree, axes, allow_none=False, move_front=move_front
   )
   return flatdef.treedef.unflatten(flat_transposes)
 
 
 def _transpose_and_split(
-  tree: A, axes, /, *, allow_none: bool = True, axis_is_source: bool = True
+  tree: A, axes, /, *, allow_none: bool = True, move_front: bool = True
 ) -> tuple[
   FlatDef[A],
   list[jax.Array | None],
   list[tp.Any],
 ]:
   flat_axes: list[int | None] = broadcast_prefix(
     axes, tree, is_leaf=lambda x: x is None
@@ -1253,25 +830,33 @@
         if axis < -len(node.shape):
           raise ValueError(
             f'Axis {axis} out of bounds for array with shape {node.shape}'
           )
         axis = len(node.shape) + axis
         flat_axes[i] = axis
 
-      if axis_is_source:
+      if node.shape == ():
+        raise ValueError(f'Cannot map over a scalar array, got {node}')
+      elif axis >= len(node.shape):
+        raise ValueError(
+          f'Axis {axis} out of bounds for array with shape {node.shape}'
+        )
+
+      if move_front:
         node = jnp.moveaxis(node, axis, 0)
       else:
         node = jnp.moveaxis(node, 0, axis)
       flat_broadcasts.append(None)
       flat_transposes.append(node)
 
   flatdef = FlatDef(type(tree), treedef, flat_axes)
 
   return flatdef, flat_transposes, flat_broadcasts
 
+
 def _unflatten_splits(
   flatdef: FlatDef[A],
   flat_transposes: list[jax.Array | None],
   flat_broadcasts: list[tp.Any] | None = None,
   /,
   *,
   allow_none: bool = True,
@@ -1324,14 +909,148 @@
   args_ = list(args)
   args_[carry_argnum] = carry_arg
   args = tuple(args_)
 
   return args
 
 
+@struct.dataclass
+class ScanBroadcasts(tp.Generic[C, B]):
+  flatdef: FlatDef[
+    tuple[tuple[tp.Any, ...], dict[str, tp.Any], list[State]]
+  ] = struct.field(pytree_node=False)
+  flat_carry: list[tp.Any] = struct.field(pytree_node=True)
+  graphdef: GraphDef[tuple[tp.Any, ...]] = struct.field(pytree_node=False)
+  filters: tuple[filterlib.Filter, ...] = struct.field(pytree_node=False)
+  f: tp.Callable[..., tuple[C, B] | C] = struct.field(pytree_node=False)
+  # options
+  carry_argnum: int = struct.field(pytree_node=False)
+  state_axes: tp.Mapping[filterlib.Filter, int] = struct.field(
+    pytree_node=False
+  )
+  split_rngs: filterlib.Filter = struct.field(pytree_node=False)
+  transform_metadata: tp.Mapping[str, tp.Any] = struct.field(pytree_node=False)
+  scan_output: bool = struct.field(pytree_node=False)
+
+
+def scan_fn(
+  carry: tuple[
+    State,  # split_rng_state
+    State,  # broadcast_rng_state
+    State,  # carry_state
+    tp.Any,  # carry_arg
+    ScanBroadcasts[C, B],  # broadcasts
+  ],
+  scan: tuple[
+    list[jax.Array | None],  # flat_scan
+  ],
+):
+  split_rng_state, broadcast_rng_state, carry_state, carry_arg, broadcasts = (
+    carry
+  )
+  (flat_scan,) = scan
+  flatdef = broadcasts.flatdef
+  flat_carry = broadcasts.flat_carry
+  graphdef, filters = broadcasts.graphdef, broadcasts.filters
+  f = broadcasts.f
+  ctx = graph.current_update_context('scan')
+
+  # merge args and kwargs
+  args, kwargs, scan_states = _unflatten_splits(flatdef, flat_scan, flat_carry)
+  # remove metadata axis name from Variable.sharding
+  if spmd.PARTITION_NAME in broadcasts.transform_metadata:
+    scan_states = [
+      spmd.remove_axis(state, index, broadcasts.transform_metadata)
+      for state, index in zip(scan_states, broadcasts.state_axes.values())
+    ]
+
+  # insert carry arg
+  args = _insert_carry_arg(args, broadcasts.carry_argnum, carry_arg)
+
+  # merge module state
+  input_graph_nodes = ctx.merge(
+    graphdef, *scan_states, carry_state, split_rng_state, broadcast_rng_state
+  )
+  (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
+
+  out = f(*args, **kwargs)
+
+  if broadcasts.scan_output:
+    if not isinstance(out, tuple) or len(out) != 2:
+      raise ValueError(
+        'Expected a tuple of length 2 as the output of the scan function, '
+        f'got {out}'
+      )
+    out = tp.cast(tuple[C, B], out)  # type: ignore[invalid-annotation]
+    carry_arg_out, scan_args_out = out
+  else:
+    out = tp.cast(C, out)  # type: ignore[invalid-annotation]
+    carry_arg_out = out
+    scan_args_out = None
+
+  (
+    (carry_arg_out, scan_args_out),
+    output_graph_nodes,
+  ) = graph.extract_graph_nodes((carry_arg_out, scan_args_out))
+
+  # split module state
+  (
+    graphdef_out,
+    rng_state_out,
+    *scan_states_out,
+    carry_state_out,
+  ) = ctx.split(  # type: ignore[misc]
+    (input_graph_nodes, output_graph_nodes),
+    rnglib.RngState,
+    *filters,
+  )
+
+  split_rng_state_out, broadcast_rng_state_out = rng_state_out.split(
+    broadcasts.split_rngs, ...
+  )
+
+  def _extract_carry_state(state: State, /):
+    if 1 in state:
+      raise ValueError(
+        f'Cannot add new carry state during scan, got {state[1]}'
+      )
+    if 0 in state:
+      _state = state[0]
+      assert isinstance(_state, State)
+      state = _state
+
+    return state
+
+  carry_state_out = _extract_carry_state(carry_state_out)
+  split_rng_state_out = _extract_carry_state(split_rng_state_out)
+  broadcast_rng_state_out = _extract_carry_state(broadcast_rng_state_out)
+
+  # override  broadcast_rng_state_out to keep the same state
+  # for the next iteration
+  broadcast_rng_state_out = broadcast_rng_state
+
+  # add metadata axis name to Variable.sharding
+  if spmd.PARTITION_NAME in broadcasts.transform_metadata:
+    scan_states_out = [
+      spmd.add_axis(state, index, broadcasts.transform_metadata)
+      for state, index in zip(scan_states_out, broadcasts.state_axes.values())
+    ]
+
+  carry_out = (
+    split_rng_state_out,
+    broadcast_rng_state_out,
+    carry_state_out,
+    carry_arg_out,
+    broadcasts,
+  )
+  scan_out = (graphdef_out, scan_args_out, scan_states_out)
+
+  return carry_out, scan_out
+
+
 def scan(
   f: F,
   *,
   length: int | None = None,
   reverse: bool = False,
   unroll: int | bool = 1,
   _split_transpose: bool = False,
@@ -1342,62 +1061,265 @@
   carry_argnum: int = 0,
   # nnx specific
   state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
   split_rngs: filterlib.Filter = ...,
   transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
   scan_output: bool = True,
 ) -> F:
-  options = ScanOptions(
-    length=length,
-    reverse=reverse,
-    unroll=unroll,
-    _split_transpose=_split_transpose,
-    in_axes=in_axes,
-    in_axes_kwargs=in_axes_kwargs,
-    out_axes=out_axes,
-    carry_argnum=carry_argnum,
-    state_axes=state_axes,
-    split_rngs=split_rngs,
-    transform_metadata=transform_metadata,
-    scan_output=scan_output,
-  )
-
   @functools.wraps(f)
-  def scan_apply_wrapper(*args, **kwargs) -> C | tuple[C, tp.Any]:
-    return scan_apply(options, f, args, kwargs)
+  @graph.update_context('scan')
+  def scan_apply_wrapper(*args, **kwargs):
+    # extract nodes
+    (args, kwargs), input_graph_nodes = graph.extract_graph_nodes(
+      (args, kwargs)
+    )
+    input_rng_streams = rnglib.backup_keys(input_graph_nodes)
 
-  return scan_apply_wrapper  # type: ignore
+    # extract carry arg
+    carry_arg, args = _extract_carry_arg(args, carry_argnum)
 
+    ctx = graph.current_update_context('scan')
+    # split module state
+    filters = (*state_axes.keys(), ...)
+    graphdef, rng_state, *scan_states, carry_state = ctx.split(  # type: ignore[misc]
+      input_graph_nodes, rnglib.RngState, *filters
+    )
 
-# -------------------------------
-# remat
-# -------------------------------
+    # transpose axes arg
+    flatdef, flat_scan, flat_carry = _transpose_and_split(
+      (args, kwargs, scan_states),
+      (in_axes, in_axes_kwargs, list(state_axes.values())),
+    )
 
+    # infer length
+    lengths: set[int] = set(
+      x.shape[0]  # type: ignore
+      for x, axis in zip(flat_scan, flatdef.flat_axes)
+      if axis is not None
+    )
 
-class RematMeta(ModuleMeta):
-  def __call__(
-    self,
-    module_constructor: tp.Callable[..., M],
-    prevent_cse: bool = True,
-    static_argnums: int | tuple[int, ...] = (),
-    policy: tp.Callable[..., bool] | None = None,
-  ) -> tp.Callable[..., 'Remat[M]']:
-    super_call = super().__call__
+    if len(lengths) > 1:
+      raise ValueError(
+        'Inconsistent lengths between state_axes states and '
+        f'arguments: {lengths}'
+      )
+    elif len(lengths) == 0:
+      if length is None:
+        raise ValueError(
+          'Cannot infer length from state_axes states or axes_arg, '
+          'please specify `length`'
+        )
+      infered_length = length
+    else:
+      infered_length = lengths.pop()
+      if length is not None and length != infered_length:
+        raise ValueError(
+          f'Specified length {length} is not the same as the inferred '
+          f'length {infered_length}'
+        )
+
+    # split rng state
+    split_rng_state, broadcast_rng_state = rng_state.split(split_rngs, ...)
+
+    broadcasts = ScanBroadcasts(
+      flatdef,
+      flat_carry,
+      graphdef,
+      filters,
+      f,
+      # options
+      carry_argnum,
+      state_axes,
+      split_rngs,
+      transform_metadata,
+      scan_output,
+    )
+    carry = (
+      split_rng_state,
+      broadcast_rng_state,
+      carry_state,
+      carry_arg,
+      broadcasts,
+    )
+    scan = (flat_scan,)
+
+    carry_out, scan_out = jax.lax.scan(
+      scan_fn,
+      carry,
+      scan,
+      length=infered_length,
+      reverse=reverse,
+      unroll=unroll,
+      _split_transpose=_split_transpose,
+    )
+    (
+      split_rng_state_out,
+      broadcast_rng_state_out,
+      carry_state_out,
+      carry_arg_out,
+      broadcasts,
+    ) = carry_out
+    graphdef_out, scan_args_out, scan_states_out = scan_out
+
+    scan_args_out, scan_states_out = _transpose_tree(
+      (scan_args_out, scan_states_out),
+      (out_axes, list(state_axes.values())),
+      move_front=False,
+    )
+
+    if carry_state_out:
+      carry_state_out = State({0: carry_state_out._mapping})
+    if split_rng_state_out:
+      split_rng_state_out = State({0: split_rng_state_out._mapping})
+    if broadcast_rng_state_out:
+      broadcast_rng_state_out = State({0: broadcast_rng_state_out._mapping})
+
+    _, output_graph_nodes = ctx.merge(
+      graphdef_out,
+      *scan_states_out,
+      carry_state_out,
+      split_rng_state_out,
+      broadcast_rng_state_out,
+    )
+
+    carry_arg_out, scan_args_out = graph.insert_graph_nodes(
+      (carry_arg_out, scan_args_out), output_graph_nodes
+    )
+
+    rnglib.restore_keys(input_rng_streams)
 
-    def create_remat(*args, **kwargs) -> Remat[M]:
-      return super_call(
+    if scan_output:
+      scan_args_out = tp.cast(B, scan_args_out)
+      return carry_arg_out, scan_args_out
+    else:
+      return carry_arg_out
+
+  return scan_apply_wrapper  # type: ignore
+
+
+class Scan(tp.Generic[M], LiftedModule[M]):
+  @staticmethod
+  def constructor(
+    module_constructor: tp.Callable[..., MA],
+    *,
+    length: int | None = None,
+    reverse: bool = False,
+    unroll: int | bool = 1,
+    _split_transpose: bool = False,
+    # extended api
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
+    in_axes_kwargs: tp.Any = 0,
+    out_axes: tp.Any = 0,
+    carry_argnum: int = 1,
+    # nnx specific
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
+    scan_output: bool = True,
+  ) -> tp.Callable[..., 'Scan[MA]']:
+    def _create_scan(*args, **kwargs):
+      return Scan(
         module_constructor=module_constructor,
         module_init_args=args,
         module_init_kwargs=kwargs,
-        prevent_cse=prevent_cse,
-        static_argnums=static_argnums,
-        policy=policy,
+        # base api
+        length=length,
+        reverse=reverse,
+        unroll=unroll,
+        _split_transpose=_split_transpose,
+        # extended api
+        in_axes=in_axes,
+        in_axes_kwargs=in_axes_kwargs,
+        out_axes=out_axes,
+        carry_argnum=carry_argnum,
+        # nnx specific
+        state_axes=state_axes,
+        split_rngs=split_rngs,
+        transform_metadata=transform_metadata,
+        scan_output=scan_output,
       )
 
-    return create_remat
+    return _create_scan
+
+  def __init__(
+    self,
+    module_constructor: tp.Callable[..., M],
+    *,
+    length: int | None = None,
+    reverse: bool = False,
+    unroll: int | bool = 1,
+    _split_transpose: bool = False,
+    # extended api
+    in_axes: int | None | tp.Sequence[tp.Any] = 0,
+    in_axes_kwargs: tp.Any = 0,
+    out_axes: tp.Any = 0,
+    carry_argnum: int = 1,
+    # nnx specific
+    state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
+    split_rngs: filterlib.Filter = ...,
+    transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
+    scan_output: bool = True,
+    # submodule args
+    module_init_args: tuple[tp.Any, ...],
+    module_init_kwargs: dict[str, tp.Any],
+  ):
+    self.module_constructor = module_constructor
+    # use Vmap to handle initialisation
+    vmapped_module = Vmap.constructor(
+      module_constructor,
+      in_axes=in_axes,
+      out_axes=None,
+      axis_name=None,
+      axis_size=length,
+      spmd_axis_name=None,
+      state_axes=state_axes,
+      split_rngs=split_rngs,
+      in_axes_kwargs=in_axes_kwargs,
+      transform_metadata=transform_metadata,
+    )(*module_init_args, **module_init_kwargs)
+    self.scan_module = vmapped_module.vmap_module
+
+    @functools.partial(
+      scan,
+      length=length,
+      reverse=reverse,
+      unroll=unroll,
+      _split_transpose=_split_transpose,
+      in_axes=in_axes,
+      in_axes_kwargs=in_axes_kwargs,
+      out_axes=out_axes,
+      carry_argnum=carry_argnum,
+      state_axes=state_axes,
+      split_rngs=split_rngs,
+      transform_metadata=transform_metadata,
+      scan_output=scan_output,
+    )
+    def scan_call_apply(
+      module, *args, _nnx_scan_accessor: DelayedAccessor, **kwargs
+    ):
+      method = _nnx_scan_accessor(module)
+      return method(*args, **kwargs)
+
+    self.scan_fn = scan_call_apply
+
+  @property
+  def _submodule(self) -> M:
+    return self.scan_module
+
+  def _call(
+    self, accessor: DelayedAccessor, *args, **kwargs
+  ) -> tuple[tp.Any, tp.Any]:
+    return self.scan_fn(
+      self._submodule, *args, _nnx_scan_accessor=accessor, **kwargs
+    )
+
+
+# -------------------------------
+# remat
+# -------------------------------
 
 
 @dataclasses.dataclass
 class RematOptions:
   prevent_cse: bool
   static_argnums: int | tuple[int, ...]
   policy: tp.Callable[..., bool] | None
@@ -1408,15 +1330,34 @@
 
     # add 1 as an offset to account for state parameter
     self.static_argnums = tuple(
       x + 1 if x >= 0 else x for x in self.static_argnums
     )
 
 
-class Remat(LiftedModule[M], metaclass=RematMeta):
+class Remat(tp.Generic[M], LiftedModule[M]):
+  @staticmethod
+  def constructor(
+    module_constructor: tp.Callable[..., MA],
+    prevent_cse: bool = True,
+    static_argnums: int | tuple[int, ...] = (),
+    policy: tp.Callable[..., bool] | None = None,
+  ) -> tp.Callable[..., 'Remat[MA]']:
+    def create_remat(*args, **kwargs):
+      return Remat(
+        module_constructor=module_constructor,
+        module_init_args=args,
+        module_init_kwargs=kwargs,
+        prevent_cse=prevent_cse,
+        static_argnums=static_argnums,
+        policy=policy,
+      )
+
+    return create_remat
+
   def __init__(
     self,
     *,
     module_constructor: tp.Callable[..., M],
     prevent_cse: bool = True,
     static_argnums: int | tuple[int, ...] = (),
     policy: tp.Callable[..., bool] | None = None,
@@ -1445,21 +1386,21 @@
 
     return remat_apply(
       self.options,
       remat_apply_call,
       (self.remat_module, *args),
     )
 
-
+@graph.update_context('remat')
 def remat_apply(
   options: RematOptions,
   f: tp.Callable[..., tp.Any],
   args: tuple[tp.Any, ...],
 ):
-  ctx = graph.UpdateContext()
+  ctx = graph.current_update_context('remat')
   args, input_nodes = graph.extract_graph_nodes(args)
   graphdef, state = ctx.split(input_nodes)
 
   def _remat_fn(state: State, *args):
     input_nodes = ctx.merge(graphdef, state)
     args = graph.insert_graph_nodes(args, input_nodes)
     out = f(*args)
@@ -1471,15 +1412,15 @@
   (new_graphdef, new_state), out = jax.checkpoint(
     _remat_fn,
     prevent_cse=options.prevent_cse,
     static_argnums=options.static_argnums,
     policy=options.policy,
   )(state, *args)
 
-  _, output_nodes = ctx.update(new_graphdef, new_state)
+  _, output_nodes = ctx.merge(new_graphdef, new_state)
   out = graph.insert_graph_nodes(out, output_nodes)
 
   return out
 
 
 def remat(
   f: F,
@@ -1501,48 +1442,47 @@
   return remat_wrapper  # type: ignore
 
 
 # -------------------------------
 # vmap
 # -------------------------------
 
+
 @dataclasses.dataclass
 class VmapOptions:
   in_axes: int | None | tp.Sequence[tp.Any]
   out_axes: tp.Any
   axis_name: AxisName | None
   axis_size: int | None
   spmd_axis_name: AxisName | tuple[AxisName, ...] | None
   # nnx specific
   state_axes: tp.Mapping[filterlib.Filter, int]
   split_rngs: filterlib.Filter
   in_axes_kwargs: tp.Any
   transform_metadata: tp.Mapping[str, tp.Any]
 
 
-class VmapMeta(ModuleMeta):
-  def __call__(
-    self,
-    module_constructor: tp.Callable[..., M],
+class Vmap(tp.Generic[M], LiftedModule[M]):
+  @staticmethod
+  def constructor(
+    module_constructor: tp.Callable[..., MA],
     *,
     in_axes: int | None | tp.Sequence[tp.Any] = 0,
     out_axes: tp.Any = 0,
     axis_name: AxisName | None = None,
     axis_size: int | None = None,
     spmd_axis_name: AxisName | tuple[AxisName, ...] | None = None,
     # nnx specific
     in_axes_kwargs: tp.Any = 0,
     state_axes: tp.Mapping[filterlib.Filter, int] = FrozenDict({...: 0}),
     split_rngs: filterlib.Filter = ...,
     transform_metadata: tp.Mapping[str, tp.Any] = FrozenDict({}),
-  ) -> tp.Callable[..., 'Vmap[M]']:
-    super_call = super().__call__
-
-    def _create_vmap(*args, **kwargs) -> Scan[M]:
-      return super_call(
+  ) -> tp.Callable[..., 'Vmap[MA]']:
+    def _create_vmap(*args, **kwargs):
+      return Vmap(
         module_constructor=module_constructor,
         in_axes=in_axes,
         out_axes=out_axes,
         axis_size=axis_size,
         axis_name=axis_name,
         spmd_axis_name=spmd_axis_name,
         # nnx specific
@@ -1553,16 +1493,14 @@
         # submodule args
         module_init_args=args,
         module_init_kwargs=kwargs,
       )
 
     return _create_vmap
 
-
-class Vmap(LiftedModule[M], metaclass=VmapMeta):
   def __init__(
     self,
     module_constructor: tp.Callable[..., M],
     *,
     in_axes: int | None | tp.Sequence[tp.Any] = 0,
     out_axes: tp.Any = 0,
     axis_name: AxisName | None = None,
@@ -1621,27 +1559,28 @@
     return vmap_apply(
       self.options,
       vmap_apply_call,
       (self._submodule, *args),
       kwargs,
     )
 
+@graph.update_context('vmap')
 def vmap_apply(
   options: VmapOptions,
   f: tp.Callable[..., A],
   args: tuple[tp.Any, ...],
   kwargs: dict[str, tp.Any],
 ) -> A:
   (args, kwargs), input_graph_nodes = graph.extract_graph_nodes((args, kwargs))
   input_rng_streams = rnglib.backup_keys(input_graph_nodes)
 
-  ctx = graph.UpdateContext()
+  ctx = graph.current_update_context('vmap')
   # split module state
   filters = (*options.state_axes.keys(), ...)
-  graphdef, rng_state, *vectorized_states, broadcast_state = ctx.split(
+  graphdef, rng_state, *vectorized_states, broadcast_state = ctx.split(  # type: ignore[misc]
     input_graph_nodes, rnglib.RngState, *filters
   )
 
   # infer length
   axis_sizes: tp.Set[int] = set()
   args_sizes = jax.tree_util.tree_map(
     lambda axis, node: jax.tree_util.tree_map(lambda x: x.shape[axis], node)
@@ -1678,15 +1617,15 @@
     axis_size = axis_sizes.pop()
     if options.axis_size is not None and options.axis_size != axis_size:
       raise ValueError(
         f'Specified axis_size {options.axis_size} is not the same as the'
         f' inferred length {axis_size}'
       )
 
-  split_keys, broadcast_keys = rnglib.fork(
+  split_keys, split_counts, broadcast_keys, broadcast_counts = rnglib.fork(
     rng_state,
     options.split_rngs,
     axis_size,
   )
 
   keys_axes = 0
   states_axes = list(options.state_axes.values())
@@ -1722,30 +1661,36 @@
       vectorized_states = [
         spmd.remove_axis(state, index, options.transform_metadata)
         for state, index in zip(vectorized_states, options.state_axes.values())
       ]
 
     # merge module state
     input_graph_nodes = ctx.merge(
-      graphdef, *vectorized_states, broadcast_state, split_keys, broadcast_keys
+      graphdef,
+      *vectorized_states,
+      broadcast_state,
+      split_keys,
+      split_counts,
+      broadcast_keys,
+      broadcast_counts,
     )
 
     (args, kwargs) = graph.insert_graph_nodes((args, kwargs), input_graph_nodes)
 
     out = f(*args, **kwargs)
 
     out, output_graph_nodes = graph.extract_graph_nodes(out)
 
     # split module state
     (
       graphdef_out,
       rng_state_out,
       *vectorized_states_out,
       broadcast_state_out,
-    ) = ctx.split(
+    ) = ctx.split(  # type: ignore[misc]
       (input_graph_nodes, output_graph_nodes),
       rnglib.RngState,
       *filters,
     )
 
     not_keys_out, split_keys_out, broadcast_keys_out = rng_state_out.split(
       rnglib.NotKey, options.split_rngs, ...
@@ -1776,15 +1721,15 @@
     graphdef_out,
     broadcast_state,
     vectorized_states,
     split_keys_out,
     out,
   ) = vmap_fn(split_keys, vectorized_states, args, kwargs)
 
-  _, output_graph_nodes = ctx.update(
+  _, output_graph_nodes = ctx.merge(
     graphdef_out,
     *vectorized_states,
     broadcast_state,
     split_keys_out,
   )
 
   out = graph.insert_graph_nodes(out, output_graph_nodes)
@@ -1824,14 +1769,15 @@
   def vmap_apply_wrapper(*args, **kwargs) -> tp.Any:
     return vmap_apply(options, f, args, kwargs)
 
   wrapper = vmap_apply_wrapper
 
   return wrapper  # type: ignore
 
+
 # -------------------------------
 # eval_shape
 # -------------------------------
 
 
 def eval_shape(
   f: tp.Callable[..., A],
@@ -1852,8 +1798,75 @@
 
   graphdef_out, state_out, out = jax.eval_shape(
     _eval_shape_fn, state, *args, **kwargs
   )
 
   output_nodes = graph.merge(graphdef_out, state_out)
   out = graph.insert_graph_nodes(out, output_nodes)
-  return out
+  return out
+
+
+# -------------------------------
+# cond
+# -------------------------------
+
+
+@dataclasses.dataclass(frozen=True)
+class CondStaticInputs(tp.Generic[A]):
+  true_fun: tp.Callable[..., A]
+  false_fun: tp.Callable[..., A]
+
+
+jax.tree_util.register_static(CondStaticInputs)
+
+
+def _cond_fun(
+  is_true: bool,
+  static_inputs: CondStaticInputs[A],
+  graphdef: GraphDef[tuple[tp.Any, ...]],
+  state: State,
+):
+  ctx = graph.current_update_context('cond')
+  fn = static_inputs.true_fun if is_true else static_inputs.false_fun
+  operands = ctx.merge(graphdef, state)
+  out = fn(*operands)
+  graphdef_out, state_out = ctx.split((operands, out))
+  return graphdef_out, state_out
+
+
+def _cond_true_fun(
+  static_inputs: CondStaticInputs[A],
+  graphdef: GraphDef[tuple[tp.Any, ...]],
+  state: State,
+):
+  return _cond_fun(True, static_inputs, graphdef, state)
+
+
+def _cond_false_fun(
+  static_inputs: CondStaticInputs[A],
+  graphdef: GraphDef[tuple[tp.Any, ...]],
+  state: State,
+):
+  return _cond_fun(False, static_inputs, graphdef, state)
+
+
+@graph.update_context('cond')
+def cond(
+  pred,
+  true_fun: tp.Callable[..., A],
+  false_fun: tp.Callable[..., A],
+  *operands,
+  **kwargs,
+) -> A:
+  ctx: graph.UpdateContext = graph.current_update_context('cond')
+  graphdef, state = ctx.split(operands)
+  graphdef_out, state_out = jax.lax.cond(
+    pred,
+    _cond_true_fun,
+    _cond_false_fun,
+    CondStaticInputs(true_fun=true_fun, false_fun=false_fun),
+    graphdef,
+    state,
+    **kwargs,
+  )
+  _operands_out, out = ctx.merge(graphdef_out, state_out)
+  return out
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/variables.py` & `flax-0.8.4/flax/nnx/nnx/variables.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,26 +21,27 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an 'AS IS' BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+# pytype: skip-file
+from __future__ import annotations
 
 import dataclasses
 import functools
-import typing as tp
 from functools import partial
+import typing as tp
 from typing import Any
 
+from flax import nnx
+from flax.nnx.nnx import reprlib, tracers
 import jax.tree_util as jtu
 
-from flax.experimental.nnx.nnx import reprlib, tracers
-from flax.experimental import nnx
-
 A = tp.TypeVar('A')
 B = tp.TypeVar('B')
 F = tp.TypeVar('F', bound=tp.Callable[..., tp.Any])
 V = tp.TypeVar('V', bound='Variable[Any]')
 GetValueHook = tp.Callable[['Variable[A]', A], A]
 SetValueHook = tp.Callable[['Variable[A]', A], A]
 CreateValueHook = tp.Callable[['Variable[A]', A], A]
@@ -65,19 +66,21 @@
 
 jtu.register_pytree_node(
   Empty,
   lambda empty: ((), None),
   lambda _0, _1: EMPTY,
 )
 
-EMPTY = Empty()
+EMPTY: Empty = Empty()
+
 
 class _Missing:
   pass
 
+
 MISSING = _Missing()
 
 
 @dataclasses.dataclass
 class VariableMetadata(tp.Generic[A]):
   raw_value: A
   set_value_hooks: tuple[SetValueHook[A], ...] = ()
@@ -218,16 +221,15 @@
     vars(self).update(metadata)
 
     # run create_value hooks
     self.raw_value = self.create_value(self.raw_value)
 
   if tp.TYPE_CHECKING:
 
-    def __getattr__(self, name: str) -> tp.Any:
-      ...
+    def __getattr__(self, name: str) -> tp.Any: ...
   else:
 
     def __setattr__(self, name: str, value: Any) -> None:
       return self._setattr(name, value)
 
   def _setattr(self, name: str, value: tp.Any):
     if not self._trace_state.is_valid():
@@ -298,20 +300,18 @@
     for hook in self.remove_axis_hooks:
       hook(self, axis_name, axis_index)
 
   def __eq__(self, other: object) -> bool:
     return type(self) is type(other) and vars(other) == vars(self)
 
   @tp.overload
-  def replace(self, value: B, **kwargs) -> 'Variable[B]':
-    ...
+  def replace(self, value: B, **kwargs) -> 'Variable[B]': ...
 
   @tp.overload
-  def replace(self, **kwargs) -> 'Variable[A]':
-    ...
+  def replace(self, **kwargs) -> 'Variable[A]': ...
 
   def replace(self, value: tp.Any = MISSING, **kwargs) -> 'Variable[tp.Any]':
     if value is not MISSING:
       kwargs['raw_value'] = value
 
     # rename `value` to `raw_value`
     if 'value' in kwargs:
@@ -381,21 +381,20 @@
       raise NotImplementedError
 
     def on_remove_axis(
       self: V, axis_name: AxisName, axis_index: AxisIndex
     ) -> V:
       raise NotImplementedError
 
-
   # operator overloads
   def __jax_array__(self):
     return self.value
 
   def __getitem__(self, key) -> tp.Any:
-    return self.value.__getitem__(key)
+    return self.value.__getitem__(key)  # type: ignore
 
   def __add__(self, other) -> A:
     return self.value.__add__(other)  # type: ignore
 
   def __sub__(self, other) -> A:
     return self.value.__sub__(other)  # type: ignore
 
@@ -568,26 +567,25 @@
 class Intermediate(Variable[A]):
   pass
 
 
 class VariableState(tp.Generic[A], reprlib.Representable):
   def __init__(
     self,
-    type: tp.Type[Variable[A]],
+    type: type[Variable[tp.Any]],
     value: A,
     **metadata,
   ):
     self.type = type
     self.value = value
     vars(self).update(metadata)
 
   if tp.TYPE_CHECKING:
 
-    def __getattr__(self, name: str) -> tp.Any:
-      ...
+    def __getattr__(self, name: str) -> tp.Any: ...
 
   def __nnx_repr__(self):
     yield reprlib.Object(type=type(self))
     yield reprlib.Attr('type', self.type.__name__)
 
     for name, value in vars(self).items():
       if name == 'type' or name.endswith('_hooks'):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/nnx/visualization.py` & `flax-0.8.4/flax/nnx/nnx/visualization.py`

 * *Files 3% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 import dataclasses
 import importlib.util
 import typing as tp
 
 import jax
 
-from flax.experimental import nnx
+from flax import nnx
 
 penzai_installed = importlib.util.find_spec('penzai') is not None
 try:
   from IPython import get_ipython
 
   in_ipython = get_ipython() is not None
 except ImportError:
@@ -36,15 +36,15 @@
   will print the objects instead.
   """
   if not penzai_installed or not in_ipython:
     for x in args:
       print(x)
     return
 
-  from penzai import pz
+  from penzai import pz  # type: ignore[import-not-found,import-untyped]
 
   with pz.ts.active_autovisualizer.set_scoped(pz.ts.ArrayAutovisualizer()):
     for x in args:
       value = to_dataclass(x)
       pz.ts.display(value, ignore_exceptions=True)
 
 
@@ -66,15 +66,15 @@
     node_dict = node_impl.node_dict(x)
     node_dict = {
       str(key): _treemap_to_dataclass(value, seen_nodes)
       for key, value in node_dict.items()
     }
     dc_type = _make_dataclass_obj(
       type(x),
-      node_dict,
+      {str(key): value for key, value in node_dict.items()},
     )
     return dc_type
   elif isinstance(x, (nnx.Variable, nnx.VariableState)):
     obj_vars = vars(x).copy()
     if 'raw_value' in obj_vars:
       obj_vars['value'] = obj_vars.pop('raw_value')
     if '_trace_state' in obj_vars:
@@ -105,17 +105,17 @@
     _to_dataclass_fn,
     node,
     is_leaf=lambda x: isinstance(x, (nnx.VariableState, nnx.State)),
   )
 
 
 def _make_dataclass_obj(
-  cls, fields: dict[str, tp.Any], penzai_dataclass: bool = True
+  cls, fields: tp.Mapping[str, tp.Any], penzai_dataclass: bool = True
 ) -> tp.Type:
-  from penzai import pz
+  from penzai import pz  # type: ignore[import-error]
 
   dataclass = pz.pytree_dataclass if penzai_dataclass else dataclasses.dataclass
   base = pz.Layer if penzai_dataclass else object
 
   attributes = {
     '__annotations__': {key: type(value) for key, value in fields.items()},
   }
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/__init__.py` & `flax-0.8.4/flax/nnx/nnx/nn/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/nn/test_attention.py` & `flax-0.8.4/flax/nnx/tests/nn/test_attention.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jax, jax.numpy as jnp
 from jax.lax import Precision
 
 from flax import linen
-from flax.experimental import nnx
+from flax import nnx
 from flax.typing import Dtype, PrecisionLike
 
 from numpy.testing import assert_array_equal
 
 import typing as tp
 from absl.testing import parameterized
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/nn/test_conv.py` & `flax-0.8.4/flax/nnx/tests/nn/test_embed.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,96 +8,65 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from collections.abc import Sequence
 import typing as tp
 
 import jax
 from absl.testing import parameterized
 from jax import numpy as jnp
-from jax.lax import Precision
 from numpy.testing import assert_array_equal
 
 from flax import linen
-from flax.experimental import nnx
-from flax.typing import PaddingLike, Dtype, PrecisionLike
+from flax import nnx
+from flax.typing import Dtype
 
 
-class TestConvLinenConsistency(parameterized.TestCase):
+class TestLinenConsistency(parameterized.TestCase):
   @parameterized.product(
-    strides=[None, (2, 3)],
-    padding=['VALID', (4, 2)],
-    input_dilation=[(2, 3)],
-    kernel_dilation=[(2, 3)],
-    feature_group_count=[3],
-    use_bias=[True, False],
-    dtype=[jnp.float32],
-    param_dtype=[jnp.float16],
-    precision=[Precision.HIGHEST],
+    input_dtype=[jnp.int16, jnp.int32],
+    num_embeddings=[1, 7],
+    dtype=[jnp.float32, jnp.float16],
+    param_dtype=[jnp.float32, jnp.float16],
   )
   def test_nnx_linen_equivalence(
     self,
-    strides: tp.Union[None, int, tp.Sequence[int]],
-    padding: PaddingLike,
-    input_dilation: tp.Union[None, int, tp.Sequence[int]],
-    kernel_dilation: tp.Union[None, int, tp.Sequence[int]],
-    feature_group_count: int,
-    use_bias: bool,
+    input_dtype: tp.Optional[Dtype],
+    num_embeddings: int,
     dtype: tp.Optional[Dtype],
     param_dtype: Dtype,
-    precision: PrecisionLike,
   ):
     key = jax.random.key(42)
     rngs = nnx.Rngs(42)
-    IN_FEATURES = 3
-    OUT_FEATURES = 6
-    INPUT_SHAPE = (24, 9, IN_FEATURES)
-    kernel_size = (7, 4)
-
-    # Cannot use string padding specification for transpose conv
-    if isinstance(input_dilation, Sequence) or input_dilation > 1:
-      padding = (4, 2)
+    IN_FEATURES = 32
+    NUM_EMBEDDINGS = num_embeddings
 
-    x = jax.numpy.ones(INPUT_SHAPE)
+    x = jax.numpy.arange(NUM_EMBEDDINGS, dtype=input_dtype)
     model_nnx = nnx.eval_shape(
-      lambda rngs: nnx.Conv(
+      lambda rngs: nnx.Embed(
+        NUM_EMBEDDINGS,
         IN_FEATURES,
-        OUT_FEATURES,
-        kernel_size,
-        strides,
-        padding=padding,
-        input_dilation=input_dilation,
-        kernel_dilation=kernel_dilation,
-        feature_group_count=feature_group_count,
-        use_bias=use_bias,
         dtype=dtype,
         param_dtype=param_dtype,
-        precision=precision,
         rngs=rngs,
       ),
       rngs,
     )
-    model = linen.Conv(
-      OUT_FEATURES,
-      kernel_size=kernel_size,
-      strides=strides,
-      padding=padding,
-      input_dilation=input_dilation,
-      kernel_dilation=kernel_dilation,
-      feature_group_count=feature_group_count,
-      use_bias=use_bias,
-      dtype=dtype,
-      param_dtype=param_dtype,
-      precision=precision,
+    model = linen.Embed(
+      NUM_EMBEDDINGS, IN_FEATURES, dtype=dtype, param_dtype=param_dtype
     )
     variables = model.init(key, x)
-    model_nnx.kernel.value = variables['params']['kernel']
-    if use_bias:
-      model_nnx.bias.value = variables['params']['bias']
+    model_nnx.embedding.value = variables['params']['embedding']
 
     out_nnx = model_nnx(x)
     out = model.apply(variables, x)
     assert_array_equal(out, out_nnx)
+
+    x = jax.numpy.ones((10,), dtype=input_dtype) * 10
+    out_nnx = model_nnx(x)
+    out = model.apply(variables, x)
+    assert isinstance(out, jax.Array)
+    assert_array_equal(out, out_nnx)
+    assert_array_equal(jax.numpy.isnan(out).all(), jax.numpy.array([True]))
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/nn/test_linear.py` & `flax-0.8.4/flax/nnx/tests/nn/test_linear.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 import jax
 import jax.numpy as jnp
 from absl.testing import parameterized
 from jax.lax import Precision
 from numpy.testing import assert_array_equal
 
 from flax import linen
-from flax.experimental import nnx
+from flax import nnx
 from flax.typing import Dtype, PrecisionLike, Shape
 
 
 class TestLinearGeneral:
   def test_basic(self):
     module = nnx.LinearGeneral(2, 3, rngs=nnx.Rngs(0))
     y = module(jnp.ones((1, 2)))
@@ -131,19 +131,21 @@
       param_dtype=param_dtype,
       precision=precision,
     )
 
     variables = model.init(key, x)
     variables['params']['kernel'] = model_nnx.kernel.value
     if bias_shape is not None:
+      assert model_nnx.bias is not None
       variables['params']['bias'] = model_nnx.bias.value
     out_nnx = model_nnx(x)
     out = model.apply(variables, x)
     assert_array_equal(out, out_nnx)
 
     variables = model.init(key, x)
     model_nnx.kernel.value = variables['params']['kernel']
     if bias_shape is not None:
+      assert model_nnx.bias is not None
       model_nnx.bias.value = variables['params']['bias']
     out_nnx = model_nnx(x)
     out = model.apply(variables, x)
     assert_array_equal(out, out_nnx)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/nn/test_normalization.py` & `flax-0.8.4/flax/nnx/tests/nn/test_normalization.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 import jax
 import jax.numpy as jnp
 from absl.testing import parameterized
 from numpy.testing import assert_array_equal
 
 from flax import linen
-from flax.experimental import nnx
+from flax import nnx
 from flax.typing import Dtype
 
 
 class TestLinenConsistency(parameterized.TestCase):
   @parameterized.product(
     dtype=[jnp.float32, jnp.float16],
     param_dtype=[jnp.float32, jnp.float16],
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/nn/test_stochastic.py` & `flax-0.8.4/flax/nnx/tests/nn/test_stochastic.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import jax.numpy as jnp
 import numpy as np
 
-from flax.experimental import nnx
+from flax import nnx
 
 import pytest
 
 
 class TestStochastic:
   def test_dropout_internal_rngs(self):
     n = 0
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_compatibility.py` & `flax-0.8.4/flax/nnx/tests/compat/test_wrappers.py`

 * *Files 24% similar despite different names*

```diff
@@ -11,26 +11,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jax
 
 from flax import linen
-from flax.experimental import nnx
+from flax import nnx
+from flax.nnx import compat
 
 
 class TestCompatibility:
   def test_functional(self):
     # Functional API for NNX Modules
-    functional = nnx.compatibility.functional(nnx.Linear)(32, 64)
+    functional = compat.functional(nnx.Linear)(32, 64)
     state = functional.init(rngs=nnx.Rngs(0))
     x = jax.numpy.ones((1, 32))
     y, updates = functional.apply(state)(x)
 
   def test_linen_wrapper(self):
     ## Wrapper API for Linen Modules
     linen_module = linen.Dense(features=64)
     x = jax.numpy.ones((1, 32))
-    module = nnx.compatibility.LinenWrapper(
-      linen_module, x, rngs=nnx.Rngs(0)
-    )  # init
+    module = compat.LinenWrapper(linen_module, x, rngs=nnx.Rngs(0))  # init
     y = module(x)  # apply
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_containers.py` & `flax-0.8.4/flax/nnx/tests/test_containers.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class TestContainers:
   def test_unbox(self):
     x = nnx.Param(
       1,
       get_value_hooks=[lambda c, x: x + 1, lambda c, x: x * 2],  # type: ignore
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_graph_utils.py` & `flax-0.8.4/flax/nnx/tests/test_graph_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,18 +9,19 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from functools import partial
+from threading import Thread
 import jax
 import pytest
 
-from flax.experimental import nnx
+from flax import nnx
 from flax import struct
 
 
 class TestGraphUtils:
   def test_flatten(self):
     a = {'a': 1, 'b': nnx.Param(2)}
     g = [a, 3, a, nnx.Param(4)]
@@ -55,17 +56,15 @@
 
   def test_unflatten_empty(self):
     a = nnx.Dict({'a': 1, 'b': nnx.Param(2)})
     g = nnx.List([a, 3, a, nnx.Param(4)])
 
     graphdef, state = nnx.split(g)
 
-    with pytest.raises(
-      ValueError, match='Expected key for Variable but was not found in state'
-    ):
+    with pytest.raises(ValueError, match='Expected key'):
       nnx.graph.unflatten(graphdef, nnx.State({}))
 
   def test_update_dynamic(self):
     a = {'a': 1, 'b': nnx.Param(2)}
     g = [a, 3, a, nnx.Param(4)]
 
     graphdef, state = nnx.split(g)
@@ -296,15 +295,15 @@
   def test_cached_unflatten(self):
     class Foo(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.a = nnx.Linear(2, 2, rngs=rngs)
         self.b = nnx.BatchNorm(2, rngs=rngs)
 
     def f(m: Foo):
-      m.a, m.b = m.b, m.a
+      m.a, m.b = m.b, m.a  # type: ignore
 
     m = Foo(rngs=nnx.Rngs(0))
     a = m.a
     b = m.b
 
     graphdef: nnx.graph.GraphDef[Foo]
     graphdef, state, ref_out_idx_out = nnx.graph.flatten(m)
@@ -395,7 +394,26 @@
     graphdef, idx_out_idx_in = static_out.value
     idx_in_ref_out = nnx.graph.compose_mapping_reversed(
       ref_out_idx_out, idx_out_idx_in
     )
     m2, _ = nnx.graph.unflatten(graphdef, state, idxmap=idx_in_ref_out)
     assert m2 is m
     assert m2.ref is m2
+
+
+class SimpleModule(nnx.Module):
+  pass
+
+
+class SimplePyTreeModule(nnx.Module, experimental_pytree=True):
+  pass
+
+
+@pytest.mark.parametrize(['x'], [(SimpleModule(),), (SimplePyTreeModule(),)])
+def test_threading(x: nnx.Module):
+  class MyThread(Thread):
+    def run(self) -> None:
+      nnx.graph.split(x)
+
+  thread = MyThread()
+  thread.start()
+  thread.join()
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_helpers.py` & `flax-0.8.4/flax/nnx/tests/test_helpers.py`

 * *Files 19% similar despite different names*

```diff
@@ -12,15 +12,19 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jax
 import jax.numpy as jnp
 import optax
 
-from flax.experimental import nnx
+from numpy.testing import assert_array_equal
+
+from flax import linen
+from flax import nnx
+
 
 class TrainState(nnx.TrainState):
   batch_stats: nnx.State
 
 
 class TestHelpers:
   def test_train_state(self):
@@ -63,7 +67,37 @@
 
     assert y.shape == (1, 4)
 
     # fake gradient
     grads = jax.tree_util.tree_map(jnp.ones_like, state.params)
     # test apply_gradients
     state = state.apply_gradients(grads)
+
+  def test_nnx_linen_sequential_equivalence(self):
+    key1, key2 = jax.random.split(jax.random.key(0), 2)
+    rngs = nnx.Rngs(0)
+    x = jax.random.uniform(key1, (3, 1, 5))
+
+    model_nnx = nnx.Sequential(
+      nnx.Linear(5, 4, rngs=rngs), nnx.Linear(4, 2, rngs=rngs)
+    )
+    model = linen.Sequential([linen.Dense(4), linen.Dense(2)])
+
+    variables = model.init(key2, x)
+    for layer_index in range(2):
+      for param in ('kernel', 'bias'):
+        variables['params'][f'layers_{layer_index}'][param] = getattr(
+          model_nnx.layers[layer_index], param
+        ).value
+    out_nnx = model_nnx(x)
+    out = model.apply(variables, x)
+    assert_array_equal(out, out_nnx)
+
+    variables = model.init(key2, x)
+    for layer_index in range(2):
+      for param in ('kernel', 'bias'):
+        getattr(model_nnx.layers[layer_index], param).value = variables[
+          'params'
+        ][f'layers_{layer_index}'][param]
+    out_nnx = model_nnx(x)
+    out = model.apply(variables, x)
+    assert_array_equal(out, out_nnx)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_ids.py` & `flax-0.8.4/flax/nnx/tests/test_ids.py`

 * *Files 13% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 
-from flax.experimental.nnx.nnx import ids
+from flax.nnx.nnx import ids
 
 
 class TestIds:
   def test_hashable(self):
     id1 = ids.uuid()
     id2 = ids.uuid()
     assert id1 == id1
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_integration.py` & `flax-0.8.4/flax/nnx/tests/test_integration.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 
 import typing as tp
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 
-from flax.experimental import nnx
+from flax import nnx
 
 A = tp.TypeVar('A')
 
 
 class TestIntegration:
   def test_shared_modules(self):
     class Block(nnx.Module):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_metrics.py` & `flax-0.8.4/flax/nnx/tests/test_metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import jax
 import jax.numpy as jnp
 
-from flax.experimental import nnx
+from flax import nnx
 
 from absl.testing import parameterized
 
 
 class TestMetrics(parameterized.TestCase):
   def test_split_merge(self):
     logits = jax.random.normal(jax.random.key(0), (5, 2))
@@ -54,13 +54,13 @@
     values = metrics.compute()
     self.assertEqual(values['accuracy'], 0.6)
     self.assertEqual(values['loss'], 2.5)
 
     metrics.update(logits=logits2, labels=labels2, values=batch_loss2)
     values = metrics.compute()
     self.assertEqual(values['accuracy'], 0.7)
-    self.assertEqual(values['loss'], 2.)
+    self.assertEqual(values['loss'], 2.0)
 
     metrics.reset()
     values = metrics.compute()
     self.assertTrue(jnp.isnan(values['accuracy']))
     self.assertTrue(jnp.isnan(values['loss']))
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_module.py` & `flax-0.8.4/flax/nnx/tests/test_module.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,27 +17,26 @@
 from typing import Any, TypeVar
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 import pytest
 
-from flax.experimental import nnx
+from flax import nnx
 
 A = TypeVar('A')
 
 
 class TestModule:
   def test_has_module_state(self):
-    class Foo(nnx.Module):
-      ...
+    class Foo(nnx.Module): ...
 
     foo = Foo()
 
-    assert hasattr(foo, '_graph_node__state')
+    assert hasattr(foo, '_object__state')
 
   def test_trace_level(self):
     m = nnx.Dict(a=nnx.Param(1))
 
     @jax.jit
     def f():
       with pytest.raises(
@@ -294,22 +293,23 @@
 
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.a = Bar()
         self.b = self.a
 
     m1 = Foo()
-    with nnx.UpdateContext() as ctx:
+    with nnx.update_context('test') as ctx:
       graphdef, state = ctx.split(m1)
       m2 = ctx.merge(graphdef, state)
       m2.a.add_field()
       new_graphdef, state = ctx.split(m2)
 
-      ctx.update(new_graphdef, state)
+      m3 = ctx.merge(new_graphdef, state)
 
+    assert m3 is m1
     assert m1.a.x == 1
     assert m1.a.y == 2
     assert m1.b.x == 1
     assert m1.b.y == 2
 
   def test_update_new_submodule(self):
     class Bar(nnx.Module):
@@ -320,43 +320,45 @@
       def __init__(self) -> None:
         self.a = Bar()
 
       def add_module(self):
         self.b = Bar()
 
     m1 = Foo()
-    ctx = nnx.UpdateContext()
-    graphdef, state = ctx.split(m1)
-    m2 = ctx.merge(graphdef, state)
-    m2.add_module()
-    new_graphdef, state = ctx.split(m2)
+    with nnx.update_context('test') as ctx:
+      graphdef, state = ctx.split(m1)
+      m2 = ctx.merge(graphdef, state)
+      m2.add_module()
+      new_graphdef, state = ctx.split(m2)
 
-    ctx.update(new_graphdef, state)
+      m3 = ctx.merge(new_graphdef, state)
 
+    assert m3 is m1
     assert m1.a.x == 1
     assert m1.b.x == 1
 
   def test_update_update_submodule(self):
     class Bar(nnx.Module):
       def __init__(self) -> None:
         self.x = 1
 
     class Foo(nnx.Module):
       def __init__(self) -> None:
         self.a = Bar()
         self.b = self.a
 
     m1 = Foo()
-    ctx = nnx.UpdateContext()
-    graphdef, state = ctx.split(m1)
-    m2 = ctx.merge(graphdef, state)
-    m2.a.x = 2
-    new_graphdef, state = ctx.split(m2)
-    ctx.update(new_graphdef, state)
+    with nnx.update_context('test') as ctx:
+      graphdef, state = ctx.split(m1)
+      m2 = ctx.merge(graphdef, state)
+      m2.a.x = 2
+      new_graphdef, state = ctx.split(m2)
+      m3 = ctx.merge(new_graphdef, state)
 
+    assert m3 is m1
     assert m1.a.x == 2
     assert m1.b.x == 2
 
   def test_update_add_shared(self):
     class Bar(nnx.Module):
       def __init__(self) -> None:
         self.x = 1
@@ -366,21 +368,22 @@
         self.a = Bar()
         self.b = self.a
 
       def add_submodule(self):
         self.c = self.a
 
     m1 = Foo()
-    ctx = nnx.UpdateContext()
-    graphdef, state = ctx.split(m1)
-    m2 = ctx.merge(graphdef, state)
-    m2.add_submodule()
-    new_graphdef, state = ctx.split(m2)
-    ctx.update(new_graphdef, state)
+    with nnx.update_context('test') as ctx:
+      graphdef, state = ctx.split(m1)
+      m2 = ctx.merge(graphdef, state)
+      m2.add_submodule()
+      new_graphdef, state = ctx.split(m2)
+      m3 = ctx.merge(new_graphdef, state)
 
+    assert m3 is m1
     assert hasattr(m1, 'c')
 
   def test_create_abstract(self):
     linear = nnx.eval_shape(lambda: nnx.Linear(2, 3, rngs=nnx.Rngs(0)))
 
     assert linear.kernel.value == jax.ShapeDtypeStruct((2, 3), jnp.float32)
     assert linear.bias.value == jax.ShapeDtypeStruct((3,), jnp.float32)
@@ -467,38 +470,14 @@
     block.set_attributes(
       deterministic=True,
       use_running_average=True,
       unknown=True,
       raise_if_not_found=False,
     )
 
-  def test_init(self):
-    class Linear(nnx.Module):
-      def __init__(self, dout, rngs: nnx.Rngs):
-        self.dout = dout
-        self.rngs = rngs
-
-      def __call__(self, x):
-        if self.is_initializing():
-          din = x.shape[-1]
-          if not hasattr(self, 'w'):
-            key = self.rngs.params()
-            self.w = nnx.Param(jax.random.uniform(key, (din, self.dout)))
-          if not hasattr(self, 'b'):
-            self.b = nnx.Param(jnp.zeros((self.dout,)))
-        return x @ self.w + self.b[None]
-
-    linear = Linear(3, nnx.Rngs(0))
-    x = jnp.ones((5, 2))
-    y = linear.init(x)
-    assert linear.w.value.shape == (2, 3)
-    assert linear.b.value.shape == (3,)
-    assert y.shape == (5, 3)
-    assert not linear.is_initializing()
-
 
 class TestModulePytree:
   def test_tree_map(self):
     class Foo(nnx.Module, experimental_pytree=True):
       def __init__(self):
         self.node = nnx.Param(1)
         self.graphdef = 1
@@ -506,14 +485,35 @@
     m = Foo()
 
     m = jax.tree_util.tree_map(lambda x: x + 1, m)
 
     assert m.node.value == 2
     assert m.graphdef == 1
 
+  def test_static(self):
+    class C(nnx.Module, experimental_pytree=True):
+      def __init__(self, x):
+        self.x = x
+
+    n = 0
+
+    @jax.jit
+    def f(x):
+      nonlocal n
+      n += 1
+
+    f(C(1))
+    assert n == 1
+    f(C(1))
+    assert n == 1
+    f(C(2))
+    assert n == 2
+    f(C(2))
+    assert n == 2
+
 
 class TestModuleDataclass:
   def test_basic(self):
     @dataclasses.dataclass
     class Foo(nnx.Module):
       a: int
       b: nnx.Variable[int]
@@ -556,31 +556,14 @@
       def __call__(self, x):
         return self.bar(x)
 
     m = DFoo(1, 1, rngs=nnx.Rngs(0))
 
     assert hasattr(m, 'bar')
 
-  def test_setup_is_called(self):
-    @dataclasses.dataclass
-    class DFoo(nnx.Module):
-      din: int
-      dout: int
-      rngs: nnx.Rngs
-
-      def setup(self):
-        self.bar = nnx.Linear(self.din, self.dout, rngs=self.rngs)
-
-      def __call__(self, x):
-        return self.bar(x)
-
-    m = DFoo(1, 1, rngs=nnx.Rngs(0))
-
-    assert hasattr(m, 'bar')
-
 
 class TestModuleDef:
   def test_apply(self):
     class Foo(nnx.Module):
       def __init__(self, c: float, *, rngs: nnx.Rngs):
         self.w = nnx.Param(jax.random.uniform(rngs.params(), ()))
         self.c = c
@@ -635,20 +618,20 @@
         ]
 
     module = Foo(rngs=nnx.Rngs(0))
 
     modules = list(module.iter_modules())
 
     assert len(modules) == 3
-    assert modules[0][0] == ()
-    assert isinstance(modules[0][1], Foo)
-    assert modules[1][0] == ('submodules', 0, 'a')
-    assert isinstance(modules[1][1], nnx.Linear)
-    assert modules[2][0] == ('submodules', 1, 'b')
-    assert isinstance(modules[2][1], nnx.Conv)
+    assert modules[0][0] == ('submodules', 0, 'a')
+    assert isinstance(modules[0][1], nnx.Linear)
+    assert modules[1][0] == ('submodules', 1, 'b')
+    assert isinstance(modules[1][1], nnx.Conv)
+    assert modules[2][0] == ()
+    assert isinstance(modules[2][1], Foo)
 
   def test_array_in_module(self):
     class Foo(nnx.Module):
       def __init__(self):
         self.a = jnp.array(1.0)
 
     foo = Foo()
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_optimizer.py` & `flax-0.8.4/flax/nnx/tests/test_optimizer.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,23 +13,24 @@
 # limitations under the License.
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 import optax
 
-from flax.experimental import nnx
+from flax import nnx
 
 from absl.testing import parameterized
 
 
 class Model(nnx.Module):
   def __init__(self, in_features, out_features, rngs):
     self.linear1 = nnx.Linear(in_features, 3, rngs=rngs)
     self.linear2 = nnx.Linear(3, out_features, rngs=rngs)
+
   def __call__(self, x):
     return self.linear2(self.linear1(x))
 
 
 class TestOptimizer(parameterized.TestCase):
   @parameterized.parameters(
     {'module_cls': nnx.Linear},
@@ -50,67 +51,70 @@
     jit_decorator=[lambda f: f, nnx.jit, jax.jit],
     optimizer=[optax.sgd, optax.adam],
   )
   def test_jit(self, module_cls, jit_decorator, optimizer):
     x = jax.random.normal(jax.random.key(0), (1, 2))
     y = jnp.ones((1, 4))
     model = module_cls(2, 4, rngs=nnx.Rngs(0))
-    tx = optimizer(1e-3) # TODO: this doesn't work with adam optimizer for some reason
+    tx = optimizer(
+      1e-3
+    )  # TODO: this doesn't work with adam optimizer for some reason
     state = nnx.Optimizer(model, tx)
 
     if jit_decorator == jax.jit:
       model_static, model_state = nnx.split(state.model)
       loss_fn = lambda graphdef, state, x, y: (
         (nnx.merge(graphdef, state)(x) - y) ** 2
       ).mean()
       initial_loss = loss_fn(model_static, model_state, x, y)
 
-      def train_step(graphdef, state, x, y):
+      def jax_jit_train_step(graphdef, state, x, y):
         state = nnx.merge(graphdef, state)
         model_static, model_state = nnx.split(state.model)
         grads = jax.grad(loss_fn, argnums=1)(model_static, model_state, x, y)
         state.update(grads)
         return state.split()
 
-      graphdef, state = jit_decorator(train_step)(*state.split(), x, y)
+      graphdef, state = jit_decorator(jax_jit_train_step)(*state.split(), x, y)
       state = nnx.merge(graphdef, state)
       new_loss = loss_fn(*nnx.split(state.model), x, y)
 
     else:
-      loss_fn = lambda model, x, y: ((model(x)-y)**2).mean()
+      loss_fn = lambda model, x, y: ((model(x) - y) ** 2).mean()
       initial_loss = loss_fn(state.model, x, y)
 
-      def train_step(optimizer: nnx.Optimizer, x, y):
+      def nnx_jit_train_step(optimizer: nnx.Optimizer, x, y):
         grads = nnx.grad(loss_fn, wrt=nnx.Param)(optimizer.model, x, y)
         optimizer.update(grads)
 
-      jit_decorator(train_step)(state, x, y)
+      jit_decorator(nnx_jit_train_step)(state, x, y)
       new_loss = loss_fn(state.model, x, y)
 
     self.assertTrue(new_loss < initial_loss)
 
   @parameterized.product(
     module_cls=[nnx.Linear, Model],
     optimizer=[optax.sgd, optax.adam],
   )
   def test_metrics(self, module_cls, optimizer):
     class TrainState(nnx.Optimizer):
       def __init__(self, model, tx, metrics):
         self.metrics = metrics
         super().__init__(model, tx)
-      def update(self, *, grads, **updates):
+
+      def update(self, *, grads, **updates):  # type: ignore[signature-mismatch]
         self.metrics.update(**updates)
         super().update(grads)
 
     x = jax.random.normal(jax.random.key(0), (1, 2))
     y = jnp.ones((1, 4))
     model = module_cls(2, 4, rngs=nnx.Rngs(0))
     tx = optax.adam(1e-3)
     metrics = nnx.metrics.Average()
     state = TrainState(model, tx, metrics)
 
-    loss_fn = lambda model: ((model(x)-y)**2).mean()
+    loss_fn = lambda model: ((model(x) - y) ** 2).mean()
     grads = nnx.grad(loss_fn, wrt=nnx.Param)(state.model)
     state.update(grads=grads, values=loss_fn(state.model))
     initial_loss = state.metrics.compute()
     state.update(grads=grads, values=loss_fn(state.model))
     self.assertTrue(state.metrics.compute() < initial_loss)
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_partitioning.py` & `flax-0.8.4/flax/nnx/tests/test_partitioning.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 import jax
 import pytest
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class TestPartitioning:
   def test_partition(self):
     m = nnx.Dict(
       a=nnx.List([nnx.Param(1), nnx.BatchStat(2)]),
       b=nnx.Param(2),
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_rngs.py` & `flax-0.8.4/flax/nnx/tests/test_rngs.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from functools import partial
 from typing import Any
 
 import jax
 import jax.numpy as jnp
 import pytest
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class TestRngs:
   def test_call(self):
     rngs = nnx.Rngs(0)
     key = rngs()
 
@@ -47,15 +47,14 @@
     assert not jnp.allclose(key0, key1)
 
     key2 = rngs.params()
     assert rngs.params.count.value == 2
     assert rngs.params.key.value is key0
     assert not jnp.allclose(key1, key2)
 
-
   def test_rng_trace_level_constraints(self):
     rngs = nnx.Rngs(0)
 
     @jax.jit
     def f():
       with pytest.raises(
         nnx.errors.TraceContextError,
@@ -122,16 +121,16 @@
       def __call__(self, x):
         x = self.linear(x)
         x = self.dropout(x, rngs=self.rngs)
         return x
 
     rngs = nnx.Rngs(params=0, dropout=1)
     m = Foo(rngs)
-    _, params, dropout_keys, param_keys, rng_counts = nnx.split(
-      m, nnx.Param, 'dropout', 'params', nnx.RngCount
+    _, params, rng_counts, dropout_keys, param_keys = nnx.split(
+      m, nnx.Param, nnx.RngCount, 'dropout', 'params'
     )
 
     assert m.rngs.params.count.value == 2
     assert m.rngs['dropout'].count.value == 0
     assert len(dropout_keys.flat_state()) == 1
     assert len(param_keys.flat_state()) == 1
     assert len(rng_counts.flat_state()) == 2
@@ -172,52 +171,67 @@
     assert y.shape == (4, 1, 3)
     assert m.rngs.params.count.value == 2
     assert m.rngs['dropout'].count.value == 1
 
   def test_state_fork_split(self):
     rngs = nnx.Rngs(params=0, dropout=1)
     graphdef, state = nnx.split(rngs, nnx.RngState)
-    split, broadcast = nnx.fork(state, ..., 4)
+    split_keys, split_counts, broadcast_keys, broadcast_counts = nnx.fork(
+      state, ..., 4
+    )
 
-    assert len(jax.tree.leaves(split)) == 2
-    assert len(jax.tree.leaves(broadcast)) == 2
-    assert split.params.key.value.shape == (4,)
-    assert split.dropout.key.value.shape == (4,)
-    assert broadcast.params.count.value == 0
-    assert broadcast.dropout.count.value == 0
+    assert len(jax.tree.leaves(split_keys)) == 2
+    assert len(jax.tree.leaves(split_counts)) == 2
+    assert len(jax.tree.leaves(broadcast_keys)) == 0
+    assert len(jax.tree.leaves(broadcast_counts)) == 0
+    assert split_keys.params.key.value.shape == (4,)
+    assert split_keys.dropout.key.value.shape == (4,)
+    assert split_counts.params.count.value == 0
+    assert split_counts.dropout.count.value == 0
 
   def test_state_fork_split_and_broadcast(self):
     rngs = nnx.Rngs(params=0, dropout=1)
     graphdef, state = nnx.split(rngs, nnx.RngState)
-    split, broadcast = nnx.fork(state, 'params', 4)
-
-    assert len(jax.tree.leaves(split)) == 1
-    assert len(jax.tree.leaves(broadcast)) == 3
-    assert split.params.key.value.shape == (4,)
-    assert broadcast.dropout.key.value.shape == ()
-    assert broadcast.params.count.value == 0
-    assert broadcast.dropout.count.value == 0
+    split_keys, split_counts, broadcast_keys, broadcast_counts = nnx.fork(
+      state, 'params', 4
+    )
 
+    assert len(jax.tree.leaves(split_keys)) == 1
+    assert len(jax.tree.leaves(split_counts)) == 1
+    assert len(jax.tree.leaves(broadcast_keys)) == 1
+    assert len(jax.tree.leaves(broadcast_counts)) == 1
+    assert split_keys.params.key.value.shape == (4,)
+    assert broadcast_keys.dropout.key.value.shape == ()
+    assert split_counts.params.count.value == 0
+    assert broadcast_counts.dropout.count.value == 0
 
   def test_state_fork_multidimensional_split(self):
     rngs = nnx.Rngs(params=0, dropout=1)
     graphdef, state = nnx.split(rngs, nnx.RngState)
-    split, broadcast = nnx.fork(state, ..., (4, None, 3))
+    split_keys, split_counts, broadcast_keys, broadcast_counts = nnx.fork(
+      state, ..., (4, None, 3)
+    )
 
-    assert len(jax.tree.leaves(split)) == 2
-    assert len(jax.tree.leaves(broadcast)) == 2
-    assert split.params.key.value.shape == (4, 1, 3)
-    assert split.dropout.key.value.shape == (4, 1, 3)
-    assert broadcast.params.count.value == 0
-    assert broadcast.dropout.count.value == 0
+    assert len(jax.tree.leaves(split_keys)) == 2
+    assert len(jax.tree.leaves(split_counts)) == 2
+    assert len(jax.tree.leaves(broadcast_keys)) == 0
+    assert len(jax.tree.leaves(broadcast_counts)) == 0
+    assert split_keys.params.key.value.shape == (4, 1, 3)
+    assert split_keys.dropout.key.value.shape == (4, 1, 3)
+    assert split_counts.params.count.value == 0
+    assert split_counts.dropout.count.value == 0
 
   def test_state_fork_multidimensional_split_mixed(self):
     rngs = nnx.Rngs(params=0, dropout=1)
     graphdef, state = nnx.split(rngs, nnx.RngState)
-    split, broadcast = nnx.fork(state, 'params', (4, None, 3))
+    split_keys, split_counts, broadcast_keys, broadcast_counts = nnx.fork(
+      state, 'params', (4, None, 3)
+    )
 
-    assert len(jax.tree.leaves(split)) == 1
-    assert len(jax.tree.leaves(broadcast)) == 3
-    assert split.params.key.value.shape == (4, 1, 3)
-    assert broadcast.dropout.key.value.shape == ()
-    assert broadcast.params.count.value == 0
-    assert broadcast.dropout.count.value == 0
+    assert len(jax.tree.leaves(split_keys)) == 1
+    assert len(jax.tree.leaves(split_counts)) == 1
+    assert len(jax.tree.leaves(broadcast_keys)) == 1
+    assert len(jax.tree.leaves(broadcast_counts)) == 1
+    assert split_keys.params.key.value.shape == (4, 1, 3)
+    assert broadcast_keys.dropout.key.value.shape == ()
+    assert split_counts.params.count.value == 0
+    assert broadcast_counts.dropout.count.value == 0
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_spmd.py` & `flax-0.8.4/flax/nnx/tests/test_spmd.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 import jax
 import jax.numpy as jnp
 import optax
 from jax._src import test_util as jtu
 from jax.experimental import mesh_utils
 from jax.sharding import Mesh, PartitionSpec
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class TestSPMD:
   @jtu.skip_on_devices('cpu', 'gpu')
   def test_init(self):
     class Foo(nnx.Module):
       def __init__(self):
@@ -40,15 +40,15 @@
     @jax.jit
     def create_module():
       return nnx.split(Foo())
 
     mesh = Mesh(mesh_utils.create_device_mesh((2, 2)), ('model', 'data'))
 
     with mesh:
-      m: Foo = nnx.merge(*create_module())
+      m: Foo = nnx.merge(*create_module())  # type: ignore[invalid-annotation]
 
     assert m.w.shape == (8, 2)
     assert m.w.sharding.shard_shape(m.w.shape) == (4, 1)
 
   def test_init_all_devices(self):
     class Foo(nnx.Module):
       def __init__(self):
@@ -65,15 +65,15 @@
     @jax.jit
     def create_module():
       return nnx.split(Foo())
 
     mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), ('model', 'data'))
 
     with mesh:
-      m: Foo = nnx.merge(*create_module())
+      m: Foo = nnx.merge(*create_module())  # type: ignore[invalid-annotation]
 
     assert m.w.value.shape == (8, 2)
     assert m.w.value.sharding.shard_shape(m.w.value.shape) == (8, 2)
 
   def test_get_partition_spec(self):
     class Foo(nnx.Module):
       def __init__(self):
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_state.py` & `flax-0.8.4/flax/nnx/tests/test_state.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,20 +8,20 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from absl.testing.absltest import TestCase
+from absl.testing import absltest
 
-from flax.experimental import nnx
+from flax import nnx
 
 
-class StateTest(TestCase):
+class StateTest(absltest.TestCase):
   def test_create_state(self):
     state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
 
     assert state['a'].value == 1
     assert state['b']['c'].value == 2
 
   def test_get_attr(self):
@@ -46,19 +46,35 @@
     state.b.c.value = 4
 
     assert issubclass(state.a.type, nnx.Param)
     assert state.a.value == 3
     assert issubclass(state.b.c.type, nnx.Param)
     assert state.b.c.value == 4
 
+  def test_add_nested_attr(self):
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
+    state.b.d = nnx.Param.state(5)
+
+    assert state['b']['d'].value == 5
+
+  def test_delete_nested_attr(self):
+    state = nnx.State({'a': nnx.Param.state(1), 'b': {'c': nnx.Param.state(2)}})
+    del state['b']['c']
+
+    assert 'c' not in state['b']
+
   def test_integer_access(self):
     class Foo(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.layers = [nnx.Linear(1, 2, rngs=rngs), nnx.Linear(2, 3, rngs=rngs)]
 
     module = Foo(rngs=nnx.Rngs(0))
     state = nnx.state(module)
 
     assert module.layers[0].kernel.value.shape == (1, 2)
     assert state.layers[0].kernel.value.shape == (1, 2)
     assert module.layers[1].kernel.value.shape == (2, 3)
     assert state.layers[1].kernel.value.shape == (2, 3)
+
+
+if __name__ == '__main__':
+  absltest.main()
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_transforms.py` & `flax-0.8.4/flax/nnx/tests/test_transforms.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,24 +8,25 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import dataclasses
 import typing as tp
 from functools import partial
 
 import jax
 import jax.numpy as jnp
 import numpy as np
 import pytest
 from jax.experimental import mesh_utils
 
-from flax.experimental import nnx
+from flax import nnx
 
 
 class TestJIT:
   def test_jit(self):
     m = nnx.Dict(a=nnx.Param(1))
 
     @nnx.jit
@@ -106,15 +107,15 @@
 
       @nnx.jit
       def __call__(self, x: jax.Array) -> jax.Array:
         nonlocal n
         n += 1
         return jnp.dot(x, self.w.value)
 
-    m = nnx.Jit(Foo)(2, 3, rngs=nnx.Rngs(0))
+    m = nnx.Jit.constructor(Foo)(2, 3, rngs=nnx.Rngs(0))
 
     y = m(jnp.ones((1, 2)))
     assert y.shape == (1, 3)
     assert n == 1
     y = m(jnp.ones((1, 2)))
     assert n == 1
 
@@ -126,15 +127,15 @@
         self.a = nnx.Linear(2, 2, rngs=rngs)
         self.b = nnx.BatchNorm(2, rngs=rngs)
 
     @nnx.jit
     def f(m: Foo):
       nonlocal n
       n += 1
-      m.a, m.b = m.b, m.a
+      m.a, m.b = m.b, m.a  # type: ignore
 
     m = Foo(rngs=nnx.Rngs(0))
     a = m.a
     b = m.b
     a_kernel = a.kernel.value
     a_bias = a.bias.value
     b_scale = b.scale.value
@@ -256,15 +257,15 @@
     assert m.b is a
 
   def test_cached_unflatten_add_self_reference(self):
     n = 0
 
     class Foo(nnx.Module):
       def __init__(self):
-        self.ref: tp.Optional[Foo] = None
+        self.ref: tp.Optional[Foo] = None  # type: ignore[name-error]
 
     @nnx.jit
     def f(m: Foo):
       nonlocal n
       n += 1
       m.ref = m
 
@@ -286,15 +287,15 @@
     assert m.ref is m
 
   def test_cached_unflatten_ref_in_output(self):
     n = 0
 
     class Foo(nnx.Module):
       def __init__(self):
-        self.ref: tp.Optional[Foo] = None
+        self.ref: tp.Optional[Foo] = None  # type: ignore[name-error]
 
     @nnx.jit
     def f(m: Foo):
       nonlocal n
       n += 1
       m.ref = m
       return m
@@ -340,15 +341,14 @@
 
     with mesh:
       constrain_object(m)
 
     m.kernel.value.sharding
 
 
-
 class TestGrad:
   def test_grad(self):
     p1 = nnx.Param(10.0)
     p2 = nnx.Param(20.0)
 
     m = nnx.Dict(
       a=nnx.List([p1, p2]),
@@ -529,15 +529,15 @@
         self.node = nnx.Variable(jnp.ones((2,)))
 
       def __call__(self, x: jax.Array) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
@@ -558,15 +558,15 @@
         self.node = nnx.Variable(jnp.ones((2,)))
 
       def __call__(self, x: jax.Array):
         x = self.linear(x)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
       scan_output=False,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
@@ -587,15 +587,15 @@
         self.node = nnx.Variable(jnp.ones((2,)))
 
       def __call__(self, x: jax.Array):
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, (x, x)
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
       out_axes=(1, 2),
     )
 
     module = MLP(rngs=nnx.Rngs(0))
@@ -622,15 +622,15 @@
       ) -> tp.Tuple[jax.Array, None]:
         assert x.shape == a.shape
         x = x + a
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
     )
 
     module = MLP(rngs=nnx.Rngs(0))
 
@@ -657,15 +657,15 @@
         assert x.shape == a.shape
         assert x.shape == b.shape
         x = x + a + b
         x = self.linear(x)
         x = nnx.gelu(x)
         return x, None
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
       in_axes=(None, None, 0, None),
     )
 
     module = MLP(rngs=nnx.Rngs(0))
@@ -693,15 +693,15 @@
       def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
         x = self.linear(x)
         x = self.bn(x)
         x = self.dropout(x, rngs=rngs)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block, state_axes={nnx.Param: 0}, length=5, scan_output=False
     )
 
     module = MLP(rngs=nnx.Rngs(0))
     module.set_attributes(deterministic=False, use_running_average=False)
 
     assert module.scan_module.linear.kernel.value.shape == (5, 3, 3)
@@ -724,15 +724,15 @@
       def __call__(self, x: jax.Array, *, rngs: nnx.Rngs) -> jax.Array:
         x = self.linear(x)
         x = self.bn(x)
         x = self.dropout(x, rngs=rngs)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
       # params is split, dropout is broadcast
       split_rngs=['dropout'],
       scan_output=False,
     )
@@ -810,22 +810,22 @@
         )
 
       def __call__(self, x: jax.Array, _) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
 
         # test sharding layer axes is not present inside scan
         state = nnx.state(self.linear)
-        assert state.kernel.value.shape == (3, 3)
-        assert state.kernel.sharding == ('din', 'dout')
-        assert state.bias.value.shape == (3,)
-        assert state.bias.sharding == ('dout',)
+        assert state.kernel.value.shape == (3, 3)  # type: ignore
+        assert state.kernel.sharding == ('din', 'dout')  # type: ignore
+        assert state.bias.value.shape == (3,)  # type: ignore
+        assert state.bias.sharding == ('dout',)  # type: ignore
 
         return x, None
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
       transform_metadata={nnx.PARTITION_NAME: 'layers'},
     )
 
     m = MLP(rngs=nnx.Rngs(0))
@@ -869,31 +869,97 @@
     class Block(nnx.Module):
       def __init__(self, *, rngs: nnx.Rngs):
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
       def __call__(self):
         return None, None
 
-    MLP = nnx.Scan(
+    MLP = nnx.Scan.constructor(
       Block,
       state_axes={nnx.Param: 0},
       length=5,
     )
 
     mlp = MLP(rngs=nnx.Rngs(0))
 
     with pytest.raises(
       TypeError, match='Expected at least 2 positional argument'
     ):
       mlp()
 
+  def test_cache_tracing_simple(self):
+    n = 0
+    x = jnp.arange(5)
+    count = jnp.array(0)
+
+    @nnx.scan
+    def f(count, x):
+      nonlocal n
+      n += 1
+      return count + 1, x**2
+
+    count, y = f(count, x)
+    assert n == 1
+    assert count == 5
+    np.testing.assert_allclose(y, x**2)
+
+    count, y = f(count, x)
+    assert n == 1
+    assert count == 10
+
+  def test_cache_tracing_object(self):
+    n = 0
+    x = jnp.arange(5)
+    count = jnp.array(0)
+
+    @dataclasses.dataclass
+    class Foo(nnx.Object):
+      @partial(nnx.vmap, axis_size=5)
+      def __init__(self, *, rngs: nnx.Rngs):
+        self.x = nnx.Param(jax.random.normal(rngs(), shape=(3,)))
+
+    foo = Foo(rngs=nnx.Rngs(0))
+    assert foo.x.value.shape == (5, 3)
+
+    @nnx.scan
+    def f(count, x, foo):
+      nonlocal n
+      n += 1
+      assert foo.x.value.shape == (3,)
+      return count + 1, x**2
+
+    count, y = f(count, x, foo)
+    assert n == 1
+    assert count == 5
+    np.testing.assert_allclose(y, x**2)
+
+    count, y = f(count, x, foo)
+    assert n == 1
+    assert count == 10
+
+  def test_scan_broadcast_keys(self):
+    rngs = nnx.Rngs(params=0, dropout=1)
+
+    @partial(nnx.scan, split_rngs='params', length=3)
+    def f(_, rngs: nnx.Rngs):
+      param_key = rngs.params()
+      dropout_key = rngs.dropout()
+      return (), (param_key, dropout_key)
+
+    _, (param_keys, dropout_keys) = f((), rngs)
+
+    assert jnp.not_equal(param_keys[0], param_keys[1])
+    assert jnp.not_equal(param_keys[1], param_keys[2])
+    assert jnp.equal(dropout_keys[0], dropout_keys[1])
+    assert jnp.equal(dropout_keys[1], dropout_keys[2])
+
 
 class TestRemat:
   def test_basic_remat(self):
-    RematLinear = nnx.Remat(nnx.Linear)
+    RematLinear = nnx.Remat.constructor(nnx.Linear)
 
     module = RematLinear(2, 3, rngs=nnx.Rngs(0))
 
     y = module(jnp.ones((1, 2)))
 
     assert y.shape == (1, 3)
 
@@ -918,17 +984,17 @@
       def __init__(self, *, rngs: nnx.Rngs):
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
       def __call__(self, x: jax.Array, _) -> tp.Tuple[jax.Array, None]:
         x = self.linear(x)
         return x, None
 
-    RematLinear = nnx.Remat(LinearBlock)
+    RematLinear = nnx.Remat.constructor(LinearBlock)
 
-    ScanRematLinear = nnx.Scan(
+    ScanRematLinear = nnx.Scan.constructor(
       RematLinear,
       state_axes={nnx.Param: 0},
       length=5,
     )
 
     m = ScanRematLinear(rngs=nnx.Rngs(0))
 
@@ -1117,15 +1183,15 @@
         self.linear = nnx.Linear(3, 3, rngs=rngs)
 
       def __call__(self, x: jax.Array) -> jax.Array:
         x = self.linear(x)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Vmap(Block, state_axes={nnx.Param: 0}, axis_size=5)
+    MLP = nnx.Vmap.constructor(Block, state_axes={nnx.Param: 0}, axis_size=5)
 
     module = MLP(rngs=nnx.Rngs(0))
 
     assert not jnp.allclose(
       module.vmap_module.linear.kernel.value[0],
       module.vmap_module.linear.kernel.value[1],
     )
@@ -1144,12 +1210,54 @@
         self.graphdef = graphdef
 
       def __call__(self, x: jax.Array) -> jax.Array:
         x = self.linear(x)
         x = nnx.gelu(x)
         return x
 
-    MLP = nnx.Vmap(Block, state_axes={nnx.Param: 0}, axis_size=5)
+    MLP = nnx.Vmap.constructor(Block, state_axes={nnx.Param: 0}, axis_size=5)
 
     module = MLP(graphdef='hello', rngs=nnx.Rngs(0))
 
     assert module.vmap_module.graphdef == 'hello'
+
+
+class TestCond:
+  def test_basic(self):
+    class TimeStep(tp.NamedTuple):
+      step: jax.Array
+      reward: jax.Array
+
+      @staticmethod
+      def zero():
+        return TimeStep(step=jnp.array(0), reward=jnp.array(0.0))
+
+    @dataclasses.dataclass
+    class Foo(nnx.Object):
+      timestep: TimeStep
+
+      def update(self):
+        def reward_2(self: Foo):
+          self.timestep = TimeStep(
+            step=self.timestep.step + 1, reward=jnp.array(2.0)
+          )
+
+        def reward_0(self: Foo):
+          self.timestep = TimeStep(
+            step=self.timestep.step + 1, reward=jnp.array(0.0)
+          )
+
+        nnx.cond(self.timestep.step % 2 == 0, reward_2, reward_0, self)
+
+    foo = Foo(timestep=TimeStep.zero())
+    foo.update()
+    assert foo.timestep.step == 1
+    assert foo.timestep.reward == 2.0
+    foo.update()
+    assert foo.timestep.step == 2
+    assert foo.timestep.reward == 0.0
+    foo.update()
+    assert foo.timestep.step == 3
+    assert foo.timestep.reward == 2.0
+    foo.update()
+    assert foo.timestep.step == 4
+    assert foo.timestep.reward == 0.0
```

### Comparing `flax-0.8.3/flax/experimental/nnx/tests/test_variable.py` & `flax-0.8.4/flax/nnx/tests/test_variable.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 
 import typing as tp
 
 import jax
 import jax.numpy as jnp
 
-from flax.experimental import nnx
+from flax import nnx
 
 A = tp.TypeVar('A')
 
 
 class TestVariableState:
   def test_pytree(self):
     r1 = nnx.VariableState(nnx.Param, 1)
@@ -52,13 +52,13 @@
     class Linear(nnx.Module):
       def __init__(self, din, dout, rngs: nnx.Rngs):
         key = rngs()
         self.w = nnx.Param(jax.random.normal(key, (din, dout)))
         self.b = nnx.Param(jax.numpy.zeros((dout,)))
 
       def __call__(self, x: jax.Array):
-        return jnp.dot(x, self.w) + self.b
+        return jnp.dot(x, self.w) + self.b  # type: ignore[arg-type]
 
     linear = Linear(3, 4, nnx.Rngs(0))
     x = jax.numpy.ones((3,))
     y = linear(x)
     assert y.shape == (4,)
```

### Comparing `flax-0.8.3/flax/ids.py` & `flax-0.8.4/flax/ids.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/io.py` & `flax-0.8.4/flax/io.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/jax_utils.py` & `flax-0.8.4/flax/jax_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/README.md` & `flax-0.8.4/flax/linen/README.md`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/__init__.py` & `flax-0.8.4/flax/linen/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/activation.py` & `flax-0.8.4/flax/linen/activation.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/attention.py` & `flax-0.8.4/flax/linen/attention.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Attention core modules for Flax."""
+from __future__ import annotations
 
 import functools
 import warnings
 from typing import Any, Callable, Optional, Union, overload
 
 import jax
 import jax.numpy as jnp
@@ -302,15 +303,19 @@
     broadcast_dropout: Use a broadcasted dropout along batch dims.
     dropout_rate: Dropout rate.
     deterministic: If False, the attention weight is masked randomly using
       dropout, whereas if True, the attention weights are deterministic.
     precision: Numerical precision of the computation see ``jax.lax.Precision``
       for details.
     kernel_init: Initializer for the kernel of the Dense layers.
+    out_kernel_init: Optional Initializer for the kernel of the output Dense layer,
+      if None, ``kernel_init`` will be used.
     bias_init: Initializer for the bias of the Dense layers.
+    out_bias_init: Optional Initializer for the bias of the output Dense layer,
+      if None, ``bias_init`` will be used.
     use_bias: Whether pointwise QKVO dense transforms use bias.
     attention_fn: dot_product_attention or compatible function. Accepts query,
       key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,
       num_heads, value_channels]``
     decode: Whether to prepare and use an autoregressive cache.
     normalize_qk: Should QK normalization be applied (arxiv.org/abs/2302.05442).
   """
@@ -321,15 +326,17 @@
   qkv_features: Optional[int] = None
   out_features: Optional[int] = None
   broadcast_dropout: bool = True
   dropout_rate: float = 0.0
   deterministic: Optional[bool] = None
   precision: PrecisionLike = None
   kernel_init: Initializer = default_kernel_init
+  out_kernel_init: Initializer | None = None
   bias_init: Initializer = initializers.zeros_init()
+  out_bias_init: Initializer | None = None
   use_bias: bool = True
   attention_fn: Callable[..., Array] = dot_product_attention
   decode: bool = False
   normalize_qk: bool = False
   force_fp32_for_softmax: bool = False
   # Deprecated, will be removed.
   qkv_dot_general: Optional[DotGeneralT] = None
@@ -594,16 +601,16 @@
         dtype=self.dtype,
         precision=self.precision,
       )
     # back to the original inputs dimensions
     out = DenseGeneral(
       features=features,
       axis=(-2, -1),
-      kernel_init=self.kernel_init,
-      bias_init=self.bias_init,
+      kernel_init=self.out_kernel_init or self.kernel_init,
+      bias_init=self.out_bias_init or self.bias_init,
       use_bias=self.use_bias,
       dtype=self.dtype,
       param_dtype=self.param_dtype,
       precision=self.precision,
       dot_general=self.out_dot_general,
       dot_general_cls=self.out_dot_general_cls,
       name='out',  # type: ignore[call-arg]
```

### Comparing `flax-0.8.3/flax/linen/batch_apply.py` & `flax-0.8.4/flax/linen/batch_apply.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/combinators.py` & `flax-0.8.4/flax/linen/combinators.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/dtypes.py` & `flax-0.8.4/flax/linen/dtypes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/experimental/layers_with_named_axes.py` & `flax-0.8.4/flax/linen/experimental/layers_with_named_axes.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/fp8_ops.py` & `flax-0.8.4/flax/linen/fp8_ops.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/initializers.py` & `flax-0.8.4/flax/linen/initializers.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/kw_only_dataclasses.py` & `flax-0.8.4/flax/linen/kw_only_dataclasses.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/linear.py` & `flax-0.8.4/flax/linen/linear.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/module.py` & `flax-0.8.4/flax/linen/module.py`

 * *Files 1% similar despite different names*

```diff
@@ -3139,3927 +3139,3975 @@
 0000c420: 696e 6974 0a20 2020 2020 206f 7220 7365  init.      or se
 0000c430: 6c66 2e5f 7374 6174 652e 696e 5f73 6574  lf._state.in_set
 0000c440: 7570 0a20 2020 2020 206f 7220 7365 6c66  up.      or self
 0000c450: 2e5f 7374 6174 652e 696e 5f63 6f6d 7061  ._state.in_compa
 0000c460: 6374 5f6d 6574 686f 640a 2020 2020 290a  ct_method.    ).
 0000c470: 0a20 2040 7072 6f70 6572 7479 0a20 2064  .  @property.  d
 0000c480: 6566 2070 6174 6828 7365 6c66 293a 0a20  ef path(self):. 
-0000c490: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
-0000c4a0: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-0000c4b0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000c4c0: 2822 4361 6e27 7420 6163 6365 7373 206d  ("Can't access m
-0000c4d0: 6f64 756c 6520 7061 7468 7320 6f6e 2075  odule paths on u
-0000c4e0: 6e62 6f75 6e64 206d 6f64 756c 6573 2e22  nbound modules."
-0000c4f0: 290a 0a20 2020 2072 6574 7572 6e20 7365  )..    return se
-0000c500: 6c66 2e73 636f 7065 2e70 6174 680a 0a20  lf.scope.path.. 
-0000c510: 2064 6566 2063 6c6f 6e65 280a 2020 2020   def clone(.    
-0000c520: 7365 6c66 3a20 4d2c 0a20 2020 202a 2c0a  self: M,.    *,.
-0000c530: 2020 2020 7061 7265 6e74 3a20 4f70 7469      parent: Opti
-0000c540: 6f6e 616c 5b55 6e69 6f6e 5b53 636f 7065  onal[Union[Scope
-0000c550: 2c20 274d 6f64 756c 6527 2c20 5f53 656e  , 'Module', _Sen
-0000c560: 7469 6e65 6c5d 5d20 3d20 4e6f 6e65 2c0a  tinel]] = None,.
-0000c570: 2020 2020 5f64 6565 705f 636c 6f6e 653a      _deep_clone:
-0000c580: 2055 6e69 6f6e 5b62 6f6f 6c2c 2077 6561   Union[bool, wea
-0000c590: 6b72 6566 2e57 6561 6b56 616c 7565 4469  kref.WeakValueDi
-0000c5a0: 6374 696f 6e61 7279 5d20 3d20 4661 6c73  ctionary] = Fals
-0000c5b0: 652c 0a20 2020 205f 7265 7365 745f 6e61  e,.    _reset_na
-0000c5c0: 6d65 733a 2062 6f6f 6c20 3d20 4661 6c73  mes: bool = Fals
-0000c5d0: 652c 0a20 2020 202a 2a75 7064 6174 6573  e,.    **updates
-0000c5e0: 2c0a 2020 2920 2d3e 204d 3a0a 2020 2020  ,.  ) -> M:.    
-0000c5f0: 2222 2243 7265 6174 6573 2061 2063 6c6f  """Creates a clo
-0000c600: 6e65 206f 6620 7468 6973 204d 6f64 756c  ne of this Modul
-0000c610: 652c 2077 6974 6820 6f70 7469 6f6e 616c  e, with optional
-0000c620: 6c79 2075 7064 6174 6564 2061 7267 756d  ly updated argum
-0000c630: 656e 7473 2e0a 0a20 2020 204e 4f54 453a  ents...    NOTE:
-0000c640: 2065 6e64 2075 7365 7273 2061 7265 2065   end users are e
-0000c650: 6e63 6f75 7261 6765 6420 746f 2075 7365  ncouraged to use
-0000c660: 2074 6865 2060 6063 6f70 7960 6020 6d65   the ``copy`` me
-0000c670: 7468 6f64 2e20 2060 6063 6c6f 6e65 6060  thod.  ``clone``
-0000c680: 2069 7320 7573 6564 0a20 2020 2020 2070   is used.      p
-0000c690: 7269 6d61 7269 6c79 2066 6f72 2069 6e74  rimarily for int
-0000c6a0: 6572 6e61 6c20 726f 7574 696e 6573 2c20  ernal routines, 
-0000c6b0: 616e 6420 6060 636f 7079 6060 206f 6666  and ``copy`` off
-0000c6c0: 6572 7320 7369 6d70 6c65 7220 6172 6775  ers simpler argu
-0000c6d0: 6d65 6e74 7320 616e 640a 2020 2020 2020  ments and.      
-0000c6e0: 6265 7474 6572 2064 6566 6175 6c74 732e  better defaults.
-0000c6f0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
-0000c700: 2020 7061 7265 6e74 3a20 5468 6520 7061    parent: The pa
-0000c710: 7265 6e74 206f 6620 7468 6520 636c 6f6e  rent of the clon
-0000c720: 652e 2054 6865 2063 6c6f 6e65 2077 696c  e. The clone wil
-0000c730: 6c20 6861 7665 206e 6f20 7061 7265 6e74  l have no parent
-0000c740: 2069 6620 6e6f 0a20 2020 2020 2020 2065   if no.        e
-0000c750: 7870 6c69 6369 7420 7061 7265 6e74 2069  xplicit parent i
-0000c760: 7320 7370 6563 6966 6965 642e 0a20 2020  s specified..   
-0000c770: 2020 205f 6465 6570 5f63 6c6f 6e65 3a20     _deep_clone: 
-0000c780: 4120 626f 6f6c 6561 6e20 6f72 2061 2077  A boolean or a w
-0000c790: 6561 6b20 7661 6c75 6520 6469 6374 696f  eak value dictio
-0000c7a0: 6e61 7279 2074 6f20 636f 6e74 726f 6c20  nary to control 
-0000c7b0: 6465 6570 2063 6c6f 6e69 6e67 0a20 2020  deep cloning.   
-0000c7c0: 2020 2020 206f 6620 7375 626d 6f64 756c       of submodul
-0000c7d0: 6573 2e20 4966 2054 7275 652c 2073 7562  es. If True, sub
-0000c7e0: 6d6f 6475 6c65 7320 7769 6c6c 2062 6520  modules will be 
-0000c7f0: 636c 6f6e 6564 2072 6563 7572 7369 7665  cloned recursive
-0000c800: 6c79 2e20 4966 2061 2077 6561 6b0a 2020  ly. If a weak.  
-0000c810: 2020 2020 2020 7661 6c75 6520 6469 6374        value dict
-0000c820: 696f 6e61 7279 2069 7320 7061 7373 6564  ionary is passed
-0000c830: 2c20 6974 2077 696c 6c20 6265 2075 7365  , it will be use
-0000c840: 6420 746f 2063 6163 6865 2063 6c6f 6e65  d to cache clone
-0000c850: 6420 7375 626d 6f64 756c 6573 2e0a 2020  d submodules..  
-0000c860: 2020 2020 2020 5468 6973 2066 6c61 6720        This flag 
-0000c870: 6973 2075 7365 6420 6279 2069 6e69 742f  is used by init/
-0000c880: 6170 706c 792f 6269 6e64 2074 6f20 6176  apply/bind to av
-0000c890: 6f69 6420 7363 6f70 6520 6c65 616b 6167  oid scope leakag
-0000c8a0: 652e 0a20 2020 2020 205f 7265 7365 745f  e..      _reset_
-0000c8b0: 6e61 6d65 733a 2049 6620 5472 7565 2c20  names: If True, 
-0000c8c0: 6060 6e61 6d65 3d4e 6f6e 6560 6020 6973  ``name=None`` is
-0000c8d0: 2061 6c73 6f20 7061 7373 6564 2074 6f20   also passed to 
-0000c8e0: 7375 626d 6f64 756c 6573 2077 6865 6e0a  submodules when.
-0000c8f0: 2020 2020 2020 2020 636c 6f6e 696e 672e          cloning.
-0000c900: 2052 6573 6574 7469 6e67 206e 616d 6573   Resetting names
-0000c910: 2069 6e20 7375 626d 6f64 756c 6573 2069   in submodules i
-0000c920: 7320 6e65 6365 7373 6172 7920 7768 656e  s necessary when
-0000c930: 2063 616c 6c69 6e67 2060 602e 756e 6269   calling ``.unbi
-0000c940: 6e64 6060 2e0a 2020 2020 2020 2a2a 7570  nd``..      **up
-0000c950: 6461 7465 733a 2041 7474 7269 6275 7465  dates: Attribute
-0000c960: 2075 7064 6174 6573 2e0a 0a20 2020 2052   updates...    R
-0000c970: 6574 7572 6e73 3a0a 2020 2020 2020 4120  eturns:.      A 
-0000c980: 636c 6f6e 6520 6f66 2074 6865 2074 6869  clone of the thi
-0000c990: 7320 4d6f 6475 6c65 2077 6974 6820 7468  s Module with th
-0000c9a0: 6520 7570 6461 7465 6420 6174 7472 6962  e updated attrib
-0000c9b0: 7574 6573 2061 6e64 2070 6172 656e 742e  utes and parent.
-0000c9c0: 0a20 2020 2022 2222 0a20 2020 2061 7474  .    """.    att
-0000c9d0: 7273 203d 207b 0a20 2020 2020 2066 2e6e  rs = {.      f.n
-0000c9e0: 616d 653a 2067 6574 6174 7472 2873 656c  ame: getattr(sel
-0000c9f0: 662c 2066 2e6e 616d 6529 2066 6f72 2066  f, f.name) for f
-0000ca00: 2069 6e20 6461 7461 636c 6173 7365 732e   in dataclasses.
-0000ca10: 6669 656c 6473 2873 656c 6629 2069 6620  fields(self) if 
-0000ca20: 662e 696e 6974 0a20 2020 207d 0a0a 2020  f.init.    }..  
-0000ca30: 2020 6174 7472 732e 7570 6461 7465 2870    attrs.update(p
-0000ca40: 6172 656e 743d 7061 7265 6e74 2c20 2a2a  arent=parent, **
-0000ca50: 7570 6461 7465 7329 0a0a 2020 2020 2320  updates)..    # 
-0000ca60: 4865 7265 2077 6520 696d 706c 656d 656e  Here we implemen
-0000ca70: 7420 6465 6570 2063 6c6f 6e69 6e67 206f  t deep cloning o
-0000ca80: 6620 7375 626d 6f64 756c 6573 2c20 7468  f submodules, th
-0000ca90: 6973 2069 7320 6e65 6365 7373 6172 7920  is is necessary 
-0000caa0: 746f 2061 766f 6964 2073 636f 7065 206c  to avoid scope l
-0000cab0: 6561 6b61 6765 0a20 2020 2023 2066 726f  eakage.    # fro
-0000cac0: 6d20 6578 7465 726e 616c 2073 7562 6d6f  m external submo
-0000cad0: 6475 6c65 7320 696e 746f 2069 6e69 742f  dules into init/
-0000cae0: 6170 706c 792f 6269 6e64 2077 6869 6c65  apply/bind while
-0000caf0: 2070 7265 7365 7276 696e 6720 7368 6172   preserving shar
-0000cb00: 696e 672d 6279 2d72 6566 6572 656e 6365  ing-by-reference
-0000cb10: 0a20 2020 2023 2072 656c 6174 696f 6e73  .    # relations
-0000cb20: 6869 7073 2062 6574 7765 656e 2073 7562  hips between sub
-0000cb30: 6d6f 6475 6c65 732e 0a20 2020 2069 6620  modules..    if 
-0000cb40: 5f64 6565 705f 636c 6f6e 6520 213d 2046  _deep_clone != F
-0000cb50: 616c 7365 3a0a 2020 2020 2020 2320 5765  alse:.      # We
-0000cb60: 2075 7365 2061 2077 6561 6b20 7661 6c75   use a weak valu
-0000cb70: 6520 6469 6374 696f 6e61 7279 2074 6f20  e dictionary to 
-0000cb80: 6361 6368 6520 636c 6f6e 6564 2073 7562  cache cloned sub
-0000cb90: 6d6f 6475 6c65 732e 2057 6865 6e20 6120  modules. When a 
-0000cba0: 7368 6172 6564 0a20 2020 2020 2023 2073  shared.      # s
-0000cbb0: 7562 6d6f 6475 6c65 2069 7320 636c 6f6e  ubmodule is clon
-0000cbc0: 6564 2c20 6974 7320 6f6e 6c79 2063 6c6f  ed, its only clo
-0000cbd0: 6e65 6420 6f6e 6365 2065 6c73 6520 6974  ned once else it
-0000cbe0: 7320 6665 7463 6865 6420 6672 6f6d 2074  s fetched from t
-0000cbf0: 6865 2063 6163 6865 2e0a 2020 2020 2020  he cache..      
-0000cc00: 6361 6368 6520 3d20 280a 2020 2020 2020  cache = (.      
-0000cc10: 2020 7765 616b 7265 662e 5765 616b 5661    weakref.WeakVa
-0000cc20: 6c75 6544 6963 7469 6f6e 6172 7928 290a  lueDictionary().
-0000cc30: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-0000cc40: 7461 6e63 6528 5f64 6565 705f 636c 6f6e  tance(_deep_clon
-0000cc50: 652c 2062 6f6f 6c29 0a20 2020 2020 2020  e, bool).       
-0000cc60: 2065 6c73 6520 5f64 6565 705f 636c 6f6e   else _deep_clon
-0000cc70: 650a 2020 2020 2020 290a 0a20 2020 2020  e.      )..     
-0000cc80: 2064 6566 2063 6c6f 6e65 5f66 6e28 6d3a   def clone_fn(m:
-0000cc90: 204d 6f64 756c 6529 202d 3e20 4d6f 6475   Module) -> Modu
-0000cca0: 6c65 3a0a 2020 2020 2020 2020 6966 2068  le:.        if h
-0000ccb0: 6173 6174 7472 286d 2c20 275f 6964 2729  asattr(m, '_id')
-0000ccc0: 3a0a 2020 2020 2020 2020 2020 6b65 7920  :.          key 
-0000ccd0: 3d20 6d2e 5f69 640a 2020 2020 2020 2020  = m._id.        
-0000cce0: 2020 6966 206b 6579 2069 6e20 6361 6368    if key in cach
-0000ccf0: 653a 0a20 2020 2020 2020 2020 2020 2072  e:.            r
-0000cd00: 6574 7572 6e20 6361 6368 655b 6b65 795d  eturn cache[key]
-0000cd10: 0a20 2020 2020 2020 2020 2065 6c73 653a  .          else:
-0000cd20: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if 
-0000cd30: 5f72 6573 6574 5f6e 616d 6573 3a0a 2020  _reset_names:.  
-0000cd40: 2020 2020 2020 2020 2020 2020 636c 6f6e              clon
-0000cd50: 6520 3d20 6d2e 636c 6f6e 6528 0a20 2020  e = m.clone(.   
-0000cd60: 2020 2020 2020 2020 2020 2020 205f 6465               _de
-0000cd70: 6570 5f63 6c6f 6e65 3d63 6163 6865 2c20  ep_clone=cache, 
-0000cd80: 5f72 6573 6574 5f6e 616d 6573 3d5f 7265  _reset_names=_re
-0000cd90: 7365 745f 6e61 6d65 732c 206e 616d 653d  set_names, name=
-0000cda0: 4e6f 6e65 0a20 2020 2020 2020 2020 2020  None.           
-0000cdb0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           
-0000cdc0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         
-0000cdd0: 2020 2020 2063 6c6f 6e65 203d 206d 2e63       clone = m.c
-0000cde0: 6c6f 6e65 285f 6465 6570 5f63 6c6f 6e65  lone(_deep_clone
-0000cdf0: 3d63 6163 6865 290a 2020 2020 2020 2020  =cache).        
-0000ce00: 2020 2020 6361 6368 655b 6b65 795d 203d      cache[key] =
-0000ce10: 2063 6c6f 6e65 0a20 2020 2020 2020 2020   clone.         
-0000ce20: 2020 2072 6574 7572 6e20 636c 6f6e 650a     return clone.
-0000ce30: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  
-0000ce40: 2020 2020 2020 2020 2320 4966 2074 6865          # If the
-0000ce50: 206d 6f64 756c 6520 646f 6573 6e27 7420   module doesn't 
-0000ce60: 6861 7665 2061 6e20 5f69 6420 6174 7472  have an _id attr
-0000ce70: 6962 7574 6520 6974 2063 6f75 6c64 2062  ibute it could b
-0000ce80: 6520 6120 6d6f 636b 206f 626a 6563 740a  e a mock object.
-0000ce90: 2020 2020 2020 2020 2020 2320 736f 2077            # so w
-0000cea0: 6520 7265 7475 726e 2069 7420 6173 2069  e return it as i
-0000ceb0: 732e 0a20 2020 2020 2020 2020 2072 6574  s..          ret
-0000cec0: 7572 6e20 6d0a 0a20 2020 2020 2023 205f  urn m..      # _
-0000ced0: 6d61 705f 7375 626d 6f64 756c 6573 2077  map_submodules w
-0000cee0: 696c 6c20 6d61 7020 6f76 6572 2061 6c6c  ill map over all
-0000cef0: 2073 7562 6d6f 6475 6c65 7320 696e 7369   submodules insi
-0000cf00: 6465 2061 7474 7273 0a20 2020 2020 2023  de attrs.      #
-0000cf10: 2076 616c 7565 2068 6572 6520 6361 6e20   value here can 
-0000cf20: 6265 2061 6e79 2070 7974 7265 652c 206e  be any pytree, n
-0000cf30: 6f6e 2d6d 6f64 756c 6520 7661 6c75 6573  on-module values
-0000cf40: 2061 7265 2069 676e 6f72 6564 0a20 2020   are ignored.   
-0000cf50: 2020 2066 6f72 2066 6965 6c64 5f6e 616d     for field_nam
-0000cf60: 652c 2076 616c 7565 2069 6e20 6174 7472  e, value in attr
-0000cf70: 732e 6974 656d 7328 293a 0a20 2020 2020  s.items():.     
-0000cf80: 2020 2069 6620 6669 656c 645f 6e61 6d65     if field_name
-0000cf90: 203d 3d20 2770 6172 656e 7427 3a0a 2020   == 'parent':.  
-0000cfa0: 2020 2020 2020 2020 636f 6e74 696e 7565          continue
-0000cfb0: 0a20 2020 2020 2020 2061 7474 7273 5b66  .        attrs[f
-0000cfc0: 6965 6c64 5f6e 616d 655d 203d 205f 6d61  ield_name] = _ma
-0000cfd0: 705f 7375 626d 6f64 756c 6573 2863 6c6f  p_submodules(clo
-0000cfe0: 6e65 5f66 6e2c 2076 616c 7565 290a 0a20  ne_fn, value).. 
-0000cff0: 2020 206d 6f64 756c 6520 3d20 7365 6c66     module = self
-0000d000: 2e5f 5f63 6c61 7373 5f5f 282a 2a61 7474  .__class__(**att
-0000d010: 7273 290a 0a20 2020 2072 6574 7572 6e20  rs)..    return 
-0000d020: 6d6f 6475 6c65 0a0a 2020 6465 6620 636f  module..  def co
-0000d030: 7079 280a 2020 2020 7365 6c66 3a20 4d2c  py(.    self: M,
-0000d040: 0a20 2020 202a 2c0a 2020 2020 7061 7265  .    *,.    pare
-0000d050: 6e74 3a20 4f70 7469 6f6e 616c 5b55 6e69  nt: Optional[Uni
-0000d060: 6f6e 5b53 636f 7065 2c20 274d 6f64 756c  on[Scope, 'Modul
-0000d070: 6527 2c20 5f53 656e 7469 6e65 6c5d 5d20  e', _Sentinel]] 
-0000d080: 3d20 5f75 6e73 7065 6369 6669 6564 5f70  = _unspecified_p
-0000d090: 6172 656e 742c 0a20 2020 206e 616d 653a  arent,.    name:
-0000d0a0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =
-0000d0b0: 204e 6f6e 652c 0a20 2020 202a 2a75 7064   None,.    **upd
-0000d0c0: 6174 6573 2c0a 2020 2920 2d3e 204d 3a0a  ates,.  ) -> M:.
-0000d0d0: 2020 2020 2222 2243 7265 6174 6573 2061      """Creates a
-0000d0e0: 2063 6f70 7920 6f66 2074 6869 7320 4d6f   copy of this Mo
-0000d0f0: 6475 6c65 2c20 7769 7468 206f 7074 696f  dule, with optio
-0000d100: 6e61 6c6c 7920 7570 6461 7465 6420 6172  nally updated ar
-0000d110: 6775 6d65 6e74 732e 0a0a 2020 2020 4172  guments...    Ar
-0000d120: 6773 3a0a 2020 2020 2020 7061 7265 6e74  gs:.      parent
-0000d130: 3a20 5468 6520 7061 7265 6e74 206f 6620  : The parent of 
-0000d140: 7468 6520 636f 7079 2e20 2042 7920 6465  the copy.  By de
-0000d150: 6661 756c 7420 7468 6520 6375 7272 656e  fault the curren
-0000d160: 7420 6d6f 6475 6c65 2069 7320 7461 6b65  t module is take
-0000d170: 6e0a 2020 2020 2020 2020 6173 2070 6172  n.        as par
-0000d180: 656e 7420 6966 206e 6f74 2065 7870 6c69  ent if not expli
-0000d190: 6369 746c 7920 7370 6563 6966 6965 642e  citly specified.
-0000d1a0: 0a20 2020 2020 206e 616d 653a 2041 206e  .      name: A n
-0000d1b0: 6577 206e 616d 6520 666f 7220 7468 6520  ew name for the 
-0000d1c0: 636f 7069 6564 204d 6f64 756c 652c 2062  copied Module, b
-0000d1d0: 7920 6465 6661 756c 7420 6120 6e65 7720  y default a new 
-0000d1e0: 6175 746f 6d61 7469 6320 6e61 6d65 0a20  automatic name. 
-0000d1f0: 2020 2020 2020 2077 696c 6c20 6265 2067         will be g
-0000d200: 6976 656e 2e0a 2020 2020 2020 2a2a 7570  iven..      **up
-0000d210: 6461 7465 733a 2041 7474 7269 6275 7465  dates: Attribute
-0000d220: 2075 7064 6174 6573 2e0a 0a20 2020 2052   updates...    R
-0000d230: 6574 7572 6e73 3a0a 2020 2020 2020 4120  eturns:.      A 
-0000d240: 636f 7079 206f 6620 7468 6520 7468 6973  copy of the this
-0000d250: 204d 6f64 756c 6520 7769 7468 2074 6865   Module with the
-0000d260: 2075 7064 6174 6564 206e 616d 652c 2070   updated name, p
-0000d270: 6172 656e 742c 2061 6e64 2061 7474 7269  arent, and attri
-0000d280: 6275 7465 732e 0a20 2020 2022 2222 0a20  butes..    """. 
-0000d290: 2020 2072 6574 7572 6e20 7365 6c66 2e63     return self.c
-0000d2a0: 6c6f 6e65 280a 2020 2020 2020 7061 7265  lone(.      pare
-0000d2b0: 6e74 3d70 6172 656e 742c 206e 616d 653d  nt=parent, name=
-0000d2c0: 6e61 6d65 2c20 5f64 6565 705f 636c 6f6e  name, _deep_clon
-0000d2d0: 653d 5472 7565 2c20 5f72 6573 6574 5f6e  e=True, _reset_n
-0000d2e0: 616d 6573 3d46 616c 7365 2c20 2a2a 7570  ames=False, **up
-0000d2f0: 6461 7465 730a 2020 2020 290a 0a20 2040  dates.    )..  @
-0000d300: 6f76 6572 6c6f 6164 0a20 2064 6566 2076  overload.  def v
-0000d310: 6172 6961 626c 6528 0a20 2020 2073 656c  ariable(.    sel
-0000d320: 662c 0a20 2020 2063 6f6c 3a20 7374 722c  f,.    col: str,
-0000d330: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
-0000d340: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
-0000d350: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
-0000d360: 2e2e 2c20 545d 5d20 3d20 4e6f 6e65 2c0a  .., T]] = None,.
-0000d370: 2020 2020 2a69 6e69 745f 6172 6773 2c0a      *init_args,.
-0000d380: 2020 2920 2d3e 2056 6172 6961 626c 655b    ) -> Variable[
-0000d390: 545d 3a0a 2020 2020 2e2e 2e0a 0a20 2040  T]:.    .....  @
-0000d3a0: 6f76 6572 6c6f 6164 0a20 2064 6566 2076  overload.  def v
-0000d3b0: 6172 6961 626c 6528 0a20 2020 2073 656c  ariable(.    sel
-0000d3c0: 662c 0a20 2020 2063 6f6c 3a20 7374 722c  f,.    col: str,
-0000d3d0: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
-0000d3e0: 2020 2020 696e 6974 5f66 6e3a 204f 7074      init_fn: Opt
-0000d3f0: 696f 6e61 6c5b 4361 6c6c 6162 6c65 5b2e  ional[Callable[.
-0000d400: 2e2e 2c20 545d 5d20 3d20 4e6f 6e65 2c0a  .., T]] = None,.
-0000d410: 2020 2020 2a69 6e69 745f 6172 6773 2c0a      *init_args,.
-0000d420: 2020 2020 756e 626f 783a 204c 6974 6572      unbox: Liter
-0000d430: 616c 5b54 7275 655d 2c0a 2020 2020 2a2a  al[True],.    **
-0000d440: 696e 6974 5f6b 7761 7267 732c 0a20 2029  init_kwargs,.  )
-0000d450: 202d 3e20 5661 7269 6162 6c65 5b54 5d3a   -> Variable[T]:
-0000d460: 0a20 2020 202e 2e2e 0a0a 2020 406f 7665  .    .....  @ove
-0000d470: 726c 6f61 640a 2020 6465 6620 7661 7269  rload.  def vari
-0000d480: 6162 6c65 280a 2020 2020 7365 6c66 2c0a  able(.    self,.
-0000d490: 2020 2020 636f 6c3a 2073 7472 2c0a 2020      col: str,.  
-0000d4a0: 2020 6e61 6d65 3a20 7374 722c 0a20 2020    name: str,.   
-0000d4b0: 2069 6e69 745f 666e 3a20 4f70 7469 6f6e   init_fn: Option
-0000d4c0: 616c 5b43 616c 6c61 626c 655b 2e2e 2e2c  al[Callable[...,
-0000d4d0: 2054 5d5d 203d 204e 6f6e 652c 0a20 2020   T]] = None,.   
-0000d4e0: 202a 696e 6974 5f61 7267 732c 0a20 2020   *init_args,.   
-0000d4f0: 2075 6e62 6f78 3a20 4c69 7465 7261 6c5b   unbox: Literal[
-0000d500: 4661 6c73 655d 2c0a 2020 2020 2a2a 696e  False],.    **in
-0000d510: 6974 5f6b 7761 7267 732c 0a20 2029 202d  it_kwargs,.  ) -
-0000d520: 3e20 5661 7269 6162 6c65 5b6d 6574 612e  > Variable[meta.
-0000d530: 4178 6973 4d65 7461 6461 7461 5b54 5d5d  AxisMetadata[T]]
-0000d540: 3a0a 2020 2020 2e2e 2e0a 0a20 2040 6f76  :.    .....  @ov
-0000d550: 6572 6c6f 6164 0a20 2064 6566 2076 6172  erload.  def var
-0000d560: 6961 626c 6528 0a20 2020 2073 656c 662c  iable(.    self,
-0000d570: 0a20 2020 2063 6f6c 3a20 7374 722c 0a20  .    col: str,. 
-0000d580: 2020 206e 616d 653a 2073 7472 2c0a 2020     name: str,.  
-0000d590: 2020 696e 6974 5f66 6e3a 204f 7074 696f    init_fn: Optio
-0000d5a0: 6e61 6c5b 4361 6c6c 6162 6c65 5b2e 2e2e  nal[Callable[...
-0000d5b0: 2c20 545d 5d20 3d20 4e6f 6e65 2c0a 2020  , T]] = None,.  
-0000d5c0: 2020 2a69 6e69 745f 6172 6773 2c0a 2020    *init_args,.  
-0000d5d0: 2020 756e 626f 783a 2062 6f6f 6c20 3d20    unbox: bool = 
-0000d5e0: 5472 7565 2c0a 2020 2020 2a2a 696e 6974  True,.    **init
-0000d5f0: 5f6b 7761 7267 732c 0a20 2029 202d 3e20  _kwargs,.  ) -> 
-0000d600: 556e 696f 6e5b 5661 7269 6162 6c65 5b54  Union[Variable[T
-0000d610: 5d2c 2056 6172 6961 626c 655b 6d65 7461  ], Variable[meta
-0000d620: 2e41 7869 734d 6574 6164 6174 615b 545d  .AxisMetadata[T]
-0000d630: 5d5d 3a0a 2020 2020 2e2e 2e0a 0a20 2064  ]]:.    .....  d
-0000d640: 6566 2076 6172 6961 626c 6528 0a20 2020  ef variable(.   
-0000d650: 2073 656c 662c 0a20 2020 2063 6f6c 3a20   self,.    col: 
-0000d660: 7374 722c 0a20 2020 206e 616d 653a 2073  str,.    name: s
-0000d670: 7472 2c0a 2020 2020 696e 6974 5f66 6e3a  tr,.    init_fn:
-0000d680: 204f 7074 696f 6e61 6c5b 4361 6c6c 6162   Optional[Callab
-0000d690: 6c65 5b2e 2e2e 2c20 545d 5d20 3d20 4e6f  le[..., T]] = No
-0000d6a0: 6e65 2c0a 2020 2020 2a69 6e69 745f 6172  ne,.    *init_ar
-0000d6b0: 6773 2c0a 2020 2020 756e 626f 783a 2062  gs,.    unbox: b
-0000d6c0: 6f6f 6c20 3d20 5472 7565 2c0a 2020 2020  ool = True,.    
-0000d6d0: 2a2a 696e 6974 5f6b 7761 7267 732c 0a20  **init_kwargs,. 
-0000d6e0: 2029 202d 3e20 556e 696f 6e5b 5661 7269   ) -> Union[Vari
-0000d6f0: 6162 6c65 5b54 5d2c 2056 6172 6961 626c  able[T], Variabl
-0000d700: 655b 6d65 7461 2e41 7869 734d 6574 6164  e[meta.AxisMetad
-0000d710: 6174 615b 545d 5d5d 3a0a 2020 2020 2222  ata[T]]]:.    ""
-0000d720: 2244 6563 6c61 7265 7320 616e 6420 7265  "Declares and re
-0000d730: 7475 726e 7320 6120 7661 7269 6162 6c65  turns a variable
-0000d740: 2069 6e20 7468 6973 204d 6f64 756c 652e   in this Module.
-0000d750: 0a0a 2020 2020 5365 6520 3a6d 6f64 3a60  ..    See :mod:`
-0000d760: 666c 6178 2e63 6f72 652e 7661 7269 6162  flax.core.variab
-0000d770: 6c65 7360 2066 6f72 206d 6f72 6520 696e  les` for more in
-0000d780: 666f 726d 6174 696f 6e2e 2053 6565 2061  formation. See a
-0000d790: 6c73 6f20 3a6d 6574 683a 6070 6172 616d  lso :meth:`param
-0000d7a0: 600a 2020 2020 666f 7220 6120 7368 6f72  `.    for a shor
-0000d7b0: 7468 616e 6420 7761 7920 746f 2064 6566  thand way to def
-0000d7c0: 696e 6520 7265 6164 2d6f 6e6c 7920 7661  ine read-only va
-0000d7d0: 7269 6162 6c65 7320 696e 2074 6865 2022  riables in the "
-0000d7e0: 7061 7261 6d73 220a 2020 2020 636f 6c6c  params".    coll
-0000d7f0: 6563 7469 6f6e 2e0a 0a20 2020 2043 6f6e  ection...    Con
-0000d800: 7472 6172 7920 746f 203a 6d65 7468 3a60  trary to :meth:`
-0000d810: 7061 7261 6d60 2c20 616c 6c20 6172 6775  param`, all argu
-0000d820: 6d65 6e74 7320 7061 7373 696e 6720 7573  ments passing us
-0000d830: 696e 6720 6060 696e 6974 5f66 6e60 6020  ing ``init_fn`` 
-0000d840: 7368 6f75 6c64 2062 650a 2020 2020 7061  should be.    pa
-0000d850: 7373 6564 206f 6e20 6578 706c 6963 6974  ssed on explicit
-0000d860: 6c79 3a3a 0a0a 2020 2020 2020 3e3e 3e20  ly::..      >>> 
-0000d870: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
-0000d880: 756c 6529 3a0a 2020 2020 2020 2e2e 2e20  ule):.      ... 
-0000d890: 2020 406e 6e2e 636f 6d70 6163 740a 2020    @nn.compact.  
-0000d8a0: 2020 2020 2e2e 2e20 2020 6465 6620 5f5f      ...   def __
-0000d8b0: 6361 6c6c 5f5f 2873 656c 662c 2078 293a  call__(self, x):
-0000d8c0: 0a20 2020 2020 202e 2e2e 2020 2020 2078  .      ...     x
-0000d8d0: 203d 206e 6e2e 4465 6e73 6528 3429 2878   = nn.Dense(4)(x
-0000d8e0: 290a 2020 2020 2020 2e2e 2e20 2020 2020  ).      ...     
-0000d8f0: 6b65 7920 3d20 7365 6c66 2e6d 616b 655f  key = self.make_
-0000d900: 726e 6728 2773 7461 7473 2729 0a20 2020  rng('stats').   
-0000d910: 2020 202e 2e2e 2020 2020 206d 6561 6e20     ...     mean 
-0000d920: 3d20 7365 6c66 2e76 6172 6961 626c 6528  = self.variable(
-0000d930: 2773 7461 7473 272c 2027 6d65 616e 272c  'stats', 'mean',
-0000d940: 206e 6e2e 696e 6974 6961 6c69 7a65 7273   nn.initializers
-0000d950: 2e6c 6563 756e 5f6e 6f72 6d61 6c28 292c  .lecun_normal(),
-0000d960: 206b 6579 2c20 782e 7368 6170 6529 0a20   key, x.shape). 
-0000d970: 2020 2020 202e 2e2e 2020 2020 202e 2e2e       ...     ...
-0000d980: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
-0000d990: 6574 7572 6e20 7820 2a20 6d65 616e 2e76  eturn x * mean.v
-0000d9a0: 616c 7565 0a20 2020 2020 203e 3e3e 2076  alue.      >>> v
-0000d9b0: 6172 6961 626c 6573 203d 2046 6f6f 2829  ariables = Foo()
-0000d9c0: 2e69 6e69 7428 7b27 7061 7261 6d73 273a  .init({'params':
-0000d9d0: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
-0000d9e0: 3029 2c20 2773 7461 7473 273a 206a 6178  0), 'stats': jax
-0000d9f0: 2e72 616e 646f 6d2e 6b65 7928 3129 7d2c  .random.key(1)},
-0000da00: 206a 6e70 2e6f 6e65 7328 2832 2c20 3329   jnp.ones((2, 3)
-0000da10: 2929 0a20 2020 2020 203e 3e3e 206a 6178  )).      >>> jax
-0000da20: 2e74 7265 655f 7574 696c 2e74 7265 655f  .tree_util.tree_
-0000da30: 6d61 7028 6a6e 702e 7368 6170 652c 2076  map(jnp.shape, v
-0000da40: 6172 6961 626c 6573 290a 2020 2020 2020  ariables).      
-0000da50: 7b27 7061 7261 6d73 273a 207b 2744 656e  {'params': {'Den
-0000da60: 7365 5f30 273a 207b 2762 6961 7327 3a20  se_0': {'bias': 
-0000da70: 2834 2c29 2c20 276b 6572 6e65 6c27 3a20  (4,), 'kernel': 
-0000da80: 2833 2c20 3429 7d7d 2c20 2773 7461 7473  (3, 4)}}, 'stats
-0000da90: 273a 207b 276d 6561 6e27 3a20 2832 2c20  ': {'mean': (2, 
-0000daa0: 3429 7d7d 0a0a 2020 2020 496e 2074 6865  4)}}..    In the
-0000dab0: 2065 7861 6d70 6c65 2061 626f 7665 2c20   example above, 
-0000dac0: 7468 6520 6675 6e63 7469 6f6e 2060 606c  the function ``l
-0000dad0: 6563 756e 5f6e 6f72 6d61 6c60 6020 6578  ecun_normal`` ex
-0000dae0: 7065 6374 7320 7477 6f20 6172 6775 6d65  pects two argume
-0000daf0: 6e74 733a 0a20 2020 2060 606b 6579 6060  nts:.    ``key``
-0000db00: 2061 6e64 2060 6073 6861 7065 6060 2c20   and ``shape``, 
-0000db10: 616e 6420 626f 7468 2068 6176 6520 746f  and both have to
-0000db20: 2062 6520 7061 7373 6564 206f 6e2e 2054   be passed on. T
-0000db30: 6865 2050 524e 4720 666f 7220 6060 7374  he PRNG for ``st
-0000db40: 6174 7360 600a 2020 2020 6861 7320 746f  ats``.    has to
-0000db50: 2062 6520 7072 6f76 6964 6564 2065 7870   be provided exp
-0000db60: 6c69 6369 746c 7920 7768 656e 2063 616c  licitly when cal
-0000db70: 6c69 6e67 203a 6d65 7468 3a60 696e 6974  ling :meth:`init
-0000db80: 6020 616e 6420 3a6d 6574 683a 6061 7070  ` and :meth:`app
-0000db90: 6c79 602e 0a0a 2020 2020 4172 6773 3a0a  ly`...    Args:.
-0000dba0: 2020 2020 2020 636f 6c3a 2054 6865 2076        col: The v
-0000dbb0: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
-0000dbc0: 6f6e 206e 616d 652e 0a20 2020 2020 206e  on name..      n
-0000dbd0: 616d 653a 2054 6865 2076 6172 6961 626c  ame: The variabl
-0000dbe0: 6520 6e61 6d65 2e0a 2020 2020 2020 696e  e name..      in
-0000dbf0: 6974 5f66 6e3a 2054 6865 2066 756e 6374  it_fn: The funct
-0000dc00: 696f 6e20 7468 6174 2077 696c 6c20 6265  ion that will be
-0000dc10: 2063 616c 6c65 6420 746f 2063 6f6d 7075   called to compu
-0000dc20: 7465 2074 6865 2069 6e69 7469 616c 2076  te the initial v
-0000dc30: 616c 7565 206f 660a 2020 2020 2020 2020  alue of.        
-0000dc40: 7468 6973 2076 6172 6961 626c 652e 2054  this variable. T
-0000dc50: 6869 7320 6675 6e63 7469 6f6e 2077 696c  his function wil
-0000dc60: 6c20 6f6e 6c79 2062 6520 6361 6c6c 6564  l only be called
-0000dc70: 2074 6865 2066 6972 7374 2074 696d 6520   the first time 
-0000dc80: 7468 6973 0a20 2020 2020 2020 2076 6172  this.        var
-0000dc90: 6961 626c 6520 6973 2075 7365 6420 696e  iable is used in
-0000dca0: 2074 6869 7320 6d6f 6475 6c65 2e20 4966   this module. If
-0000dcb0: 204e 6f6e 652c 2074 6865 2076 6172 6961   None, the varia
-0000dcc0: 626c 6520 6d75 7374 2061 6c72 6561 6479  ble must already
-0000dcd0: 2062 650a 2020 2020 2020 2020 696e 6974   be.        init
-0000dce0: 6961 6c69 7a65 6420 6f74 6865 7277 6973  ialized otherwis
-0000dcf0: 6520 616e 2065 7272 6f72 2069 7320 7261  e an error is ra
-0000dd00: 6973 6564 2e0a 2020 2020 2020 2a69 6e69  ised..      *ini
-0000dd10: 745f 6172 6773 3a20 5468 6520 706f 7369  t_args: The posi
-0000dd20: 7469 6f6e 616c 2061 7267 756d 656e 7473  tional arguments
-0000dd30: 2074 6f20 7061 7373 2074 6f20 696e 6974   to pass to init
-0000dd40: 5f66 6e2e 0a20 2020 2020 2075 6e62 6f78  _fn..      unbox
-0000dd50: 3a20 4966 2054 7275 652c 2060 6041 7869  : If True, ``Axi
-0000dd60: 734d 6574 6164 6174 6160 6020 696e 7374  sMetadata`` inst
-0000dd70: 616e 6365 7320 6172 6520 7265 706c 6163  ances are replac
-0000dd80: 6564 2062 7920 7468 6569 7220 756e 626f  ed by their unbo
-0000dd90: 7865 640a 2020 2020 2020 2020 7661 6c75  xed.        valu
-0000dda0: 652c 2073 6565 2060 6066 6c61 782e 6e6e  e, see ``flax.nn
-0000ddb0: 2e6d 6574 612e 756e 626f 7860 6020 2864  .meta.unbox`` (d
-0000ddc0: 6566 6175 6c74 3a20 5472 7565 292e 0a20  efault: True).. 
-0000ddd0: 2020 2020 202a 2a69 6e69 745f 6b77 6172       **init_kwar
-0000dde0: 6773 3a20 5468 6520 6b65 792d 776f 7264  gs: The key-word
-0000ddf0: 2061 7267 756d 656e 7473 2074 6f20 7061   arguments to pa
-0000de00: 7373 2074 6f20 696e 6974 5f66 6e0a 0a20  ss to init_fn.. 
-0000de10: 2020 2052 6574 7572 6e73 3a0a 2020 2020     Returns:.    
-0000de20: 2020 4120 3a63 6c61 7373 3a60 666c 6178    A :class:`flax
-0000de30: 2e63 6f72 652e 7661 7269 6162 6c65 732e  .core.variables.
-0000de40: 5661 7269 6162 6c65 6020 7468 6174 2063  Variable` that c
-0000de50: 616e 2062 6520 7265 6164 206f 7220 7365  an be read or se
-0000de60: 7420 7669 610a 2020 2020 2020 222e 7661  t via.      ".va
-0000de70: 6c75 6522 2061 7474 7269 6275 7465 2e20  lue" attribute. 
-0000de80: 5468 726f 7773 2061 6e20 6572 726f 7220  Throws an error 
-0000de90: 6966 2074 6865 2076 6172 6961 626c 6520  if the variable 
-0000dea0: 6578 6973 7473 2061 6c72 6561 6479 2e0a  exists already..
-0000deb0: 2020 2020 2222 220a 2020 2020 6966 206e      """.    if n
-0000dec0: 6f74 2073 656c 662e 5f69 6e69 7469 616c  ot self._initial
-0000ded0: 697a 6174 696f 6e5f 616c 6c6f 7765 643a  ization_allowed:
-0000dee0: 0a20 2020 2020 2072 6169 7365 2056 616c  .      raise Val
-0000def0: 7565 4572 726f 7228 0a20 2020 2020 2020  ueError(.       
-0000df00: 2027 5661 7269 6162 6c65 7320 6d75 7374   'Variables must
-0000df10: 2062 6520 696e 6974 6961 6c69 7a65 6420   be initialized 
-0000df20: 696e 2060 7365 7475 7028 2960 206f 7220  in `setup()` or 
-0000df30: 696e 2061 206d 6574 686f 6420 270a 2020  in a method '.  
-0000df40: 2020 2020 2020 2777 7261 7070 6564 2069        'wrapped i
-0000df50: 6e20 6040 636f 6d70 6163 7460 270a 2020  n `@compact`'.  
-0000df60: 2020 2020 290a 2020 2020 6966 2073 656c      ).    if sel
-0000df70: 662e 5f6e 616d 655f 7461 6b65 6e28 6e61  f._name_taken(na
-0000df80: 6d65 2c20 636f 6c6c 6563 7469 6f6e 3d63  me, collection=c
-0000df90: 6f6c 293a 0a20 2020 2020 2072 6169 7365  ol):.      raise
-0000dfa0: 2065 7272 6f72 732e 4e61 6d65 496e 5573   errors.NameInUs
-0000dfb0: 6545 7272 6f72 2827 7661 7269 6162 6c65  eError('variable
-0000dfc0: 272c 206e 616d 652c 2073 656c 662e 5f5f  ', name, self.__
-0000dfd0: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
-0000dfe0: 290a 2020 2020 6173 7365 7274 2073 656c  ).    assert sel
-0000dff0: 662e 7363 6f70 6520 6973 206e 6f74 204e  f.scope is not N
-0000e000: 6f6e 650a 2020 2020 7620 3d20 7365 6c66  one.    v = self
-0000e010: 2e73 636f 7065 2e76 6172 6961 626c 6528  .scope.variable(
-0000e020: 0a20 2020 2020 2063 6f6c 2c20 6e61 6d65  .      col, name
-0000e030: 2c20 696e 6974 5f66 6e2c 202a 696e 6974  , init_fn, *init
-0000e040: 5f61 7267 732c 2075 6e62 6f78 3d75 6e62  _args, unbox=unb
-0000e050: 6f78 2c20 2a2a 696e 6974 5f6b 7761 7267  ox, **init_kwarg
-0000e060: 730a 2020 2020 290a 2020 2020 7365 6c66  s.    ).    self
-0000e070: 2e5f 7374 6174 652e 6368 696c 6472 656e  ._state.children
-0000e080: 5b6e 616d 655d 203d 2063 6f6c 0a20 2020  [name] = col.   
-0000e090: 2072 6574 7572 6e20 760a 0a20 2040 6f76   return v..  @ov
-0000e0a0: 6572 6c6f 6164 0a20 2064 6566 2070 6172  erload.  def par
-0000e0b0: 616d 280a 2020 2020 7365 6c66 2c20 6e61  am(.    self, na
-0000e0c0: 6d65 3a20 7374 722c 2069 6e69 745f 666e  me: str, init_fn
-0000e0d0: 3a20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  : Callable[..., 
-0000e0e0: 545d 2c20 2a69 6e69 745f 6172 6773 2c0a  T], *init_args,.
-0000e0f0: 2020 2920 2d3e 2054 3a0a 2020 2020 2e2e    ) -> T:.    ..
-0000e100: 2e0a 0a20 2040 6f76 6572 6c6f 6164 0a20  ...  @overload. 
-0000e110: 2064 6566 2070 6172 616d 280a 2020 2020   def param(.    
-0000e120: 7365 6c66 2c0a 2020 2020 6e61 6d65 3a20  self,.    name: 
-0000e130: 7374 722c 0a20 2020 2069 6e69 745f 666e  str,.    init_fn
-0000e140: 3a20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  : Callable[..., 
-0000e150: 545d 2c0a 2020 2020 2a69 6e69 745f 6172  T],.    *init_ar
-0000e160: 6773 2c0a 2020 2020 756e 626f 783a 204c  gs,.    unbox: L
-0000e170: 6974 6572 616c 5b54 7275 655d 2c0a 2020  iteral[True],.  
-0000e180: 2020 2a2a 696e 6974 5f6b 7761 7267 732c    **init_kwargs,
-0000e190: 0a20 2029 202d 3e20 543a 0a20 2020 202e  .  ) -> T:.    .
-0000e1a0: 2e2e 0a0a 2020 406f 7665 726c 6f61 640a  ....  @overload.
-0000e1b0: 2020 6465 6620 7061 7261 6d28 0a20 2020    def param(.   
-0000e1c0: 2073 656c 662c 0a20 2020 206e 616d 653a   self,.    name:
-0000e1d0: 2073 7472 2c0a 2020 2020 696e 6974 5f66   str,.    init_f
-0000e1e0: 6e3a 2043 616c 6c61 626c 655b 2e2e 2e2c  n: Callable[...,
-0000e1f0: 2054 5d2c 0a20 2020 202a 696e 6974 5f61   T],.    *init_a
-0000e200: 7267 732c 0a20 2020 2075 6e62 6f78 3a20  rgs,.    unbox: 
-0000e210: 4c69 7465 7261 6c5b 4661 6c73 655d 2c0a  Literal[False],.
-0000e220: 2020 2020 2a2a 696e 6974 5f6b 7761 7267      **init_kwarg
-0000e230: 732c 0a20 2029 202d 3e20 6d65 7461 2e41  s,.  ) -> meta.A
-0000e240: 7869 734d 6574 6164 6174 615b 545d 3a0a  xisMetadata[T]:.
-0000e250: 2020 2020 2e2e 2e0a 0a20 2040 6f76 6572      .....  @over
-0000e260: 6c6f 6164 0a20 2064 6566 2070 6172 616d  load.  def param
-0000e270: 280a 2020 2020 7365 6c66 2c0a 2020 2020  (.    self,.    
-0000e280: 6e61 6d65 3a20 7374 722c 0a20 2020 2069  name: str,.    i
-0000e290: 6e69 745f 666e 3a20 4361 6c6c 6162 6c65  nit_fn: Callable
-0000e2a0: 5b2e 2e2e 2c20 545d 2c0a 2020 2020 2a69  [..., T],.    *i
-0000e2b0: 6e69 745f 6172 6773 2c0a 2020 2020 756e  nit_args,.    un
-0000e2c0: 626f 783a 2062 6f6f 6c2c 0a20 2020 202a  box: bool,.    *
-0000e2d0: 2a69 6e69 745f 6b77 6172 6773 2c0a 2020  *init_kwargs,.  
-0000e2e0: 2920 2d3e 2055 6e69 6f6e 5b54 2c20 6d65  ) -> Union[T, me
-0000e2f0: 7461 2e41 7869 734d 6574 6164 6174 615b  ta.AxisMetadata[
-0000e300: 545d 5d3a 0a20 2020 202e 2e2e 0a0a 2020  T]]:.    .....  
-0000e310: 6465 6620 7061 7261 6d28 0a20 2020 2073  def param(.    s
-0000e320: 656c 662c 0a20 2020 206e 616d 653a 2073  elf,.    name: s
-0000e330: 7472 2c0a 2020 2020 696e 6974 5f66 6e3a  tr,.    init_fn:
-0000e340: 2043 616c 6c61 626c 655b 2e2e 2e2c 2054   Callable[..., T
-0000e350: 5d2c 0a20 2020 202a 696e 6974 5f61 7267  ],.    *init_arg
-0000e360: 732c 0a20 2020 2075 6e62 6f78 3a20 626f  s,.    unbox: bo
-0000e370: 6f6c 203d 2054 7275 652c 0a20 2020 202a  ol = True,.    *
-0000e380: 2a69 6e69 745f 6b77 6172 6773 2c0a 2020  *init_kwargs,.  
-0000e390: 2920 2d3e 2055 6e69 6f6e 5b54 2c20 6d65  ) -> Union[T, me
-0000e3a0: 7461 2e41 7869 734d 6574 6164 6174 615b  ta.AxisMetadata[
-0000e3b0: 545d 5d3a 0a20 2020 2022 2222 4465 636c  T]]:.    """Decl
-0000e3c0: 6172 6573 2061 6e64 2072 6574 7572 6e73  ares and returns
-0000e3d0: 2061 2070 6172 616d 6574 6572 2069 6e20   a parameter in 
-0000e3e0: 7468 6973 204d 6f64 756c 652e 0a0a 2020  this Module...  
-0000e3f0: 2020 5061 7261 6d65 7465 7273 2061 7265    Parameters are
-0000e400: 2072 6561 642d 6f6e 6c79 2076 6172 6961   read-only varia
-0000e410: 626c 6573 2069 6e20 7468 6520 636f 6c6c  bles in the coll
-0000e420: 6563 7469 6f6e 206e 616d 6564 2022 7061  ection named "pa
-0000e430: 7261 6d73 222e 2053 6565 0a20 2020 203a  rams". See.    :
-0000e440: 6d6f 643a 6066 6c61 782e 636f 7265 2e76  mod:`flax.core.v
-0000e450: 6172 6961 626c 6573 6020 666f 7220 6d6f  ariables` for mo
-0000e460: 7265 2064 6574 6169 6c73 206f 6e20 7661  re details on va
-0000e470: 7269 6162 6c65 732e 0a0a 2020 2020 5468  riables...    Th
-0000e480: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
-0000e490: 206f 6620 6060 696e 6974 5f66 6e60 6020   of ``init_fn`` 
-0000e4a0: 6973 2061 7373 756d 6564 2074 6f20 6265  is assumed to be
-0000e4b0: 2061 2050 524e 4720 6b65 792c 2077 6869   a PRNG key, whi
-0000e4c0: 6368 2069 730a 2020 2020 7072 6f76 6964  ch is.    provid
-0000e4d0: 6564 2061 7574 6f6d 6174 6963 616c 6c79  ed automatically
-0000e4e0: 2061 6e64 2064 6f65 7320 6e6f 7420 6861   and does not ha
-0000e4f0: 7665 2074 6f20 6265 2070 6173 7365 6420  ve to be passed 
-0000e500: 7573 696e 6720 6060 696e 6974 5f61 7267  using ``init_arg
-0000e510: 7360 600a 2020 2020 6f72 2060 6069 6e69  s``.    or ``ini
-0000e520: 745f 6b77 6172 6773 6060 3a3a 0a0a 2020  t_kwargs``::..  
-0000e530: 2020 2020 3e3e 3e20 636c 6173 7320 466f      >>> class Fo
-0000e540: 6f28 6e6e 2e4d 6f64 756c 6529 3a0a 2020  o(nn.Module):.  
-0000e550: 2020 2020 2e2e 2e20 2020 406e 6e2e 636f      ...   @nn.co
-0000e560: 6d70 6163 740a 2020 2020 2020 2e2e 2e20  mpact.      ... 
-0000e570: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
-0000e580: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
-0000e590: 2e2e 2020 2020 2078 203d 206e 6e2e 4465  ..     x = nn.De
-0000e5a0: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
-0000e5b0: 2e2e 2e20 2020 2020 6d65 616e 203d 2073  ...     mean = s
-0000e5c0: 656c 662e 7061 7261 6d28 276d 6561 6e27  elf.param('mean'
-0000e5d0: 2c20 6e6e 2e69 6e69 7469 616c 697a 6572  , nn.initializer
-0000e5e0: 732e 6c65 6375 6e5f 6e6f 726d 616c 2829  s.lecun_normal()
-0000e5f0: 2c20 782e 7368 6170 6529 0a20 2020 2020  , x.shape).     
-0000e600: 202e 2e2e 2020 2020 202e 2e2e 0a20 2020   ...     ....   
-0000e610: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
-0000e620: 6e20 7820 2a20 6d65 616e 0a20 2020 2020  n x * mean.     
-0000e630: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
-0000e640: 2046 6f6f 2829 2e69 6e69 7428 7b27 7061   Foo().init({'pa
-0000e650: 7261 6d73 273a 206a 6178 2e72 616e 646f  rams': jax.rando
-0000e660: 6d2e 6b65 7928 3029 2c20 2773 7461 7473  m.key(0), 'stats
-0000e670: 273a 206a 6178 2e72 616e 646f 6d2e 6b65  ': jax.random.ke
-0000e680: 7928 3129 7d2c 206a 6e70 2e6f 6e65 7328  y(1)}, jnp.ones(
-0000e690: 2832 2c20 3329 2929 0a20 2020 2020 203e  (2, 3))).      >
-0000e6a0: 3e3e 206a 6178 2e74 7265 655f 7574 696c  >> jax.tree_util
-0000e6b0: 2e74 7265 655f 6d61 7028 6a6e 702e 7368  .tree_map(jnp.sh
-0000e6c0: 6170 652c 2076 6172 6961 626c 6573 290a  ape, variables).
-0000e6d0: 2020 2020 2020 7b27 7061 7261 6d73 273a        {'params':
-0000e6e0: 207b 2744 656e 7365 5f30 273a 207b 2762   {'Dense_0': {'b
-0000e6f0: 6961 7327 3a20 2834 2c29 2c20 276b 6572  ias': (4,), 'ker
-0000e700: 6e65 6c27 3a20 2833 2c20 3429 7d2c 2027  nel': (3, 4)}, '
-0000e710: 6d65 616e 273a 2028 322c 2034 297d 7d0a  mean': (2, 4)}}.
-0000e720: 0a20 2020 2049 6e20 7468 6520 6578 616d  .    In the exam
-0000e730: 706c 6520 6162 6f76 652c 2074 6865 2066  ple above, the f
-0000e740: 756e 6374 696f 6e20 6060 6c65 6375 6e5f  unction ``lecun_
-0000e750: 6e6f 726d 616c 6060 2065 7870 6563 7473  normal`` expects
-0000e760: 2074 776f 2061 7267 756d 656e 7473 3a0a   two arguments:.
-0000e770: 2020 2020 6060 6b65 7960 6020 616e 6420      ``key`` and 
-0000e780: 6060 7368 6170 6560 602c 2062 7574 206f  ``shape``, but o
-0000e790: 6e6c 7920 6060 7368 6170 6560 6020 6861  nly ``shape`` ha
-0000e7a0: 7320 746f 2062 6520 7072 6f76 6964 6564  s to be provided
-0000e7b0: 2065 7870 6c69 6369 746c 793b 0a20 2020   explicitly;.   
-0000e7c0: 2060 606b 6579 6060 2069 7320 7365 7420   ``key`` is set 
-0000e7d0: 6175 746f 6d61 7469 6361 6c6c 7920 7573  automatically us
-0000e7e0: 696e 6720 7468 6520 5052 4e47 2066 6f72  ing the PRNG for
-0000e7f0: 2060 6070 6172 616d 7360 6020 7468 6174   ``params`` that
-0000e800: 2069 7320 7061 7373 6564 0a20 2020 2077   is passed.    w
-0000e810: 6865 6e20 696e 6974 6961 6c69 7a69 6e67  hen initializing
-0000e820: 2074 6865 206d 6f64 756c 6520 7573 696e   the module usin
-0000e830: 6720 3a6d 6574 683a 6069 6e69 7460 2e0a  g :meth:`init`..
-0000e840: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
-0000e850: 206e 616d 653a 2054 6865 2070 6172 616d   name: The param
-0000e860: 6574 6572 206e 616d 652e 0a20 2020 2020  eter name..     
-0000e870: 2069 6e69 745f 666e 3a20 5468 6520 6675   init_fn: The fu
-0000e880: 6e63 7469 6f6e 2074 6861 7420 7769 6c6c  nction that will
-0000e890: 2062 6520 6361 6c6c 6564 2074 6f20 636f   be called to co
-0000e8a0: 6d70 7574 6520 7468 6520 696e 6974 6961  mpute the initia
-0000e8b0: 6c20 7661 6c75 6520 6f66 0a20 2020 2020  l value of.     
-0000e8c0: 2020 2074 6869 7320 7661 7269 6162 6c65     this variable
-0000e8d0: 2e20 5468 6973 2066 756e 6374 696f 6e20  . This function 
-0000e8e0: 7769 6c6c 206f 6e6c 7920 6265 2063 616c  will only be cal
-0000e8f0: 6c65 6420 7468 6520 6669 7273 7420 7469  led the first ti
-0000e900: 6d65 2074 6869 730a 2020 2020 2020 2020  me this.        
-0000e910: 7061 7261 6d65 7465 7220 6973 2075 7365  parameter is use
-0000e920: 6420 696e 2074 6869 7320 6d6f 6475 6c65  d in this module
-0000e930: 2e0a 2020 2020 2020 2a69 6e69 745f 6172  ..      *init_ar
-0000e940: 6773 3a20 5468 6520 706f 7369 7469 6f6e  gs: The position
-0000e950: 616c 2061 7267 756d 656e 7473 2074 6f20  al arguments to 
-0000e960: 7061 7373 2074 6f20 696e 6974 5f66 6e2e  pass to init_fn.
-0000e970: 0a20 2020 2020 2075 6e62 6f78 3a20 4966  .      unbox: If
-0000e980: 2054 7275 652c 2060 6041 7869 734d 6574   True, ``AxisMet
-0000e990: 6164 6174 6160 6020 696e 7374 616e 6365  adata`` instance
-0000e9a0: 7320 6172 6520 7265 706c 6163 6564 2062  s are replaced b
-0000e9b0: 7920 7468 6569 7220 756e 626f 7865 640a  y their unboxed.
-0000e9c0: 2020 2020 2020 2020 7661 6c75 652c 2073          value, s
-0000e9d0: 6565 2060 6066 6c61 782e 6e6e 2e6d 6574  ee ``flax.nn.met
-0000e9e0: 612e 756e 626f 7860 6020 2864 6566 6175  a.unbox`` (defau
-0000e9f0: 6c74 3a20 5472 7565 292e 0a20 2020 2020  lt: True)..     
-0000ea00: 202a 2a69 6e69 745f 6b77 6172 6773 3a20   **init_kwargs: 
-0000ea10: 5468 6520 6b65 792d 776f 7264 2061 7267  The key-word arg
-0000ea20: 756d 656e 7473 2074 6f20 7061 7373 2074  uments to pass t
-0000ea30: 6f20 696e 6974 5f66 6e2e 0a0a 2020 2020  o init_fn...    
-0000ea40: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
-0000ea50: 6865 2076 616c 7565 206f 6620 7468 6520  he value of the 
-0000ea60: 696e 6974 6961 6c69 7a65 6420 7061 7261  initialized para
-0000ea70: 6d65 7465 722e 2054 6872 6f77 7320 616e  meter. Throws an
-0000ea80: 2065 7272 6f72 2069 6620 7468 6520 7061   error if the pa
-0000ea90: 7261 6d65 7465 720a 2020 2020 2020 6578  rameter.      ex
-0000eaa0: 6973 7473 2061 6c72 6561 6479 2e0a 2020  ists already..  
-0000eab0: 2020 2222 220a 2020 2020 6966 206e 6f74    """.    if not
-0000eac0: 2073 656c 662e 5f69 6e69 7469 616c 697a   self._initializ
-0000ead0: 6174 696f 6e5f 616c 6c6f 7765 643a 0a20  ation_allowed:. 
-0000eae0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-0000eaf0: 4572 726f 7228 0a20 2020 2020 2020 2027  Error(.        '
-0000eb00: 5061 7261 6d65 7465 7273 206d 7573 7420  Parameters must 
-0000eb10: 6265 2069 6e69 7469 616c 697a 6564 2069  be initialized i
-0000eb20: 6e20 6073 6574 7570 2829 6020 6f72 2069  n `setup()` or i
-0000eb30: 6e20 6120 6d65 7468 6f64 2027 0a20 2020  n a method '.   
-0000eb40: 2020 2020 2027 7772 6170 7065 6420 696e       'wrapped in
-0000eb50: 2060 4063 6f6d 7061 6374 6027 0a20 2020   `@compact`'.   
-0000eb60: 2020 2029 0a20 2020 2069 6620 7365 6c66     ).    if self
-0000eb70: 2e5f 6e61 6d65 5f74 616b 656e 286e 616d  ._name_taken(nam
-0000eb80: 652c 2063 6f6c 6c65 6374 696f 6e3d 2770  e, collection='p
-0000eb90: 6172 616d 7327 293a 0a20 2020 2020 2072  arams'):.      r
-0000eba0: 6169 7365 2065 7272 6f72 732e 4e61 6d65  aise errors.Name
-0000ebb0: 496e 5573 6545 7272 6f72 2827 7061 7261  InUseError('para
-0000ebc0: 6d27 2c20 6e61 6d65 2c20 7365 6c66 2e5f  m', name, self._
-0000ebd0: 5f63 6c61 7373 5f5f 2e5f 5f6e 616d 655f  _class__.__name_
-0000ebe0: 5f29 0a20 2020 2061 7373 6572 7420 7365  _).    assert se
-0000ebf0: 6c66 2e73 636f 7065 2069 7320 6e6f 7420  lf.scope is not 
-0000ec00: 4e6f 6e65 0a20 2020 2076 203d 2073 656c  None.    v = sel
-0000ec10: 662e 7363 6f70 652e 7061 7261 6d28 6e61  f.scope.param(na
-0000ec20: 6d65 2c20 696e 6974 5f66 6e2c 202a 696e  me, init_fn, *in
-0000ec30: 6974 5f61 7267 732c 2075 6e62 6f78 3d75  it_args, unbox=u
-0000ec40: 6e62 6f78 2c20 2a2a 696e 6974 5f6b 7761  nbox, **init_kwa
-0000ec50: 7267 7329 0a20 2020 2073 656c 662e 5f73  rgs).    self._s
-0000ec60: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
-0000ec70: 6d65 5d20 3d20 2770 6172 616d 7327 0a20  me] = 'params'. 
-0000ec80: 2020 2072 6574 7572 6e20 760a 0a20 2064     return v..  d
-0000ec90: 6566 2068 6173 5f76 6172 6961 626c 6528  ef has_variable(
-0000eca0: 7365 6c66 2c20 636f 6c3a 2073 7472 2c20  self, col: str, 
-0000ecb0: 6e61 6d65 3a20 7374 7229 202d 3e20 626f  name: str) -> bo
-0000ecc0: 6f6c 3a0a 2020 2020 2222 2243 6865 636b  ol:.    """Check
-0000ecd0: 7320 6966 2061 2076 6172 6961 626c 6520  s if a variable 
-0000ece0: 6f66 2067 6976 656e 2063 6f6c 6c65 6374  of given collect
-0000ecf0: 696f 6e20 616e 6420 6e61 6d65 2065 7869  ion and name exi
-0000ed00: 7374 7320 696e 2074 6869 7320 4d6f 6475  sts in this Modu
-0000ed10: 6c65 2e0a 0a20 2020 2053 6565 203a 6d6f  le...    See :mo
-0000ed20: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
-0000ed30: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
-0000ed40: 2065 7870 6c61 6e61 7469 6f6e 206f 6e20   explanation on 
-0000ed50: 7661 7269 6162 6c65 7320 616e 640a 2020  variables and.  
-0000ed60: 2020 636f 6c6c 6563 7469 6f6e 732e 0a0a    collections...
-0000ed70: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-0000ed80: 636f 6c3a 2054 6865 2076 6172 6961 626c  col: The variabl
-0000ed90: 6520 636f 6c6c 6563 7469 6f6e 206e 616d  e collection nam
-0000eda0: 652e 0a20 2020 2020 206e 616d 653a 2054  e..      name: T
-0000edb0: 6865 206e 616d 6520 6f66 2074 6865 2076  he name of the v
-0000edc0: 6172 6961 626c 652e 0a0a 2020 2020 5265  ariable...    Re
-0000edd0: 7475 726e 733a 0a20 2020 2020 2054 7275  turns:.      Tru
-0000ede0: 6520 6966 2074 6865 2076 6172 6961 626c  e if the variabl
-0000edf0: 6520 6578 6973 7473 2e0a 2020 2020 2222  e exists..    ""
-0000ee00: 220a 2020 2020 6966 2073 656c 662e 7363  ".    if self.sc
-0000ee10: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
-0000ee20: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-0000ee30: 726f 7228 2243 616e 2774 2061 6363 6573  ror("Can't acces
-0000ee40: 7320 7661 7269 6162 6c65 7320 6f6e 2075  s variables on u
-0000ee50: 6e62 6f75 6e64 206d 6f64 756c 6573 2229  nbound modules")
-0000ee60: 0a20 2020 2072 6574 7572 6e20 7365 6c66  .    return self
-0000ee70: 2e73 636f 7065 2e68 6173 5f76 6172 6961  .scope.has_varia
-0000ee80: 626c 6528 636f 6c2c 206e 616d 6529 0a0a  ble(col, name)..
-0000ee90: 2020 6465 6620 6973 5f6d 7574 6162 6c65    def is_mutable
-0000eea0: 5f63 6f6c 6c65 6374 696f 6e28 7365 6c66  _collection(self
-0000eeb0: 2c20 636f 6c3a 2073 7472 2920 2d3e 2062  , col: str) -> b
-0000eec0: 6f6f 6c3a 0a20 2020 2022 2222 5265 7475  ool:.    """Retu
-0000eed0: 726e 7320 7472 7565 2069 6620 7468 6520  rns true if the 
-0000eee0: 636f 6c6c 6563 7469 6f6e 2060 6063 6f6c  collection ``col
-0000eef0: 6060 2069 7320 6d75 7461 626c 652e 2222  `` is mutable.""
-0000ef00: 220a 2020 2020 6966 2073 656c 662e 7363  ".    if self.sc
-0000ef10: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
-0000ef20: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
-0000ef30: 726f 7228 2243 616e 2774 2063 6865 636b  ror("Can't check
-0000ef40: 206d 7574 6162 696c 6974 7920 6f6e 2075   mutability on u
-0000ef50: 6e62 6f75 6e64 206d 6f64 756c 6573 2229  nbound modules")
-0000ef60: 0a20 2020 2072 6574 7572 6e20 7365 6c66  .    return self
-0000ef70: 2e73 636f 7065 2e69 735f 6d75 7461 626c  .scope.is_mutabl
-0000ef80: 655f 636f 6c6c 6563 7469 6f6e 2863 6f6c  e_collection(col
-0000ef90: 290a 0a20 2064 6566 2068 6173 5f72 6e67  )..  def has_rng
-0000efa0: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-0000efb0: 2920 2d3e 2062 6f6f 6c3a 0a20 2020 2022  ) -> bool:.    "
-0000efc0: 2222 5265 7475 726e 7320 7472 7565 2069  ""Returns true i
-0000efd0: 6620 6120 5052 4e47 5365 7175 656e 6365  f a PRNGSequence
-0000efe0: 2077 6974 6820 6e61 6d65 2060 606e 616d   with name ``nam
-0000eff0: 6560 6020 6578 6973 7473 2e22 2222 0a20  e`` exists.""". 
-0000f000: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
-0000f010: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-0000f020: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-0000f030: 2822 4361 6e27 7420 7175 6572 7920 666f  ("Can't query fo
-0000f040: 7220 524e 4773 206f 6e20 756e 626f 756e  r RNGs on unboun
-0000f050: 6420 6d6f 6475 6c65 7322 290a 2020 2020  d modules").    
-0000f060: 7265 7475 726e 2073 656c 662e 7363 6f70  return self.scop
-0000f070: 652e 6861 735f 726e 6728 6e61 6d65 290a  e.has_rng(name).
-0000f080: 0a20 2064 6566 206d 616b 655f 726e 6728  .  def make_rng(
-0000f090: 7365 6c66 2c20 6e61 6d65 3a20 7374 7220  self, name: str 
-0000f0a0: 3d20 2770 6172 616d 7327 2920 2d3e 2050  = 'params') -> P
-0000f0b0: 524e 474b 6579 3a0a 2020 2020 2222 2252  RNGKey:.    """R
-0000f0c0: 6574 7572 6e73 2061 206e 6577 2052 4e47  eturns a new RNG
-0000f0d0: 206b 6579 2066 726f 6d20 6120 6769 7665   key from a give
-0000f0e0: 6e20 524e 4720 7365 7175 656e 6365 2066  n RNG sequence f
-0000f0f0: 6f72 2074 6869 7320 4d6f 6475 6c65 2e0a  or this Module..
-0000f100: 0a20 2020 2054 6865 206e 6577 2052 4e47  .    The new RNG
-0000f110: 206b 6579 2069 7320 7370 6c69 7420 6672   key is split fr
-0000f120: 6f6d 2074 6865 2070 7265 7669 6f75 7320  om the previous 
-0000f130: 6f6e 652e 2054 6875 732c 2065 7665 7279  one. Thus, every
-0000f140: 2063 616c 6c20 746f 0a20 2020 2060 606d   call to.    ``m
-0000f150: 616b 655f 726e 6760 6020 7265 7475 726e  ake_rng`` return
-0000f160: 7320 6120 6e65 7720 524e 4720 6b65 792c  s a new RNG key,
-0000f170: 2077 6869 6c65 2073 7469 6c6c 2067 7561   while still gua
-0000f180: 7261 6e74 6565 696e 6720 6675 6c6c 0a20  ranteeing full. 
-0000f190: 2020 2072 6570 726f 6475 6369 6269 6c69     reproducibili
-0000f1a0: 7479 2e0a 0a20 2020 202e 2e20 6e6f 7465  ty...    .. note
-0000f1b0: 3a3a 0a20 2020 2020 2049 6620 616e 2069  ::.      If an i
-0000f1c0: 6e76 616c 6964 206e 616d 6520 6973 2070  nvalid name is p
-0000f1d0: 6173 7365 6420 2869 2e65 2e20 6e6f 2052  assed (i.e. no R
-0000f1e0: 4e47 206b 6579 2077 6173 2070 6173 7365  NG key was passe
-0000f1f0: 6420 6279 0a20 2020 2020 2074 6865 2075  d by.      the u
-0000f200: 7365 7220 696e 2060 602e 696e 6974 6060  ser in ``.init``
-0000f210: 206f 7220 6060 2e61 7070 6c79 6060 2066   or ``.apply`` f
-0000f220: 6f72 2074 6869 7320 6e61 6d65 292c 2074  or this name), t
-0000f230: 6865 6e20 6060 6e61 6d65 6060 0a20 2020  hen ``name``.   
-0000f240: 2020 2077 696c 6c20 6465 6661 756c 7420     will default 
-0000f250: 746f 2060 6027 7061 7261 6d73 2760 602e  to ``'params'``.
-0000f260: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
-0000f270: 0a20 2020 2020 203e 3e3e 2069 6d70 6f72  .      >>> impor
-0000f280: 7420 6a61 780a 2020 2020 2020 3e3e 3e20  t jax.      >>> 
-0000f290: 696d 706f 7274 2066 6c61 782e 6c69 6e65  import flax.line
-0000f2a0: 6e20 6173 206e 6e0a 0a20 2020 2020 203e  n as nn..      >
-0000f2b0: 3e3e 2063 6c61 7373 2050 6172 616d 734d  >> class ParamsM
-0000f2c0: 6f64 756c 6528 6e6e 2e4d 6f64 756c 6529  odule(nn.Module)
-0000f2d0: 3a0a 2020 2020 2020 2e2e 2e20 2020 6465  :.      ...   de
-0000f2e0: 6620 5f5f 6361 6c6c 5f5f 2873 656c 6629  f __call__(self)
-0000f2f0: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
-0000f300: 7265 7475 726e 2073 656c 662e 6d61 6b65  return self.make
-0000f310: 5f72 6e67 2827 7061 7261 6d73 2729 0a20  _rng('params'). 
-0000f320: 2020 2020 203e 3e3e 2063 6c61 7373 204f       >>> class O
-0000f330: 7468 6572 4d6f 6475 6c65 286e 6e2e 4d6f  therModule(nn.Mo
-0000f340: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
-0000f350: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-0000f360: 7365 6c66 293a 0a20 2020 2020 202e 2e2e  self):.      ...
-0000f370: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-0000f380: 2e6d 616b 655f 726e 6728 276f 7468 6572  .make_rng('other
-0000f390: 2729 0a0a 2020 2020 2020 3e3e 3e20 6b65  ')..      >>> ke
-0000f3a0: 7920 3d20 6a61 782e 7261 6e64 6f6d 2e6b  y = jax.random.k
-0000f3b0: 6579 2830 290a 2020 2020 2020 3e3e 3e20  ey(0).      >>> 
-0000f3c0: 7061 7261 6d73 5f6f 7574 2c20 5f20 3d20  params_out, _ = 
-0000f3d0: 5061 7261 6d73 4d6f 6475 6c65 2829 2e69  ParamsModule().i
-0000f3e0: 6e69 745f 7769 7468 5f6f 7574 7075 7428  nit_with_output(
-0000f3f0: 7b27 7061 7261 6d73 273a 206b 6579 7d29  {'params': key})
-0000f400: 0a20 2020 2020 203e 3e3e 2023 2073 656c  .      >>> # sel
-0000f410: 662e 6d61 6b65 5f72 6e67 2827 6f74 6865  f.make_rng('othe
-0000f420: 7227 2920 7769 6c6c 2064 6566 6175 6c74  r') will default
-0000f430: 2074 6f20 7573 696e 6720 7468 6520 2770   to using the 'p
-0000f440: 6172 616d 7327 2052 4e47 2073 7472 6561  arams' RNG strea
-0000f450: 6d0a 2020 2020 2020 3e3e 3e20 6f74 6865  m.      >>> othe
-0000f460: 725f 6f75 742c 205f 203d 204f 7468 6572  r_out, _ = Other
-0000f470: 4d6f 6475 6c65 2829 2e69 6e69 745f 7769  Module().init_wi
-0000f480: 7468 5f6f 7574 7075 7428 7b27 7061 7261  th_output({'para
-0000f490: 6d73 273a 206b 6579 7d29 0a20 2020 2020  ms': key}).     
-0000f4a0: 203e 3e3e 2061 7373 6572 7420 7061 7261   >>> assert para
-0000f4b0: 6d73 5f6f 7574 203d 3d20 6f74 6865 725f  ms_out == other_
-0000f4c0: 6f75 740a 0a20 2020 204c 6561 726e 206d  out..    Learn m
-0000f4d0: 6f72 6520 6162 6f75 7420 524e 4727 7320  ore about RNG's 
-0000f4e0: 6279 2072 6561 6469 6e67 2074 6865 2046  by reading the F
-0000f4f0: 6c61 7820 524e 4720 6775 6964 653a 0a20  lax RNG guide:. 
-0000f500: 2020 2068 7474 7073 3a2f 2f66 6c61 782e     https://flax.
-0000f510: 7265 6164 7468 6564 6f63 732e 696f 2f65  readthedocs.io/e
-0000f520: 6e2f 6c61 7465 7374 2f67 7569 6465 732f  n/latest/guides/
-0000f530: 666c 6178 5f66 756e 6461 6d65 6e74 616c  flax_fundamental
-0000f540: 732f 726e 675f 6775 6964 652e 6874 6d6c  s/rng_guide.html
-0000f550: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
-0000f560: 2020 6e61 6d65 3a20 5468 6520 524e 4720    name: The RNG 
-0000f570: 7365 7175 656e 6365 206e 616d 652e 0a0a  sequence name...
-0000f580: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   
-0000f590: 2020 2054 6865 206e 6577 6c79 2067 656e     The newly gen
-0000f5a0: 6572 6174 6564 2052 4e47 206b 6579 2e0a  erated RNG key..
-0000f5b0: 2020 2020 2222 220a 2020 2020 6966 2073      """.    if s
-0000f5c0: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
-0000f5d0: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
-0000f5e0: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
-0000f5f0: 2075 7365 2052 4e47 7320 6f6e 2075 6e62   use RNGs on unb
-0000f600: 6f75 6e64 206d 6f64 756c 6573 2229 0a20  ound modules"). 
-0000f610: 2020 2072 6574 7572 6e20 7365 6c66 2e73     return self.s
-0000f620: 636f 7065 2e6d 616b 655f 726e 6728 6e61  cope.make_rng(na
-0000f630: 6d65 290a 0a20 2064 6566 2069 735f 696e  me)..  def is_in
-0000f640: 6974 6961 6c69 7a69 6e67 2873 656c 6629  itializing(self)
-0000f650: 202d 3e20 626f 6f6c 3a0a 2020 2020 2222   -> bool:.    ""
-0000f660: 2252 6574 7572 6e73 2054 7275 6520 6966  "Returns True if
-0000f670: 2072 756e 6e69 6e67 2075 6e64 6572 2073   running under s
-0000f680: 656c 662e 696e 6974 282e 2e2e 2920 6f72  elf.init(...) or
-0000f690: 206e 6e2e 696e 6974 282e 2e2e 2928 292e   nn.init(...)().
-0000f6a0: 0a0a 2020 2020 5468 6973 2069 7320 6120  ..    This is a 
-0000f6b0: 6865 6c70 6572 206d 6574 686f 6420 746f  helper method to
-0000f6c0: 2068 616e 646c 6520 7468 6520 636f 6d6d   handle the comm
-0000f6d0: 6f6e 2063 6173 6520 6f66 2073 696d 706c  on case of simpl
-0000f6e0: 6520 696e 6974 6961 6c69 7a61 7469 6f6e  e initialization
-0000f6f0: 0a20 2020 2077 6865 7265 2077 6520 7769  .    where we wi
-0000f700: 7368 2074 6f20 6861 7665 2073 6574 7570  sh to have setup
-0000f710: 206c 6f67 6963 206f 6363 7572 2077 6865   logic occur whe
-0000f720: 6e20 6f6e 6c79 2063 616c 6c65 6420 756e  n only called un
-0000f730: 6465 720a 2020 2020 6060 6d6f 6475 6c65  der.    ``module
-0000f740: 2e69 6e69 7460 6020 6f72 2060 606e 6e2e  .init`` or ``nn.
-0000f750: 696e 6974 6060 2e20 2046 6f72 206d 6f72  init``.  For mor
-0000f760: 6520 636f 6d70 6c69 6361 7465 6420 6d75  e complicated mu
-0000f770: 6c74 692d 7068 6173 650a 2020 2020 696e  lti-phase.    in
-0000f780: 6974 6961 6c69 7a61 7469 6f6e 2073 6365  itialization sce
-0000f790: 6e61 7269 6f73 2069 7420 6973 2062 6574  narios it is bet
-0000f7a0: 7465 7220 746f 2074 6573 7420 666f 7220  ter to test for 
-0000f7b0: 7468 6520 6d75 7461 6269 6c69 7479 206f  the mutability o
-0000f7c0: 660a 2020 2020 7061 7274 6963 756c 6172  f.    particular
-0000f7d0: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-0000f7e0: 7469 6f6e 7320 6f72 2066 6f72 2074 6865  tions or for the
-0000f7f0: 2070 7265 7365 6e63 6520 6f66 2070 6172   presence of par
-0000f800: 7469 6375 6c61 720a 2020 2020 7661 7269  ticular.    vari
-0000f810: 6162 6c65 7320 7468 6174 2070 6f74 656e  ables that poten
-0000f820: 7469 616c 6c79 206e 6565 6420 746f 2062  tially need to b
-0000f830: 6520 696e 6974 6961 6c69 7a65 642e 0a20  e initialized.. 
-0000f840: 2020 2022 2222 0a20 2020 2069 6620 7365     """.    if se
-0000f850: 6c66 2e73 636f 7065 2069 7320 4e6f 6e65  lf.scope is None
-0000f860: 3a0a 2020 2020 2020 7261 6973 6520 5661  :.      raise Va
-0000f870: 6c75 6545 7272 6f72 2822 4361 6e27 7420  lueError("Can't 
-0000f880: 6368 6563 6b20 6966 2072 756e 6e69 6e67  check if running
-0000f890: 2075 6e64 6572 2069 6e69 7428 2920 6f6e   under init() on
-0000f8a0: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-0000f8b0: 2229 0a20 2020 2072 6574 7572 6e20 7365  ").    return se
-0000f8c0: 6c66 2e73 636f 7065 2e67 6574 5f66 6c61  lf.scope.get_fla
-0000f8d0: 6728 2769 6e69 7469 616c 697a 696e 6727  g('initializing'
-0000f8e0: 2c20 4661 6c73 6529 0a0a 2020 6465 6620  , False)..  def 
-0000f8f0: 5f6d 6f64 756c 655f 6368 6563 6b73 2873  _module_checks(s
-0000f900: 656c 6629 3a0a 2020 2020 2222 2252 756e  elf):.    """Run
-0000f910: 2073 7461 6e64 6172 6420 7275 6e74 696d   standard runtim
-0000f920: 6520 6368 6563 6b73 2e22 2222 0a0a 2020  e checks."""..  
-0000f930: 2020 6966 206e 6f74 2069 7369 6e73 7461    if not isinsta
-0000f940: 6e63 6528 7365 6c66 2c20 4d6f 6475 6c65  nce(self, Module
-0000f950: 293a 0a20 2020 2020 2072 6169 7365 2065  ):.      raise e
-0000f960: 7272 6f72 732e 496e 7661 6c69 6449 6e73  rrors.InvalidIns
-0000f970: 7461 6e63 654d 6f64 756c 6545 7272 6f72  tanceModuleError
-0000f980: 2829 0a0a 2020 2020 6f76 6572 7269 6464  ()..    overridd
-0000f990: 656e 5f70 6f73 745f 696e 6974 203d 2073  en_post_init = s
-0000f9a0: 656c 662e 5f5f 706f 7374 5f69 6e69 745f  elf.__post_init_
-0000f9b0: 5f20 213d 204d 6f64 756c 652e 5f5f 706f  _ != Module.__po
-0000f9c0: 7374 5f69 6e69 745f 5f0a 2020 2020 6966  st_init__.    if
-0000f9d0: 206f 7665 7272 6964 6465 6e5f 706f 7374   overridden_post
-0000f9e0: 5f69 6e69 7420 616e 6420 6e6f 7420 6861  _init and not ha
-0000f9f0: 7361 7474 7228 7365 6c66 2c20 275f 6964  sattr(self, '_id
-0000fa00: 2729 3a0a 2020 2020 2020 7261 6973 6520  '):.      raise 
-0000fa10: 6572 726f 7273 2e49 6e63 6f72 7265 6374  errors.Incorrect
-0000fa20: 506f 7374 496e 6974 4f76 6572 7269 6465  PostInitOverride
-0000fa30: 4572 726f 7228 290a 0a20 2040 7472 6163  Error()..  @trac
-0000fa40: 6562 6163 6b5f 7574 696c 2e61 7069 5f62  eback_util.api_b
-0000fa50: 6f75 6e64 6172 790a 2020 6465 6620 6269  oundary.  def bi
-0000fa60: 6e64 280a 2020 2020 7365 6c66 3a20 4d2c  nd(.    self: M,
-0000fa70: 0a20 2020 2076 6172 6961 626c 6573 3a20  .    variables: 
-0000fa80: 5661 7269 6162 6c65 4469 6374 2c0a 2020  VariableDict,.  
-0000fa90: 2020 2a61 7267 732c 0a20 2020 2072 6e67    *args,.    rng
-0000faa0: 733a 204f 7074 696f 6e61 6c5b 524e 4753  s: Optional[RNGS
-0000fab0: 6571 7565 6e63 6573 5d20 3d20 4e6f 6e65  equences] = None
-0000fac0: 2c0a 2020 2020 6d75 7461 626c 653a 2043  ,.    mutable: C
-0000fad0: 6f6c 6c65 6374 696f 6e46 696c 7465 7220  ollectionFilter 
-0000fae0: 3d20 4661 6c73 652c 0a20 2029 202d 3e20  = False,.  ) -> 
-0000faf0: 4d3a 0a20 2020 2022 2222 4372 6561 7465  M:.    """Create
-0000fb00: 7320 616e 2069 6e74 6572 6163 7469 7665  s an interactive
-0000fb10: 204d 6f64 756c 6520 696e 7374 616e 6365   Module instance
-0000fb20: 2062 7920 6269 6e64 696e 6720 7661 7269   by binding vari
-0000fb30: 6162 6c65 7320 616e 6420 524e 4773 2e0a  ables and RNGs..
-0000fb40: 0a20 2020 2060 6062 696e 6460 6020 7072  .    ``bind`` pr
-0000fb50: 6f76 6964 6573 2061 6e20 2269 6e74 6572  ovides an "inter
-0000fb60: 6163 7469 7665 2220 696e 7374 616e 6365  active" instance
-0000fb70: 206f 6620 6120 4d6f 6475 6c65 2064 6972   of a Module dir
-0000fb80: 6563 746c 7920 7769 7468 6f75 740a 2020  ectly without.  
-0000fb90: 2020 7472 616e 7366 6f72 6d69 6e67 2061    transforming a
-0000fba0: 2066 756e 6374 696f 6e20 7769 7468 2060   function with `
-0000fbb0: 6061 7070 6c79 6060 2e20 5468 6973 2069  `apply``. This i
-0000fbc0: 7320 7061 7274 6963 756c 6172 6c79 2075  s particularly u
-0000fbd0: 7365 6675 6c20 666f 720a 2020 2020 6465  seful for.    de
-0000fbe0: 6275 6767 696e 6720 616e 6420 696e 7465  bugging and inte
-0000fbf0: 7261 6374 6976 6520 7573 6520 6361 7365  ractive use case
-0000fc00: 7320 6c69 6b65 206e 6f74 6562 6f6f 6b73  s like notebooks
-0000fc10: 2077 6865 7265 2061 2066 756e 6374 696f   where a functio
-0000fc20: 6e20 776f 756c 640a 2020 2020 6c69 6d69  n would.    limi
-0000fc30: 7420 7468 6520 6162 696c 6974 7920 746f  t the ability to
-0000fc40: 2073 706c 6974 2075 7020 636f 6465 2069   split up code i
-0000fc50: 6e74 6f20 6469 6666 6572 656e 7420 6365  nto different ce
-0000fc60: 6c6c 732e 0a0a 2020 2020 4f6e 6365 2074  lls...    Once t
-0000fc70: 6865 2076 6172 6961 626c 6573 2028 616e  he variables (an
-0000fc80: 6420 6f70 7469 6f6e 616c 6c79 2052 4e47  d optionally RNG
-0000fc90: 7329 2061 7265 2062 6f75 6e64 2074 6f20  s) are bound to 
-0000fca0: 6120 6060 4d6f 6475 6c65 6060 2069 740a  a ``Module`` it.
-0000fcb0: 2020 2020 6265 636f 6d65 7320 6120 7374      becomes a st
-0000fcc0: 6174 6566 756c 206f 626a 6563 742e 204e  ateful object. N
-0000fcd0: 6f74 6520 7468 6174 2069 6469 6f6d 6174  ote that idiomat
-0000fce0: 6963 204a 4158 2069 7320 6675 6e63 7469  ic JAX is functi
-0000fcf0: 6f6e 616c 2061 6e64 0a20 2020 2074 6865  onal and.    the
-0000fd00: 7265 666f 7265 2061 6e20 696e 7465 7261  refore an intera
-0000fd10: 6374 6976 6520 696e 7374 616e 6365 2064  ctive instance d
-0000fd20: 6f65 7320 6e6f 7420 6d69 7820 7765 6c6c  oes not mix well
-0000fd30: 2077 6974 6820 7661 6e69 6c6c 6120 4a41   with vanilla JA
-0000fd40: 5820 4150 4973 2e0a 2020 2020 6060 6269  X APIs..    ``bi
-0000fd50: 6e64 2829 6060 2073 686f 756c 6420 6f6e  nd()`` should on
-0000fd60: 6c79 2062 6520 7573 6564 2066 6f72 2069  ly be used for i
-0000fd70: 6e74 6572 6163 7469 7665 2065 7870 6572  nteractive exper
-0000fd80: 696d 656e 7461 7469 6f6e 2c20 616e 6420  imentation, and 
-0000fd90: 696e 2061 6c6c 0a20 2020 206f 7468 6572  in all.    other
-0000fda0: 2063 6173 6573 2077 6520 7374 726f 6e67   cases we strong
-0000fdb0: 6c79 2065 6e63 6f75 7261 6765 2075 7365  ly encourage use
-0000fdc0: 7273 2074 6f20 7573 6520 6060 6170 706c  rs to use ``appl
-0000fdd0: 7928 2960 6020 696e 7374 6561 642e 0a0a  y()`` instead...
-0000fde0: 2020 2020 4578 616d 706c 653a 3a0a 0a20      Example::.. 
-0000fdf0: 2020 2020 203e 3e3e 2069 6d70 6f72 7420       >>> import 
-0000fe00: 6a61 780a 2020 2020 2020 3e3e 3e20 696d  jax.      >>> im
-0000fe10: 706f 7274 206a 6178 2e6e 756d 7079 2061  port jax.numpy a
-0000fe20: 7320 6a6e 700a 2020 2020 2020 3e3e 3e20  s jnp.      >>> 
-0000fe30: 696d 706f 7274 2066 6c61 782e 6c69 6e65  import flax.line
-0000fe40: 6e20 6173 206e 6e0a 0a20 2020 2020 203e  n as nn..      >
-0000fe50: 3e3e 2063 6c61 7373 2041 7574 6f45 6e63  >> class AutoEnc
-0000fe60: 6f64 6572 286e 6e2e 4d6f 6475 6c65 293a  oder(nn.Module):
-0000fe70: 0a20 2020 2020 202e 2e2e 2020 2064 6566  .      ...   def
-0000fe80: 2073 6574 7570 2873 656c 6629 3a0a 2020   setup(self):.  
-0000fe90: 2020 2020 2e2e 2e20 2020 2020 7365 6c66      ...     self
-0000fea0: 2e65 6e63 6f64 6572 203d 206e 6e2e 4465  .encoder = nn.De
-0000feb0: 6e73 6528 3329 0a20 2020 2020 202e 2e2e  nse(3).      ...
-0000fec0: 2020 2020 2073 656c 662e 6465 636f 6465       self.decode
-0000fed0: 7220 3d20 6e6e 2e44 656e 7365 2835 290a  r = nn.Dense(5).
-0000fee0: 2020 2020 2020 2e2e 2e0a 2020 2020 2020        ....      
-0000fef0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
-0000ff00: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
-0000ff10: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
-0000ff20: 6e20 7365 6c66 2e64 6563 6f64 6572 2873  n self.decoder(s
-0000ff30: 656c 662e 656e 636f 6465 7228 7829 290a  elf.encoder(x)).
-0000ff40: 0a20 2020 2020 203e 3e3e 2078 203d 206a  .      >>> x = j
-0000ff50: 6e70 2e6f 6e65 7328 2831 362c 2039 2929  np.ones((16, 9))
-0000ff60: 0a20 2020 2020 203e 3e3e 2061 6520 3d20  .      >>> ae = 
-0000ff70: 4175 746f 456e 636f 6465 7228 290a 2020  AutoEncoder().  
-0000ff80: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
-0000ff90: 7320 3d20 6165 2e69 6e69 7428 6a61 782e  s = ae.init(jax.
-0000ffa0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2078  random.key(0), x
-0000ffb0: 290a 2020 2020 2020 3e3e 3e20 6d6f 6465  ).      >>> mode
-0000ffc0: 6c20 3d20 6165 2e62 696e 6428 7661 7269  l = ae.bind(vari
-0000ffd0: 6162 6c65 7329 0a20 2020 2020 203e 3e3e  ables).      >>>
-0000ffe0: 207a 203d 206d 6f64 656c 2e65 6e63 6f64   z = model.encod
-0000fff0: 6572 2878 290a 2020 2020 2020 3e3e 3e20  er(x).      >>> 
-00010000: 785f 7265 636f 6e73 7472 7563 7465 6420  x_reconstructed 
-00010010: 3d20 6d6f 6465 6c2e 6465 636f 6465 7228  = model.decoder(
-00010020: 7a29 0a0a 2020 2020 4172 6773 3a0a 2020  z)..    Args:.  
-00010030: 2020 2020 7661 7269 6162 6c65 733a 2041      variables: A
-00010040: 2064 6963 7469 6f6e 6172 7920 636f 6e74   dictionary cont
-00010050: 6169 6e69 6e67 2076 6172 6961 626c 6573  aining variables
-00010060: 206b 6579 6564 2062 7920 7661 7269 6162   keyed by variab
-00010070: 6c65 0a20 2020 2020 2020 2063 6f6c 6c65  le.        colle
-00010080: 6374 696f 6e73 2e20 5365 6520 3a6d 6f64  ctions. See :mod
-00010090: 3a60 666c 6178 2e63 6f72 652e 7661 7269  :`flax.core.vari
-000100a0: 6162 6c65 7360 2066 6f72 206d 6f72 6520  ables` for more 
-000100b0: 6465 7461 696c 7320 6162 6f75 740a 2020  details about.  
-000100c0: 2020 2020 2020 7661 7269 6162 6c65 732e        variables.
-000100d0: 0a20 2020 2020 202a 6172 6773 3a20 4e61  .      *args: Na
-000100e0: 6d65 6420 6172 6775 6d65 6e74 7320 286e  med arguments (n
-000100f0: 6f74 2075 7365 6429 2e0a 2020 2020 2020  ot used)..      
-00010100: 726e 6773 3a20 6120 6469 6374 206f 6620  rngs: a dict of 
-00010110: 5052 4e47 4b65 7973 2074 6f20 696e 6974  PRNGKeys to init
-00010120: 6961 6c69 7a65 2074 6865 2050 524e 4720  ialize the PRNG 
-00010130: 7365 7175 656e 6365 732e 0a20 2020 2020  sequences..     
-00010140: 206d 7574 6162 6c65 3a20 4361 6e20 6265   mutable: Can be
-00010150: 2062 6f6f 6c2c 2073 7472 2c20 6f72 206c   bool, str, or l
-00010160: 6973 742e 2053 7065 6369 6669 6573 2077  ist. Specifies w
-00010170: 6869 6368 2063 6f6c 6c65 6374 696f 6e73  hich collections
-00010180: 2073 686f 756c 6420 6265 0a20 2020 2020   should be.     
-00010190: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
-000101a0: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
-000101b0: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
-000101c0: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
-000101d0: 2060 6073 7472 6060 3a0a 2020 2020 2020   ``str``:.      
-000101e0: 2020 5468 6520 6e61 6d65 206f 6620 6120    The name of a 
-000101f0: 7369 6e67 6c65 206d 7574 6162 6c65 2063  single mutable c
-00010200: 6f6c 6c65 6374 696f 6e2e 2060 606c 6973  ollection. ``lis
-00010210: 7460 603a 2041 206c 6973 7420 6f66 206e  t``: A list of n
-00010220: 616d 6573 206f 660a 2020 2020 2020 2020  ames of.        
-00010230: 6d75 7461 626c 6520 636f 6c6c 6563 7469  mutable collecti
-00010240: 6f6e 732e 0a0a 2020 2020 5265 7475 726e  ons...    Return
-00010250: 733a 0a20 2020 2020 2041 2063 6f70 7920  s:.      A copy 
-00010260: 6f66 2074 6869 7320 696e 7374 616e 6365  of this instance
-00010270: 2077 6974 6820 626f 756e 6420 7661 7269   with bound vari
-00010280: 6162 6c65 7320 616e 6420 524e 4773 2e0a  ables and RNGs..
-00010290: 2020 2020 2222 220a 2020 2020 4d6f 6475      """.    Modu
-000102a0: 6c65 2e5f 6d6f 6475 6c65 5f63 6865 636b  le._module_check
-000102b0: 7328 7365 6c66 290a 0a20 2020 2064 656c  s(self)..    del
-000102c0: 2061 7267 730a 2020 2020 7363 6f70 6520   args.    scope 
-000102d0: 3d20 636f 7265 2e62 696e 6428 7661 7269  = core.bind(vari
-000102e0: 6162 6c65 732c 2072 6e67 733d 726e 6773  ables, rngs=rngs
-000102f0: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
-00010300: 6529 0a20 2020 2072 6574 7572 6e20 7365  e).    return se
-00010310: 6c66 2e63 6c6f 6e65 2870 6172 656e 743d  lf.clone(parent=
-00010320: 7363 6f70 652c 205f 6465 6570 5f63 6c6f  scope, _deep_clo
-00010330: 6e65 3d54 7275 6529 0a0a 2020 6465 6620  ne=True)..  def 
-00010340: 756e 6269 6e64 2873 656c 663a 204d 2920  unbind(self: M) 
-00010350: 2d3e 2054 7570 6c65 5b4d 2c20 5661 7269  -> Tuple[M, Vari
-00010360: 6162 6c65 4469 6374 5d3a 0a20 2020 2022  ableDict]:.    "
-00010370: 2222 5265 7475 726e 7320 616e 2075 6e62  ""Returns an unb
-00010380: 6f75 6e64 2063 6f70 7920 6f66 2061 204d  ound copy of a M
-00010390: 6f64 756c 6520 616e 6420 6974 7320 7661  odule and its va
-000103a0: 7269 6162 6c65 732e 0a0a 2020 2020 6060  riables...    ``
-000103b0: 756e 6269 6e64 6060 2068 656c 7073 2063  unbind`` helps c
-000103c0: 7265 6174 6520 6120 7374 6174 656c 6573  reate a stateles
-000103d0: 7320 7665 7273 696f 6e20 6f66 2061 2062  s version of a b
-000103e0: 6f75 6e64 204d 6f64 756c 652e 0a0a 2020  ound Module...  
-000103f0: 2020 416e 2065 7861 6d70 6c65 206f 6620    An example of 
-00010400: 6120 636f 6d6d 6f6e 2075 7365 2063 6173  a common use cas
-00010410: 653a 2074 6f20 6578 7472 6163 7420 6120  e: to extract a 
-00010420: 7375 622d 4d6f 6475 6c65 2064 6566 696e  sub-Module defin
-00010430: 6564 2069 6e73 6964 650a 2020 2020 6060  ed inside.    ``
-00010440: 7365 7475 7028 2960 6020 616e 6420 6974  setup()`` and it
-00010450: 7320 636f 7272 6573 706f 6e64 696e 6720  s corresponding 
-00010460: 7661 7269 6162 6c65 733a 2031 2920 7465  variables: 1) te
-00010470: 6d70 6f72 6172 696c 7920 6060 6269 6e64  mporarily ``bind
-00010480: 6060 2074 6865 0a20 2020 2070 6172 656e  `` the.    paren
-00010490: 7420 4d6f 6475 6c65 3b20 616e 6420 7468  t Module; and th
-000104a0: 656e 2032 2920 6060 756e 6269 6e64 6060  en 2) ``unbind``
-000104b0: 2074 6865 2064 6573 6972 6564 2073 7562   the desired sub
-000104c0: 2d4d 6f64 756c 652e 2028 5265 6361 6c6c  -Module. (Recall
-000104d0: 2074 6861 740a 2020 2020 6060 7365 7475   that.    ``setu
-000104e0: 7028 2960 6020 6973 206f 6e6c 7920 6361  p()`` is only ca
-000104f0: 6c6c 6564 2077 6865 6e20 7468 6520 4d6f  lled when the Mo
-00010500: 6475 6c65 2069 7320 626f 756e 642e 293a  dule is bound.):
-00010510: 3a0a 0a20 2020 2020 203e 3e3e 2063 6c61  :..      >>> cla
-00010520: 7373 2045 6e63 6f64 6572 286e 6e2e 4d6f  ss Encoder(nn.Mo
-00010530: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
-00010540: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
-00010550: 2020 2020 202e 2e2e 2020 2064 6566 205f       ...   def _
-00010560: 5f63 616c 6c5f 5f28 7365 6c66 2c20 7829  _call__(self, x)
-00010570: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
-00010580: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
-00010590: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
-000105a0: 6528 3235 3629 2878 290a 0a20 2020 2020  e(256)(x)..     
-000105b0: 203e 3e3e 2063 6c61 7373 2044 6563 6f64   >>> class Decod
-000105c0: 6572 286e 6e2e 4d6f 6475 6c65 293a 0a20  er(nn.Module):. 
-000105d0: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
-000105e0: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
-000105f0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-00010600: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
-00010610: 2e2e 2e20 2020 2020 2e2e 2e0a 2020 2020  ...     ....    
-00010620: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
-00010630: 206e 6e2e 4465 6e73 6528 3738 3429 2878   nn.Dense(784)(x
-00010640: 290a 0a20 2020 2020 203e 3e3e 2063 6c61  )..      >>> cla
-00010650: 7373 2041 7574 6f45 6e63 6f64 6572 286e  ss AutoEncoder(n
-00010660: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
-00010670: 202e 2e2e 2020 2064 6566 2073 6574 7570   ...   def setup
-00010680: 2873 656c 6629 3a0a 2020 2020 2020 2e2e  (self):.      ..
-00010690: 2e20 2020 2020 7365 6c66 2e65 6e63 6f64  .     self.encod
-000106a0: 6572 203d 2045 6e63 6f64 6572 2829 0a20  er = Encoder(). 
-000106b0: 2020 2020 202e 2e2e 2020 2020 2073 656c       ...     sel
-000106c0: 662e 6465 636f 6465 7220 3d20 4465 636f  f.decoder = Deco
-000106d0: 6465 7228 290a 2020 2020 2020 2e2e 2e0a  der().      ....
-000106e0: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
-000106f0: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
-00010700: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
-00010710: 2072 6574 7572 6e20 7365 6c66 2e64 6563   return self.dec
-00010720: 6f64 6572 2873 656c 662e 656e 636f 6465  oder(self.encode
-00010730: 7228 7829 290a 0a20 2020 2020 203e 3e3e  r(x))..      >>>
-00010740: 206d 6f64 756c 6520 3d20 4175 746f 456e   module = AutoEn
-00010750: 636f 6465 7228 290a 2020 2020 2020 3e3e  coder().      >>
-00010760: 3e20 7661 7269 6162 6c65 7320 3d20 6d6f  > variables = mo
-00010770: 6475 6c65 2e69 6e69 7428 6a61 782e 7261  dule.init(jax.ra
-00010780: 6e64 6f6d 2e6b 6579 2830 292c 206a 6e70  ndom.key(0), jnp
-00010790: 2e6f 6e65 7328 2831 2c20 3738 3429 2929  .ones((1, 784)))
-000107a0: 0a0a 2020 2020 2020 3e3e 3e20 2320 4578  ..      >>> # Ex
-000107b0: 7472 6163 7420 7468 6520 456e 636f 6465  tract the Encode
-000107c0: 7220 7375 622d 4d6f 6475 6c65 2061 6e64  r sub-Module and
-000107d0: 2069 7473 2076 6172 6961 626c 6573 0a20   its variables. 
-000107e0: 2020 2020 203e 3e3e 2065 6e63 6f64 6572       >>> encoder
-000107f0: 2c20 656e 636f 6465 725f 7661 7273 203d  , encoder_vars =
-00010800: 206d 6f64 756c 652e 6269 6e64 2876 6172   module.bind(var
-00010810: 6961 626c 6573 292e 656e 636f 6465 722e  iables).encoder.
-00010820: 756e 6269 6e64 2829 0a0a 2020 2020 5265  unbind()..    Re
-00010830: 7475 726e 733a 0a20 2020 2020 2041 2074  turns:.      A t
-00010840: 7570 6c65 2077 6974 6820 616e 2075 6e62  uple with an unb
-00010850: 6f75 6e64 2063 6f70 7920 6f66 2074 6869  ound copy of thi
-00010860: 7320 4d6f 6475 6c65 2061 6e64 2069 7473  s Module and its
-00010870: 2076 6172 6961 626c 6573 2e0a 2020 2020   variables..    
-00010880: 2222 220a 2020 2020 4d6f 6475 6c65 2e5f  """.    Module._
-00010890: 6d6f 6475 6c65 5f63 6865 636b 7328 7365  module_checks(se
-000108a0: 6c66 290a 0a20 2020 2069 6620 7365 6c66  lf)..    if self
-000108b0: 2e73 636f 7065 2069 7320 4e6f 6e65 3a0a  .scope is None:.
-000108c0: 2020 2020 2020 7261 6973 6520 6572 726f        raise erro
-000108d0: 7273 2e43 616c 6c55 6e62 696e 644f 6e55  rs.CallUnbindOnU
-000108e0: 6e62 6f75 6e64 4d6f 6475 6c65 4572 726f  nboundModuleErro
-000108f0: 7228 290a 0a20 2020 2076 6172 6961 626c  r()..    variabl
-00010900: 6573 203d 2073 656c 662e 7661 7269 6162  es = self.variab
-00010910: 6c65 730a 2020 2020 6d6f 6475 6c65 203d  les.    module =
-00010920: 2073 656c 662e 636c 6f6e 6528 5f64 6565   self.clone(_dee
-00010930: 705f 636c 6f6e 653d 5472 7565 2c20 5f72  p_clone=True, _r
-00010940: 6573 6574 5f6e 616d 6573 3d54 7275 652c  eset_names=True,
-00010950: 206e 616d 653d 4e6f 6e65 290a 2020 2020   name=None).    
-00010960: 7265 7475 726e 206d 6f64 756c 652c 2076  return module, v
-00010970: 6172 6961 626c 6573 0a0a 2020 4074 7261  ariables..  @tra
-00010980: 6365 6261 636b 5f75 7469 6c2e 6170 695f  ceback_util.api_
-00010990: 626f 756e 6461 7279 0a20 2064 6566 2061  boundary.  def a
-000109a0: 7070 6c79 280a 2020 2020 7365 6c66 2c0a  pply(.    self,.
-000109b0: 2020 2020 7661 7269 6162 6c65 733a 2056      variables: V
-000109c0: 6172 6961 626c 6544 6963 742c 0a20 2020  ariableDict,.   
-000109d0: 202a 6172 6773 2c0a 2020 2020 726e 6773   *args,.    rngs
-000109e0: 3a20 4f70 7469 6f6e 616c 5b55 6e69 6f6e  : Optional[Union
-000109f0: 5b50 524e 474b 6579 2c20 524e 4753 6571  [PRNGKey, RNGSeq
-00010a00: 7565 6e63 6573 5d5d 203d 204e 6f6e 652c  uences]] = None,
-00010a10: 0a20 2020 206d 6574 686f 643a 2055 6e69  .    method: Uni
-00010a20: 6f6e 5b43 616c 6c61 626c 655b 2e2e 2e2c  on[Callable[...,
-00010a30: 2041 6e79 5d2c 2073 7472 2c20 4e6f 6e65   Any], str, None
-00010a40: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6d75  ] = None,.    mu
-00010a50: 7461 626c 653a 2043 6f6c 6c65 6374 696f  table: Collectio
-00010a60: 6e46 696c 7465 7220 3d20 4661 6c73 652c  nFilter = False,
-00010a70: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
-00010a80: 6572 6d65 6469 6174 6573 3a20 556e 696f  ermediates: Unio
-00010a90: 6e5b 626f 6f6c 2c20 4361 6c6c 6162 6c65  n[bool, Callable
-00010aa0: 5b5b 274d 6f64 756c 6527 2c20 7374 725d  [['Module', str]
-00010ab0: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
-00010ac0: 2c0a 2020 2020 2a2a 6b77 6172 6773 2c0a  ,.    **kwargs,.
-00010ad0: 2020 2920 2d3e 2055 6e69 6f6e 5b41 6e79    ) -> Union[Any
-00010ae0: 2c20 5475 706c 655b 416e 792c 2055 6e69  , Tuple[Any, Uni
-00010af0: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
-00010b00: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
-00010b10: 2041 6e79 5d5d 5d5d 3a0a 2020 2020 2222   Any]]]]:.    ""
-00010b20: 2241 7070 6c69 6573 2061 206d 6f64 756c  "Applies a modul
-00010b30: 6520 6d65 7468 6f64 2074 6f20 7661 7269  e method to vari
-00010b40: 6162 6c65 7320 616e 6420 7265 7475 726e  ables and return
-00010b50: 7320 6f75 7470 7574 2061 6e64 206d 6f64  s output and mod
-00010b60: 6966 6965 6420 7661 7269 6162 6c65 732e  ified variables.
-00010b70: 0a0a 2020 2020 4e6f 7465 2074 6861 7420  ..    Note that 
-00010b80: 6060 6d65 7468 6f64 6060 2073 686f 756c  ``method`` shoul
-00010b90: 6420 6265 2073 6574 2069 6620 6f6e 6520  d be set if one 
-00010ba0: 776f 756c 6420 6c69 6b65 2074 6f20 6361  would like to ca
-00010bb0: 6c6c 2060 6061 7070 6c79 6060 206f 6e20  ll ``apply`` on 
-00010bc0: 610a 2020 2020 6469 6666 6572 656e 7420  a.    different 
-00010bd0: 636c 6173 7320 6d65 7468 6f64 2074 6861  class method tha
-00010be0: 6e20 6060 5f5f 6361 6c6c 5f5f 6060 2e20  n ``__call__``. 
-00010bf0: 466f 7220 696e 7374 616e 6365 2c20 7375  For instance, su
-00010c00: 7070 6f73 6520 610a 2020 2020 5472 616e  ppose a.    Tran
-00010c10: 7366 6f72 6d65 7220 6d6f 6475 6c65 7320  sformer modules 
-00010c20: 6861 7320 6120 6d65 7468 6f64 2063 616c  has a method cal
-00010c30: 6c65 6420 6060 656e 636f 6465 6060 2c20  led ``encode``, 
-00010c40: 7468 656e 2074 6865 2066 6f6c 6c6f 7769  then the followi
-00010c50: 6e67 2063 616c 6c73 0a20 2020 2060 6061  ng calls.    ``a
-00010c60: 7070 6c79 6060 206f 6e20 7468 6174 206d  pply`` on that m
-00010c70: 6574 686f 643a 3a0a 0a20 2020 2020 203e  ethod::..      >
-00010c80: 3e3e 2069 6d70 6f72 7420 666c 6178 2e6c  >> import flax.l
-00010c90: 696e 656e 2061 7320 6e6e 0a20 2020 2020  inen as nn.     
-00010ca0: 203e 3e3e 2069 6d70 6f72 7420 6a61 782c   >>> import jax,
-00010cb0: 206a 6178 2e6e 756d 7079 2061 7320 6a6e   jax.numpy as jn
-00010cc0: 700a 2020 2020 2020 3e3e 3e20 696d 706f  p.      >>> impo
-00010cd0: 7274 206e 756d 7079 2061 7320 6e70 0a0a  rt numpy as np..
-00010ce0: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-00010cf0: 5472 616e 7366 6f72 6d65 7228 6e6e 2e4d  Transformer(nn.M
-00010d00: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
-00010d10: 2e20 2020 6465 6620 656e 636f 6465 2873  .   def encode(s
-00010d20: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
-00010d30: 2e2e 2020 2020 202e 2e2e 0a0a 2020 2020  ..     .....    
-00010d40: 2020 3e3e 3e20 7820 3d20 6a6e 702e 6f6e    >>> x = jnp.on
-00010d50: 6573 2828 3136 2c20 3929 290a 2020 2020  es((16, 9)).    
-00010d60: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 5472    >>> model = Tr
-00010d70: 616e 7366 6f72 6d65 7228 290a 2020 2020  ansformer().    
-00010d80: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
-00010d90: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
-00010da0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
-00010db0: 782c 206d 6574 686f 643d 5472 616e 7366  x, method=Transf
-00010dc0: 6f72 6d65 722e 656e 636f 6465 290a 0a20  ormer.encode).. 
-00010dd0: 2020 2020 203e 3e3e 2065 6e63 6f64 6564       >>> encoded
-00010de0: 203d 206d 6f64 656c 2e61 7070 6c79 2876   = model.apply(v
-00010df0: 6172 6961 626c 6573 2c20 782c 206d 6574  ariables, x, met
-00010e00: 686f 643d 5472 616e 7366 6f72 6d65 722e  hod=Transformer.
-00010e10: 656e 636f 6465 290a 0a20 2020 2049 6620  encode)..    If 
-00010e20: 6120 6675 6e63 7469 6f6e 2069 6e73 7461  a function insta
-00010e30: 6e63 6520 6973 2070 726f 7669 6465 642c  nce is provided,
-00010e40: 2074 6865 2075 6e62 6f75 6e64 2066 756e   the unbound fun
-00010e50: 6374 696f 6e20 6973 2075 7365 642e 2046  ction is used. F
-00010e60: 6f72 0a20 2020 2069 6e73 7461 6e63 652c  or.    instance,
-00010e70: 2074 6865 2065 7861 6d70 6c65 2062 656c   the example bel
-00010e80: 6f77 2069 7320 6571 7569 7661 6c65 6e74  ow is equivalent
-00010e90: 2074 6f20 7468 6520 6f6e 6520 6162 6f76   to the one abov
-00010ea0: 653a 3a0a 0a20 2020 2020 203e 3e3e 2065  e::..      >>> e
-00010eb0: 6e63 6f64 6564 203d 206d 6f64 656c 2e61  ncoded = model.a
-00010ec0: 7070 6c79 2876 6172 6961 626c 6573 2c20  pply(variables, 
-00010ed0: 782c 206d 6574 686f 643d 6d6f 6465 6c2e  x, method=model.
-00010ee0: 656e 636f 6465 290a 0a20 2020 2059 6f75  encode)..    You
-00010ef0: 2063 616e 2061 6c73 6f20 7061 7373 2061   can also pass a
-00010f00: 2073 7472 696e 6720 746f 2061 2063 616c   string to a cal
-00010f10: 6c61 626c 6520 6174 7472 6962 7574 6520  lable attribute 
-00010f20: 6f66 2074 6865 206d 6f64 756c 652e 2046  of the module. F
-00010f30: 6f72 0a20 2020 2065 7861 6d70 6c65 2c20  or.    example, 
-00010f40: 7468 6520 7072 6576 696f 7573 2063 616e  the previous can
-00010f50: 2062 6520 7772 6974 7465 6e20 6173 3a3a   be written as::
-00010f60: 0a0a 2020 2020 2020 3e3e 3e20 656e 636f  ..      >>> enco
-00010f70: 6465 6420 3d20 6d6f 6465 6c2e 6170 706c  ded = model.appl
-00010f80: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
-00010f90: 6d65 7468 6f64 3d27 656e 636f 6465 2729  method='encode')
-00010fa0: 0a0a 2020 2020 4e6f 7465 2060 606d 6574  ..    Note ``met
-00010fb0: 686f 6460 6020 6361 6e20 616c 736f 2062  hod`` can also b
-00010fc0: 6520 6120 6675 6e63 7469 6f6e 2074 6861  e a function tha
-00010fd0: 7420 6973 206e 6f74 2064 6566 696e 6564  t is not defined
-00010fe0: 2069 6e0a 2020 2020 6060 5472 616e 7366   in.    ``Transf
-00010ff0: 6f72 6d65 7260 602e 2049 6e20 7468 6174  ormer``. In that
-00011000: 2063 6173 652c 2074 6865 2066 756e 6374   case, the funct
-00011010: 696f 6e20 7368 6f75 6c64 2068 6176 6520  ion should have 
-00011020: 6174 206c 6561 7374 206f 6e65 0a20 2020  at least one.   
-00011030: 2061 7267 756d 656e 7420 7265 7072 6573   argument repres
-00011040: 656e 7469 6e67 2061 6e20 696e 7374 616e  enting an instan
-00011050: 6365 206f 6620 7468 6520 4d6f 6475 6c65  ce of the Module
-00011060: 2063 6c61 7373 3a3a 0a0a 2020 2020 2020   class::..      
-00011070: 3e3e 3e20 6465 6620 6f74 6865 725f 666e  >>> def other_fn
-00011080: 2869 6e73 7461 6e63 652c 2078 293a 0a20  (instance, x):. 
-00011090: 2020 2020 202e 2e2e 2020 2023 2069 6e73       ...   # ins
-000110a0: 7461 6e63 652e 736f 6d65 5f6d 6f64 756c  tance.some_modul
-000110b0: 655f 6174 7472 282e 2e2e 290a 2020 2020  e_attr(...).    
-000110c0: 2020 2e2e 2e20 2020 696e 7374 616e 6365    ...   instance
-000110d0: 2e65 6e63 6f64 650a 2020 2020 2020 2e2e  .encode.      ..
-000110e0: 2e20 2020 2e2e 2e0a 0a20 2020 2020 203e  .   .....      >
-000110f0: 3e3e 206d 6f64 656c 2e61 7070 6c79 2876  >> model.apply(v
-00011100: 6172 6961 626c 6573 2c20 782c 206d 6574  ariables, x, met
-00011110: 686f 643d 6f74 6865 725f 666e 290a 0a20  hod=other_fn).. 
-00011120: 2020 2049 6620 796f 7520 7061 7373 2061     If you pass a
-00011130: 2073 696e 676c 6520 6060 5052 4e47 4b65   single ``PRNGKe
-00011140: 7960 602c 2046 6c61 7820 7769 6c6c 2075  y``, Flax will u
-00011150: 7365 2069 7420 746f 2066 6565 6420 7468  se it to feed th
-00011160: 6520 6060 2770 6172 616d 7327 6060 0a20  e ``'params'``. 
-00011170: 2020 2052 4e47 2073 7472 6561 6d2e 2020     RNG stream.  
-00011180: 4966 2079 6f75 2077 616e 7420 746f 2075  If you want to u
-00011190: 7365 2061 2064 6966 6665 7265 6e74 2052  se a different R
-000111a0: 4e47 2073 7472 6561 6d20 6f72 206e 6565  NG stream or nee
-000111b0: 6420 746f 2075 7365 0a20 2020 206d 756c  d to use.    mul
-000111c0: 7469 706c 6520 7374 7265 616d 732c 2079  tiple streams, y
-000111d0: 6f75 2063 616e 2070 6173 7320 6120 6469  ou can pass a di
-000111e0: 6374 696f 6e61 7279 206d 6170 7069 6e67  ctionary mapping
-000111f0: 2065 6163 6820 524e 4720 7374 7265 616d   each RNG stream
-00011200: 206e 616d 650a 2020 2020 746f 2069 7473   name.    to its
-00011210: 2063 6f72 7265 7370 6f6e 6469 6e67 2060   corresponding `
-00011220: 6050 524e 474b 6579 6060 2074 6f20 6060  `PRNGKey`` to ``
-00011230: 6170 706c 7960 602e 2049 6620 6060 7365  apply``. If ``se
-00011240: 6c66 2e6d 616b 655f 726e 6728 6e61 6d65  lf.make_rng(name
-00011250: 2960 600a 2020 2020 6973 2063 616c 6c65  )``.    is calle
-00011260: 6420 6f6e 2061 6e20 524e 4720 7374 7265  d on an RNG stre
-00011270: 616d 206e 616d 6520 7468 6174 2069 736e  am name that isn
-00011280: 2774 2070 6173 7365 6420 6279 2074 6865  't passed by the
-00011290: 2075 7365 722c 2069 7420 7769 6c6c 0a20   user, it will. 
-000112a0: 2020 2064 6566 6175 6c74 2074 6f20 7573     default to us
-000112b0: 696e 6720 7468 6520 6060 2770 6172 616d  ing the ``'param
-000112c0: 7327 6060 2052 4e47 2073 7472 6561 6d2e  s'`` RNG stream.
-000112d0: 0a0a 2020 2020 4578 616d 706c 653a 3a0a  ..    Example::.
-000112e0: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
-000112f0: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
-00011300: 0a20 2020 2020 202e 2e2e 2020 2040 6e6e  .      ...   @nn
-00011310: 2e63 6f6d 7061 6374 0a20 2020 2020 202e  .compact.      .
-00011320: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
-00011330: 5f28 7365 6c66 2c20 782c 2061 6464 5f6e  _(self, x, add_n
-00011340: 6f69 7365 3d46 616c 7365 293a 0a20 2020  oise=False):.   
-00011350: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
-00011360: 6e2e 4465 6e73 6528 3136 2928 7829 0a20  n.Dense(16)(x). 
-00011370: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
-00011380: 206e 6e2e 7265 6c75 2878 290a 2020 2020   nn.relu(x).    
-00011390: 2020 2e2e 2e0a 2020 2020 2020 2e2e 2e20    ....      ... 
-000113a0: 2020 2020 6966 2061 6464 5f6e 6f69 7365      if add_noise
-000113b0: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
-000113c0: 2020 2320 4164 6420 6761 7573 7369 616e    # Add gaussian
-000113d0: 206e 6f69 7365 0a20 2020 2020 202e 2e2e   noise.      ...
-000113e0: 2020 2020 2020 206e 6f69 7365 5f6b 6579         noise_key
-000113f0: 203d 2073 656c 662e 6d61 6b65 5f72 6e67   = self.make_rng
-00011400: 2827 6e6f 6973 6527 290a 2020 2020 2020  ('noise').      
-00011410: 2e2e 2e20 2020 2020 2020 7820 3d20 7820  ...       x = x 
-00011420: 2b20 6a61 782e 7261 6e64 6f6d 2e6e 6f72  + jax.random.nor
-00011430: 6d61 6c28 6e6f 6973 655f 6b65 792c 2078  mal(noise_key, x
-00011440: 2e73 6861 7065 290a 2020 2020 2020 2e2e  .shape).      ..
-00011450: 2e0a 2020 2020 2020 2e2e 2e20 2020 2020  ..      ...     
-00011460: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
-00011470: 3129 2878 290a 0a20 2020 2020 203e 3e3e  1)(x)..      >>>
-00011480: 2078 203d 206a 6e70 2e65 6d70 7479 2828   x = jnp.empty((
-00011490: 312c 2037 2929 0a20 2020 2020 203e 3e3e  1, 7)).      >>>
-000114a0: 206d 6f64 756c 6520 3d20 466f 6f28 290a   module = Foo().
-000114b0: 2020 2020 2020 3e3e 3e20 726e 6773 203d        >>> rngs =
-000114c0: 207b 2770 6172 616d 7327 3a20 6a61 782e   {'params': jax.
-000114d0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2027  random.key(0), '
-000114e0: 6e6f 6973 6527 3a20 6a61 782e 7261 6e64  noise': jax.rand
-000114f0: 6f6d 2e6b 6579 2831 297d 0a20 2020 2020  om.key(1)}.     
-00011500: 203e 3e3e 2076 6172 6961 626c 6573 203d   >>> variables =
-00011510: 206d 6f64 756c 652e 696e 6974 2872 6e67   module.init(rng
-00011520: 732c 2078 290a 2020 2020 2020 3e3e 3e20  s, x).      >>> 
-00011530: 6f75 7430 203d 206d 6f64 756c 652e 6170  out0 = module.ap
-00011540: 706c 7928 7661 7269 6162 6c65 732c 2078  ply(variables, x
-00011550: 2c20 6164 645f 6e6f 6973 653d 5472 7565  , add_noise=True
-00011560: 2c20 726e 6773 3d72 6e67 7329 0a0a 2020  , rngs=rngs)..  
-00011570: 2020 2020 3e3e 3e20 726e 6773 5b27 6e6f      >>> rngs['no
-00011580: 6973 6527 5d20 3d20 6a61 782e 7261 6e64  ise'] = jax.rand
-00011590: 6f6d 2e6b 6579 2830 290a 2020 2020 2020  om.key(0).      
-000115a0: 3e3e 3e20 6f75 7431 203d 206d 6f64 756c  >>> out1 = modul
-000115b0: 652e 6170 706c 7928 7661 7269 6162 6c65  e.apply(variable
-000115c0: 732c 2078 2c20 6164 645f 6e6f 6973 653d  s, x, add_noise=
-000115d0: 5472 7565 2c20 726e 6773 3d72 6e67 7329  True, rngs=rngs)
-000115e0: 0a20 2020 2020 203e 3e3e 2023 2064 6966  .      >>> # dif
-000115f0: 6665 7265 6e74 206f 7574 7075 7420 286b  ferent output (k
-00011600: 6579 2831 2920 7673 206b 6579 2830 2929  ey(1) vs key(0))
-00011610: 0a20 2020 2020 203e 3e3e 206e 702e 7465  .      >>> np.te
-00011620: 7374 696e 672e 6173 7365 7274 5f72 6169  sting.assert_rai
-00011630: 7365 7328 4173 7365 7274 696f 6e45 7272  ses(AssertionErr
-00011640: 6f72 2c20 6e70 2e74 6573 7469 6e67 2e61  or, np.testing.a
-00011650: 7373 6572 745f 616c 6c63 6c6f 7365 2c20  ssert_allclose, 
-00011660: 6f75 7430 2c20 6f75 7431 290a 0a20 2020  out0, out1)..   
-00011670: 2020 203e 3e3e 2064 656c 2072 6e67 735b     >>> del rngs[
-00011680: 276e 6f69 7365 275d 0a20 2020 2020 203e  'noise'].      >
-00011690: 3e3e 2023 2073 656c 662e 6d61 6b65 5f72  >> # self.make_r
-000116a0: 6e67 2827 6e6f 6973 6527 2920 7769 6c6c  ng('noise') will
-000116b0: 2064 6566 6175 6c74 2074 6f20 7573 696e   default to usin
-000116c0: 6720 7468 6520 2770 6172 616d 7327 2052  g the 'params' R
-000116d0: 4e47 2073 7472 6561 6d0a 2020 2020 2020  NG stream.      
-000116e0: 3e3e 3e20 6f75 7432 203d 206d 6f64 756c  >>> out2 = modul
-000116f0: 652e 6170 706c 7928 7661 7269 6162 6c65  e.apply(variable
-00011700: 732c 2078 2c20 6164 645f 6e6f 6973 653d  s, x, add_noise=
-00011710: 5472 7565 2c20 726e 6773 3d72 6e67 7329  True, rngs=rngs)
-00011720: 0a20 2020 2020 203e 3e3e 2023 2073 616d  .      >>> # sam
-00011730: 6520 6f75 7470 7574 2028 6b65 7928 3029  e output (key(0)
-00011740: 290a 2020 2020 2020 3e3e 3e20 6e70 2e74  ).      >>> np.t
-00011750: 6573 7469 6e67 2e61 7373 6572 745f 616c  esting.assert_al
-00011760: 6c63 6c6f 7365 286f 7574 312c 206f 7574  lclose(out1, out
-00011770: 3229 0a0a 2020 2020 2020 3e3e 3e20 2320  2)..      >>> # 
-00011780: 7061 7373 696e 6720 696e 2061 2073 696e  passing in a sin
-00011790: 676c 6520 6b65 7920 6973 2065 7175 6976  gle key is equiv
-000117a0: 616c 656e 7420 746f 2070 6173 7369 6e67  alent to passing
-000117b0: 2069 6e20 7b27 7061 7261 6d73 273a 206b   in {'params': k
-000117c0: 6579 7d0a 2020 2020 2020 3e3e 3e20 6f75  ey}.      >>> ou
-000117d0: 7433 203d 206d 6f64 756c 652e 6170 706c  t3 = module.appl
-000117e0: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
-000117f0: 6164 645f 6e6f 6973 653d 5472 7565 2c20  add_noise=True, 
-00011800: 726e 6773 3d6a 6178 2e72 616e 646f 6d2e  rngs=jax.random.
-00011810: 6b65 7928 3029 290a 2020 2020 2020 3e3e  key(0)).      >>
-00011820: 3e20 2320 7361 6d65 206f 7574 7075 7420  > # same output 
-00011830: 286b 6579 2830 2929 0a20 2020 2020 203e  (key(0)).      >
-00011840: 3e3e 206e 702e 7465 7374 696e 672e 6173  >> np.testing.as
-00011850: 7365 7274 5f61 6c6c 636c 6f73 6528 6f75  sert_allclose(ou
-00011860: 7432 2c20 6f75 7433 290a 0a20 2020 2041  t2, out3)..    A
-00011870: 7267 733a 0a20 2020 2020 2076 6172 6961  rgs:.      varia
-00011880: 626c 6573 3a20 4120 6469 6374 696f 6e61  bles: A dictiona
-00011890: 7279 2063 6f6e 7461 696e 696e 6720 7661  ry containing va
-000118a0: 7269 6162 6c65 7320 6b65 7965 6420 6279  riables keyed by
-000118b0: 2076 6172 6961 626c 650a 2020 2020 2020   variable.      
-000118c0: 2020 636f 6c6c 6563 7469 6f6e 732e 2053    collections. S
-000118d0: 6565 203a 6d6f 643a 6066 6c61 782e 636f  ee :mod:`flax.co
-000118e0: 7265 2e76 6172 6961 626c 6573 6020 666f  re.variables` fo
-000118f0: 7220 6d6f 7265 2064 6574 6169 6c73 2061  r more details a
-00011900: 626f 7574 0a20 2020 2020 2020 2076 6172  bout.        var
-00011910: 6961 626c 6573 2e0a 2020 2020 2020 2a61  iables..      *a
-00011920: 7267 733a 204e 616d 6564 2061 7267 756d  rgs: Named argum
-00011930: 656e 7473 2070 6173 7365 6420 746f 2074  ents passed to t
-00011940: 6865 2073 7065 6369 6669 6564 2061 7070  he specified app
-00011950: 6c79 206d 6574 686f 642e 0a20 2020 2020  ly method..     
-00011960: 2072 6e67 733a 2061 2064 6963 7420 6f66   rngs: a dict of
-00011970: 2050 524e 474b 6579 7320 746f 2069 6e69   PRNGKeys to ini
-00011980: 7469 616c 697a 6520 7468 6520 5052 4e47  tialize the PRNG
-00011990: 2073 6571 7565 6e63 6573 2e20 5468 6520   sequences. The 
-000119a0: 2270 6172 616d 7322 0a20 2020 2020 2020  "params".       
-000119b0: 2050 524e 4720 7365 7175 656e 6365 2069   PRNG sequence i
-000119c0: 7320 7573 6564 2074 6f20 696e 6974 6961  s used to initia
-000119d0: 6c69 7a65 2070 6172 616d 6574 6572 732e  lize parameters.
-000119e0: 0a20 2020 2020 206d 6574 686f 643a 2041  .      method: A
-000119f0: 2066 756e 6374 696f 6e20 746f 2063 616c   function to cal
-00011a00: 6c20 6170 706c 7920 6f6e 2e20 5468 6973  l apply on. This
-00011a10: 2069 7320 6765 6e65 7261 6c6c 7920 6120   is generally a 
-00011a20: 6675 6e63 7469 6f6e 2069 6e20 7468 650a  function in the.
-00011a30: 2020 2020 2020 2020 6d6f 6475 6c65 2e20          module. 
-00011a40: 4966 2070 726f 7669 6465 642c 2061 7070  If provided, app
-00011a50: 6c69 6573 2074 6869 7320 6d65 7468 6f64  lies this method
-00011a60: 2e20 4966 206e 6f74 2070 726f 7669 6465  . If not provide
-00011a70: 642c 2061 7070 6c69 6573 2074 6865 0a20  d, applies the. 
-00011a80: 2020 2020 2020 2060 605f 5f63 616c 6c5f         ``__call_
-00011a90: 5f60 6020 6d65 7468 6f64 206f 6620 7468  _`` method of th
-00011aa0: 6520 6d6f 6475 6c65 2e20 4120 7374 7269  e module. A stri
-00011ab0: 6e67 2063 616e 2061 6c73 6f20 6265 2070  ng can also be p
-00011ac0: 726f 7669 6465 6420 746f 0a20 2020 2020  rovided to.     
-00011ad0: 2020 2073 7065 6369 6679 2061 206d 6574     specify a met
-00011ae0: 686f 6420 6279 206e 616d 652e 0a20 2020  hod by name..   
-00011af0: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
-00011b00: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
-00011b10: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
-00011b20: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
-00011b30: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
-00011b40: 2020 2020 2074 7265 6174 6564 2061 7320       treated as 
-00011b50: 6d75 7461 626c 653a 2060 6062 6f6f 6c60  mutable: ``bool`
-00011b60: 603a 2061 6c6c 2f6e 6f20 636f 6c6c 6563  `: all/no collec
-00011b70: 7469 6f6e 7320 6172 6520 6d75 7461 626c  tions are mutabl
-00011b80: 652e 2060 6073 7472 6060 3a0a 2020 2020  e. ``str``:.    
-00011b90: 2020 2020 5468 6520 6e61 6d65 206f 6620      The name of 
-00011ba0: 6120 7369 6e67 6c65 206d 7574 6162 6c65  a single mutable
-00011bb0: 2063 6f6c 6c65 6374 696f 6e2e 2060 606c   collection. ``l
-00011bc0: 6973 7460 603a 2041 206c 6973 7420 6f66  ist``: A list of
-00011bd0: 206e 616d 6573 206f 660a 2020 2020 2020   names of.      
-00011be0: 2020 6d75 7461 626c 6520 636f 6c6c 6563    mutable collec
-00011bf0: 7469 6f6e 732e 0a20 2020 2020 2063 6170  tions..      cap
-00011c00: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00011c10: 6573 3a20 4966 2060 6054 7275 6560 602c  es: If ``True``,
-00011c20: 2063 6170 7475 7265 7320 696e 7465 726d   captures interm
-00011c30: 6564 6961 7465 2072 6574 7572 6e20 7661  ediate return va
-00011c40: 6c75 6573 206f 660a 2020 2020 2020 2020  lues of.        
-00011c50: 616c 6c20 4d6f 6475 6c65 7320 696e 7369  all Modules insi
-00011c60: 6465 2074 6865 2022 696e 7465 726d 6564  de the "intermed
-00011c70: 6961 7465 7322 2063 6f6c 6c65 6374 696f  iates" collectio
-00011c80: 6e2e 2042 7920 6465 6661 756c 742c 206f  n. By default, o
-00011c90: 6e6c 7920 7468 650a 2020 2020 2020 2020  nly the.        
-00011ca0: 7265 7475 726e 2076 616c 7565 7320 6f66  return values of
-00011cb0: 2061 6c6c 2060 605f 5f63 616c 6c5f 5f60   all ``__call__`
-00011cc0: 6020 6d65 7468 6f64 7320 6172 6520 7374  ` methods are st
-00011cd0: 6f72 6564 2e20 4120 6675 6e63 7469 6f6e  ored. A function
-00011ce0: 2063 616e 2062 650a 2020 2020 2020 2020   can be.        
-00011cf0: 7061 7373 6564 2074 6f20 6368 616e 6765  passed to change
-00011d00: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
-00011d10: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
-00011d20: 2066 756e 6374 696f 6e20 7461 6b65 7320   function takes 
-00011d30: 7468 650a 2020 2020 2020 2020 4d6f 6475  the.        Modu
-00011d40: 6c65 2069 6e73 7461 6e63 6520 616e 6420  le instance and 
-00011d50: 6d65 7468 6f64 206e 616d 6520 616e 6420  method name and 
-00011d60: 7265 7475 726e 7320 6120 626f 6f6c 2069  returns a bool i
-00011d70: 6e64 6963 6174 696e 6720 7768 6574 6865  ndicating whethe
-00011d80: 720a 2020 2020 2020 2020 7468 6520 6f75  r.        the ou
-00011d90: 7470 7574 206f 6620 7468 6174 206d 6574  tput of that met
-00011da0: 686f 6420 696e 766f 6361 7469 6f6e 2073  hod invocation s
-00011db0: 686f 756c 6420 6265 2073 746f 7265 642e  hould be stored.
-00011dc0: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
-00011dd0: 204b 6579 776f 7264 2061 7267 756d 656e   Keyword argumen
-00011de0: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
-00011df0: 2073 7065 6369 6669 6564 2061 7070 6c79   specified apply
-00011e00: 206d 6574 686f 642e 0a0a 2020 2020 5265   method...    Re
-00011e10: 7475 726e 733a 0a20 2020 2020 2049 6620  turns:.      If 
-00011e20: 6060 6d75 7461 626c 6560 6020 6973 2046  ``mutable`` is F
-00011e30: 616c 7365 2c20 7265 7475 726e 7320 6f75  alse, returns ou
-00011e40: 7470 7574 2e20 4966 2061 6e79 2063 6f6c  tput. If any col
-00011e50: 6c65 6374 696f 6e73 2061 7265 0a20 2020  lections are.   
-00011e60: 2020 206d 7574 6162 6c65 2c20 7265 7475     mutable, retu
-00011e70: 726e 7320 6060 286f 7574 7075 742c 2076  rns ``(output, v
-00011e80: 6172 7329 6060 2c20 7768 6572 6520 6060  ars)``, where ``
-00011e90: 7661 7273 6060 2061 7265 2069 7320 6120  vars`` are is a 
-00011ea0: 6469 6374 0a20 2020 2020 206f 6620 7468  dict.      of th
-00011eb0: 6520 6d6f 6469 6669 6564 2063 6f6c 6c65  e modified colle
-00011ec0: 6374 696f 6e73 2e0a 2020 2020 2222 220a  ctions..    """.
-00011ed0: 2020 2020 4d6f 6475 6c65 2e5f 6d6f 6475      Module._modu
-00011ee0: 6c65 5f63 6865 636b 7328 7365 6c66 290a  le_checks(self).
-00011ef0: 0a20 2020 2069 6620 726e 6773 2069 7320  .    if rngs is 
-00011f00: 6e6f 7420 4e6f 6e65 2061 6e64 206e 6f74  not None and not
-00011f10: 2069 7369 6e73 7461 6e63 6528 726e 6773   isinstance(rngs
-00011f20: 2c20 6469 6374 293a 0a20 2020 2020 2069  , dict):.      i
-00011f30: 6620 6e6f 7420 636f 7265 2e73 636f 7065  f not core.scope
-00011f40: 2e5f 6973 5f76 616c 6964 5f72 6e67 2872  ._is_valid_rng(r
-00011f50: 6e67 7329 3a0a 2020 2020 2020 2020 7261  ngs):.        ra
-00011f60: 6973 6520 6572 726f 7273 2e49 6e76 616c  ise errors.Inval
-00011f70: 6964 526e 6745 7272 6f72 280a 2020 2020  idRngError(.    
-00011f80: 2020 2020 2020 2752 4e47 7320 7368 6f75        'RNGs shou
-00011f90: 6c64 2062 6520 6f66 2073 6861 7065 2028  ld be of shape (
-00011fa0: 322c 2920 6f72 2050 524e 474b 6579 2069  2,) or PRNGKey i
-00011fb0: 6e20 4d6f 6475 6c65 2027 0a20 2020 2020  n Module '.     
-00011fc0: 2020 2020 2066 277b 7365 6c66 2e5f 5f63       f'{self.__c
-00011fd0: 6c61 7373 5f5f 2e5f 5f6e 616d 655f 5f7d  lass__.__name__}
-00011fe0: 2c20 6275 7420 726e 6773 2061 7265 3a20  , but rngs are: 
-00011ff0: 7b72 6e67 737d 270a 2020 2020 2020 2020  {rngs}'.        
-00012000: 290a 2020 2020 2020 726e 6773 203d 207b  ).      rngs = {
-00012010: 2770 6172 616d 7327 3a20 726e 6773 7d0a  'params': rngs}.
-00012020: 0a20 2020 2069 6620 6973 696e 7374 616e  .    if isinstan
-00012030: 6365 286d 6574 686f 642c 2073 7472 293a  ce(method, str):
-00012040: 0a20 2020 2020 2061 7474 7269 6275 7465  .      attribute
-00012050: 5f6e 616d 6520 3d20 6d65 7468 6f64 0a20  _name = method. 
-00012060: 2020 2020 206d 6574 686f 6420 3d20 6765       method = ge
-00012070: 7461 7474 7228 7365 6c66 2c20 6174 7472  tattr(self, attr
-00012080: 6962 7574 655f 6e61 6d65 290a 2020 2020  ibute_name).    
-00012090: 2020 6966 206e 6f74 2063 616c 6c61 626c    if not callabl
-000120a0: 6528 6d65 7468 6f64 293a 0a20 2020 2020  e(method):.     
-000120b0: 2020 2063 6c61 7373 5f6e 616d 6520 3d20     class_name = 
-000120c0: 7479 7065 2873 656c 6629 2e5f 5f6e 616d  type(self).__nam
-000120d0: 655f 5f0a 2020 2020 2020 2020 7261 6973  e__.        rais
-000120e0: 6520 5479 7065 4572 726f 7228 0a20 2020  e TypeError(.   
-000120f0: 2020 2020 2020 2066 2227 7b63 6c61 7373         f"'{class
-00012100: 5f6e 616d 657d 2e7b 6174 7472 6962 7574  _name}.{attribut
-00012110: 655f 6e61 6d65 7d27 206d 7573 7420 6265  e_name}' must be
-00012120: 2061 2063 616c 6c61 626c 652c 2067 6f74   a callable, got
-00012130: 220a 2020 2020 2020 2020 2020 6627 207b  ".          f' {
-00012140: 7479 7065 286d 6574 686f 6429 7d2e 270a  type(method)}.'.
-00012150: 2020 2020 2020 2020 290a 2020 2020 2020          ).      
-00012160: 2320 6966 2074 6865 2060 6d65 7468 6f64  # if the `method
-00012170: 6020 7374 7269 6e67 2069 7320 6120 7375  ` string is a su
-00012180: 626d 6f64 756c 652c 2077 6520 6372 6561  bmodule, we crea
-00012190: 7465 2061 206c 616d 6264 6120 6675 6e63  te a lambda func
-000121a0: 7469 6f6e 0a20 2020 2020 2023 2074 6861  tion.      # tha
-000121b0: 7420 6361 6c6c 7320 7468 6520 7375 626d  t calls the subm
-000121c0: 6f64 756c 652c 2066 6f72 7761 7264 696e  odule, forwardin
-000121d0: 6720 616c 6c20 6172 6775 6d65 6e74 732e  g all arguments.
-000121e0: 0a20 2020 2020 2069 6620 6973 696e 7374  .      if isinst
-000121f0: 616e 6365 286d 6574 686f 642c 204d 6f64  ance(method, Mod
-00012200: 756c 6529 3a0a 2020 2020 2020 2020 6d65  ule):.        me
-00012210: 7468 6f64 203d 206c 616d 6264 6120 7365  thod = lambda se
-00012220: 6c66 2c20 2a61 7267 732c 202a 2a6b 7761  lf, *args, **kwa
-00012230: 7267 733a 2067 6574 6174 7472 2873 656c  rgs: getattr(sel
-00012240: 662c 2061 7474 7269 6275 7465 5f6e 616d  f, attribute_nam
-00012250: 6529 280a 2020 2020 2020 2020 2020 2a61  e)(.          *a
-00012260: 7267 732c 202a 2a6b 7761 7267 730a 2020  rgs, **kwargs.  
-00012270: 2020 2020 2020 290a 2020 2020 656c 6966        ).    elif
-00012280: 206d 6574 686f 6420 6973 204e 6f6e 653a   method is None:
-00012290: 0a20 2020 2020 206d 6574 686f 6420 3d20  .      method = 
-000122a0: 7365 6c66 2e5f 5f63 616c 6c5f 5f0a 2020  self.__call__.  
-000122b0: 2020 6d65 7468 6f64 203d 205f 6765 745f    method = _get_
-000122c0: 756e 626f 756e 645f 666e 286d 6574 686f  unbound_fn(metho
-000122d0: 6429 0a20 2020 2072 6574 7572 6e20 6170  d).    return ap
-000122e0: 706c 7928 0a20 2020 2020 206d 6574 686f  ply(.      metho
-000122f0: 642c 0a20 2020 2020 2073 656c 662c 0a20  d,.      self,. 
-00012300: 2020 2020 206d 7574 6162 6c65 3d6d 7574       mutable=mut
-00012310: 6162 6c65 2c0a 2020 2020 2020 6361 7074  able,.      capt
-00012320: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
-00012330: 733d 6361 7074 7572 655f 696e 7465 726d  s=capture_interm
-00012340: 6564 6961 7465 732c 0a20 2020 2029 2876  ediates,.    )(v
-00012350: 6172 6961 626c 6573 2c20 2a61 7267 732c  ariables, *args,
-00012360: 202a 2a6b 7761 7267 732c 2072 6e67 733d   **kwargs, rngs=
-00012370: 726e 6773 290a 0a20 2040 7472 6163 6562  rngs)..  @traceb
-00012380: 6163 6b5f 7574 696c 2e61 7069 5f62 6f75  ack_util.api_bou
-00012390: 6e64 6172 790a 2020 6465 6620 696e 6974  ndary.  def init
-000123a0: 5f77 6974 685f 6f75 7470 7574 280a 2020  _with_output(.  
-000123b0: 2020 7365 6c66 2c0a 2020 2020 726e 6773    self,.    rngs
-000123c0: 3a20 556e 696f 6e5b 5052 4e47 4b65 792c  : Union[PRNGKey,
-000123d0: 2052 4e47 5365 7175 656e 6365 735d 2c0a   RNGSequences],.
-000123e0: 2020 2020 2a61 7267 732c 0a20 2020 206d      *args,.    m
-000123f0: 6574 686f 643a 2055 6e69 6f6e 5b43 616c  ethod: Union[Cal
-00012400: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d2c  lable[..., Any],
-00012410: 2073 7472 2c20 4e6f 6e65 5d20 3d20 4e6f   str, None] = No
-00012420: 6e65 2c0a 2020 2020 6d75 7461 626c 653a  ne,.    mutable:
-00012430: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
-00012440: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
-00012450: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
-00012460: 2020 2063 6170 7475 7265 5f69 6e74 6572     capture_inter
-00012470: 6d65 6469 6174 6573 3a20 556e 696f 6e5b  mediates: Union[
-00012480: 626f 6f6c 2c20 4361 6c6c 6162 6c65 5b5b  bool, Callable[[
-00012490: 274d 6f64 756c 6527 2c20 7374 725d 2c20  'Module', str], 
-000124a0: 626f 6f6c 5d5d 203d 2046 616c 7365 2c0a  bool]] = False,.
-000124b0: 2020 2020 2a2a 6b77 6172 6773 2c0a 2020      **kwargs,.  
-000124c0: 2920 2d3e 2054 7570 6c65 5b41 6e79 2c20  ) -> Tuple[Any, 
-000124d0: 556e 696f 6e5b 4672 6f7a 656e 5661 7269  Union[FrozenVari
-000124e0: 6162 6c65 4469 6374 2c20 4469 6374 5b73  ableDict, Dict[s
-000124f0: 7472 2c20 416e 795d 5d5d 3a0a 2020 2020  tr, Any]]]:.    
-00012500: 2222 2249 6e69 7469 616c 697a 6573 2061  """Initializes a
-00012510: 206d 6f64 756c 6520 6d65 7468 6f64 2077   module method w
-00012520: 6974 6820 7661 7269 6162 6c65 7320 616e  ith variables an
-00012530: 6420 7265 7475 726e 7320 6f75 7470 7574  d returns output
-00012540: 2061 6e64 206d 6f64 6966 6965 6420 7661   and modified va
-00012550: 7269 6162 6c65 732e 0a0a 2020 2020 4172  riables...    Ar
-00012560: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
-00012570: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
-00012580: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-00012590: 7469 6f6e 732e 0a20 2020 2020 202a 6172  tions..      *ar
-000125a0: 6773 3a20 4e61 6d65 6420 6172 6775 6d65  gs: Named argume
-000125b0: 6e74 7320 7061 7373 6564 2074 6f20 7468  nts passed to th
-000125c0: 6520 696e 6974 2066 756e 6374 696f 6e2e  e init function.
-000125d0: 0a20 2020 2020 206d 6574 686f 643a 2041  .      method: A
-000125e0: 6e20 6f70 7469 6f6e 616c 206d 6574 686f  n optional metho
-000125f0: 642e 2049 6620 7072 6f76 6964 6564 2c20  d. If provided, 
-00012600: 6170 706c 6965 7320 7468 6973 206d 6574  applies this met
-00012610: 686f 642e 2049 6620 6e6f 740a 2020 2020  hod. If not.    
-00012620: 2020 2020 7072 6f76 6964 6564 2c20 6170      provided, ap
-00012630: 706c 6965 7320 7468 6520 6060 5f5f 6361  plies the ``__ca
-00012640: 6c6c 5f5f 6060 206d 6574 686f 642e 2041  ll__`` method. A
-00012650: 2073 7472 696e 6720 6361 6e20 616c 736f   string can also
-00012660: 2062 650a 2020 2020 2020 2020 7072 6f76   be.        prov
-00012670: 6964 6564 2074 6f20 7370 6563 6966 7920  ided to specify 
-00012680: 6120 6d65 7468 6f64 2062 7920 6e61 6d65  a method by name
-00012690: 2e0a 2020 2020 2020 6d75 7461 626c 653a  ..      mutable:
-000126a0: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
-000126b0: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
-000126c0: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
-000126d0: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
-000126e0: 650a 2020 2020 2020 2020 7472 6561 7465  e.        treate
-000126f0: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
-00012700: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
-00012710: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
-00012720: 7574 6162 6c65 2e20 6060 7374 7260 603a  utable. ``str``:
-00012730: 0a20 2020 2020 2020 2054 6865 206e 616d  .        The nam
-00012740: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
-00012750: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
-00012760: 2e20 6060 6c69 7374 6060 3a20 4120 6c69  . ``list``: A li
-00012770: 7374 206f 6620 6e61 6d65 7320 6f66 0a20  st of names of. 
-00012780: 2020 2020 2020 206d 7574 6162 6c65 2063         mutable c
-00012790: 6f6c 6c65 6374 696f 6e73 2e20 4279 2064  ollections. By d
-000127a0: 6566 6175 6c74 2c20 616c 6c20 636f 6c6c  efault, all coll
-000127b0: 6563 7469 6f6e 7320 6578 6365 7074 2022  ections except "
-000127c0: 696e 7465 726d 6564 6961 7465 7322 0a20  intermediates". 
-000127d0: 2020 2020 2020 2061 7265 206d 7574 6162         are mutab
-000127e0: 6c65 2e0a 2020 2020 2020 6361 7074 7572  le..      captur
-000127f0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
-00012800: 2049 6620 6060 5472 7565 6060 2c20 6361   If ``True``, ca
-00012810: 7074 7572 6573 2069 6e74 6572 6d65 6469  ptures intermedi
-00012820: 6174 6520 7265 7475 726e 2076 616c 7565  ate return value
-00012830: 7320 6f66 0a20 2020 2020 2020 2061 6c6c  s of.        all
-00012840: 204d 6f64 756c 6573 2069 6e73 6964 6520   Modules inside 
-00012850: 7468 6520 2269 6e74 6572 6d65 6469 6174  the "intermediat
-00012860: 6573 2220 636f 6c6c 6563 7469 6f6e 2e20  es" collection. 
-00012870: 4279 2064 6566 6175 6c74 206f 6e6c 7920  By default only 
-00012880: 7468 650a 2020 2020 2020 2020 7265 7475  the.        retu
-00012890: 726e 2076 616c 7565 7320 6f66 2061 6c6c  rn values of all
-000128a0: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
-000128b0: 7468 6f64 7320 6172 6520 7374 6f72 6564  thods are stored
-000128c0: 2e20 4120 6675 6e63 7469 6f6e 2063 616e  . A function can
-000128d0: 2062 650a 2020 2020 2020 2020 7061 7373   be.        pass
-000128e0: 6564 2074 6f20 6368 616e 6765 2074 6865  ed to change the
-000128f0: 2066 696c 7465 7220 6265 6861 7669 6f72   filter behavior
-00012900: 2e20 5468 6520 6669 6c74 6572 2066 756e  . The filter fun
-00012910: 6374 696f 6e20 7461 6b65 7320 7468 650a  ction takes the.
-00012920: 2020 2020 2020 2020 4d6f 6475 6c65 2069          Module i
-00012930: 6e73 7461 6e63 6520 616e 6420 6d65 7468  nstance and meth
-00012940: 6f64 206e 616d 6520 616e 6420 7265 7475  od name and retu
-00012950: 726e 7320 6120 626f 6f6c 2069 6e64 6963  rns a bool indic
-00012960: 6174 696e 6720 7768 6574 6865 720a 2020  ating whether.  
-00012970: 2020 2020 2020 7468 6520 6f75 7470 7574        the output
-00012980: 206f 6620 7468 6174 206d 6574 686f 6420   of that method 
-00012990: 696e 766f 6361 7469 6f6e 2073 686f 756c  invocation shoul
-000129a0: 6420 6265 2073 746f 7265 642e 0a20 2020  d be stored..   
-000129b0: 2020 202a 2a6b 7761 7267 733a 204b 6579     **kwargs: Key
-000129c0: 776f 7264 2061 7267 756d 656e 7473 2070  word arguments p
-000129d0: 6173 7365 6420 746f 2074 6865 2069 6e69  assed to the ini
-000129e0: 7420 6675 6e63 7469 6f6e 2e0a 0a20 2020  t function...   
-000129f0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-00012a00: 6060 286f 7574 7075 742c 2076 6172 7329  ``(output, vars)
-00012a10: 6060 2c20 7768 6572 6520 6060 7661 7273  ``, where ``vars
-00012a20: 6060 2061 7265 2069 7320 6120 6469 6374  `` are is a dict
-00012a30: 206f 6620 7468 6520 6d6f 6469 6669 6564   of the modified
-00012a40: 0a20 2020 2020 2063 6f6c 6c65 6374 696f  .      collectio
-00012a50: 6e73 2e0a 2020 2020 2222 220a 2020 2020  ns..    """.    
-00012a60: 4d6f 6475 6c65 2e5f 6d6f 6475 6c65 5f63  Module._module_c
-00012a70: 6865 636b 7328 7365 6c66 290a 0a20 2020  hecks(self)..   
-00012a80: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan
-00012a90: 6365 2872 6e67 732c 2064 6963 7429 3a0a  ce(rngs, dict):.
-00012aa0: 2020 2020 2020 6966 206e 6f74 2063 6f72        if not cor
-00012ab0: 652e 7363 6f70 652e 5f69 735f 7661 6c69  e.scope._is_vali
-00012ac0: 645f 726e 6728 726e 6773 293a 0a20 2020  d_rng(rngs):.   
-00012ad0: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
-00012ae0: 732e 496e 7661 6c69 6452 6e67 4572 726f  s.InvalidRngErro
-00012af0: 7228 0a20 2020 2020 2020 2020 2027 524e  r(.          'RN
-00012b00: 4773 2073 686f 756c 6420 6265 206f 6620  Gs should be of 
-00012b10: 7368 6170 6520 2832 2c29 206f 7220 5052  shape (2,) or PR
-00012b20: 4e47 4b65 7920 696e 204d 6f64 756c 6520  NGKey in Module 
-00012b30: 270a 2020 2020 2020 2020 2020 6627 7b73  '.          f'{s
-00012b40: 656c 662e 5f5f 636c 6173 735f 5f2e 5f5f  elf.__class__.__
-00012b50: 6e61 6d65 5f5f 7d2c 2062 7574 2072 6e67  name__}, but rng
-00012b60: 7320 6172 653a 207b 726e 6773 7d27 0a20  s are: {rngs}'. 
-00012b70: 2020 2020 2020 2029 0a20 2020 2020 2072         ).      r
-00012b80: 6e67 7320 3d20 7b27 7061 7261 6d73 273a  ngs = {'params':
-00012b90: 2072 6e67 737d 0a0a 2020 2020 6966 2069   rngs}..    if i
-00012ba0: 7369 6e73 7461 6e63 6528 6d65 7468 6f64  sinstance(method
-00012bb0: 2c20 7374 7229 3a0a 2020 2020 2020 6174  , str):.      at
-00012bc0: 7472 6962 7574 655f 6e61 6d65 203d 206d  tribute_name = m
-00012bd0: 6574 686f 640a 2020 2020 2020 6d65 7468  ethod.      meth
-00012be0: 6f64 203d 2067 6574 6174 7472 2873 656c  od = getattr(sel
-00012bf0: 662c 2061 7474 7269 6275 7465 5f6e 616d  f, attribute_nam
-00012c00: 6529 0a20 2020 2020 2069 6620 6e6f 7420  e).      if not 
-00012c10: 6361 6c6c 6162 6c65 286d 6574 686f 6429  callable(method)
-00012c20: 3a0a 2020 2020 2020 2020 636c 6173 735f  :.        class_
-00012c30: 6e61 6d65 203d 2074 7970 6528 7365 6c66  name = type(self
-00012c40: 292e 5f5f 6e61 6d65 5f5f 0a20 2020 2020  ).__name__.     
-00012c50: 2020 2072 6169 7365 2054 7970 6545 7272     raise TypeErr
-00012c60: 6f72 280a 2020 2020 2020 2020 2020 6622  or(.          f"
-00012c70: 277b 636c 6173 735f 6e61 6d65 7d2e 7b61  '{class_name}.{a
-00012c80: 7474 7269 6275 7465 5f6e 616d 657d 2720  ttribute_name}' 
-00012c90: 6d75 7374 2062 6520 6120 6361 6c6c 6162  must be a callab
-00012ca0: 6c65 2c20 676f 7422 0a20 2020 2020 2020  le, got".       
-00012cb0: 2020 2066 2720 7b74 7970 6528 6d65 7468     f' {type(meth
-00012cc0: 6f64 297d 2e27 0a20 2020 2020 2020 2029  od)}.'.        )
-00012cd0: 0a20 2020 2065 6c69 6620 6d65 7468 6f64  .    elif method
-00012ce0: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-00012cf0: 6d65 7468 6f64 203d 2073 656c 662e 5f5f  method = self.__
-00012d00: 6361 6c6c 5f5f 0a20 2020 206d 6574 686f  call__.    metho
-00012d10: 6420 3d20 5f67 6574 5f75 6e62 6f75 6e64  d = _get_unbound
-00012d20: 5f66 6e28 6d65 7468 6f64 290a 2020 2020  _fn(method).    
-00012d30: 7265 7475 726e 2069 6e69 745f 7769 7468  return init_with
-00012d40: 5f6f 7574 7075 7428 0a20 2020 2020 206d  _output(.      m
-00012d50: 6574 686f 642c 0a20 2020 2020 2073 656c  ethod,.      sel
-00012d60: 662c 0a20 2020 2020 206d 7574 6162 6c65  f,.      mutable
-00012d70: 3d6d 7574 6162 6c65 2c0a 2020 2020 2020  =mutable,.      
-00012d80: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-00012d90: 6961 7465 733d 6361 7074 7572 655f 696e  iates=capture_in
-00012da0: 7465 726d 6564 6961 7465 732c 0a20 2020  termediates,.   
-00012db0: 2029 2872 6e67 732c 202a 6172 6773 2c20   )(rngs, *args, 
-00012dc0: 2a2a 6b77 6172 6773 290a 0a20 2040 7472  **kwargs)..  @tr
-00012dd0: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
-00012de0: 5f62 6f75 6e64 6172 790a 2020 6465 6620  _boundary.  def 
-00012df0: 696e 6974 280a 2020 2020 7365 6c66 2c0a  init(.    self,.
-00012e00: 2020 2020 726e 6773 3a20 556e 696f 6e5b      rngs: Union[
-00012e10: 5052 4e47 4b65 792c 2052 4e47 5365 7175  PRNGKey, RNGSequ
-00012e20: 656e 6365 735d 2c0a 2020 2020 2a61 7267  ences],.    *arg
-00012e30: 732c 0a20 2020 206d 6574 686f 643a 2055  s,.    method: U
-00012e40: 6e69 6f6e 5b43 616c 6c61 626c 655b 2e2e  nion[Callable[..
-00012e50: 2e2c 2041 6e79 5d2c 2073 7472 2c20 4e6f  ., Any], str, No
-00012e60: 6e65 5d20 3d20 4e6f 6e65 2c0a 2020 2020  ne] = None,.    
-00012e70: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
-00012e80: 696f 6e46 696c 7465 7220 3d20 4465 6e79  ionFilter = Deny
-00012e90: 4c69 7374 2827 696e 7465 726d 6564 6961  List('intermedia
-00012ea0: 7465 7327 292c 0a20 2020 2063 6170 7475  tes'),.    captu
-00012eb0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
-00012ec0: 3a20 556e 696f 6e5b 626f 6f6c 2c20 4361  : Union[bool, Ca
-00012ed0: 6c6c 6162 6c65 5b5b 274d 6f64 756c 6527  llable[['Module'
-00012ee0: 2c20 7374 725d 2c20 626f 6f6c 5d5d 203d  , str], bool]] =
-00012ef0: 2046 616c 7365 2c0a 2020 2020 2a2a 6b77   False,.    **kw
-00012f00: 6172 6773 2c0a 2020 2920 2d3e 2055 6e69  args,.  ) -> Uni
-00012f10: 6f6e 5b46 726f 7a65 6e56 6172 6961 626c  on[FrozenVariabl
-00012f20: 6544 6963 742c 2044 6963 745b 7374 722c  eDict, Dict[str,
-00012f30: 2041 6e79 5d5d 3a0a 2020 2020 2222 2249   Any]]:.    """I
-00012f40: 6e69 7469 616c 697a 6573 2061 206d 6f64  nitializes a mod
-00012f50: 756c 6520 6d65 7468 6f64 2077 6974 6820  ule method with 
-00012f60: 7661 7269 6162 6c65 7320 616e 6420 7265  variables and re
-00012f70: 7475 726e 7320 6d6f 6469 6669 6564 2076  turns modified v
-00012f80: 6172 6961 626c 6573 2e0a 0a20 2020 2060  ariables...    `
-00012f90: 6069 6e69 7460 6020 7461 6b65 7320 6173  `init`` takes as
-00012fa0: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
-00012fb0: 6569 7468 6572 2061 2073 696e 676c 6520  either a single 
-00012fc0: 6060 5052 4e47 4b65 7960 602c 206f 7220  ``PRNGKey``, or 
-00012fd0: 610a 2020 2020 6469 6374 696f 6e61 7279  a.    dictionary
-00012fe0: 206d 6170 7069 6e67 2076 6172 6961 626c   mapping variabl
-00012ff0: 6520 636f 6c6c 6563 7469 6f6e 7320 6e61  e collections na
-00013000: 6d65 7320 746f 2074 6865 6972 2060 6050  mes to their ``P
-00013010: 524e 474b 6579 7360 602c 2061 6e64 0a20  RNGKeys``, and. 
-00013020: 2020 2077 696c 6c20 6361 6c6c 2060 606d     will call ``m
-00013030: 6574 686f 6460 6020 2877 6869 6368 2069  ethod`` (which i
-00013040: 7320 7468 6520 6d6f 6475 6c65 2773 2060  s the module's `
-00013050: 605f 5f63 616c 6c5f 5f60 6020 6675 6e63  `__call__`` func
-00013060: 7469 6f6e 2062 790a 2020 2020 6465 6661  tion by.    defa
-00013070: 756c 7429 2070 6173 7369 6e67 2060 602a  ult) passing ``*
-00013080: 6172 6773 6060 2061 6e64 2060 602a 2a6b  args`` and ``**k
-00013090: 7761 7267 7360 602c 2061 6e64 2072 6574  wargs``, and ret
-000130a0: 7572 6e73 0a20 2020 2061 2064 6963 7469  urns.    a dicti
-000130b0: 6f6e 6172 7920 6f66 2069 6e69 7469 616c  onary of initial
-000130c0: 697a 6564 2076 6172 6961 626c 6573 2e0a  ized variables..
-000130d0: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
-000130e0: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-000130f0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
-00013100: 6e0a 2020 2020 2020 3e3e 3e20 696d 706f  n.      >>> impo
-00013110: 7274 206a 6178 2c20 6a61 782e 6e75 6d70  rt jax, jax.nump
-00013120: 7920 6173 206a 6e70 0a20 2020 2020 203e  y as jnp.      >
-00013130: 3e3e 2069 6d70 6f72 7420 6e75 6d70 7920  >> import numpy 
-00013140: 6173 206e 700a 0a20 2020 2020 203e 3e3e  as np..      >>>
-00013150: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
-00013160: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
-00013170: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
-00013180: 2020 2020 202e 2e2e 2020 2064 6566 205f       ...   def _
-00013190: 5f63 616c 6c5f 5f28 7365 6c66 2c20 782c  _call__(self, x,
-000131a0: 2074 7261 696e 293a 0a20 2020 2020 202e   train):.      .
-000131b0: 2e2e 2020 2020 2078 203d 206e 6e2e 4465  ..     x = nn.De
-000131c0: 6e73 6528 3136 2928 7829 0a20 2020 2020  nse(16)(x).     
-000131d0: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
-000131e0: 4261 7463 684e 6f72 6d28 7573 655f 7275  BatchNorm(use_ru
-000131f0: 6e6e 696e 675f 6176 6572 6167 653d 6e6f  nning_average=no
-00013200: 7420 7472 6169 6e29 2878 290a 2020 2020  t train)(x).    
-00013210: 2020 2e2e 2e20 2020 2020 7820 3d20 6e6e    ...     x = nn
-00013220: 2e72 656c 7528 7829 0a20 2020 2020 202e  .relu(x).      .
-00013230: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
-00013240: 2e44 656e 7365 2831 2928 7829 0a0a 2020  .Dense(1)(x)..  
-00013250: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
-00013260: 656d 7074 7928 2831 2c20 3729 290a 2020  empty((1, 7)).  
-00013270: 2020 2020 3e3e 3e20 6d6f 6475 6c65 203d      >>> module =
-00013280: 2046 6f6f 2829 0a20 2020 2020 203e 3e3e   Foo().      >>>
-00013290: 206b 6579 203d 206a 6178 2e72 616e 646f   key = jax.rando
-000132a0: 6d2e 6b65 7928 3029 0a20 2020 2020 203e  m.key(0).      >
-000132b0: 3e3e 2076 6172 6961 626c 6573 203d 206d  >> variables = m
-000132c0: 6f64 756c 652e 696e 6974 286b 6579 2c20  odule.init(key, 
-000132d0: 782c 2074 7261 696e 3d46 616c 7365 290a  x, train=False).
-000132e0: 0a20 2020 2049 6620 796f 7520 7061 7373  .    If you pass
-000132f0: 2061 2073 696e 676c 6520 6060 5052 4e47   a single ``PRNG
-00013300: 4b65 7960 602c 2046 6c61 7820 7769 6c6c  Key``, Flax will
-00013310: 2075 7365 2069 7420 746f 2066 6565 6420   use it to feed 
-00013320: 7468 6520 6060 2770 6172 616d 7327 6060  the ``'params'``
-00013330: 0a20 2020 2052 4e47 2073 7472 6561 6d2e  .    RNG stream.
-00013340: 2020 4966 2079 6f75 2077 616e 7420 746f    If you want to
-00013350: 2075 7365 2061 2064 6966 6665 7265 6e74   use a different
-00013360: 2052 4e47 2073 7472 6561 6d20 6f72 206e   RNG stream or n
-00013370: 6565 6420 746f 2075 7365 0a20 2020 206d  eed to use.    m
-00013380: 756c 7469 706c 6520 7374 7265 616d 732c  ultiple streams,
-00013390: 2079 6f75 2063 616e 2070 6173 7320 6120   you can pass a 
-000133a0: 6469 6374 696f 6e61 7279 206d 6170 7069  dictionary mappi
-000133b0: 6e67 2065 6163 6820 524e 4720 7374 7265  ng each RNG stre
-000133c0: 616d 206e 616d 650a 2020 2020 746f 2069  am name.    to i
-000133d0: 7473 2063 6f72 7265 7370 6f6e 6469 6e67  ts corresponding
-000133e0: 2060 6050 524e 474b 6579 6060 2074 6f20   ``PRNGKey`` to 
-000133f0: 6060 696e 6974 6060 2e20 4966 2060 6073  ``init``. If ``s
-00013400: 656c 662e 6d61 6b65 5f72 6e67 286e 616d  elf.make_rng(nam
-00013410: 6529 6060 0a20 2020 2069 7320 6361 6c6c  e)``.    is call
-00013420: 6564 206f 6e20 616e 2052 4e47 2073 7472  ed on an RNG str
-00013430: 6561 6d20 6e61 6d65 2074 6861 7420 6973  eam name that is
-00013440: 6e27 7420 7061 7373 6564 2062 7920 7468  n't passed by th
-00013450: 6520 7573 6572 2c20 6974 2077 696c 6c0a  e user, it will.
-00013460: 2020 2020 6465 6661 756c 7420 746f 2075      default to u
-00013470: 7369 6e67 2074 6865 2060 6027 7061 7261  sing the ``'para
-00013480: 6d73 2760 6020 524e 4720 7374 7265 616d  ms'`` RNG stream
-00013490: 2e0a 0a20 2020 2045 7861 6d70 6c65 3a3a  ...    Example::
-000134a0: 0a0a 2020 2020 2020 3e3e 3e20 636c 6173  ..      >>> clas
-000134b0: 7320 466f 6f28 6e6e 2e4d 6f64 756c 6529  s Foo(nn.Module)
-000134c0: 3a0a 2020 2020 2020 2e2e 2e20 2020 406e  :.      ...   @n
-000134d0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
-000134e0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
-000134f0: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
-00013500: 2020 202e 2e2e 2020 2020 2078 203d 206e     ...     x = n
-00013510: 6e2e 4465 6e73 6528 3136 2928 7829 0a20  n.Dense(16)(x). 
-00013520: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
-00013530: 206e 6e2e 7265 6c75 2878 290a 2020 2020   nn.relu(x).    
-00013540: 2020 2e2e 2e0a 2020 2020 2020 2e2e 2e20    ....      ... 
-00013550: 2020 2020 6f74 6865 725f 7661 7269 6162      other_variab
-00013560: 6c65 203d 2073 656c 662e 7661 7269 6162  le = self.variab
-00013570: 6c65 280a 2020 2020 2020 2e2e 2e20 2020  le(.      ...   
-00013580: 2020 2020 276f 7468 6572 5f63 6f6c 6c65      'other_colle
-00013590: 6374 696f 6e27 2c0a 2020 2020 2020 2e2e  ction',.      ..
-000135a0: 2e20 2020 2020 2020 276f 7468 6572 5f76  .       'other_v
-000135b0: 6172 6961 626c 6527 2c0a 2020 2020 2020  ariable',.      
-000135c0: 2e2e 2e20 2020 2020 2020 6c61 6d62 6461  ...       lambda
-000135d0: 2078 3a20 6a61 782e 7261 6e64 6f6d 2e6e   x: jax.random.n
-000135e0: 6f72 6d61 6c28 7365 6c66 2e6d 616b 655f  ormal(self.make_
-000135f0: 726e 6728 276f 7468 6572 5f72 6e67 2729  rng('other_rng')
-00013600: 2c20 782e 7368 6170 6529 2c0a 2020 2020  , x.shape),.    
-00013610: 2020 2e2e 2e20 2020 2020 2020 782c 0a20    ...       x,. 
-00013620: 2020 2020 202e 2e2e 2020 2020 2029 0a20       ...     ). 
-00013630: 2020 2020 202e 2e2e 2020 2020 2078 203d       ...     x =
-00013640: 2078 202b 206f 7468 6572 5f76 6172 6961   x + other_varia
-00013650: 626c 652e 7661 6c75 650a 2020 2020 2020  ble.value.      
-00013660: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
-00013670: 2020 7265 7475 726e 206e 6e2e 4465 6e73    return nn.Dens
-00013680: 6528 3129 2878 290a 0a20 2020 2020 203e  e(1)(x)..      >
-00013690: 3e3e 206d 6f64 756c 6520 3d20 466f 6f28  >> module = Foo(
-000136a0: 290a 2020 2020 2020 3e3e 3e20 726e 6773  ).      >>> rngs
-000136b0: 203d 207b 2770 6172 616d 7327 3a20 6a61   = {'params': ja
-000136c0: 782e 7261 6e64 6f6d 2e6b 6579 2830 292c  x.random.key(0),
-000136d0: 2027 6f74 6865 725f 726e 6727 3a20 6a61   'other_rng': ja
-000136e0: 782e 7261 6e64 6f6d 2e6b 6579 2831 297d  x.random.key(1)}
-000136f0: 0a20 2020 2020 203e 3e3e 2076 6172 6961  .      >>> varia
-00013700: 626c 6573 3020 3d20 6d6f 6475 6c65 2e69  bles0 = module.i
-00013710: 6e69 7428 726e 6773 2c20 7829 0a0a 2020  nit(rngs, x)..  
-00013720: 2020 2020 3e3e 3e20 726e 6773 5b27 6f74      >>> rngs['ot
-00013730: 6865 725f 726e 6727 5d20 3d20 6a61 782e  her_rng'] = jax.
-00013740: 7261 6e64 6f6d 2e6b 6579 2830 290a 2020  random.key(0).  
-00013750: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
-00013760: 7331 203d 206d 6f64 756c 652e 696e 6974  s1 = module.init
-00013770: 2872 6e67 732c 2078 290a 2020 2020 2020  (rngs, x).      
-00013780: 3e3e 3e20 2320 6571 7569 7661 6c65 6e74  >>> # equivalent
-00013790: 2070 6172 616d 7320 286b 6579 2830 2929   params (key(0))
-000137a0: 0a20 2020 2020 203e 3e3e 205f 203d 206a  .      >>> _ = j
-000137b0: 6178 2e74 7265 655f 7574 696c 2e74 7265  ax.tree_util.tre
-000137c0: 655f 6d61 7028 0a20 2020 2020 202e 2e2e  e_map(.      ...
-000137d0: 2020 206e 702e 7465 7374 696e 672e 6173     np.testing.as
-000137e0: 7365 7274 5f61 6c6c 636c 6f73 652c 2076  sert_allclose, v
-000137f0: 6172 6961 626c 6573 305b 2770 6172 616d  ariables0['param
-00013800: 7327 5d2c 2076 6172 6961 626c 6573 315b  s'], variables1[
-00013810: 2770 6172 616d 7327 5d0a 2020 2020 2020  'params'].      
-00013820: 2e2e 2e20 290a 2020 2020 2020 3e3e 3e20  ... ).      >>> 
-00013830: 2320 6469 6666 6572 656e 7420 6f74 6865  # different othe
-00013840: 725f 7661 7269 6162 6c65 2028 6b65 7928  r_variable (key(
-00013850: 3129 2076 7320 6b65 7928 3029 290a 2020  1) vs key(0)).  
-00013860: 2020 2020 3e3e 3e20 6e70 2e74 6573 7469      >>> np.testi
-00013870: 6e67 2e61 7373 6572 745f 7261 6973 6573  ng.assert_raises
-00013880: 280a 2020 2020 2020 2e2e 2e20 2020 4173  (.      ...   As
-00013890: 7365 7274 696f 6e45 7272 6f72 2c0a 2020  sertionError,.  
-000138a0: 2020 2020 2e2e 2e20 2020 6e70 2e74 6573      ...   np.tes
-000138b0: 7469 6e67 2e61 7373 6572 745f 616c 6c63  ting.assert_allc
-000138c0: 6c6f 7365 2c0a 2020 2020 2020 2e2e 2e20  lose,.      ... 
-000138d0: 2020 7661 7269 6162 6c65 7330 5b27 6f74    variables0['ot
-000138e0: 6865 725f 636f 6c6c 6563 7469 6f6e 275d  her_collection']
-000138f0: 5b27 6f74 6865 725f 7661 7269 6162 6c65  ['other_variable
-00013900: 275d 2c0a 2020 2020 2020 2e2e 2e20 2020  '],.      ...   
-00013910: 7661 7269 6162 6c65 7331 5b27 6f74 6865  variables1['othe
-00013920: 725f 636f 6c6c 6563 7469 6f6e 275d 5b27  r_collection']['
-00013930: 6f74 6865 725f 7661 7269 6162 6c65 275d  other_variable']
-00013940: 2c0a 2020 2020 2020 2e2e 2e20 290a 0a20  ,.      ... ).. 
-00013950: 2020 2020 203e 3e3e 2064 656c 2072 6e67       >>> del rng
-00013960: 735b 276f 7468 6572 5f72 6e67 275d 0a20  s['other_rng']. 
-00013970: 2020 2020 203e 3e3e 2023 2073 656c 662e       >>> # self.
-00013980: 6d61 6b65 5f72 6e67 2827 6f74 6865 725f  make_rng('other_
-00013990: 726e 6727 2920 7769 6c6c 2064 6566 6175  rng') will defau
-000139a0: 6c74 2074 6f20 7573 696e 6720 7468 6520  lt to using the 
-000139b0: 2770 6172 616d 7327 2052 4e47 2073 7472  'params' RNG str
-000139c0: 6561 6d0a 2020 2020 2020 3e3e 3e20 7661  eam.      >>> va
-000139d0: 7269 6162 6c65 7332 203d 206d 6f64 756c  riables2 = modul
-000139e0: 652e 696e 6974 2872 6e67 732c 2078 290a  e.init(rngs, x).
-000139f0: 2020 2020 2020 3e3e 3e20 2320 6571 7569        >>> # equi
-00013a00: 7661 6c65 6e74 2070 6172 616d 7320 286b  valent params (k
-00013a10: 6579 2830 2929 0a20 2020 2020 203e 3e3e  ey(0)).      >>>
-00013a20: 205f 203d 206a 6178 2e74 7265 655f 7574   _ = jax.tree_ut
-00013a30: 696c 2e74 7265 655f 6d61 7028 0a20 2020  il.tree_map(.   
-00013a40: 2020 202e 2e2e 2020 206e 702e 7465 7374     ...   np.test
-00013a50: 696e 672e 6173 7365 7274 5f61 6c6c 636c  ing.assert_allcl
-00013a60: 6f73 652c 2076 6172 6961 626c 6573 315b  ose, variables1[
-00013a70: 2770 6172 616d 7327 5d2c 2076 6172 6961  'params'], varia
-00013a80: 626c 6573 325b 2770 6172 616d 7327 5d0a  bles2['params'].
-00013a90: 2020 2020 2020 2e2e 2e20 290a 2020 2020        ... ).    
-00013aa0: 2020 3e3e 3e20 2320 6571 7569 7661 6c65    >>> # equivale
-00013ab0: 6e74 206f 7468 6572 5f76 6172 6961 626c  nt other_variabl
-00013ac0: 6520 286b 6579 2830 2929 0a20 2020 2020  e (key(0)).     
-00013ad0: 203e 3e3e 206e 702e 7465 7374 696e 672e   >>> np.testing.
-00013ae0: 6173 7365 7274 5f61 6c6c 636c 6f73 6528  assert_allclose(
-00013af0: 0a20 2020 2020 202e 2e2e 2020 2076 6172  .      ...   var
-00013b00: 6961 626c 6573 315b 276f 7468 6572 5f63  iables1['other_c
-00013b10: 6f6c 6c65 6374 696f 6e27 5d5b 276f 7468  ollection']['oth
-00013b20: 6572 5f76 6172 6961 626c 6527 5d2c 0a20  er_variable'],. 
-00013b30: 2020 2020 202e 2e2e 2020 2076 6172 6961       ...   varia
-00013b40: 626c 6573 325b 276f 7468 6572 5f63 6f6c  bles2['other_col
-00013b50: 6c65 6374 696f 6e27 5d5b 276f 7468 6572  lection']['other
-00013b60: 5f76 6172 6961 626c 6527 5d2c 0a20 2020  _variable'],.   
-00013b70: 2020 202e 2e2e 2029 0a0a 2020 2020 2020     ... )..      
-00013b80: 3e3e 3e20 2320 7061 7373 696e 6720 696e  >>> # passing in
-00013b90: 2061 2073 696e 676c 6520 6b65 7920 6973   a single key is
-00013ba0: 2065 7175 6976 616c 656e 7420 746f 2070   equivalent to p
-00013bb0: 6173 7369 6e67 2069 6e20 7b27 7061 7261  assing in {'para
-00013bc0: 6d73 273a 206b 6579 7d0a 2020 2020 2020  ms': key}.      
-00013bd0: 3e3e 3e20 7661 7269 6162 6c65 7333 203d  >>> variables3 =
-00013be0: 206d 6f64 756c 652e 696e 6974 286a 6178   module.init(jax
-00013bf0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
-00013c00: 7829 0a20 2020 2020 203e 3e3e 2023 2065  x).      >>> # e
-00013c10: 7175 6976 616c 656e 7420 7061 7261 6d73  quivalent params
-00013c20: 2028 6b65 7928 3029 290a 2020 2020 2020   (key(0)).      
-00013c30: 3e3e 3e20 5f20 3d20 6a61 782e 7472 6565  >>> _ = jax.tree
-00013c40: 5f75 7469 6c2e 7472 6565 5f6d 6170 280a  _util.tree_map(.
-00013c50: 2020 2020 2020 2e2e 2e20 2020 6e70 2e74        ...   np.t
-00013c60: 6573 7469 6e67 2e61 7373 6572 745f 616c  esting.assert_al
-00013c70: 6c63 6c6f 7365 2c20 7661 7269 6162 6c65  lclose, variable
-00013c80: 7332 5b27 7061 7261 6d73 275d 2c20 7661  s2['params'], va
-00013c90: 7269 6162 6c65 7333 5b27 7061 7261 6d73  riables3['params
-00013ca0: 275d 0a20 2020 2020 202e 2e2e 2029 0a20  '].      ... ). 
-00013cb0: 2020 2020 203e 3e3e 2023 2065 7175 6976       >>> # equiv
-00013cc0: 616c 656e 7420 6f74 6865 725f 7661 7269  alent other_vari
-00013cd0: 6162 6c65 2028 6b65 7928 3029 290a 2020  able (key(0)).  
-00013ce0: 2020 2020 3e3e 3e20 6e70 2e74 6573 7469      >>> np.testi
-00013cf0: 6e67 2e61 7373 6572 745f 616c 6c63 6c6f  ng.assert_allclo
-00013d00: 7365 280a 2020 2020 2020 2e2e 2e20 2020  se(.      ...   
-00013d10: 7661 7269 6162 6c65 7332 5b27 6f74 6865  variables2['othe
-00013d20: 725f 636f 6c6c 6563 7469 6f6e 275d 5b27  r_collection']['
-00013d30: 6f74 6865 725f 7661 7269 6162 6c65 275d  other_variable']
-00013d40: 2c0a 2020 2020 2020 2e2e 2e20 2020 7661  ,.      ...   va
-00013d50: 7269 6162 6c65 7333 5b27 6f74 6865 725f  riables3['other_
-00013d60: 636f 6c6c 6563 7469 6f6e 275d 5b27 6f74  collection']['ot
-00013d70: 6865 725f 7661 7269 6162 6c65 275d 2c0a  her_variable'],.
-00013d80: 2020 2020 2020 2e2e 2e20 290a 0a20 2020        ... )..   
-00013d90: 204a 6974 7469 6e67 2060 6069 6e69 7460   Jitting ``init`
-00013da0: 6020 696e 6974 6961 6c69 7a65 7320 6120  ` initializes a 
-00013db0: 6d6f 6465 6c20 6c61 7a69 6c79 2075 7369  model lazily usi
-00013dc0: 6e67 206f 6e6c 7920 7468 6520 7368 6170  ng only the shap
-00013dd0: 6573 206f 6620 7468 650a 2020 2020 7072  es of the.    pr
-00013de0: 6f76 6964 6564 2061 7267 756d 656e 7473  ovided arguments
-00013df0: 2c20 616e 6420 6176 6f69 6473 2063 6f6d  , and avoids com
-00013e00: 7075 7469 6e67 2074 6865 2066 6f72 7761  puting the forwa
-00013e10: 7264 2070 6173 7320 7769 7468 2061 6374  rd pass with act
-00013e20: 7561 6c0a 2020 2020 7661 6c75 6573 2e20  ual.    values. 
-00013e30: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
-00013e40: 203e 3e3e 206d 6f64 756c 6520 3d20 6e6e   >>> module = nn
-00013e50: 2e44 656e 7365 2831 290a 2020 2020 2020  .Dense(1).      
-00013e60: 3e3e 3e20 696e 6974 5f6a 6974 203d 206a  >>> init_jit = j
-00013e70: 6178 2e6a 6974 286d 6f64 756c 652e 696e  ax.jit(module.in
-00013e80: 6974 290a 2020 2020 2020 3e3e 3e20 7661  it).      >>> va
-00013e90: 7269 6162 6c65 7320 3d20 696e 6974 5f6a  riables = init_j
-00013ea0: 6974 286a 6178 2e72 616e 646f 6d2e 6b65  it(jax.random.ke
-00013eb0: 7928 3029 2c20 7829 0a0a 2020 2020 6060  y(0), x)..    ``
-00013ec0: 696e 6974 6060 2069 7320 6120 6c69 6768  init`` is a ligh
-00013ed0: 7420 7772 6170 7065 7220 6f76 6572 2060  t wrapper over `
-00013ee0: 6061 7070 6c79 6060 2c20 736f 206f 7468  `apply``, so oth
-00013ef0: 6572 2060 6061 7070 6c79 6060 2061 7267  er ``apply`` arg
-00013f00: 756d 656e 7473 0a20 2020 206c 696b 6520  uments.    like 
-00013f10: 6060 6d65 7468 6f64 6060 2c20 6060 6d75  ``method``, ``mu
-00013f20: 7461 626c 6560 602c 2061 6e64 2060 6063  table``, and ``c
-00013f30: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
-00013f40: 6174 6573 6060 2061 7265 2061 6c73 6f0a  ates`` are also.
-00013f50: 2020 2020 6176 6169 6c61 626c 652e 0a0a      available...
-00013f60: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
-00013f70: 726e 6773 3a20 5468 6520 726e 6773 2066  rngs: The rngs f
-00013f80: 6f72 2074 6865 2076 6172 6961 626c 6520  or the variable 
-00013f90: 636f 6c6c 6563 7469 6f6e 732e 0a20 2020  collections..   
-00013fa0: 2020 202a 6172 6773 3a20 4e61 6d65 6420     *args: Named 
-00013fb0: 6172 6775 6d65 6e74 7320 7061 7373 6564  arguments passed
-00013fc0: 2074 6f20 7468 6520 696e 6974 2066 756e   to the init fun
-00013fd0: 6374 696f 6e2e 0a20 2020 2020 206d 6574  ction..      met
-00013fe0: 686f 643a 2041 6e20 6f70 7469 6f6e 616c  hod: An optional
-00013ff0: 206d 6574 686f 642e 2049 6620 7072 6f76   method. If prov
-00014000: 6964 6564 2c20 6170 706c 6965 7320 7468  ided, applies th
-00014010: 6973 206d 6574 686f 642e 2049 6620 6e6f  is method. If no
-00014020: 740a 2020 2020 2020 2020 7072 6f76 6964  t.        provid
-00014030: 6564 2c20 6170 706c 6965 7320 7468 6520  ed, applies the 
-00014040: 6060 5f5f 6361 6c6c 5f5f 6060 206d 6574  ``__call__`` met
-00014050: 686f 642e 2041 2073 7472 696e 6720 6361  hod. A string ca
-00014060: 6e20 616c 736f 2062 6520 7072 6f76 6964  n also be provid
-00014070: 6564 0a20 2020 2020 2020 2074 6f20 7370  ed.        to sp
-00014080: 6563 6966 7920 6120 6d65 7468 6f64 2062  ecify a method b
-00014090: 7920 6e61 6d65 2e0a 2020 2020 2020 6d75  y name..      mu
-000140a0: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-000140b0: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-000140c0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-000140d0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-000140e0: 6f75 6c64 2062 650a 2020 2020 2020 2020  ould be.        
-000140f0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
-00014100: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
-00014110: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
-00014120: 2061 7265 206d 7574 6162 6c65 2e20 6060   are mutable. ``
-00014130: 7374 7260 603a 0a20 2020 2020 2020 2054  str``:.        T
-00014140: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
-00014150: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-00014160: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-00014170: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
-00014180: 7320 6f66 0a20 2020 2020 2020 206d 7574  s of.        mut
-00014190: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
-000141a0: 2e20 4279 2064 6566 6175 6c74 2061 6c6c  . By default all
-000141b0: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
-000141c0: 6570 7420 2269 6e74 6572 6d65 6469 6174  ept "intermediat
-000141d0: 6573 220a 2020 2020 2020 2020 6172 6520  es".        are 
-000141e0: 6d75 7461 626c 652e 0a20 2020 2020 2063  mutable..      c
-000141f0: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
-00014200: 6174 6573 3a20 4966 2060 6054 7275 6560  ates: If ``True`
-00014210: 602c 2063 6170 7475 7265 7320 696e 7465  `, captures inte
-00014220: 726d 6564 6961 7465 2072 6574 7572 6e20  rmediate return 
-00014230: 7661 6c75 6573 206f 660a 2020 2020 2020  values of.      
-00014240: 2020 616c 6c20 4d6f 6475 6c65 7320 696e    all Modules in
-00014250: 7369 6465 2074 6865 2022 696e 7465 726d  side the "interm
-00014260: 6564 6961 7465 7322 2063 6f6c 6c65 6374  ediates" collect
-00014270: 696f 6e2e 2042 7920 6465 6661 756c 7420  ion. By default 
-00014280: 6f6e 6c79 2074 6865 0a20 2020 2020 2020  only the.       
-00014290: 2072 6574 7572 6e20 7661 6c75 6573 206f   return values o
-000142a0: 6620 616c 6c20 6060 5f5f 6361 6c6c 5f5f  f all ``__call__
-000142b0: 6060 206d 6574 686f 6473 2061 7265 2073  `` methods are s
-000142c0: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
-000142d0: 6e20 6361 6e20 6265 0a20 2020 2020 2020  n can be.       
-000142e0: 2070 6173 7365 6420 746f 2063 6861 6e67   passed to chang
-000142f0: 6520 7468 6520 6669 6c74 6572 2062 6568  e the filter beh
-00014300: 6176 696f 722e 2054 6865 2066 696c 7465  avior. The filte
-00014310: 7220 6675 6e63 7469 6f6e 2074 616b 6573  r function takes
-00014320: 2074 6865 0a20 2020 2020 2020 204d 6f64   the.        Mod
-00014330: 756c 6520 696e 7374 616e 6365 2061 6e64  ule instance and
-00014340: 206d 6574 686f 6420 6e61 6d65 2061 6e64   method name and
-00014350: 2072 6574 7572 6e73 2061 2062 6f6f 6c20   returns a bool 
-00014360: 696e 6469 6361 7469 6e67 2077 6865 7468  indicating wheth
-00014370: 6572 0a20 2020 2020 2020 2074 6865 206f  er.        the o
-00014380: 7574 7075 7420 6f66 2074 6861 7420 6d65  utput of that me
-00014390: 7468 6f64 2069 6e76 6f63 6174 696f 6e20  thod invocation 
-000143a0: 7368 6f75 6c64 2062 6520 7374 6f72 6564  should be stored
-000143b0: 2e0a 2020 2020 2020 2a2a 6b77 6172 6773  ..      **kwargs
-000143c0: 3a20 4b65 7977 6f72 6420 6172 6775 6d65  : Keyword argume
-000143d0: 6e74 7320 7061 7373 6564 2074 6f20 7468  nts passed to th
-000143e0: 6520 696e 6974 2066 756e 6374 696f 6e2e  e init function.
-000143f0: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. 
-00014400: 2020 2020 2054 6865 2069 6e69 7469 616c       The initial
-00014410: 697a 6564 2076 6172 6961 626c 6520 6469  ized variable di
-00014420: 6374 2e0a 2020 2020 2222 220a 2020 2020  ct..    """.    
-00014430: 4d6f 6475 6c65 2e5f 6d6f 6475 6c65 5f63  Module._module_c
-00014440: 6865 636b 7328 7365 6c66 290a 0a20 2020  hecks(self)..   
-00014450: 205f 2c20 765f 6f75 7420 3d20 7365 6c66   _, v_out = self
-00014460: 2e69 6e69 745f 7769 7468 5f6f 7574 7075  .init_with_outpu
-00014470: 7428 0a20 2020 2020 2072 6e67 732c 0a20  t(.      rngs,. 
-00014480: 2020 2020 202a 6172 6773 2c0a 2020 2020       *args,.    
-00014490: 2020 6d65 7468 6f64 3d6d 6574 686f 642c    method=method,
-000144a0: 0a20 2020 2020 206d 7574 6162 6c65 3d6d  .      mutable=m
-000144b0: 7574 6162 6c65 2c0a 2020 2020 2020 6361  utable,.      ca
-000144c0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-000144d0: 7465 733d 6361 7074 7572 655f 696e 7465  tes=capture_inte
-000144e0: 726d 6564 6961 7465 732c 0a20 2020 2020  rmediates,.     
-000144f0: 202a 2a6b 7761 7267 732c 0a20 2020 2029   **kwargs,.    )
-00014500: 0a20 2020 2072 6574 7572 6e20 765f 6f75  .    return v_ou
-00014510: 740a 0a20 2040 7472 6163 6562 6163 6b5f  t..  @traceback_
-00014520: 7574 696c 2e61 7069 5f62 6f75 6e64 6172  util.api_boundar
-00014530: 790a 2020 6465 6620 6c61 7a79 5f69 6e69  y.  def lazy_ini
-00014540: 7428 0a20 2020 2073 656c 662c 0a20 2020  t(.    self,.   
-00014550: 2072 6e67 733a 2055 6e69 6f6e 5b50 524e   rngs: Union[PRN
-00014560: 474b 6579 2c20 524e 4753 6571 7565 6e63  GKey, RNGSequenc
-00014570: 6573 5d2c 0a20 2020 202a 6172 6773 2c0a  es],.    *args,.
-00014580: 2020 2020 6d65 7468 6f64 3a20 4f70 7469      method: Opti
-00014590: 6f6e 616c 5b43 616c 6c61 626c 655b 2e2e  onal[Callable[..
-000145a0: 2e2c 2041 6e79 5d5d 203d 204e 6f6e 652c  ., Any]] = None,
-000145b0: 0a20 2020 206d 7574 6162 6c65 3a20 436f  .    mutable: Co
-000145c0: 6c6c 6563 7469 6f6e 4669 6c74 6572 203d  llectionFilter =
-000145d0: 2044 656e 794c 6973 7428 2769 6e74 6572   DenyList('inter
-000145e0: 6d65 6469 6174 6573 2729 2c0a 2020 2020  mediates'),.    
-000145f0: 2a2a 6b77 6172 6773 2c0a 2020 2920 2d3e  **kwargs,.  ) ->
-00014600: 2046 726f 7a65 6e56 6172 6961 626c 6544   FrozenVariableD
-00014610: 6963 743a 0a20 2020 2022 2222 496e 6974  ict:.    """Init
-00014620: 6961 6c69 7a65 7320 6120 6d6f 6475 6c65  ializes a module
-00014630: 2077 6974 686f 7574 2063 6f6d 7075 7469   without computi
-00014640: 6e67 206f 6e20 616e 2061 6374 7561 6c20  ng on an actual 
-00014650: 696e 7075 742e 0a0a 2020 2020 6c61 7a79  input...    lazy
-00014660: 5f69 6e69 7420 7769 6c6c 2069 6e69 7469  _init will initi
-00014670: 616c 697a 6520 7468 6520 7661 7269 6162  alize the variab
-00014680: 6c65 7320 7769 7468 6f75 7420 646f 696e  les without doin
-00014690: 6720 756e 6e65 6365 7373 6172 7920 636f  g unnecessary co
-000146a0: 6d70 7574 652e 0a20 2020 2054 6865 2069  mpute..    The i
-000146b0: 6e70 7574 2064 6174 6120 7368 6f75 6c64  nput data should
-000146c0: 2062 6520 7061 7373 6564 2061 7320 6120   be passed as a 
-000146d0: 6060 6a61 782e 5368 6170 6544 7479 7065  ``jax.ShapeDtype
-000146e0: 5374 7275 6374 6060 2077 6869 6368 0a20  Struct`` which. 
-000146f0: 2020 2073 7065 6369 6669 6573 2074 6865     specifies the
-00014700: 2073 6861 7065 2061 6e64 2064 7479 7065   shape and dtype
-00014710: 206f 6620 7468 6520 696e 7075 7420 6275   of the input bu
-00014720: 7420 6e6f 2063 6f6e 6372 6574 6520 6461  t no concrete da
-00014730: 7461 2e0a 0a20 2020 2045 7861 6d70 6c65  ta...    Example
-00014740: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
-00014750: 6465 6c20 3d20 6e6e 2e44 656e 7365 2866  del = nn.Dense(f
-00014760: 6561 7475 7265 733d 3235 3629 0a20 2020  eatures=256).   
-00014770: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
-00014780: 203d 206d 6f64 656c 2e6c 617a 795f 696e   = model.lazy_in
-00014790: 6974 280a 2020 2020 2020 2e2e 2e20 2020  it(.      ...   
-000147a0: 2020 6a61 782e 7261 6e64 6f6d 2e6b 6579    jax.random.key
-000147b0: 2830 292c 206a 6178 2e53 6861 7065 4474  (0), jax.ShapeDt
-000147c0: 7970 6553 7472 7563 7428 2831 2c20 3132  ypeStruct((1, 12
-000147d0: 3829 2c20 6a6e 702e 666c 6f61 7433 3229  8), jnp.float32)
-000147e0: 290a 0a20 2020 2054 6865 2061 7267 7320  )..    The args 
-000147f0: 616e 6420 6b77 6172 6773 2061 7267 7320  and kwargs args 
-00014800: 7061 7373 6564 2074 6f20 6060 6c61 7a79  passed to ``lazy
-00014810: 5f69 6e69 7460 6020 6361 6e20 6265 2061  _init`` can be a
-00014820: 206d 6978 206f 660a 2020 2020 636f 6e63   mix of.    conc
-00014830: 7265 7465 2028 6a61 7820 6172 7261 7973  rete (jax arrays
-00014840: 2c20 7363 616c 6172 732c 2062 6f6f 6c73  , scalars, bools
-00014850: 2920 616e 6420 6162 7374 7261 6374 2028  ) and abstract (
-00014860: 5368 6170 6544 7479 7065 5374 7275 6374  ShapeDtypeStruct
-00014870: 290a 2020 2020 7661 6c75 6573 2e20 436f  ).    values. Co
-00014880: 6e63 7265 7465 2076 616c 7565 7320 6172  ncrete values ar
-00014890: 6520 6f6e 6c79 206e 6563 6573 7361 7279  e only necessary
-000148a0: 2066 6f72 2061 7267 756d 656e 7473 2074   for arguments t
-000148b0: 6861 7420 6166 6665 6374 0a20 2020 2074  hat affect.    t
-000148c0: 6865 2069 6e69 7469 616c 697a 6174 696f  he initializatio
-000148d0: 6e20 6f66 2076 6172 6961 626c 6573 2e20  n of variables. 
-000148e0: 466f 7220 6578 616d 706c 652c 2074 6865  For example, the
-000148f0: 206d 6f64 656c 206d 6967 6874 2065 7870   model might exp
-00014900: 6563 740a 2020 2020 6120 6b65 7977 6f72  ect.    a keywor
-00014910: 6420 6172 6720 7468 6174 2065 6e61 626c  d arg that enabl
-00014920: 6573 2f64 6973 6162 6c65 7320 6120 7375  es/disables a su
-00014930: 6270 6172 7420 6f66 2074 6865 206d 6f64  bpart of the mod
-00014940: 656c 2e0a 2020 2020 496e 2074 6869 7320  el..    In this 
-00014950: 6361 7365 2c20 616e 2065 7870 6c69 6369  case, an explici
-00014960: 7420 7661 6c75 6520 2854 7275 652f 466c  t value (True/Fl
-00014970: 6173 6529 2073 686f 756c 6420 6265 2070  ase) should be p
-00014980: 6173 7365 6420 6f74 6865 7277 6973 650a  assed otherwise.
-00014990: 2020 2020 6060 6c61 7a79 5f69 6e69 7460      ``lazy_init`
-000149a0: 6020 6361 6e6e 6f74 2069 6e66 6572 2077  ` cannot infer w
-000149b0: 6869 6368 2076 6172 6961 626c 6573 2073  hich variables s
-000149c0: 686f 756c 6420 6265 2069 6e69 7469 616c  hould be initial
-000149d0: 697a 6564 2e0a 0a20 2020 2041 7267 733a  ized...    Args:
-000149e0: 0a20 2020 2020 2072 6e67 733a 2054 6865  .      rngs: The
-000149f0: 2072 6e67 7320 666f 7220 7468 6520 7661   rngs for the va
-00014a00: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
-00014a10: 6e73 2e0a 2020 2020 2020 2a61 7267 733a  ns..      *args:
-00014a20: 2061 7267 756d 656e 7473 2070 6173 7365   arguments passe
-00014a30: 6420 746f 2074 6865 2069 6e69 7420 6675  d to the init fu
-00014a40: 6e63 7469 6f6e 2e0a 2020 2020 2020 6d65  nction..      me
-00014a50: 7468 6f64 3a20 416e 206f 7074 696f 6e61  thod: An optiona
-00014a60: 6c20 6d65 7468 6f64 2e20 4966 2070 726f  l method. If pro
-00014a70: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
-00014a80: 6869 7320 6d65 7468 6f64 2e20 4966 206e  his method. If n
-00014a90: 6f74 0a20 2020 2020 2020 2070 726f 7669  ot.        provi
-00014aa0: 6465 642c 2061 7070 6c69 6573 2074 6865  ded, applies the
-00014ab0: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
-00014ac0: 7468 6f64 2e0a 2020 2020 2020 6d75 7461  thod..      muta
-00014ad0: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
-00014ae0: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
-00014af0: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
-00014b00: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
-00014b10: 6c64 2062 650a 2020 2020 2020 2020 7472  ld be.        tr
-00014b20: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
-00014b30: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
-00014b40: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
-00014b50: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
-00014b60: 7260 603a 0a20 2020 2020 2020 2054 6865  r``:.        The
-00014b70: 206e 616d 6520 6f66 2061 2073 696e 676c   name of a singl
-00014b80: 6520 6d75 7461 626c 6520 636f 6c6c 6563  e mutable collec
-00014b90: 7469 6f6e 2e20 6060 6c69 7374 6060 3a20  tion. ``list``: 
-00014ba0: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
-00014bb0: 6f66 0a20 2020 2020 2020 206d 7574 6162  of.        mutab
-00014bc0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2e20  le collections. 
-00014bd0: 4279 2064 6566 6175 6c74 2061 6c6c 2063  By default all c
-00014be0: 6f6c 6c65 6374 696f 6e73 2065 7863 6570  ollections excep
-00014bf0: 7420 2269 6e74 6572 6d65 6469 6174 6573  t "intermediates
-00014c00: 220a 2020 2020 2020 2020 6172 6520 6d75  ".        are mu
-00014c10: 7461 626c 652e 0a20 2020 2020 202a 2a6b  table..      **k
-00014c20: 7761 7267 733a 204b 6579 776f 7264 2061  wargs: Keyword a
-00014c30: 7267 756d 656e 7473 2070 6173 7365 6420  rguments passed 
-00014c40: 746f 2074 6865 2069 6e69 7420 6675 6e63  to the init func
-00014c50: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
-00014c60: 6e73 3a0a 2020 2020 2020 5468 6520 696e  ns:.      The in
-00014c70: 6974 6961 6c69 7a65 6420 7661 7269 6162  itialized variab
-00014c80: 6c65 2064 6963 742e 0a20 2020 2022 2222  le dict..    """
-00014c90: 0a20 2020 204d 6f64 756c 652e 5f6d 6f64  .    Module._mod
-00014ca0: 756c 655f 6368 6563 6b73 2873 656c 6629  ule_checks(self)
-00014cb0: 0a0a 2020 2020 6465 6620 6c61 7a79 5f77  ..    def lazy_w
-00014cc0: 7261 7070 6572 2872 6e67 732c 202a 6172  rapper(rngs, *ar
-00014cd0: 6773 2c20 2a2a 6b77 6172 6773 293a 0a20  gs, **kwargs):. 
-00014ce0: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-00014cf0: 2e69 6e69 7428 726e 6773 2c20 2a61 7267  .init(rngs, *arg
-00014d00: 732c 206d 6574 686f 643d 6d65 7468 6f64  s, method=method
-00014d10: 2c20 6d75 7461 626c 653d 6d75 7461 626c  , mutable=mutabl
-00014d20: 652c 202a 2a6b 7761 7267 7329 0a0a 2020  e, **kwargs)..  
-00014d30: 2020 7265 7475 726e 2070 6172 7469 616c    return partial
-00014d40: 5f65 7661 6c2e 6c61 7a79 5f69 6e69 7428  _eval.lazy_init(
-00014d50: 6c61 7a79 5f77 7261 7070 6572 2928 726e  lazy_wrapper)(rn
-00014d60: 6773 2c20 2a61 7267 732c 202a 2a6b 7761  gs, *args, **kwa
-00014d70: 7267 7329 0a0a 2020 4070 726f 7065 7274  rgs)..  @propert
-00014d80: 790a 2020 6465 6620 7661 7269 6162 6c65  y.  def variable
-00014d90: 7328 7365 6c66 2920 2d3e 2056 6172 6961  s(self) -> Varia
-00014da0: 626c 6544 6963 743a 0a20 2020 2022 2222  bleDict:.    """
-00014db0: 5265 7475 726e 7320 7468 6520 7661 7269  Returns the vari
-00014dc0: 6162 6c65 7320 696e 2074 6869 7320 6d6f  ables in this mo
-00014dd0: 6475 6c65 2e22 2222 0a20 2020 2069 6620  dule.""".    if 
-00014de0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
-00014df0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
-00014e00: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
-00014e10: 7420 6163 6365 7373 2076 6172 6961 626c  t access variabl
-00014e20: 6573 206f 6e20 756e 626f 756e 6420 6d6f  es on unbound mo
-00014e30: 6475 6c65 7322 290a 2020 2020 7265 7475  dules").    retu
-00014e40: 726e 2073 656c 662e 7363 6f70 652e 7661  rn self.scope.va
-00014e50: 7269 6162 6c65 7328 290a 0a20 2064 6566  riables()..  def
-00014e60: 2067 6574 5f76 6172 6961 626c 6528 7365   get_variable(se
-00014e70: 6c66 2c20 636f 6c3a 2073 7472 2c20 6e61  lf, col: str, na
-00014e80: 6d65 3a20 7374 722c 2064 6566 6175 6c74  me: str, default
-00014e90: 3a20 4f70 7469 6f6e 616c 5b54 5d20 3d20  : Optional[T] = 
-00014ea0: 4e6f 6e65 2920 2d3e 2054 3a0a 2020 2020  None) -> T:.    
-00014eb0: 2222 2252 6574 7269 6576 6573 2074 6865  """Retrieves the
-00014ec0: 2076 616c 7565 206f 6620 6120 5661 7269   value of a Vari
-00014ed0: 6162 6c65 2e0a 0a20 2020 2041 7267 733a  able...    Args:
-00014ee0: 0a20 2020 2020 2063 6f6c 3a20 7468 6520  .      col: the 
-00014ef0: 7661 7269 6162 6c65 2063 6f6c 6c65 6374  variable collect
-00014f00: 696f 6e2e 0a20 2020 2020 206e 616d 653a  ion..      name:
-00014f10: 2074 6865 206e 616d 6520 6f66 2074 6865   the name of the
-00014f20: 2076 6172 6961 626c 652e 0a20 2020 2020   variable..     
-00014f30: 2064 6566 6175 6c74 3a20 7468 6520 6465   default: the de
-00014f40: 6661 756c 7420 7661 6c75 6520 746f 2072  fault value to r
-00014f50: 6574 7572 6e20 6966 2074 6865 2076 6172  eturn if the var
-00014f60: 6961 626c 6520 646f 6573 206e 6f74 2065  iable does not e
-00014f70: 7869 7374 2069 6e0a 2020 2020 2020 2020  xist in.        
-00014f80: 7468 6973 2073 636f 7065 2e0a 0a20 2020  this scope...   
-00014f90: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      
-00014fa0: 5468 6520 7661 6c75 6520 6f66 2074 6865  The value of the
-00014fb0: 2069 6e70 7574 2076 6172 6961 626c 652c   input variable,
-00014fc0: 206f 6620 7468 6520 6465 6661 756c 7420   of the default 
-00014fd0: 7661 6c75 6520 6966 2074 6865 2076 6172  value if the var
-00014fe0: 6961 626c 650a 2020 2020 2020 646f 6573  iable.      does
-00014ff0: 6e27 7420 6578 6973 7420 696e 2074 6869  n't exist in thi
-00015000: 7320 7363 6f70 652e 0a20 2020 2022 2222  s scope..    """
-00015010: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
-00015020: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
-00015030: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-00015040: 6f72 2822 4361 6e27 7420 6163 6365 7373  or("Can't access
-00015050: 2076 6172 6961 626c 6573 206f 6e20 756e   variables on un
-00015060: 626f 756e 6420 6d6f 6475 6c65 7322 290a  bound modules").
-00015070: 2020 2020 7265 7475 726e 2073 656c 662e      return self.
-00015080: 7363 6f70 652e 6765 745f 7661 7269 6162  scope.get_variab
-00015090: 6c65 2863 6f6c 2c20 6e61 6d65 2c20 6465  le(col, name, de
-000150a0: 6661 756c 7429 0a0a 2020 6465 6620 7075  fault)..  def pu
-000150b0: 745f 7661 7269 6162 6c65 2873 656c 662c  t_variable(self,
-000150c0: 2063 6f6c 3a20 7374 722c 206e 616d 653a   col: str, name:
-000150d0: 2073 7472 2c20 7661 6c75 653a 2041 6e79   str, value: Any
-000150e0: 293a 0a20 2020 2022 2222 5570 6461 7465  ):.    """Update
-000150f0: 7320 7468 6520 7661 6c75 6520 6f66 2074  s the value of t
-00015100: 6865 2067 6976 656e 2076 6172 6961 626c  he given variabl
-00015110: 6520 6966 2069 7420 6973 206d 7574 6162  e if it is mutab
-00015120: 6c65 2c20 6f72 2061 6e20 6572 726f 7220  le, or an error 
-00015130: 6f74 6865 7277 6973 652e 0a0a 2020 2020  otherwise...    
-00015140: 4172 6773 3a0a 2020 2020 2020 636f 6c3a  Args:.      col:
-00015150: 2074 6865 2076 6172 6961 626c 6520 636f   the variable co
-00015160: 6c6c 6563 7469 6f6e 2e0a 2020 2020 2020  llection..      
-00015170: 6e61 6d65 3a20 7468 6520 6e61 6d65 206f  name: the name o
-00015180: 6620 7468 6520 7661 7269 6162 6c65 2e0a  f the variable..
-00015190: 2020 2020 2020 7661 6c75 653a 2074 6865        value: the
-000151a0: 206e 6577 2076 616c 7565 206f 6620 7468   new value of th
-000151b0: 6520 7661 7269 6162 6c65 2e0a 2020 2020  e variable..    
-000151c0: 2222 220a 2020 2020 6966 2073 656c 662e  """.    if self.
-000151d0: 7363 6f70 6520 6973 204e 6f6e 653a 0a20  scope is None:. 
-000151e0: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-000151f0: 4572 726f 7228 2243 616e 2774 2061 6363  Error("Can't acc
-00015200: 6573 7320 7661 7269 6162 6c65 7320 6f6e  ess variables on
-00015210: 2075 6e62 6f75 6e64 206d 6f64 756c 6573   unbound modules
-00015220: 2229 0a20 2020 2073 656c 662e 7363 6f70  ").    self.scop
-00015230: 652e 7075 745f 7661 7269 6162 6c65 2863  e.put_variable(c
-00015240: 6f6c 2c20 6e61 6d65 2c20 7661 6c75 6529  ol, name, value)
-00015250: 0a0a 2020 406f 7665 726c 6f61 640a 2020  ..  @overload.  
-00015260: 6465 6620 736f 7728 7365 6c66 2c20 636f  def sow(self, co
-00015270: 6c3a 2073 7472 2c20 6e61 6d65 3a20 7374  l: str, name: st
-00015280: 722c 2076 616c 7565 3a20 416e 7929 202d  r, value: Any) -
-00015290: 3e20 626f 6f6c 3a0a 2020 2020 2e2e 2e0a  > bool:.    ....
-000152a0: 0a20 2040 6f76 6572 6c6f 6164 0a20 2064  .  @overload.  d
-000152b0: 6566 2073 6f77 280a 2020 2020 7365 6c66  ef sow(.    self
-000152c0: 2c0a 2020 2020 636f 6c3a 2073 7472 2c0a  ,.    col: str,.
-000152d0: 2020 2020 6e61 6d65 3a20 7374 722c 0a20      name: str,. 
-000152e0: 2020 2076 616c 7565 3a20 542c 0a20 2020     value: T,.   
-000152f0: 2072 6564 7563 655f 666e 3a20 4361 6c6c   reduce_fn: Call
-00015300: 6162 6c65 5b5b 4b2c 2054 5d2c 204b 5d20  able[[K, T], K] 
-00015310: 3d20 7475 706c 655f 7265 6475 6365 2c0a  = tuple_reduce,.
-00015320: 2020 2020 696e 6974 5f66 6e3a 2043 616c      init_fn: Cal
-00015330: 6c61 626c 655b 5b5d 2c20 4b5d 203d 2074  lable[[], K] = t
-00015340: 7570 6c65 5f69 6e69 742c 2020 2320 7479  uple_init,  # ty
-00015350: 7065 3a20 6967 6e6f 7265 0a20 2029 202d  pe: ignore.  ) -
-00015360: 3e20 626f 6f6c 3a0a 2020 2020 2e2e 2e0a  > bool:.    ....
-00015370: 0a20 2064 6566 2073 6f77 280a 2020 2020  .  def sow(.    
-00015380: 7365 6c66 2c0a 2020 2020 636f 6c3a 2073  self,.    col: s
-00015390: 7472 2c0a 2020 2020 6e61 6d65 3a20 7374  tr,.    name: st
-000153a0: 722c 0a20 2020 2076 616c 7565 3a20 542c  r,.    value: T,
-000153b0: 0a20 2020 2072 6564 7563 655f 666e 3a20  .    reduce_fn: 
-000153c0: 4361 6c6c 6162 6c65 5b5b 4b2c 2054 5d2c  Callable[[K, T],
-000153d0: 204b 5d20 3d20 7475 706c 655f 7265 6475   K] = tuple_redu
-000153e0: 6365 2c0a 2020 2020 696e 6974 5f66 6e3a  ce,.    init_fn:
-000153f0: 2043 616c 6c61 626c 655b 5b5d 2c20 4b5d   Callable[[], K]
-00015400: 203d 2074 7570 6c65 5f69 6e69 742c 2020   = tuple_init,  
-00015410: 2320 7479 7065 3a20 6967 6e6f 7265 0a20  # type: ignore. 
-00015420: 2029 202d 3e20 626f 6f6c 3a0a 2020 2020   ) -> bool:.    
-00015430: 2222 2253 746f 7265 7320 6120 7661 6c75  """Stores a valu
-00015440: 6520 696e 2061 2063 6f6c 6c65 6374 696f  e in a collectio
-00015450: 6e2e 0a0a 2020 2020 436f 6c6c 6563 7469  n...    Collecti
-00015460: 6f6e 7320 6361 6e20 6265 2075 7365 6420  ons can be used 
-00015470: 746f 2063 6f6c 6c65 6374 2069 6e74 6572  to collect inter
-00015480: 6d65 6469 6174 6520 7661 6c75 6573 2077  mediate values w
-00015490: 6974 686f 7574 0a20 2020 2074 6865 206f  ithout.    the o
-000154a0: 7665 7268 6561 6420 6f66 2065 7870 6c69  verhead of expli
-000154b0: 6369 746c 7920 7061 7373 696e 6720 6120  citly passing a 
-000154c0: 636f 6e74 6169 6e65 7220 7468 726f 7567  container throug
-000154d0: 6820 6561 6368 204d 6f64 756c 6520 6361  h each Module ca
-000154e0: 6c6c 2e0a 0a20 2020 2049 6620 7468 6520  ll...    If the 
-000154f0: 7461 7267 6574 2063 6f6c 6c65 6374 696f  target collectio
-00015500: 6e20 6973 206e 6f74 206d 7574 6162 6c65  n is not mutable
-00015510: 2060 6073 6f77 6060 2062 6568 6176 6573   ``sow`` behaves
-00015520: 206c 696b 6520 6120 6e6f 2d6f 700a 2020   like a no-op.  
-00015530: 2020 616e 6420 7265 7475 726e 7320 6060    and returns ``
-00015540: 4661 6c73 6560 602e 0a0a 2020 2020 4578  False``...    Ex
-00015550: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
-00015560: 3e3e 2069 6d70 6f72 7420 6a61 780a 2020  >> import jax.  
-00015570: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
-00015580: 6178 2e6e 756d 7079 2061 7320 6a6e 700a  ax.numpy as jnp.
-00015590: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
-000155a0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
-000155b0: 6e0a 0a20 2020 2020 203e 3e3e 2063 6c61  n..      >>> cla
-000155c0: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
-000155d0: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
-000155e0: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
-000155f0: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
-00015600: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
-00015610: 2020 2020 2e2e 2e20 2020 2020 6820 3d20      ...     h = 
-00015620: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
-00015630: 2020 2020 202e 2e2e 2020 2020 2073 656c       ...     sel
-00015640: 662e 736f 7728 2769 6e74 6572 6d65 6469  f.sow('intermedi
-00015650: 6174 6573 272c 2027 6827 2c20 6829 0a20  ates', 'h', h). 
-00015660: 2020 2020 202e 2e2e 2020 2020 2072 6574       ...     ret
-00015670: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
-00015680: 6829 0a0a 2020 2020 2020 3e3e 3e20 7820  h)..      >>> x 
-00015690: 3d20 6a6e 702e 6f6e 6573 2828 3136 2c20  = jnp.ones((16, 
-000156a0: 3929 290a 2020 2020 2020 3e3e 3e20 6d6f  9)).      >>> mo
-000156b0: 6465 6c20 3d20 466f 6f28 290a 2020 2020  del = Foo().    
-000156c0: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
-000156d0: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
-000156e0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
-000156f0: 7829 0a20 2020 2020 203e 3e3e 2079 2c20  x).      >>> y, 
-00015700: 7374 6174 6520 3d20 6d6f 6465 6c2e 6170  state = model.ap
-00015710: 706c 7928 7661 7269 6162 6c65 732c 2078  ply(variables, x
-00015720: 2c20 6d75 7461 626c 653d 5b27 696e 7465  , mutable=['inte
-00015730: 726d 6564 6961 7465 7327 5d29 0a20 2020  rmediates']).   
-00015740: 2020 203e 3e3e 206a 6178 2e74 7265 652e     >>> jax.tree.
-00015750: 6d61 7028 6a6e 702e 7368 6170 652c 2073  map(jnp.shape, s
-00015760: 7461 7465 5b27 696e 7465 726d 6564 6961  tate['intermedia
-00015770: 7465 7327 5d29 0a20 2020 2020 207b 2768  tes']).      {'h
-00015780: 273a 2028 2831 362c 2034 292c 297d 0a0a  ': ((16, 4),)}..
-00015790: 2020 2020 4279 2064 6566 6175 6c74 2074      By default t
-000157a0: 6865 2076 616c 7565 7320 6172 6520 7374  he values are st
-000157b0: 6f72 6564 2069 6e20 6120 7475 706c 6520  ored in a tuple 
-000157c0: 616e 6420 6561 6368 2073 746f 7265 6420  and each stored 
-000157d0: 7661 6c75 650a 2020 2020 6973 2061 7070  value.    is app
-000157e0: 656e 6465 6420 6174 2074 6865 2065 6e64  ended at the end
-000157f0: 2e20 5468 6973 2077 6179 2061 6c6c 2069  . This way all i
-00015800: 6e74 6572 6d65 6469 6174 6573 2063 616e  ntermediates can
-00015810: 2062 6520 7472 6163 6b65 6420 7768 656e   be tracked when
-00015820: 0a20 2020 2074 6865 2073 616d 6520 6d6f  .    the same mo
-00015830: 6475 6c65 2069 7320 6361 6c6c 6564 206d  dule is called m
-00015840: 756c 7469 706c 6520 7469 6d65 732e 2041  ultiple times. A
-00015850: 6c74 6572 6e61 7469 7665 6c79 2c20 6120  lternatively, a 
-00015860: 6375 7374 6f6d 0a20 2020 2069 6e69 742f  custom.    init/
-00015870: 7265 6475 6365 2066 756e 6374 696f 6e20  reduce function 
-00015880: 6361 6e20 6265 2070 6173 7365 643a 3a0a  can be passed::.
-00015890: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
-000158a0: 2046 6f6f 3228 6e6e 2e4d 6f64 756c 6529   Foo2(nn.Module)
-000158b0: 3a0a 2020 2020 2020 2e2e 2e20 2020 406e  :.      ...   @n
-000158c0: 6e2e 636f 6d70 6163 740a 2020 2020 2020  n.compact.      
-000158d0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
-000158e0: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
-000158f0: 2020 202e 2e2e 2020 2020 2069 6e69 745f     ...     init_
-00015900: 666e 203d 206c 616d 6264 613a 2030 0a20  fn = lambda: 0. 
-00015910: 2020 2020 202e 2e2e 2020 2020 2072 6564       ...     red
-00015920: 7563 655f 666e 203d 206c 616d 6264 6120  uce_fn = lambda 
-00015930: 612c 2062 3a20 6120 2b20 620a 2020 2020  a, b: a + b.    
-00015940: 2020 2e2e 2e20 2020 2020 7365 6c66 2e73    ...     self.s
-00015950: 6f77 2827 696e 7465 726d 6564 6961 7465  ow('intermediate
-00015960: 7327 2c20 2768 272c 2078 2c0a 2020 2020  s', 'h', x,.    
-00015970: 2020 2e2e 2e20 2020 2020 2020 2020 2020    ...           
-00015980: 2020 2020 696e 6974 5f66 6e3d 696e 6974      init_fn=init
-00015990: 5f66 6e2c 2072 6564 7563 655f 666e 3d72  _fn, reduce_fn=r
-000159a0: 6564 7563 655f 666e 290a 2020 2020 2020  educe_fn).      
-000159b0: 2e2e 2e20 2020 2020 7365 6c66 2e73 6f77  ...     self.sow
-000159c0: 2827 696e 7465 726d 6564 6961 7465 7327  ('intermediates'
-000159d0: 2c20 2768 272c 2078 202a 2032 2c0a 2020  , 'h', x * 2,.  
-000159e0: 2020 2020 2e2e 2e20 2020 2020 2020 2020      ...         
-000159f0: 2020 2020 2020 696e 6974 5f66 6e3d 696e        init_fn=in
-00015a00: 6974 5f66 6e2c 2072 6564 7563 655f 666e  it_fn, reduce_fn
-00015a10: 3d72 6564 7563 655f 666e 290a 2020 2020  =reduce_fn).    
-00015a20: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
-00015a30: 2078 0a0a 2020 2020 2020 3e3e 3e20 7820   x..      >>> x 
-00015a40: 3d20 6a6e 702e 6f6e 6573 2828 312c 2031  = jnp.ones((1, 1
-00015a50: 2929 0a20 2020 2020 203e 3e3e 206d 6f64  )).      >>> mod
-00015a60: 656c 203d 2046 6f6f 3228 290a 2020 2020  el = Foo2().    
-00015a70: 2020 3e3e 3e20 7661 7269 6162 6c65 7320    >>> variables 
-00015a80: 3d20 6d6f 6465 6c2e 696e 6974 286a 6178  = model.init(jax
-00015a90: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
-00015aa0: 7829 0a20 2020 2020 203e 3e3e 2079 2c20  x).      >>> y, 
-00015ab0: 7374 6174 6520 3d20 6d6f 6465 6c2e 6170  state = model.ap
-00015ac0: 706c 7928 0a20 2020 2020 202e 2e2e 2020  ply(.      ...  
-00015ad0: 2020 2076 6172 6961 626c 6573 2c20 782c     variables, x,
-00015ae0: 206d 7574 6162 6c65 3d5b 2769 6e74 6572   mutable=['inter
-00015af0: 6d65 6469 6174 6573 275d 290a 2020 2020  mediates']).    
-00015b00: 2020 3e3e 3e20 7072 696e 7428 7374 6174    >>> print(stat
-00015b10: 655b 2769 6e74 6572 6d65 6469 6174 6573  e['intermediates
-00015b20: 275d 290a 2020 2020 2020 7b27 6827 3a20  ']).      {'h': 
-00015b30: 4172 7261 7928 5b5b 332e 5d5d 2c20 6474  Array([[3.]], dt
-00015b40: 7970 653d 666c 6f61 7433 3229 7d0a 0a20  ype=float32)}.. 
-00015b50: 2020 2041 7267 733a 0a20 2020 2020 2063     Args:.      c
-00015b60: 6f6c 3a20 5468 6520 6e61 6d65 206f 6620  ol: The name of 
-00015b70: 7468 6520 7661 7269 6162 6c65 2063 6f6c  the variable col
-00015b80: 6c65 6374 696f 6e2e 0a20 2020 2020 206e  lection..      n
-00015b90: 616d 653a 2054 6865 206e 616d 6520 6f66  ame: The name of
-00015ba0: 2074 6865 2076 6172 6961 626c 652e 0a20   the variable.. 
-00015bb0: 2020 2020 2076 616c 7565 3a20 5468 6520       value: The 
-00015bc0: 7661 6c75 6520 6f66 2074 6865 2076 6172  value of the var
-00015bd0: 6961 626c 652e 0a20 2020 2020 2072 6564  iable..      red
-00015be0: 7563 655f 666e 3a20 5468 6520 6675 6e63  uce_fn: The func
-00015bf0: 7469 6f6e 2075 7365 6420 746f 2063 6f6d  tion used to com
-00015c00: 6269 6e65 2074 6865 2065 7869 7374 696e  bine the existin
-00015c10: 6720 7661 6c75 6520 7769 7468 2074 6865  g value with the
-00015c20: 206e 6577 0a20 2020 2020 2020 2076 616c   new.        val
-00015c30: 7565 2e20 5468 6520 6465 6661 756c 7420  ue. The default 
-00015c40: 6973 2074 6f20 6170 7065 6e64 2074 6865  is to append the
-00015c50: 2076 616c 7565 2074 6f20 6120 7475 706c   value to a tupl
-00015c60: 652e 0a20 2020 2020 2069 6e69 745f 666e  e..      init_fn
-00015c70: 3a20 466f 7220 7468 6520 6669 7273 7420  : For the first 
-00015c80: 7661 6c75 6520 7374 6f72 6564 2c20 6060  value stored, ``
-00015c90: 7265 6475 6365 5f66 6e60 6020 7769 6c6c  reduce_fn`` will
-00015ca0: 2062 6520 7061 7373 6564 2074 6865 2072   be passed the r
-00015cb0: 6573 756c 740a 2020 2020 2020 2020 6f66  esult.        of
-00015cc0: 2060 6069 6e69 745f 666e 6060 2074 6f67   ``init_fn`` tog
-00015cd0: 6574 6865 7220 7769 7468 2074 6865 2076  ether with the v
-00015ce0: 616c 7565 2074 6f20 6265 2073 746f 7265  alue to be store
-00015cf0: 642e 2054 6865 2064 6566 6175 6c74 2069  d. The default i
-00015d00: 7320 616e 0a20 2020 2020 2020 2065 6d70  s an.        emp
-00015d10: 7479 2074 7570 6c65 2e0a 0a20 2020 2052  ty tuple...    R
-00015d20: 6574 7572 6e73 3a0a 2020 2020 2020 6060  eturns:.      ``
-00015d30: 5472 7565 6060 2069 6620 7468 6520 7661  True`` if the va
-00015d40: 6c75 6520 6861 7320 6265 656e 2073 746f  lue has been sto
-00015d50: 7265 6420 7375 6363 6573 7366 756c 6c79  red successfully
-00015d60: 2c20 6060 4661 6c73 6560 6020 6f74 6865  , ``False`` othe
-00015d70: 7277 6973 652e 0a20 2020 2022 2222 0a20  rwise..    """. 
-00015d80: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
-00015d90: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
-00015da0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-00015db0: 2822 4361 6e27 7420 7374 6f72 6520 7661  ("Can't store va
-00015dc0: 7269 6162 6c65 7320 6f6e 2075 6e62 6f75  riables on unbou
-00015dd0: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
-00015de0: 2069 6620 6e6f 7420 7365 6c66 2e73 636f   if not self.sco
-00015df0: 7065 2e69 735f 6d75 7461 626c 655f 636f  pe.is_mutable_co
-00015e00: 6c6c 6563 7469 6f6e 2863 6f6c 293a 0a20  llection(col):. 
-00015e10: 2020 2020 2072 6574 7572 6e20 4661 6c73       return Fals
-00015e20: 650a 2020 2020 6966 2073 656c 662e 7363  e.    if self.sc
-00015e30: 6f70 652e 6861 735f 7661 7269 6162 6c65  ope.has_variable
-00015e40: 2863 6f6c 2c20 6e61 6d65 293a 0a20 2020  (col, name):.   
-00015e50: 2020 2078 7320 3d20 7365 6c66 2e73 636f     xs = self.sco
-00015e60: 7065 2e67 6574 5f76 6172 6961 626c 6528  pe.get_variable(
-00015e70: 636f 6c2c 206e 616d 6529 0a20 2020 2065  col, name).    e
-00015e80: 6c73 653a 0a20 2020 2020 2073 656c 662e  lse:.      self.
-00015e90: 7363 6f70 652e 7265 7365 7276 6528 6e61  scope.reserve(na
-00015ea0: 6d65 2c20 636f 6c29 0a20 2020 2020 2073  me, col).      s
-00015eb0: 656c 662e 5f73 7461 7465 2e63 6869 6c64  elf._state.child
-00015ec0: 7265 6e5b 6e61 6d65 5d20 3d20 636f 6c0a  ren[name] = col.
-00015ed0: 2020 2020 2020 7873 203d 2069 6e69 745f        xs = init_
-00015ee0: 666e 2829 0a20 2020 2078 7320 3d20 7265  fn().    xs = re
-00015ef0: 6475 6365 5f66 6e28 7873 2c20 7661 6c75  duce_fn(xs, valu
-00015f00: 6529 0a20 2020 2073 656c 662e 7363 6f70  e).    self.scop
-00015f10: 652e 7075 745f 7661 7269 6162 6c65 2863  e.put_variable(c
-00015f20: 6f6c 2c20 6e61 6d65 2c20 7873 290a 2020  ol, name, xs).  
-00015f30: 2020 7265 7475 726e 2054 7275 650a 0a20    return True.. 
-00015f40: 2064 6566 2070 6572 7475 7262 280a 2020   def perturb(.  
-00015f50: 2020 7365 6c66 2c20 6e61 6d65 3a20 7374    self, name: st
-00015f60: 722c 2076 616c 7565 3a20 542c 2063 6f6c  r, value: T, col
-00015f70: 6c65 6374 696f 6e3a 2073 7472 203d 2027  lection: str = '
-00015f80: 7065 7274 7572 6261 7469 6f6e 7327 0a20  perturbations'. 
-00015f90: 2029 202d 3e20 543a 0a20 2020 2022 2222   ) -> T:.    """
-00015fa0: 4164 6420 616e 207a 6572 6f2d 7661 6c75  Add an zero-valu
-00015fb0: 6520 7661 7269 6162 6c65 2028 2770 6572  e variable ('per
-00015fc0: 7475 7262 6174 696f 6e27 2920 746f 2074  turbation') to t
-00015fd0: 6865 2069 6e74 6572 6d65 6469 6174 6520  he intermediate 
-00015fe0: 7661 6c75 652e 0a0a 2020 2020 5468 6520  value...    The 
-00015ff0: 6772 6164 6965 6e74 206f 6620 6060 7661  gradient of ``va
-00016000: 6c75 6560 6020 776f 756c 6420 6265 2074  lue`` would be t
-00016010: 6865 2073 616d 6520 6173 2074 6865 2067  he same as the g
-00016020: 7261 6469 656e 7420 6f66 2074 6869 730a  radient of this.
-00016030: 2020 2020 7065 7274 7572 6261 7469 6f6e      perturbation
-00016040: 2076 6172 6961 626c 652e 2054 6865 7265   variable. There
-00016050: 666f 7265 2c20 6966 2079 6f75 2064 6566  fore, if you def
-00016060: 696e 6520 796f 7572 206c 6f73 7320 6675  ine your loss fu
-00016070: 6e63 7469 6f6e 2077 6974 680a 2020 2020  nction with.    
-00016080: 626f 7468 2070 6172 616d 7320 616e 6420  both params and 
-00016090: 7065 7274 7572 6261 7469 6f6e 7320 6173  perturbations as
-000160a0: 2073 7461 6e64 616c 6f6e 6520 6172 6775   standalone argu
-000160b0: 6d65 6e74 732c 2079 6f75 2063 616e 2067  ments, you can g
-000160c0: 6574 2074 6865 0a20 2020 2069 6e74 6572  et the.    inter
-000160d0: 6d65 6469 6174 6520 6772 6164 6965 6e74  mediate gradient
-000160e0: 7320 6f66 2060 6076 616c 7565 6060 2062  s of ``value`` b
-000160f0: 7920 7275 6e6e 696e 6720 6060 6a61 782e  y running ``jax.
-00016100: 6772 6164 6060 206f 6e20 7468 6520 7065  grad`` on the pe
-00016110: 7274 7572 6261 7469 6f6e 0a20 2020 2061  rturbation.    a
-00016120: 7267 756d 656e 742e 0a0a 2020 2020 2e2e  rgument...    ..
-00016130: 206e 6f74 653a 3a0a 2020 2020 2020 5468   note::.      Th
-00016140: 6973 2069 7320 616e 2065 7870 6572 696d  is is an experim
-00016150: 656e 7461 6c20 4150 4920 616e 6420 6d61  ental API and ma
-00016160: 7920 6265 2074 7765 616b 6564 206c 6174  y be tweaked lat
-00016170: 6572 2066 6f72 2062 6574 7465 720a 2020  er for better.  
-00016180: 2020 2020 7065 7266 6f72 6d61 6e63 6520      performance 
-00016190: 616e 6420 7573 6162 696c 6974 792e 0a20  and usability.. 
-000161a0: 2020 2020 2041 7420 6974 7320 6375 7272       At its curr
-000161b0: 656e 7420 7374 6167 652c 2069 7420 6372  ent stage, it cr
-000161c0: 6561 7465 7320 6578 7472 6120 6475 6d6d  eates extra dumm
-000161d0: 7920 7661 7269 6162 6c65 7320 7468 6174  y variables that
-000161e0: 206f 6363 7570 6965 7320 6578 7472 610a   occupies extra.
-000161f0: 2020 2020 2020 6d65 6d6f 7279 2073 7061        memory spa
-00016200: 6365 2e20 5573 6520 6974 206f 6e6c 7920  ce. Use it only 
-00016210: 746f 2064 6562 7567 2067 7261 6469 656e  to debug gradien
-00016220: 7473 2069 6e20 7472 6169 6e69 6e67 2e0a  ts in training..
-00016230: 0a20 2020 2045 7861 6d70 6c65 3a3a 0a0a  .    Example::..
-00016240: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
-00016250: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
-00016260: 2020 2020 2020 2e2e 2e20 2020 406e 6e2e        ...   @nn.
-00016270: 636f 6d70 6163 740a 2020 2020 2020 2e2e  compact.      ..
-00016280: 2e20 2020 6465 6620 5f5f 6361 6c6c 5f5f  .   def __call__
-00016290: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
-000162a0: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
-000162b0: 4465 6e73 6528 3329 2878 290a 2020 2020  Dense(3)(x).    
-000162c0: 2020 2e2e 2e20 2020 2020 7820 3d20 7365    ...     x = se
-000162d0: 6c66 2e70 6572 7475 7262 2827 6465 6e73  lf.perturb('dens
-000162e0: 6533 272c 2078 290a 2020 2020 2020 2e2e  e3', x).      ..
-000162f0: 2e20 2020 2020 7265 7475 726e 206e 6e2e  .     return nn.
-00016300: 4465 6e73 6528 3229 2878 290a 0a20 2020  Dense(2)(x)..   
-00016310: 2020 203e 3e3e 2064 6566 206c 6f73 7328     >>> def loss(
-00016320: 7661 7269 6162 6c65 732c 2069 6e70 7574  variables, input
-00016330: 732c 2074 6172 6765 7473 293a 0a20 2020  s, targets):.   
-00016340: 2020 202e 2e2e 2020 2070 7265 6473 203d     ...   preds =
-00016350: 206d 6f64 656c 2e61 7070 6c79 2876 6172   model.apply(var
-00016360: 6961 626c 6573 2c20 696e 7075 7473 290a  iables, inputs).
-00016370: 2020 2020 2020 2e2e 2e20 2020 7265 7475        ...   retu
-00016380: 726e 206a 6e70 2e73 7175 6172 6528 7072  rn jnp.square(pr
-00016390: 6564 7320 2d20 7461 7267 6574 7329 2e6d  eds - targets).m
-000163a0: 6561 6e28 290a 0a20 2020 2020 203e 3e3e  ean()..      >>>
-000163b0: 2078 203d 206a 6e70 2e6f 6e65 7328 2832   x = jnp.ones((2
-000163c0: 2c20 3929 290a 2020 2020 2020 3e3e 3e20  , 9)).      >>> 
-000163d0: 7920 3d20 6a6e 702e 6f6e 6573 2828 322c  y = jnp.ones((2,
-000163e0: 2032 2929 0a20 2020 2020 203e 3e3e 206d   2)).      >>> m
-000163f0: 6f64 656c 203d 2046 6f6f 2829 0a20 2020  odel = Foo().   
-00016400: 2020 203e 3e3e 2076 6172 6961 626c 6573     >>> variables
-00016410: 203d 206d 6f64 656c 2e69 6e69 7428 6a61   = model.init(ja
-00016420: 782e 7261 6e64 6f6d 2e6b 6579 2830 292c  x.random.key(0),
-00016430: 2078 290a 2020 2020 2020 3e3e 3e20 696e   x).      >>> in
-00016440: 746d 5f67 7261 6473 203d 206a 6178 2e67  tm_grads = jax.g
-00016450: 7261 6428 6c6f 7373 2c20 6172 676e 756d  rad(loss, argnum
-00016460: 733d 3029 2876 6172 6961 626c 6573 2c20  s=0)(variables, 
-00016470: 782c 2079 290a 2020 2020 2020 3e3e 3e20  x, y).      >>> 
-00016480: 7072 696e 7428 696e 746d 5f67 7261 6473  print(intm_grads
-00016490: 5b27 7065 7274 7572 6261 7469 6f6e 7327  ['perturbations'
-000164a0: 5d5b 2764 656e 7365 3327 5d29 0a20 2020  ]['dense3']).   
-000164b0: 2020 205b 5b2d 312e 3435 3639 3234 2020     [[-1.456924  
-000164c0: 202d 302e 3434 3333 3235 3337 2020 302e   -0.44332537  0.
-000164d0: 3032 3432 3238 3437 5d0a 2020 2020 2020  02422847].      
-000164e0: 205b 2d31 2e34 3536 3932 3420 2020 2d30   [-1.456924   -0
-000164f0: 2e34 3433 3332 3533 3720 2030 2e30 3234  .44332537  0.024
-00016500: 3232 3834 375d 5d0a 0a20 2020 2049 6620  22847]]..    If 
-00016510: 7065 7274 7572 6261 7469 6f6e 7320 6172  perturbations ar
-00016520: 6520 6e6f 7420 7061 7373 6564 2074 6f20  e not passed to 
-00016530: 6060 6170 706c 7960 602c 2060 6070 6572  ``apply``, ``per
-00016540: 7475 7262 6060 2062 6568 6176 6573 206c  turb`` behaves l
-00016550: 696b 6520 6120 6e6f 2d6f 700a 2020 2020  ike a no-op.    
-00016560: 736f 2079 6f75 2063 616e 2065 6173 696c  so you can easil
-00016570: 7920 6469 7361 626c 6520 7468 6520 6265  y disable the be
-00016580: 6861 7669 6f72 2077 6865 6e20 6e6f 7420  havior when not 
-00016590: 6e65 6564 6564 3a3a 0a0a 2020 2020 2020  needed::..      
-000165a0: 3e3e 3e20 6d6f 6465 6c2e 6170 706c 7928  >>> model.apply(
-000165b0: 7661 7269 6162 6c65 732c 2078 2920 2320  variables, x) # 
-000165c0: 776f 726b 7320 6173 2065 7870 6563 7465  works as expecte
-000165d0: 640a 2020 2020 2020 4172 7261 7928 5b5b  d.      Array([[
-000165e0: 2d31 2e30 3938 3031 3238 202c 202d 302e  -1.0980128 , -0.
-000165f0: 3637 3936 3137 3335 5d2c 0a20 2020 2020  67961735],.     
-00016600: 2020 2020 2020 2020 5b2d 312e 3039 3830          [-1.0980
-00016610: 3132 3820 2c20 2d30 2e36 3739 3631 3733  128 , -0.6796173
-00016620: 355d 5d2c 2064 7479 7065 3d66 6c6f 6174  5]], dtype=float
-00016630: 3332 290a 2020 2020 2020 3e3e 3e20 6d6f  32).      >>> mo
-00016640: 6465 6c2e 6170 706c 7928 7b27 7061 7261  del.apply({'para
-00016650: 6d73 273a 2076 6172 6961 626c 6573 5b27  ms': variables['
-00016660: 7061 7261 6d73 275d 7d2c 2078 2920 2320  params']}, x) # 
-00016670: 6265 6861 7665 7320 6c69 6b65 2061 206e  behaves like a n
-00016680: 6f2d 6f70 0a20 2020 2020 2041 7272 6179  o-op.      Array
-00016690: 285b 5b2d 312e 3039 3830 3132 3820 2c20  ([[-1.0980128 , 
-000166a0: 2d30 2e36 3739 3631 3733 355d 2c0a 2020  -0.67961735],.  
-000166b0: 2020 2020 2020 2020 2020 205b 2d31 2e30             [-1.0
-000166c0: 3938 3031 3238 202c 202d 302e 3637 3936  980128 , -0.6796
-000166d0: 3137 3335 5d5d 2c20 6474 7970 653d 666c  1735]], dtype=fl
-000166e0: 6f61 7433 3229 0a20 2020 2020 203e 3e3e  oat32).      >>>
-000166f0: 2069 6e74 6d5f 6772 6164 7320 3d20 6a61   intm_grads = ja
-00016700: 782e 6772 6164 286c 6f73 732c 2061 7267  x.grad(loss, arg
-00016710: 6e75 6d73 3d30 2928 7b27 7061 7261 6d73  nums=0)({'params
-00016720: 273a 2076 6172 6961 626c 6573 5b27 7061  ': variables['pa
-00016730: 7261 6d73 275d 7d2c 2078 2c20 7929 0a20  rams']}, x, y). 
-00016740: 2020 2020 203e 3e3e 2027 7065 7274 7572       >>> 'pertur
-00016750: 6261 7469 6f6e 7327 206e 6f74 2069 6e20  bations' not in 
-00016760: 696e 746d 5f67 7261 6473 0a20 2020 2020  intm_grads.     
-00016770: 2054 7275 650a 2020 2020 2222 220a 2020   True.    """.  
-00016780: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
-00016790: 6973 204e 6f6e 653a 0a20 2020 2020 2072  is None:.      r
-000167a0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-000167b0: 2243 616e 2774 2073 746f 7265 2076 6172  "Can't store var
-000167c0: 6961 626c 6573 206f 6e20 756e 626f 756e  iables on unboun
-000167d0: 6420 6d6f 6475 6c65 7322 290a 0a20 2020  d modules")..   
-000167e0: 2069 6620 7365 6c66 2e69 735f 6d75 7461   if self.is_muta
-000167f0: 626c 655f 636f 6c6c 6563 7469 6f6e 2863  ble_collection(c
-00016800: 6f6c 6c65 6374 696f 6e29 3a0a 2020 2020  ollection):.    
-00016810: 2020 6966 206e 6f74 2073 656c 662e 7363    if not self.sc
-00016820: 6f70 652e 6861 735f 7661 7269 6162 6c65  ope.has_variable
-00016830: 2863 6f6c 6c65 6374 696f 6e2c 206e 616d  (collection, nam
-00016840: 6529 3a0a 2020 2020 2020 2020 7365 6c66  e):.        self
-00016850: 2e73 636f 7065 2e72 6573 6572 7665 286e  .scope.reserve(n
-00016860: 616d 652c 2063 6f6c 6c65 6374 696f 6e29  ame, collection)
-00016870: 0a20 2020 2020 2020 2073 656c 662e 5f73  .        self._s
-00016880: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
-00016890: 6d65 5d20 3d20 636f 6c6c 6563 7469 6f6e  me] = collection
-000168a0: 0a20 2020 2020 2020 2073 656c 662e 7363  .        self.sc
-000168b0: 6f70 652e 7075 745f 7661 7269 6162 6c65  ope.put_variable
-000168c0: 2863 6f6c 6c65 6374 696f 6e2c 206e 616d  (collection, nam
-000168d0: 652c 206a 6e70 2e7a 6572 6f73 5f6c 696b  e, jnp.zeros_lik
-000168e0: 6528 7661 6c75 6529 2920 2023 2074 7970  e(value))  # typ
-000168f0: 653a 2069 676e 6f72 650a 0a20 2020 2069  e: ignore..    i
-00016900: 6620 636f 6c6c 6563 7469 6f6e 2069 6e20  f collection in 
-00016910: 7365 6c66 2e73 636f 7065 2e72 6f6f 742e  self.scope.root.
-00016920: 5f76 6172 6961 626c 6573 3a0a 2020 2020  _variables:.    
-00016930: 2020 6966 2073 656c 662e 7363 6f70 652e    if self.scope.
-00016940: 6861 735f 7661 7269 6162 6c65 2863 6f6c  has_variable(col
-00016950: 6c65 6374 696f 6e2c 206e 616d 6529 3a0a  lection, name):.
-00016960: 2020 2020 2020 2020 7661 6c75 6520 2b3d          value +=
-00016970: 2073 656c 662e 7363 6f70 652e 6765 745f   self.scope.get_
-00016980: 7661 7269 6162 6c65 2863 6f6c 6c65 6374  variable(collect
-00016990: 696f 6e2c 206e 616d 6529 2020 2320 7479  ion, name)  # ty
-000169a0: 7065 3a20 6967 6e6f 7265 0a20 2020 2020  pe: ignore.     
-000169b0: 2065 6c73 653a 0a20 2020 2020 2020 2072   else:.        r
-000169c0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
-000169d0: 6622 5065 7274 7572 6261 7469 6f6e 2063  f"Perturbation c
-000169e0: 6f6c 6c65 6374 696f 6e20 7b63 6f6c 6c65  ollection {colle
-000169f0: 6374 696f 6e7d 2070 7265 7365 6e74 2c20  ction} present, 
-00016a00: 6275 7420 220a 2020 2020 2020 2020 2020  but ".          
-00016a10: 2020 2020 2020 2020 2020 2020 2020 2066                 f
-00016a20: 226d 6973 7369 6e67 2070 6572 7475 7262  "missing perturb
-00016a30: 6174 696f 6e20 7661 7269 6162 6c65 207b  ation variable {
-00016a40: 6e61 6d65 7d22 290a 0a20 2020 2072 6574  name}")..    ret
-00016a50: 7572 6e20 7661 6c75 650a 0a20 2064 6566  urn value..  def
-00016a60: 2074 6162 756c 6174 6528 0a20 2020 2073   tabulate(.    s
-00016a70: 656c 662c 0a20 2020 2072 6e67 733a 2055  elf,.    rngs: U
-00016a80: 6e69 6f6e 5b50 524e 474b 6579 2c20 524e  nion[PRNGKey, RN
-00016a90: 4753 6571 7565 6e63 6573 5d2c 0a20 2020  GSequences],.   
-00016aa0: 202a 6172 6773 2c0a 2020 2020 6465 7074   *args,.    dept
-00016ab0: 683a 204f 7074 696f 6e61 6c5b 696e 745d  h: Optional[int]
-00016ac0: 203d 204e 6f6e 652c 0a20 2020 2073 686f   = None,.    sho
-00016ad0: 775f 7265 7065 6174 6564 3a20 626f 6f6c  w_repeated: bool
-00016ae0: 203d 2046 616c 7365 2c0a 2020 2020 6d75   = False,.    mu
-00016af0: 7461 626c 653a 2043 6f6c 6c65 6374 696f  table: Collectio
-00016b00: 6e46 696c 7465 7220 3d20 4465 6e79 4c69  nFilter = DenyLi
-00016b10: 7374 2827 696e 7465 726d 6564 6961 7465  st('intermediate
-00016b20: 7327 292c 0a20 2020 2063 6f6e 736f 6c65  s'),.    console
-00016b30: 5f6b 7761 7267 733a 204f 7074 696f 6e61  _kwargs: Optiona
-00016b40: 6c5b 4d61 7070 696e 675b 7374 722c 2041  l[Mapping[str, A
-00016b50: 6e79 5d5d 203d 204e 6f6e 652c 0a20 2020  ny]] = None,.   
-00016b60: 2074 6162 6c65 5f6b 7761 7267 733a 204d   table_kwargs: M
-00016b70: 6170 7069 6e67 5b73 7472 2c20 416e 795d  apping[str, Any]
-00016b80: 203d 204d 6170 7069 6e67 5072 6f78 7954   = MappingProxyT
-00016b90: 7970 6528 7b7d 292c 0a20 2020 2063 6f6c  ype({}),.    col
-00016ba0: 756d 6e5f 6b77 6172 6773 3a20 4d61 7070  umn_kwargs: Mapp
-00016bb0: 696e 675b 7374 722c 2041 6e79 5d20 3d20  ing[str, Any] = 
-00016bc0: 4d61 7070 696e 6750 726f 7879 5479 7065  MappingProxyType
-00016bd0: 287b 7d29 2c0a 2020 2020 636f 6d70 7574  ({}),.    comput
-00016be0: 655f 666c 6f70 733a 2062 6f6f 6c20 3d20  e_flops: bool = 
-00016bf0: 4661 6c73 652c 0a20 2020 2063 6f6d 7075  False,.    compu
-00016c00: 7465 5f76 6a70 5f66 6c6f 7073 3a20 626f  te_vjp_flops: bo
-00016c10: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    
-00016c20: 2a2a 6b77 6172 6773 2c0a 2020 2920 2d3e  **kwargs,.  ) ->
-00016c30: 2073 7472 3a0a 2020 2020 2222 2243 7265   str:.    """Cre
-00016c40: 6174 6573 2061 2073 756d 6d61 7279 206f  ates a summary o
-00016c50: 6620 7468 6520 4d6f 6475 6c65 2072 6570  f the Module rep
-00016c60: 7265 7365 6e74 6564 2061 7320 6120 7461  resented as a ta
-00016c70: 626c 652e 0a0a 2020 2020 5468 6973 206d  ble...    This m
-00016c80: 6574 686f 6420 6861 7320 7468 6520 7361  ethod has the sa
-00016c90: 6d65 2073 6967 6e61 7475 7265 2061 6e64  me signature and
-00016ca0: 2069 6e74 6572 6e61 6c6c 7920 6361 6c6c   internally call
-00016cb0: 7320 6060 4d6f 6475 6c65 2e69 6e69 7460  s ``Module.init`
-00016cc0: 602c 0a20 2020 2062 7574 2069 6e73 7465  `,.    but inste
-00016cd0: 6164 206f 6620 7265 7475 726e 696e 6720  ad of returning 
-00016ce0: 7468 6520 7661 7269 6162 6c65 732c 2069  the variables, i
-00016cf0: 7420 7265 7475 726e 7320 7468 6520 7374  t returns the st
-00016d00: 7269 6e67 2073 756d 6d61 7269 7a69 6e67  ring summarizing
-00016d10: 0a20 2020 2074 6865 204d 6f64 756c 6520  .    the Module 
-00016d20: 696e 2061 2074 6162 6c65 2e20 6060 7461  in a table. ``ta
-00016d30: 6275 6c61 7465 6060 2075 7365 7320 6060  bulate`` uses ``
-00016d40: 6a61 782e 6576 616c 5f73 6861 7065 6060  jax.eval_shape``
-00016d50: 2074 6f20 7275 6e20 7468 6520 666f 7277   to run the forw
-00016d60: 6172 640a 2020 2020 636f 6d70 7574 6174  ard.    computat
-00016d70: 696f 6e20 7769 7468 6f75 7420 636f 6e73  ion without cons
-00016d80: 756d 696e 6720 616e 7920 464c 4f50 7320  uming any FLOPs 
-00016d90: 6f72 2061 6c6c 6f63 6174 696e 6720 6d65  or allocating me
-00016da0: 6d6f 7279 2e0a 0a20 2020 2041 6464 6974  mory...    Addit
-00016db0: 696f 6e61 6c20 6172 6775 6d65 6e74 7320  ional arguments 
-00016dc0: 6361 6e20 6265 2070 6173 7365 6420 696e  can be passed in
-00016dd0: 746f 2074 6865 2060 6063 6f6e 736f 6c65  to the ``console
-00016de0: 5f6b 7761 7267 7360 6020 6172 6775 6d65  _kwargs`` argume
-00016df0: 6e74 2c20 666f 720a 2020 2020 6578 616d  nt, for.    exam
-00016e00: 706c 652c 2060 607b 2777 6964 7468 273a  ple, ``{'width':
-00016e10: 2031 3230 7d60 602e 2046 6f72 2061 2066   120}``. For a f
-00016e20: 756c 6c20 6c69 7374 206f 6620 6060 636f  ull list of ``co
-00016e30: 6e73 6f6c 655f 6b77 6172 6773 6060 2061  nsole_kwargs`` a
-00016e40: 7267 756d 656e 7473 2c0a 2020 2020 7365  rguments,.    se
-00016e50: 653a 0a20 2020 2068 7474 7073 3a2f 2f72  e:.    https://r
-00016e60: 6963 682e 7265 6164 7468 6564 6f63 732e  ich.readthedocs.
-00016e70: 696f 2f65 6e2f 7374 6162 6c65 2f72 6566  io/en/stable/ref
-00016e80: 6572 656e 6365 2f63 6f6e 736f 6c65 2e68  erence/console.h
-00016e90: 746d 6c23 7269 6368 2e63 6f6e 736f 6c65  tml#rich.console
-00016ea0: 2e43 6f6e 736f 6c65 0a0a 2020 2020 4578  .Console..    Ex
-00016eb0: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
-00016ec0: 3e3e 2069 6d70 6f72 7420 666c 6178 2e6c  >> import flax.l
-00016ed0: 696e 656e 2061 7320 6e6e 0a20 2020 2020  inen as nn.     
-00016ee0: 203e 3e3e 2069 6d70 6f72 7420 6a61 782c   >>> import jax,
-00016ef0: 206a 6178 2e6e 756d 7079 2061 7320 6a6e   jax.numpy as jn
-00016f00: 700a 0a20 2020 2020 203e 3e3e 2063 6c61  p..      >>> cla
-00016f10: 7373 2046 6f6f 286e 6e2e 4d6f 6475 6c65  ss Foo(nn.Module
-00016f20: 293a 0a20 2020 2020 202e 2e2e 2020 2040  ):.      ...   @
-00016f30: 6e6e 2e63 6f6d 7061 6374 0a20 2020 2020  nn.compact.     
-00016f40: 202e 2e2e 2020 2064 6566 205f 5f63 616c   ...   def __cal
-00016f50: 6c5f 5f28 7365 6c66 2c20 7829 3a0a 2020  l__(self, x):.  
-00016f60: 2020 2020 2e2e 2e20 2020 2020 6820 3d20      ...     h = 
-00016f70: 6e6e 2e44 656e 7365 2834 2928 7829 0a20  nn.Dense(4)(x). 
-00016f80: 2020 2020 202e 2e2e 2020 2020 2072 6574       ...     ret
-00016f90: 7572 6e20 6e6e 2e44 656e 7365 2832 2928  urn nn.Dense(2)(
-00016fa0: 6829 0a0a 2020 2020 2020 3e3e 3e20 7820  h)..      >>> x 
-00016fb0: 3d20 6a6e 702e 6f6e 6573 2828 3136 2c20  = jnp.ones((16, 
-00016fc0: 3929 290a 0a20 2020 2020 203e 3e3e 2023  9))..      >>> #
-00016fd0: 2070 7269 6e74 2846 6f6f 2829 2e74 6162   print(Foo().tab
-00016fe0: 756c 6174 6528 0a20 2020 2020 203e 3e3e  ulate(.      >>>
-00016ff0: 2023 2020 2020 206a 6178 2e72 616e 646f   #     jax.rando
-00017000: 6d2e 6b65 7928 3029 2c20 782c 2063 6f6d  m.key(0), x, com
-00017010: 7075 7465 5f66 6c6f 7073 3d54 7275 652c  pute_flops=True,
-00017020: 2063 6f6d 7075 7465 5f76 6a70 5f66 6c6f   compute_vjp_flo
-00017030: 7073 3d54 7275 6529 290a 0a20 2020 2054  ps=True))..    T
-00017040: 6869 7320 6769 7665 7320 7468 6520 666f  his gives the fo
-00017050: 6c6c 6f77 696e 6720 6f75 7470 7574 3a3a  llowing output::
-00017060: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
-00017070: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017080: 2020 2020 2020 2020 2020 2020 2020 466f                Fo
-00017090: 6f20 5375 6d6d 6172 790a 2020 2020 2020  o Summary.      
-000170a0: e294 8fe2 9481 e294 81e2 9481 e294 81e2  ................
-000170b0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000170c0: b3e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000170d0: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
-000170e0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-000170f0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00017100: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
-00017110: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00017120: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00017130: e294 81e2 9481 e294 81e2 94b3 e294 81e2  ................
-00017140: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00017150: 81e2 94b3 e294 81e2 9481 e294 81e2 9481  ................
-00017160: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00017170: 9481 e294 81e2 94b3 e294 81e2 9481 e294  ................
-00017180: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00017190: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000171a0: 9481 e294 81e2 9481 e294 81e2 9493 0a20  ............... 
-000171b0: 2020 2020 20e2 9483 2070 6174 6820 2020       ... path   
-000171c0: 20e2 9483 206d 6f64 756c 6520 e294 8320   ... module ... 
-000171d0: 696e 7075 7473 2020 2020 2020 2020 e294  inputs        ..
-000171e0: 8320 6f75 7470 7574 7320 2020 2020 2020  . outputs       
-000171f0: e294 8320 666c 6f70 7320 e294 8320 766a  ... flops ... vj
-00017200: 705f 666c 6f70 7320 e294 8320 7061 7261  p_flops ... para
-00017210: 6d73 2020 2020 2020 2020 2020 e294 830a  ms          ....
-00017220: 2020 2020 2020 e294 a1e2 9481 e294 81e2        ..........
-00017230: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00017240: 81e2 9481 e295 87e2 9481 e294 81e2 9481  ................
-00017250: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00017260: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
-00017270: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00017280: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00017290: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
-000172a0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-000172b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000172c0: 9587 e294 81e2 9481 e294 81e2 9481 e294  ................
-000172d0: 81e2 9481 e294 81e2 9587 e294 81e2 9481  ................
-000172e0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-000172f0: 9481 e294 81e2 9481 e294 81e2 9587 e294  ................
-00017300: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
-00017310: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
-00017320: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
-00017330: 81e2 94a9 0a20 2020 2020 20e2 9482 2020  .....      ...  
-00017340: 2020 2020 2020 20e2 9482 2046 6f6f 2020         ... Foo  
-00017350: 2020 e294 8220 666c 6f61 7433 325b 3136    ... float32[16
-00017360: 2c39 5d20 e294 8220 666c 6f61 7433 325b  ,9] ... float32[
-00017370: 3136 2c32 5d20 e294 8220 3135 3034 2020  16,2] ... 1504  
-00017380: e294 8220 3434 3630 2020 2020 2020 e294  ... 4460      ..
-00017390: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-000173a0: 2020 e294 820a 2020 2020 2020 e294 9ce2    ....      ....
-000173b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000173c0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
-000173d0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000173e0: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-000173f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017400: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017410: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-00017420: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017430: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017440: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
-00017450: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
-00017460: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017470: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017480: 80e2 94bc e294 80e2 9480 e294 80e2 9480  ................
-00017490: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-000174a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000174b0: 80e2 9480 e294 80e2 94a4 0a20 2020 2020  ...........     
-000174c0: 20e2 9482 2044 656e 7365 5f30 20e2 9482   ... Dense_0 ...
-000174d0: 2044 656e 7365 2020 e294 8220 666c 6f61   Dense  ... floa
-000174e0: 7433 325b 3136 2c39 5d20 e294 8220 666c  t32[16,9] ... fl
-000174f0: 6f61 7433 325b 3136 2c34 5d20 e294 8220  oat32[16,4] ... 
-00017500: 3132 3136 2020 e294 8220 3336 3230 2020  1216  ... 3620  
-00017510: 2020 2020 e294 8220 6269 6173 3a20 2020      ... bias:   
-00017520: 2020 2020 2020 2020 e294 820a 2020 2020          ....    
-00017530: 2020 e294 8220 2020 2020 2020 2020 e294    ...         ..
-00017540: 8220 2020 2020 2020 20e2 9482 2020 2020  .        ...    
-00017550: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
-00017560: 2020 2020 2020 2020 2020 2020 20e2 9482               ...
-00017570: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00017580: 2020 2020 20e2 9482 2066 6c6f 6174 3332       ... float32
-00017590: 5b34 5d20 2020 2020 20e2 9482 0a20 2020  [4]      ....   
-000175a0: 2020 20e2 9482 2020 2020 2020 2020 20e2     ...         .
-000175b0: 9482 2020 2020 2020 2020 e294 8220 2020  ..        ...   
-000175c0: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-000175d0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-000175e0: 8220 2020 2020 2020 e294 8220 2020 2020  .       ...     
-000175f0: 2020 2020 2020 e294 8220 6b65 726e 656c        ... kernel
-00017600: 3a20 2020 2020 2020 2020 e294 820a 2020  :         ....  
-00017610: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-00017620: e294 8220 2020 2020 2020 20e2 9482 2020  ...        ...  
-00017630: 2020 2020 2020 2020 2020 2020 20e2 9482               ...
-00017640: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
-00017650: 9482 2020 2020 2020 20e2 9482 2020 2020  ..       ...    
-00017660: 2020 2020 2020 20e2 9482 2066 6c6f 6174         ... float
-00017670: 3332 5b39 2c34 5d20 2020 20e2 9482 0a20  32[9,4]    .... 
-00017680: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00017690: 20e2 9482 2020 2020 2020 2020 e294 8220   ...        ... 
-000176a0: 2020 2020 2020 2020 2020 2020 2020 e294                ..
-000176b0: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-000176c0: e294 8220 2020 2020 2020 e294 8220 2020  ...       ...   
-000176d0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-000176e0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
-000176f0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00017700: 2020 e294 8220 2020 2020 2020 20e2 9482    ...        ...
-00017710: 2020 2020 2020 2020 2020 2020 2020 20e2                 .
-00017720: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
-00017730: 20e2 9482 2020 2020 2020 20e2 9482 2020   ...       ...  
-00017740: 2020 2020 2020 2020 20e2 9482 2034 3020           ... 40 
-00017750: 2831 3630 2042 2920 2020 2020 20e2 9482  (160 B)      ...
-00017760: 0a20 2020 2020 20e2 949c e294 80e2 9480  .      .........
+0000c490: 2020 2022 2222 4765 7420 7468 6520 7061     """Get the pa
+0000c4a0: 7468 206f 6620 7468 6973 204d 6f64 756c  th of this Modul
+0000c4b0: 652e 2054 6f70 2d6c 6576 656c 2072 6f6f  e. Top-level roo
+0000c4c0: 7420 6d6f 6475 6c65 7320 6861 7665 2061  t modules have a
+0000c4d0: 6e20 656d 7074 7920 7061 7468 2060 6028  n empty path ``(
+0000c4e0: 2960 602e 0a20 2020 204e 6f74 6520 7468  )``..    Note th
+0000c4f0: 6174 2074 6869 7320 6d65 7468 6f64 2063  at this method c
+0000c500: 616e 206f 6e6c 7920 6265 2075 7365 6420  an only be used 
+0000c510: 6f6e 2062 6f75 6e64 206d 6f64 756c 6573  on bound modules
+0000c520: 2074 6861 7420 6861 7665 2061 2076 616c   that have a val
+0000c530: 6964 2073 636f 7065 2e0a 0a20 2020 2045  id scope...    E
+0000c540: 7861 6d70 6c65 2075 7361 6765 3a3a 0a0a  xample usage::..
+0000c550: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+0000c560: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+0000c570: 6e0a 2020 2020 2020 3e3e 3e20 696d 706f  n.      >>> impo
+0000c580: 7274 206a 6178 2c20 6a61 782e 6e75 6d70  rt jax, jax.nump
+0000c590: 7920 6173 206a 6e70 0a0a 2020 2020 2020  y as jnp..      
+0000c5a0: 3e3e 3e20 636c 6173 7320 5375 624d 6f64  >>> class SubMod
+0000c5b0: 656c 286e 6e2e 4d6f 6475 6c65 293a 0a20  el(nn.Module):. 
+0000c5c0: 2020 2020 202e 2e2e 2020 2040 6e6e 2e63       ...   @nn.c
+0000c5d0: 6f6d 7061 6374 0a20 2020 2020 202e 2e2e  ompact.      ...
+0000c5e0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+0000c5f0: 7365 6c66 2c20 7829 3a0a 2020 2020 2020  self, x):.      
+0000c600: 2e2e 2e20 2020 2020 7072 696e 7428 6627  ...     print(f'
+0000c610: 5375 624d 6f64 656c 2070 6174 683a 207b  SubModel path: {
+0000c620: 7365 6c66 2e70 6174 687d 2729 0a20 2020  self.path}').   
+0000c630: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
+0000c640: 6e20 780a 0a20 2020 2020 203e 3e3e 2063  n x..      >>> c
+0000c650: 6c61 7373 204d 6f64 656c 286e 6e2e 4d6f  lass Model(nn.Mo
+0000c660: 6475 6c65 293a 0a20 2020 2020 202e 2e2e  dule):.      ...
+0000c670: 2020 2040 6e6e 2e63 6f6d 7061 6374 0a20     @nn.compact. 
+0000c680: 2020 2020 202e 2e2e 2020 2064 6566 205f       ...   def _
+0000c690: 5f63 616c 6c5f 5f28 7365 6c66 2c20 7829  _call__(self, x)
+0000c6a0: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
+0000c6b0: 7072 696e 7428 6627 4d6f 6465 6c20 7061  print(f'Model pa
+0000c6c0: 7468 3a20 7b73 656c 662e 7061 7468 7d27  th: {self.path}'
+0000c6d0: 290a 2020 2020 2020 2e2e 2e20 2020 2020  ).      ...     
+0000c6e0: 7265 7475 726e 2053 7562 4d6f 6465 6c28  return SubModel(
+0000c6f0: 2928 7829 0a0a 2020 2020 2020 3e3e 3e20  )(x)..      >>> 
+0000c700: 6d6f 6465 6c20 3d20 4d6f 6465 6c28 290a  model = Model().
+0000c710: 2020 2020 2020 3e3e 3e20 7661 7269 6162        >>> variab
+0000c720: 6c65 7320 3d20 6d6f 6465 6c2e 696e 6974  les = model.init
+0000c730: 286a 6178 2e72 616e 646f 6d2e 6b65 7928  (jax.random.key(
+0000c740: 3029 2c20 6a6e 702e 6f6e 6573 2828 312c  0), jnp.ones((1,
+0000c750: 2032 2929 290a 2020 2020 2020 4d6f 6465   2))).      Mode
+0000c760: 6c20 7061 7468 3a20 2829 0a20 2020 2020  l path: ().     
+0000c770: 2053 7562 4d6f 6465 6c20 7061 7468 3a20   SubModel path: 
+0000c780: 2827 5375 624d 6f64 656c 5f30 272c 290a  ('SubModel_0',).
+0000c790: 2020 2020 2222 220a 0a20 2020 2069 6620      """..    if 
+0000c7a0: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+0000c7b0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+0000c7c0: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+0000c7d0: 7420 6163 6365 7373 206d 6f64 756c 6520  t access module 
+0000c7e0: 7061 7468 7320 6f6e 2075 6e62 6f75 6e64  paths on unbound
+0000c7f0: 206d 6f64 756c 6573 2e22 290a 0a20 2020   modules.")..   
+0000c800: 2072 6574 7572 6e20 7365 6c66 2e73 636f   return self.sco
+0000c810: 7065 2e70 6174 680a 0a20 2064 6566 2063  pe.path..  def c
+0000c820: 6c6f 6e65 280a 2020 2020 7365 6c66 3a20  lone(.    self: 
+0000c830: 4d2c 0a20 2020 202a 2c0a 2020 2020 7061  M,.    *,.    pa
+0000c840: 7265 6e74 3a20 4f70 7469 6f6e 616c 5b55  rent: Optional[U
+0000c850: 6e69 6f6e 5b53 636f 7065 2c20 274d 6f64  nion[Scope, 'Mod
+0000c860: 756c 6527 2c20 5f53 656e 7469 6e65 6c5d  ule', _Sentinel]
+0000c870: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 5f64  ] = None,.    _d
+0000c880: 6565 705f 636c 6f6e 653a 2055 6e69 6f6e  eep_clone: Union
+0000c890: 5b62 6f6f 6c2c 2077 6561 6b72 6566 2e57  [bool, weakref.W
+0000c8a0: 6561 6b56 616c 7565 4469 6374 696f 6e61  eakValueDictiona
+0000c8b0: 7279 5d20 3d20 4661 6c73 652c 0a20 2020  ry] = False,.   
+0000c8c0: 205f 7265 7365 745f 6e61 6d65 733a 2062   _reset_names: b
+0000c8d0: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   
+0000c8e0: 202a 2a75 7064 6174 6573 2c0a 2020 2920   **updates,.  ) 
+0000c8f0: 2d3e 204d 3a0a 2020 2020 2222 2243 7265  -> M:.    """Cre
+0000c900: 6174 6573 2061 2063 6c6f 6e65 206f 6620  ates a clone of 
+0000c910: 7468 6973 204d 6f64 756c 652c 2077 6974  this Module, wit
+0000c920: 6820 6f70 7469 6f6e 616c 6c79 2075 7064  h optionally upd
+0000c930: 6174 6564 2061 7267 756d 656e 7473 2e0a  ated arguments..
+0000c940: 0a20 2020 204e 4f54 453a 2065 6e64 2075  .    NOTE: end u
+0000c950: 7365 7273 2061 7265 2065 6e63 6f75 7261  sers are encoura
+0000c960: 6765 6420 746f 2075 7365 2074 6865 2060  ged to use the `
+0000c970: 6063 6f70 7960 6020 6d65 7468 6f64 2e20  `copy`` method. 
+0000c980: 2060 6063 6c6f 6e65 6060 2069 7320 7573   ``clone`` is us
+0000c990: 6564 0a20 2020 2020 2070 7269 6d61 7269  ed.      primari
+0000c9a0: 6c79 2066 6f72 2069 6e74 6572 6e61 6c20  ly for internal 
+0000c9b0: 726f 7574 696e 6573 2c20 616e 6420 6060  routines, and ``
+0000c9c0: 636f 7079 6060 206f 6666 6572 7320 7369  copy`` offers si
+0000c9d0: 6d70 6c65 7220 6172 6775 6d65 6e74 7320  mpler arguments 
+0000c9e0: 616e 640a 2020 2020 2020 6265 7474 6572  and.      better
+0000c9f0: 2064 6566 6175 6c74 732e 0a0a 2020 2020   defaults...    
+0000ca00: 4172 6773 3a0a 2020 2020 2020 7061 7265  Args:.      pare
+0000ca10: 6e74 3a20 5468 6520 7061 7265 6e74 206f  nt: The parent o
+0000ca20: 6620 7468 6520 636c 6f6e 652e 2054 6865  f the clone. The
+0000ca30: 2063 6c6f 6e65 2077 696c 6c20 6861 7665   clone will have
+0000ca40: 206e 6f20 7061 7265 6e74 2069 6620 6e6f   no parent if no
+0000ca50: 0a20 2020 2020 2020 2065 7870 6c69 6369  .        explici
+0000ca60: 7420 7061 7265 6e74 2069 7320 7370 6563  t parent is spec
+0000ca70: 6966 6965 642e 0a20 2020 2020 205f 6465  ified..      _de
+0000ca80: 6570 5f63 6c6f 6e65 3a20 4120 626f 6f6c  ep_clone: A bool
+0000ca90: 6561 6e20 6f72 2061 2077 6561 6b20 7661  ean or a weak va
+0000caa0: 6c75 6520 6469 6374 696f 6e61 7279 2074  lue dictionary t
+0000cab0: 6f20 636f 6e74 726f 6c20 6465 6570 2063  o control deep c
+0000cac0: 6c6f 6e69 6e67 0a20 2020 2020 2020 206f  loning.        o
+0000cad0: 6620 7375 626d 6f64 756c 6573 2e20 4966  f submodules. If
+0000cae0: 2054 7275 652c 2073 7562 6d6f 6475 6c65   True, submodule
+0000caf0: 7320 7769 6c6c 2062 6520 636c 6f6e 6564  s will be cloned
+0000cb00: 2072 6563 7572 7369 7665 6c79 2e20 4966   recursively. If
+0000cb10: 2061 2077 6561 6b0a 2020 2020 2020 2020   a weak.        
+0000cb20: 7661 6c75 6520 6469 6374 696f 6e61 7279  value dictionary
+0000cb30: 2069 7320 7061 7373 6564 2c20 6974 2077   is passed, it w
+0000cb40: 696c 6c20 6265 2075 7365 6420 746f 2063  ill be used to c
+0000cb50: 6163 6865 2063 6c6f 6e65 6420 7375 626d  ache cloned subm
+0000cb60: 6f64 756c 6573 2e0a 2020 2020 2020 2020  odules..        
+0000cb70: 5468 6973 2066 6c61 6720 6973 2075 7365  This flag is use
+0000cb80: 6420 6279 2069 6e69 742f 6170 706c 792f  d by init/apply/
+0000cb90: 6269 6e64 2074 6f20 6176 6f69 6420 7363  bind to avoid sc
+0000cba0: 6f70 6520 6c65 616b 6167 652e 0a20 2020  ope leakage..   
+0000cbb0: 2020 205f 7265 7365 745f 6e61 6d65 733a     _reset_names:
+0000cbc0: 2049 6620 5472 7565 2c20 6060 6e61 6d65   If True, ``name
+0000cbd0: 3d4e 6f6e 6560 6020 6973 2061 6c73 6f20  =None`` is also 
+0000cbe0: 7061 7373 6564 2074 6f20 7375 626d 6f64  passed to submod
+0000cbf0: 756c 6573 2077 6865 6e0a 2020 2020 2020  ules when.      
+0000cc00: 2020 636c 6f6e 696e 672e 2052 6573 6574    cloning. Reset
+0000cc10: 7469 6e67 206e 616d 6573 2069 6e20 7375  ting names in su
+0000cc20: 626d 6f64 756c 6573 2069 7320 6e65 6365  bmodules is nece
+0000cc30: 7373 6172 7920 7768 656e 2063 616c 6c69  ssary when calli
+0000cc40: 6e67 2060 602e 756e 6269 6e64 6060 2e0a  ng ``.unbind``..
+0000cc50: 2020 2020 2020 2a2a 7570 6461 7465 733a        **updates:
+0000cc60: 2041 7474 7269 6275 7465 2075 7064 6174   Attribute updat
+0000cc70: 6573 2e0a 0a20 2020 2052 6574 7572 6e73  es...    Returns
+0000cc80: 3a0a 2020 2020 2020 4120 636c 6f6e 6520  :.      A clone 
+0000cc90: 6f66 2074 6865 2074 6869 7320 4d6f 6475  of the this Modu
+0000cca0: 6c65 2077 6974 6820 7468 6520 7570 6461  le with the upda
+0000ccb0: 7465 6420 6174 7472 6962 7574 6573 2061  ted attributes a
+0000ccc0: 6e64 2070 6172 656e 742e 0a20 2020 2022  nd parent..    "
+0000ccd0: 2222 0a20 2020 2061 7474 7273 203d 207b  "".    attrs = {
+0000cce0: 0a20 2020 2020 2066 2e6e 616d 653a 2067  .      f.name: g
+0000ccf0: 6574 6174 7472 2873 656c 662c 2066 2e6e  etattr(self, f.n
+0000cd00: 616d 6529 2066 6f72 2066 2069 6e20 6461  ame) for f in da
+0000cd10: 7461 636c 6173 7365 732e 6669 656c 6473  taclasses.fields
+0000cd20: 2873 656c 6629 2069 6620 662e 696e 6974  (self) if f.init
+0000cd30: 0a20 2020 207d 0a0a 2020 2020 6174 7472  .    }..    attr
+0000cd40: 732e 7570 6461 7465 2870 6172 656e 743d  s.update(parent=
+0000cd50: 7061 7265 6e74 2c20 2a2a 7570 6461 7465  parent, **update
+0000cd60: 7329 0a0a 2020 2020 2320 4865 7265 2077  s)..    # Here w
+0000cd70: 6520 696d 706c 656d 656e 7420 6465 6570  e implement deep
+0000cd80: 2063 6c6f 6e69 6e67 206f 6620 7375 626d   cloning of subm
+0000cd90: 6f64 756c 6573 2c20 7468 6973 2069 7320  odules, this is 
+0000cda0: 6e65 6365 7373 6172 7920 746f 2061 766f  necessary to avo
+0000cdb0: 6964 2073 636f 7065 206c 6561 6b61 6765  id scope leakage
+0000cdc0: 0a20 2020 2023 2066 726f 6d20 6578 7465  .    # from exte
+0000cdd0: 726e 616c 2073 7562 6d6f 6475 6c65 7320  rnal submodules 
+0000cde0: 696e 746f 2069 6e69 742f 6170 706c 792f  into init/apply/
+0000cdf0: 6269 6e64 2077 6869 6c65 2070 7265 7365  bind while prese
+0000ce00: 7276 696e 6720 7368 6172 696e 672d 6279  rving sharing-by
+0000ce10: 2d72 6566 6572 656e 6365 0a20 2020 2023  -reference.    #
+0000ce20: 2072 656c 6174 696f 6e73 6869 7073 2062   relationships b
+0000ce30: 6574 7765 656e 2073 7562 6d6f 6475 6c65  etween submodule
+0000ce40: 732e 0a20 2020 2069 6620 5f64 6565 705f  s..    if _deep_
+0000ce50: 636c 6f6e 6520 213d 2046 616c 7365 3a0a  clone != False:.
+0000ce60: 2020 2020 2020 2320 5765 2075 7365 2061        # We use a
+0000ce70: 2077 6561 6b20 7661 6c75 6520 6469 6374   weak value dict
+0000ce80: 696f 6e61 7279 2074 6f20 6361 6368 6520  ionary to cache 
+0000ce90: 636c 6f6e 6564 2073 7562 6d6f 6475 6c65  cloned submodule
+0000cea0: 732e 2057 6865 6e20 6120 7368 6172 6564  s. When a shared
+0000ceb0: 0a20 2020 2020 2023 2073 7562 6d6f 6475  .      # submodu
+0000cec0: 6c65 2069 7320 636c 6f6e 6564 2c20 6974  le is cloned, it
+0000ced0: 7320 6f6e 6c79 2063 6c6f 6e65 6420 6f6e  s only cloned on
+0000cee0: 6365 2065 6c73 6520 6974 7320 6665 7463  ce else its fetc
+0000cef0: 6865 6420 6672 6f6d 2074 6865 2063 6163  hed from the cac
+0000cf00: 6865 2e0a 2020 2020 2020 6361 6368 6520  he..      cache 
+0000cf10: 3d20 280a 2020 2020 2020 2020 7765 616b  = (.        weak
+0000cf20: 7265 662e 5765 616b 5661 6c75 6544 6963  ref.WeakValueDic
+0000cf30: 7469 6f6e 6172 7928 290a 2020 2020 2020  tionary().      
+0000cf40: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(
+0000cf50: 5f64 6565 705f 636c 6f6e 652c 2062 6f6f  _deep_clone, boo
+0000cf60: 6c29 0a20 2020 2020 2020 2065 6c73 6520  l).        else 
+0000cf70: 5f64 6565 705f 636c 6f6e 650a 2020 2020  _deep_clone.    
+0000cf80: 2020 290a 0a20 2020 2020 2064 6566 2063    )..      def c
+0000cf90: 6c6f 6e65 5f66 6e28 6d3a 204d 6f64 756c  lone_fn(m: Modul
+0000cfa0: 6529 202d 3e20 4d6f 6475 6c65 3a0a 2020  e) -> Module:.  
+0000cfb0: 2020 2020 2020 6966 2068 6173 6174 7472        if hasattr
+0000cfc0: 286d 2c20 275f 6964 2729 3a0a 2020 2020  (m, '_id'):.    
+0000cfd0: 2020 2020 2020 6b65 7920 3d20 6d2e 5f69        key = m._i
+0000cfe0: 640a 2020 2020 2020 2020 2020 6966 206b  d.          if k
+0000cff0: 6579 2069 6e20 6361 6368 653a 0a20 2020  ey in cache:.   
+0000d000: 2020 2020 2020 2020 2072 6574 7572 6e20           return 
+0000d010: 6361 6368 655b 6b65 795d 0a20 2020 2020  cache[key].     
+0000d020: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
+0000d030: 2020 2020 2020 2069 6620 5f72 6573 6574         if _reset
+0000d040: 5f6e 616d 6573 3a0a 2020 2020 2020 2020  _names:.        
+0000d050: 2020 2020 2020 636c 6f6e 6520 3d20 6d2e        clone = m.
+0000d060: 636c 6f6e 6528 0a20 2020 2020 2020 2020  clone(.         
+0000d070: 2020 2020 2020 205f 6465 6570 5f63 6c6f         _deep_clo
+0000d080: 6e65 3d63 6163 6865 2c20 5f72 6573 6574  ne=cache, _reset
+0000d090: 5f6e 616d 6573 3d5f 7265 7365 745f 6e61  _names=_reset_na
+0000d0a0: 6d65 732c 206e 616d 653d 4e6f 6e65 0a20  mes, name=None. 
+0000d0b0: 2020 2020 2020 2020 2020 2020 2029 0a20               ). 
+0000d0c0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+0000d0d0: 0a20 2020 2020 2020 2020 2020 2020 2063  .              c
+0000d0e0: 6c6f 6e65 203d 206d 2e63 6c6f 6e65 285f  lone = m.clone(_
+0000d0f0: 6465 6570 5f63 6c6f 6e65 3d63 6163 6865  deep_clone=cache
+0000d100: 290a 2020 2020 2020 2020 2020 2020 6361  ).            ca
+0000d110: 6368 655b 6b65 795d 203d 2063 6c6f 6e65  che[key] = clone
+0000d120: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret
+0000d130: 7572 6e20 636c 6f6e 650a 2020 2020 2020  urn clone.      
+0000d140: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
+0000d150: 2020 2320 4966 2074 6865 206d 6f64 756c    # If the modul
+0000d160: 6520 646f 6573 6e27 7420 6861 7665 2061  e doesn't have a
+0000d170: 6e20 5f69 6420 6174 7472 6962 7574 6520  n _id attribute 
+0000d180: 6974 2063 6f75 6c64 2062 6520 6120 6d6f  it could be a mo
+0000d190: 636b 206f 626a 6563 740a 2020 2020 2020  ck object.      
+0000d1a0: 2020 2020 2320 736f 2077 6520 7265 7475      # so we retu
+0000d1b0: 726e 2069 7420 6173 2069 732e 0a20 2020  rn it as is..   
+0000d1c0: 2020 2020 2020 2072 6574 7572 6e20 6d0a         return m.
+0000d1d0: 0a20 2020 2020 2023 205f 6d61 705f 7375  .      # _map_su
+0000d1e0: 626d 6f64 756c 6573 2077 696c 6c20 6d61  bmodules will ma
+0000d1f0: 7020 6f76 6572 2061 6c6c 2073 7562 6d6f  p over all submo
+0000d200: 6475 6c65 7320 696e 7369 6465 2061 7474  dules inside att
+0000d210: 7273 0a20 2020 2020 2023 2076 616c 7565  rs.      # value
+0000d220: 2068 6572 6520 6361 6e20 6265 2061 6e79   here can be any
+0000d230: 2070 7974 7265 652c 206e 6f6e 2d6d 6f64   pytree, non-mod
+0000d240: 756c 6520 7661 6c75 6573 2061 7265 2069  ule values are i
+0000d250: 676e 6f72 6564 0a20 2020 2020 2066 6f72  gnored.      for
+0000d260: 2066 6965 6c64 5f6e 616d 652c 2076 616c   field_name, val
+0000d270: 7565 2069 6e20 6174 7472 732e 6974 656d  ue in attrs.item
+0000d280: 7328 293a 0a20 2020 2020 2020 2069 6620  s():.        if 
+0000d290: 6669 656c 645f 6e61 6d65 203d 3d20 2770  field_name == 'p
+0000d2a0: 6172 656e 7427 3a0a 2020 2020 2020 2020  arent':.        
+0000d2b0: 2020 636f 6e74 696e 7565 0a20 2020 2020    continue.     
+0000d2c0: 2020 2061 7474 7273 5b66 6965 6c64 5f6e     attrs[field_n
+0000d2d0: 616d 655d 203d 205f 6d61 705f 7375 626d  ame] = _map_subm
+0000d2e0: 6f64 756c 6573 2863 6c6f 6e65 5f66 6e2c  odules(clone_fn,
+0000d2f0: 2076 616c 7565 290a 0a20 2020 206d 6f64   value)..    mod
+0000d300: 756c 6520 3d20 7365 6c66 2e5f 5f63 6c61  ule = self.__cla
+0000d310: 7373 5f5f 282a 2a61 7474 7273 290a 0a20  ss__(**attrs).. 
+0000d320: 2020 2072 6574 7572 6e20 6d6f 6475 6c65     return module
+0000d330: 0a0a 2020 6465 6620 636f 7079 280a 2020  ..  def copy(.  
+0000d340: 2020 7365 6c66 3a20 4d2c 0a20 2020 202a    self: M,.    *
+0000d350: 2c0a 2020 2020 7061 7265 6e74 3a20 4f70  ,.    parent: Op
+0000d360: 7469 6f6e 616c 5b55 6e69 6f6e 5b53 636f  tional[Union[Sco
+0000d370: 7065 2c20 274d 6f64 756c 6527 2c20 5f53  pe, 'Module', _S
+0000d380: 656e 7469 6e65 6c5d 5d20 3d20 5f75 6e73  entinel]] = _uns
+0000d390: 7065 6369 6669 6564 5f70 6172 656e 742c  pecified_parent,
+0000d3a0: 0a20 2020 206e 616d 653a 204f 7074 696f  .    name: Optio
+0000d3b0: 6e61 6c5b 7374 725d 203d 204e 6f6e 652c  nal[str] = None,
+0000d3c0: 0a20 2020 202a 2a75 7064 6174 6573 2c0a  .    **updates,.
+0000d3d0: 2020 2920 2d3e 204d 3a0a 2020 2020 2222    ) -> M:.    ""
+0000d3e0: 2243 7265 6174 6573 2061 2063 6f70 7920  "Creates a copy 
+0000d3f0: 6f66 2074 6869 7320 4d6f 6475 6c65 2c20  of this Module, 
+0000d400: 7769 7468 206f 7074 696f 6e61 6c6c 7920  with optionally 
+0000d410: 7570 6461 7465 6420 6172 6775 6d65 6e74  updated argument
+0000d420: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  
+0000d430: 2020 2020 7061 7265 6e74 3a20 5468 6520      parent: The 
+0000d440: 7061 7265 6e74 206f 6620 7468 6520 636f  parent of the co
+0000d450: 7079 2e20 2042 7920 6465 6661 756c 7420  py.  By default 
+0000d460: 7468 6520 6375 7272 656e 7420 6d6f 6475  the current modu
+0000d470: 6c65 2069 7320 7461 6b65 6e0a 2020 2020  le is taken.    
+0000d480: 2020 2020 6173 2070 6172 656e 7420 6966      as parent if
+0000d490: 206e 6f74 2065 7870 6c69 6369 746c 7920   not explicitly 
+0000d4a0: 7370 6563 6966 6965 642e 0a20 2020 2020  specified..     
+0000d4b0: 206e 616d 653a 2041 206e 6577 206e 616d   name: A new nam
+0000d4c0: 6520 666f 7220 7468 6520 636f 7069 6564  e for the copied
+0000d4d0: 204d 6f64 756c 652c 2062 7920 6465 6661   Module, by defa
+0000d4e0: 756c 7420 6120 6e65 7720 6175 746f 6d61  ult a new automa
+0000d4f0: 7469 6320 6e61 6d65 0a20 2020 2020 2020  tic name.       
+0000d500: 2077 696c 6c20 6265 2067 6976 656e 2e0a   will be given..
+0000d510: 2020 2020 2020 2a2a 7570 6461 7465 733a        **updates:
+0000d520: 2041 7474 7269 6275 7465 2075 7064 6174   Attribute updat
+0000d530: 6573 2e0a 0a20 2020 2052 6574 7572 6e73  es...    Returns
+0000d540: 3a0a 2020 2020 2020 4120 636f 7079 206f  :.      A copy o
+0000d550: 6620 7468 6520 7468 6973 204d 6f64 756c  f the this Modul
+0000d560: 6520 7769 7468 2074 6865 2075 7064 6174  e with the updat
+0000d570: 6564 206e 616d 652c 2070 6172 656e 742c  ed name, parent,
+0000d580: 2061 6e64 2061 7474 7269 6275 7465 732e   and attributes.
+0000d590: 0a20 2020 2022 2222 0a20 2020 2072 6574  .    """.    ret
+0000d5a0: 7572 6e20 7365 6c66 2e63 6c6f 6e65 280a  urn self.clone(.
+0000d5b0: 2020 2020 2020 7061 7265 6e74 3d70 6172        parent=par
+0000d5c0: 656e 742c 206e 616d 653d 6e61 6d65 2c20  ent, name=name, 
+0000d5d0: 5f64 6565 705f 636c 6f6e 653d 5472 7565  _deep_clone=True
+0000d5e0: 2c20 5f72 6573 6574 5f6e 616d 6573 3d46  , _reset_names=F
+0000d5f0: 616c 7365 2c20 2a2a 7570 6461 7465 730a  alse, **updates.
+0000d600: 2020 2020 290a 0a20 2040 6f76 6572 6c6f      )..  @overlo
+0000d610: 6164 0a20 2064 6566 2076 6172 6961 626c  ad.  def variabl
+0000d620: 6528 0a20 2020 2073 656c 662c 0a20 2020  e(.    self,.   
+0000d630: 2063 6f6c 3a20 7374 722c 0a20 2020 206e   col: str,.    n
+0000d640: 616d 653a 2073 7472 2c0a 2020 2020 696e  ame: str,.    in
+0000d650: 6974 5f66 6e3a 204f 7074 696f 6e61 6c5b  it_fn: Optional[
+0000d660: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 545d  Callable[..., T]
+0000d670: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 2a69  ] = None,.    *i
+0000d680: 6e69 745f 6172 6773 2c0a 2020 2920 2d3e  nit_args,.  ) ->
+0000d690: 2056 6172 6961 626c 655b 545d 3a0a 2020   Variable[T]:.  
+0000d6a0: 2020 2e2e 2e0a 0a20 2040 6f76 6572 6c6f    .....  @overlo
+0000d6b0: 6164 0a20 2064 6566 2076 6172 6961 626c  ad.  def variabl
+0000d6c0: 6528 0a20 2020 2073 656c 662c 0a20 2020  e(.    self,.   
+0000d6d0: 2063 6f6c 3a20 7374 722c 0a20 2020 206e   col: str,.    n
+0000d6e0: 616d 653a 2073 7472 2c0a 2020 2020 696e  ame: str,.    in
+0000d6f0: 6974 5f66 6e3a 204f 7074 696f 6e61 6c5b  it_fn: Optional[
+0000d700: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 545d  Callable[..., T]
+0000d710: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 2a69  ] = None,.    *i
+0000d720: 6e69 745f 6172 6773 2c0a 2020 2020 756e  nit_args,.    un
+0000d730: 626f 783a 204c 6974 6572 616c 5b54 7275  box: Literal[Tru
+0000d740: 655d 2c0a 2020 2020 2a2a 696e 6974 5f6b  e],.    **init_k
+0000d750: 7761 7267 732c 0a20 2029 202d 3e20 5661  wargs,.  ) -> Va
+0000d760: 7269 6162 6c65 5b54 5d3a 0a20 2020 202e  riable[T]:.    .
+0000d770: 2e2e 0a0a 2020 406f 7665 726c 6f61 640a  ....  @overload.
+0000d780: 2020 6465 6620 7661 7269 6162 6c65 280a    def variable(.
+0000d790: 2020 2020 7365 6c66 2c0a 2020 2020 636f      self,.    co
+0000d7a0: 6c3a 2073 7472 2c0a 2020 2020 6e61 6d65  l: str,.    name
+0000d7b0: 3a20 7374 722c 0a20 2020 2069 6e69 745f  : str,.    init_
+0000d7c0: 666e 3a20 4f70 7469 6f6e 616c 5b43 616c  fn: Optional[Cal
+0000d7d0: 6c61 626c 655b 2e2e 2e2c 2054 5d5d 203d  lable[..., T]] =
+0000d7e0: 204e 6f6e 652c 0a20 2020 202a 696e 6974   None,.    *init
+0000d7f0: 5f61 7267 732c 0a20 2020 2075 6e62 6f78  _args,.    unbox
+0000d800: 3a20 4c69 7465 7261 6c5b 4661 6c73 655d  : Literal[False]
+0000d810: 2c0a 2020 2020 2a2a 696e 6974 5f6b 7761  ,.    **init_kwa
+0000d820: 7267 732c 0a20 2029 202d 3e20 5661 7269  rgs,.  ) -> Vari
+0000d830: 6162 6c65 5b6d 6574 612e 4178 6973 4d65  able[meta.AxisMe
+0000d840: 7461 6461 7461 5b54 5d5d 3a0a 2020 2020  tadata[T]]:.    
+0000d850: 2e2e 2e0a 0a20 2040 6f76 6572 6c6f 6164  .....  @overload
+0000d860: 0a20 2064 6566 2076 6172 6961 626c 6528  .  def variable(
+0000d870: 0a20 2020 2073 656c 662c 0a20 2020 2063  .    self,.    c
+0000d880: 6f6c 3a20 7374 722c 0a20 2020 206e 616d  ol: str,.    nam
+0000d890: 653a 2073 7472 2c0a 2020 2020 696e 6974  e: str,.    init
+0000d8a0: 5f66 6e3a 204f 7074 696f 6e61 6c5b 4361  _fn: Optional[Ca
+0000d8b0: 6c6c 6162 6c65 5b2e 2e2e 2c20 545d 5d20  llable[..., T]] 
+0000d8c0: 3d20 4e6f 6e65 2c0a 2020 2020 2a69 6e69  = None,.    *ini
+0000d8d0: 745f 6172 6773 2c0a 2020 2020 756e 626f  t_args,.    unbo
+0000d8e0: 783a 2062 6f6f 6c20 3d20 5472 7565 2c0a  x: bool = True,.
+0000d8f0: 2020 2020 2a2a 696e 6974 5f6b 7761 7267      **init_kwarg
+0000d900: 732c 0a20 2029 202d 3e20 556e 696f 6e5b  s,.  ) -> Union[
+0000d910: 5661 7269 6162 6c65 5b54 5d2c 2056 6172  Variable[T], Var
+0000d920: 6961 626c 655b 6d65 7461 2e41 7869 734d  iable[meta.AxisM
+0000d930: 6574 6164 6174 615b 545d 5d5d 3a0a 2020  etadata[T]]]:.  
+0000d940: 2020 2e2e 2e0a 0a20 2064 6566 2076 6172    .....  def var
+0000d950: 6961 626c 6528 0a20 2020 2073 656c 662c  iable(.    self,
+0000d960: 0a20 2020 2063 6f6c 3a20 7374 722c 0a20  .    col: str,. 
+0000d970: 2020 206e 616d 653a 2073 7472 2c0a 2020     name: str,.  
+0000d980: 2020 696e 6974 5f66 6e3a 204f 7074 696f    init_fn: Optio
+0000d990: 6e61 6c5b 4361 6c6c 6162 6c65 5b2e 2e2e  nal[Callable[...
+0000d9a0: 2c20 545d 5d20 3d20 4e6f 6e65 2c0a 2020  , T]] = None,.  
+0000d9b0: 2020 2a69 6e69 745f 6172 6773 2c0a 2020    *init_args,.  
+0000d9c0: 2020 756e 626f 783a 2062 6f6f 6c20 3d20    unbox: bool = 
+0000d9d0: 5472 7565 2c0a 2020 2020 2a2a 696e 6974  True,.    **init
+0000d9e0: 5f6b 7761 7267 732c 0a20 2029 202d 3e20  _kwargs,.  ) -> 
+0000d9f0: 556e 696f 6e5b 5661 7269 6162 6c65 5b54  Union[Variable[T
+0000da00: 5d2c 2056 6172 6961 626c 655b 6d65 7461  ], Variable[meta
+0000da10: 2e41 7869 734d 6574 6164 6174 615b 545d  .AxisMetadata[T]
+0000da20: 5d5d 3a0a 2020 2020 2222 2244 6563 6c61  ]]:.    """Decla
+0000da30: 7265 7320 616e 6420 7265 7475 726e 7320  res and returns 
+0000da40: 6120 7661 7269 6162 6c65 2069 6e20 7468  a variable in th
+0000da50: 6973 204d 6f64 756c 652e 0a0a 2020 2020  is Module...    
+0000da60: 5365 6520 3a6d 6f64 3a60 666c 6178 2e63  See :mod:`flax.c
+0000da70: 6f72 652e 7661 7269 6162 6c65 7360 2066  ore.variables` f
+0000da80: 6f72 206d 6f72 6520 696e 666f 726d 6174  or more informat
+0000da90: 696f 6e2e 2053 6565 2061 6c73 6f20 3a6d  ion. See also :m
+0000daa0: 6574 683a 6070 6172 616d 600a 2020 2020  eth:`param`.    
+0000dab0: 666f 7220 6120 7368 6f72 7468 616e 6420  for a shorthand 
+0000dac0: 7761 7920 746f 2064 6566 696e 6520 7265  way to define re
+0000dad0: 6164 2d6f 6e6c 7920 7661 7269 6162 6c65  ad-only variable
+0000dae0: 7320 696e 2074 6865 2022 7061 7261 6d73  s in the "params
+0000daf0: 220a 2020 2020 636f 6c6c 6563 7469 6f6e  ".    collection
+0000db00: 2e0a 0a20 2020 2043 6f6e 7472 6172 7920  ...    Contrary 
+0000db10: 746f 203a 6d65 7468 3a60 7061 7261 6d60  to :meth:`param`
+0000db20: 2c20 616c 6c20 6172 6775 6d65 6e74 7320  , all arguments 
+0000db30: 7061 7373 696e 6720 7573 696e 6720 6060  passing using ``
+0000db40: 696e 6974 5f66 6e60 6020 7368 6f75 6c64  init_fn`` should
+0000db50: 2062 650a 2020 2020 7061 7373 6564 206f   be.    passed o
+0000db60: 6e20 6578 706c 6963 6974 6c79 3a3a 0a0a  n explicitly::..
+0000db70: 2020 2020 2020 3e3e 3e20 636c 6173 7320        >>> class 
+0000db80: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
+0000db90: 2020 2020 2020 2e2e 2e20 2020 406e 6e2e        ...   @nn.
+0000dba0: 636f 6d70 6163 740a 2020 2020 2020 2e2e  compact.      ..
+0000dbb0: 2e20 2020 6465 6620 5f5f 6361 6c6c 5f5f  .   def __call__
+0000dbc0: 2873 656c 662c 2078 293a 0a20 2020 2020  (self, x):.     
+0000dbd0: 202e 2e2e 2020 2020 2078 203d 206e 6e2e   ...     x = nn.
+0000dbe0: 4465 6e73 6528 3429 2878 290a 2020 2020  Dense(4)(x).    
+0000dbf0: 2020 2e2e 2e20 2020 2020 6b65 7920 3d20    ...     key = 
+0000dc00: 7365 6c66 2e6d 616b 655f 726e 6728 2773  self.make_rng('s
+0000dc10: 7461 7473 2729 0a20 2020 2020 202e 2e2e  tats').      ...
+0000dc20: 2020 2020 206d 6561 6e20 3d20 7365 6c66       mean = self
+0000dc30: 2e76 6172 6961 626c 6528 2773 7461 7473  .variable('stats
+0000dc40: 272c 2027 6d65 616e 272c 206e 6e2e 696e  ', 'mean', nn.in
+0000dc50: 6974 6961 6c69 7a65 7273 2e6c 6563 756e  itializers.lecun
+0000dc60: 5f6e 6f72 6d61 6c28 292c 206b 6579 2c20  _normal(), key, 
+0000dc70: 782e 7368 6170 6529 0a20 2020 2020 202e  x.shape).      .
+0000dc80: 2e2e 2020 2020 202e 2e2e 0a20 2020 2020  ..     ....     
+0000dc90: 202e 2e2e 2020 2020 2072 6574 7572 6e20   ...     return 
+0000dca0: 7820 2a20 6d65 616e 2e76 616c 7565 0a20  x * mean.value. 
+0000dcb0: 2020 2020 203e 3e3e 2076 6172 6961 626c       >>> variabl
+0000dcc0: 6573 203d 2046 6f6f 2829 2e69 6e69 7428  es = Foo().init(
+0000dcd0: 7b27 7061 7261 6d73 273a 206a 6178 2e72  {'params': jax.r
+0000dce0: 616e 646f 6d2e 6b65 7928 3029 2c20 2773  andom.key(0), 's
+0000dcf0: 7461 7473 273a 206a 6178 2e72 616e 646f  tats': jax.rando
+0000dd00: 6d2e 6b65 7928 3129 7d2c 206a 6e70 2e6f  m.key(1)}, jnp.o
+0000dd10: 6e65 7328 2832 2c20 3329 2929 0a20 2020  nes((2, 3))).   
+0000dd20: 2020 203e 3e3e 206a 6178 2e74 7265 655f     >>> jax.tree_
+0000dd30: 7574 696c 2e74 7265 655f 6d61 7028 6a6e  util.tree_map(jn
+0000dd40: 702e 7368 6170 652c 2076 6172 6961 626c  p.shape, variabl
+0000dd50: 6573 290a 2020 2020 2020 7b27 7061 7261  es).      {'para
+0000dd60: 6d73 273a 207b 2744 656e 7365 5f30 273a  ms': {'Dense_0':
+0000dd70: 207b 2762 6961 7327 3a20 2834 2c29 2c20   {'bias': (4,), 
+0000dd80: 276b 6572 6e65 6c27 3a20 2833 2c20 3429  'kernel': (3, 4)
+0000dd90: 7d7d 2c20 2773 7461 7473 273a 207b 276d  }}, 'stats': {'m
+0000dda0: 6561 6e27 3a20 2832 2c20 3429 7d7d 0a0a  ean': (2, 4)}}..
+0000ddb0: 2020 2020 496e 2074 6865 2065 7861 6d70      In the examp
+0000ddc0: 6c65 2061 626f 7665 2c20 7468 6520 6675  le above, the fu
+0000ddd0: 6e63 7469 6f6e 2060 606c 6563 756e 5f6e  nction ``lecun_n
+0000dde0: 6f72 6d61 6c60 6020 6578 7065 6374 7320  ormal`` expects 
+0000ddf0: 7477 6f20 6172 6775 6d65 6e74 733a 0a20  two arguments:. 
+0000de00: 2020 2060 606b 6579 6060 2061 6e64 2060     ``key`` and `
+0000de10: 6073 6861 7065 6060 2c20 616e 6420 626f  `shape``, and bo
+0000de20: 7468 2068 6176 6520 746f 2062 6520 7061  th have to be pa
+0000de30: 7373 6564 206f 6e2e 2054 6865 2050 524e  ssed on. The PRN
+0000de40: 4720 666f 7220 6060 7374 6174 7360 600a  G for ``stats``.
+0000de50: 2020 2020 6861 7320 746f 2062 6520 7072      has to be pr
+0000de60: 6f76 6964 6564 2065 7870 6c69 6369 746c  ovided explicitl
+0000de70: 7920 7768 656e 2063 616c 6c69 6e67 203a  y when calling :
+0000de80: 6d65 7468 3a60 696e 6974 6020 616e 6420  meth:`init` and 
+0000de90: 3a6d 6574 683a 6061 7070 6c79 602e 0a0a  :meth:`apply`...
+0000dea0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      
+0000deb0: 636f 6c3a 2054 6865 2076 6172 6961 626c  col: The variabl
+0000dec0: 6520 636f 6c6c 6563 7469 6f6e 206e 616d  e collection nam
+0000ded0: 652e 0a20 2020 2020 206e 616d 653a 2054  e..      name: T
+0000dee0: 6865 2076 6172 6961 626c 6520 6e61 6d65  he variable name
+0000def0: 2e0a 2020 2020 2020 696e 6974 5f66 6e3a  ..      init_fn:
+0000df00: 2054 6865 2066 756e 6374 696f 6e20 7468   The function th
+0000df10: 6174 2077 696c 6c20 6265 2063 616c 6c65  at will be calle
+0000df20: 6420 746f 2063 6f6d 7075 7465 2074 6865  d to compute the
+0000df30: 2069 6e69 7469 616c 2076 616c 7565 206f   initial value o
+0000df40: 660a 2020 2020 2020 2020 7468 6973 2076  f.        this v
+0000df50: 6172 6961 626c 652e 2054 6869 7320 6675  ariable. This fu
+0000df60: 6e63 7469 6f6e 2077 696c 6c20 6f6e 6c79  nction will only
+0000df70: 2062 6520 6361 6c6c 6564 2074 6865 2066   be called the f
+0000df80: 6972 7374 2074 696d 6520 7468 6973 0a20  irst time this. 
+0000df90: 2020 2020 2020 2076 6172 6961 626c 6520         variable 
+0000dfa0: 6973 2075 7365 6420 696e 2074 6869 7320  is used in this 
+0000dfb0: 6d6f 6475 6c65 2e20 4966 204e 6f6e 652c  module. If None,
+0000dfc0: 2074 6865 2076 6172 6961 626c 6520 6d75   the variable mu
+0000dfd0: 7374 2061 6c72 6561 6479 2062 650a 2020  st already be.  
+0000dfe0: 2020 2020 2020 696e 6974 6961 6c69 7a65        initialize
+0000dff0: 6420 6f74 6865 7277 6973 6520 616e 2065  d otherwise an e
+0000e000: 7272 6f72 2069 7320 7261 6973 6564 2e0a  rror is raised..
+0000e010: 2020 2020 2020 2a69 6e69 745f 6172 6773        *init_args
+0000e020: 3a20 5468 6520 706f 7369 7469 6f6e 616c  : The positional
+0000e030: 2061 7267 756d 656e 7473 2074 6f20 7061   arguments to pa
+0000e040: 7373 2074 6f20 696e 6974 5f66 6e2e 0a20  ss to init_fn.. 
+0000e050: 2020 2020 2075 6e62 6f78 3a20 4966 2054       unbox: If T
+0000e060: 7275 652c 2060 6041 7869 734d 6574 6164  rue, ``AxisMetad
+0000e070: 6174 6160 6020 696e 7374 616e 6365 7320  ata`` instances 
+0000e080: 6172 6520 7265 706c 6163 6564 2062 7920  are replaced by 
+0000e090: 7468 6569 7220 756e 626f 7865 640a 2020  their unboxed.  
+0000e0a0: 2020 2020 2020 7661 6c75 652c 2073 6565        value, see
+0000e0b0: 2060 6066 6c61 782e 6e6e 2e6d 6574 612e   ``flax.nn.meta.
+0000e0c0: 756e 626f 7860 6020 2864 6566 6175 6c74  unbox`` (default
+0000e0d0: 3a20 5472 7565 292e 0a20 2020 2020 202a  : True)..      *
+0000e0e0: 2a69 6e69 745f 6b77 6172 6773 3a20 5468  *init_kwargs: Th
+0000e0f0: 6520 6b65 792d 776f 7264 2061 7267 756d  e key-word argum
+0000e100: 656e 7473 2074 6f20 7061 7373 2074 6f20  ents to pass to 
+0000e110: 696e 6974 5f66 6e0a 0a20 2020 2052 6574  init_fn..    Ret
+0000e120: 7572 6e73 3a0a 2020 2020 2020 4120 3a63  urns:.      A :c
+0000e130: 6c61 7373 3a60 666c 6178 2e63 6f72 652e  lass:`flax.core.
+0000e140: 7661 7269 6162 6c65 732e 5661 7269 6162  variables.Variab
+0000e150: 6c65 6020 7468 6174 2063 616e 2062 6520  le` that can be 
+0000e160: 7265 6164 206f 7220 7365 7420 7669 610a  read or set via.
+0000e170: 2020 2020 2020 222e 7661 6c75 6522 2061        ".value" a
+0000e180: 7474 7269 6275 7465 2e20 5468 726f 7773  ttribute. Throws
+0000e190: 2061 6e20 6572 726f 7220 6966 2074 6865   an error if the
+0000e1a0: 2076 6172 6961 626c 6520 6578 6973 7473   variable exists
+0000e1b0: 2061 6c72 6561 6479 2e0a 2020 2020 2222   already..    ""
+0000e1c0: 220a 2020 2020 6966 206e 6f74 2073 656c  ".    if not sel
+0000e1d0: 662e 5f69 6e69 7469 616c 697a 6174 696f  f._initializatio
+0000e1e0: 6e5f 616c 6c6f 7765 643a 0a20 2020 2020  n_allowed:.     
+0000e1f0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+0000e200: 7228 0a20 2020 2020 2020 2027 5661 7269  r(.        'Vari
+0000e210: 6162 6c65 7320 6d75 7374 2062 6520 696e  ables must be in
+0000e220: 6974 6961 6c69 7a65 6420 696e 2060 7365  itialized in `se
+0000e230: 7475 7028 2960 206f 7220 696e 2061 206d  tup()` or in a m
+0000e240: 6574 686f 6420 270a 2020 2020 2020 2020  ethod '.        
+0000e250: 2777 7261 7070 6564 2069 6e20 6040 636f  'wrapped in `@co
+0000e260: 6d70 6163 7460 270a 2020 2020 2020 290a  mpact`'.      ).
+0000e270: 2020 2020 6966 2073 656c 662e 5f6e 616d      if self._nam
+0000e280: 655f 7461 6b65 6e28 6e61 6d65 2c20 636f  e_taken(name, co
+0000e290: 6c6c 6563 7469 6f6e 3d63 6f6c 293a 0a20  llection=col):. 
+0000e2a0: 2020 2020 2072 6169 7365 2065 7272 6f72       raise error
+0000e2b0: 732e 4e61 6d65 496e 5573 6545 7272 6f72  s.NameInUseError
+0000e2c0: 2827 7661 7269 6162 6c65 272c 206e 616d  ('variable', nam
+0000e2d0: 652c 2073 656c 662e 5f5f 636c 6173 735f  e, self.__class_
+0000e2e0: 5f2e 5f5f 6e61 6d65 5f5f 290a 2020 2020  _.__name__).    
+0000e2f0: 6173 7365 7274 2073 656c 662e 7363 6f70  assert self.scop
+0000e300: 6520 6973 206e 6f74 204e 6f6e 650a 2020  e is not None.  
+0000e310: 2020 7620 3d20 7365 6c66 2e73 636f 7065    v = self.scope
+0000e320: 2e76 6172 6961 626c 6528 0a20 2020 2020  .variable(.     
+0000e330: 2063 6f6c 2c20 6e61 6d65 2c20 696e 6974   col, name, init
+0000e340: 5f66 6e2c 202a 696e 6974 5f61 7267 732c  _fn, *init_args,
+0000e350: 2075 6e62 6f78 3d75 6e62 6f78 2c20 2a2a   unbox=unbox, **
+0000e360: 696e 6974 5f6b 7761 7267 730a 2020 2020  init_kwargs.    
+0000e370: 290a 2020 2020 7365 6c66 2e5f 7374 6174  ).    self._stat
+0000e380: 652e 6368 696c 6472 656e 5b6e 616d 655d  e.children[name]
+0000e390: 203d 2063 6f6c 0a20 2020 2072 6574 7572   = col.    retur
+0000e3a0: 6e20 760a 0a20 2040 6f76 6572 6c6f 6164  n v..  @overload
+0000e3b0: 0a20 2064 6566 2070 6172 616d 280a 2020  .  def param(.  
+0000e3c0: 2020 7365 6c66 2c20 6e61 6d65 3a20 7374    self, name: st
+0000e3d0: 722c 2069 6e69 745f 666e 3a20 4361 6c6c  r, init_fn: Call
+0000e3e0: 6162 6c65 5b2e 2e2e 2c20 545d 2c20 2a69  able[..., T], *i
+0000e3f0: 6e69 745f 6172 6773 2c0a 2020 2920 2d3e  nit_args,.  ) ->
+0000e400: 2054 3a0a 2020 2020 2e2e 2e0a 0a20 2040   T:.    .....  @
+0000e410: 6f76 6572 6c6f 6164 0a20 2064 6566 2070  overload.  def p
+0000e420: 6172 616d 280a 2020 2020 7365 6c66 2c0a  aram(.    self,.
+0000e430: 2020 2020 6e61 6d65 3a20 7374 722c 0a20      name: str,. 
+0000e440: 2020 2069 6e69 745f 666e 3a20 4361 6c6c     init_fn: Call
+0000e450: 6162 6c65 5b2e 2e2e 2c20 545d 2c0a 2020  able[..., T],.  
+0000e460: 2020 2a69 6e69 745f 6172 6773 2c0a 2020    *init_args,.  
+0000e470: 2020 756e 626f 783a 204c 6974 6572 616c    unbox: Literal
+0000e480: 5b54 7275 655d 2c0a 2020 2020 2a2a 696e  [True],.    **in
+0000e490: 6974 5f6b 7761 7267 732c 0a20 2029 202d  it_kwargs,.  ) -
+0000e4a0: 3e20 543a 0a20 2020 202e 2e2e 0a0a 2020  > T:.    .....  
+0000e4b0: 406f 7665 726c 6f61 640a 2020 6465 6620  @overload.  def 
+0000e4c0: 7061 7261 6d28 0a20 2020 2073 656c 662c  param(.    self,
+0000e4d0: 0a20 2020 206e 616d 653a 2073 7472 2c0a  .    name: str,.
+0000e4e0: 2020 2020 696e 6974 5f66 6e3a 2043 616c      init_fn: Cal
+0000e4f0: 6c61 626c 655b 2e2e 2e2c 2054 5d2c 0a20  lable[..., T],. 
+0000e500: 2020 202a 696e 6974 5f61 7267 732c 0a20     *init_args,. 
+0000e510: 2020 2075 6e62 6f78 3a20 4c69 7465 7261     unbox: Litera
+0000e520: 6c5b 4661 6c73 655d 2c0a 2020 2020 2a2a  l[False],.    **
+0000e530: 696e 6974 5f6b 7761 7267 732c 0a20 2029  init_kwargs,.  )
+0000e540: 202d 3e20 6d65 7461 2e41 7869 734d 6574   -> meta.AxisMet
+0000e550: 6164 6174 615b 545d 3a0a 2020 2020 2e2e  adata[T]:.    ..
+0000e560: 2e0a 0a20 2040 6f76 6572 6c6f 6164 0a20  ...  @overload. 
+0000e570: 2064 6566 2070 6172 616d 280a 2020 2020   def param(.    
+0000e580: 7365 6c66 2c0a 2020 2020 6e61 6d65 3a20  self,.    name: 
+0000e590: 7374 722c 0a20 2020 2069 6e69 745f 666e  str,.    init_fn
+0000e5a0: 3a20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  : Callable[..., 
+0000e5b0: 545d 2c0a 2020 2020 2a69 6e69 745f 6172  T],.    *init_ar
+0000e5c0: 6773 2c0a 2020 2020 756e 626f 783a 2062  gs,.    unbox: b
+0000e5d0: 6f6f 6c2c 0a20 2020 202a 2a69 6e69 745f  ool,.    **init_
+0000e5e0: 6b77 6172 6773 2c0a 2020 2920 2d3e 2055  kwargs,.  ) -> U
+0000e5f0: 6e69 6f6e 5b54 2c20 6d65 7461 2e41 7869  nion[T, meta.Axi
+0000e600: 734d 6574 6164 6174 615b 545d 5d3a 0a20  sMetadata[T]]:. 
+0000e610: 2020 202e 2e2e 0a0a 2020 6465 6620 7061     .....  def pa
+0000e620: 7261 6d28 0a20 2020 2073 656c 662c 0a20  ram(.    self,. 
+0000e630: 2020 206e 616d 653a 2073 7472 2c0a 2020     name: str,.  
+0000e640: 2020 696e 6974 5f66 6e3a 2043 616c 6c61    init_fn: Calla
+0000e650: 626c 655b 2e2e 2e2c 2054 5d2c 0a20 2020  ble[..., T],.   
+0000e660: 202a 696e 6974 5f61 7267 732c 0a20 2020   *init_args,.   
+0000e670: 2075 6e62 6f78 3a20 626f 6f6c 203d 2054   unbox: bool = T
+0000e680: 7275 652c 0a20 2020 202a 2a69 6e69 745f  rue,.    **init_
+0000e690: 6b77 6172 6773 2c0a 2020 2920 2d3e 2055  kwargs,.  ) -> U
+0000e6a0: 6e69 6f6e 5b54 2c20 6d65 7461 2e41 7869  nion[T, meta.Axi
+0000e6b0: 734d 6574 6164 6174 615b 545d 5d3a 0a20  sMetadata[T]]:. 
+0000e6c0: 2020 2022 2222 4465 636c 6172 6573 2061     """Declares a
+0000e6d0: 6e64 2072 6574 7572 6e73 2061 2070 6172  nd returns a par
+0000e6e0: 616d 6574 6572 2069 6e20 7468 6973 204d  ameter in this M
+0000e6f0: 6f64 756c 652e 0a0a 2020 2020 5061 7261  odule...    Para
+0000e700: 6d65 7465 7273 2061 7265 2072 6561 642d  meters are read-
+0000e710: 6f6e 6c79 2076 6172 6961 626c 6573 2069  only variables i
+0000e720: 6e20 7468 6520 636f 6c6c 6563 7469 6f6e  n the collection
+0000e730: 206e 616d 6564 2022 7061 7261 6d73 222e   named "params".
+0000e740: 2053 6565 0a20 2020 203a 6d6f 643a 6066   See.    :mod:`f
+0000e750: 6c61 782e 636f 7265 2e76 6172 6961 626c  lax.core.variabl
+0000e760: 6573 6020 666f 7220 6d6f 7265 2064 6574  es` for more det
+0000e770: 6169 6c73 206f 6e20 7661 7269 6162 6c65  ails on variable
+0000e780: 732e 0a0a 2020 2020 5468 6520 6669 7273  s...    The firs
+0000e790: 7420 6172 6775 6d65 6e74 206f 6620 6060  t argument of ``
+0000e7a0: 696e 6974 5f66 6e60 6020 6973 2061 7373  init_fn`` is ass
+0000e7b0: 756d 6564 2074 6f20 6265 2061 2050 524e  umed to be a PRN
+0000e7c0: 4720 6b65 792c 2077 6869 6368 2069 730a  G key, which is.
+0000e7d0: 2020 2020 7072 6f76 6964 6564 2061 7574      provided aut
+0000e7e0: 6f6d 6174 6963 616c 6c79 2061 6e64 2064  omatically and d
+0000e7f0: 6f65 7320 6e6f 7420 6861 7665 2074 6f20  oes not have to 
+0000e800: 6265 2070 6173 7365 6420 7573 696e 6720  be passed using 
+0000e810: 6060 696e 6974 5f61 7267 7360 600a 2020  ``init_args``.  
+0000e820: 2020 6f72 2060 6069 6e69 745f 6b77 6172    or ``init_kwar
+0000e830: 6773 6060 3a3a 0a0a 2020 2020 2020 3e3e  gs``::..      >>
+0000e840: 3e20 636c 6173 7320 466f 6f28 6e6e 2e4d  > class Foo(nn.M
+0000e850: 6f64 756c 6529 3a0a 2020 2020 2020 2e2e  odule):.      ..
+0000e860: 2e20 2020 406e 6e2e 636f 6d70 6163 740a  .   @nn.compact.
+0000e870: 2020 2020 2020 2e2e 2e20 2020 6465 6620        ...   def 
+0000e880: 5f5f 6361 6c6c 5f5f 2873 656c 662c 2078  __call__(self, x
+0000e890: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
+0000e8a0: 2078 203d 206e 6e2e 4465 6e73 6528 3429   x = nn.Dense(4)
+0000e8b0: 2878 290a 2020 2020 2020 2e2e 2e20 2020  (x).      ...   
+0000e8c0: 2020 6d65 616e 203d 2073 656c 662e 7061    mean = self.pa
+0000e8d0: 7261 6d28 276d 6561 6e27 2c20 6e6e 2e69  ram('mean', nn.i
+0000e8e0: 6e69 7469 616c 697a 6572 732e 6c65 6375  nitializers.lecu
+0000e8f0: 6e5f 6e6f 726d 616c 2829 2c20 782e 7368  n_normal(), x.sh
+0000e900: 6170 6529 0a20 2020 2020 202e 2e2e 2020  ape).      ...  
+0000e910: 2020 202e 2e2e 0a20 2020 2020 202e 2e2e     ....      ...
+0000e920: 2020 2020 2072 6574 7572 6e20 7820 2a20       return x * 
+0000e930: 6d65 616e 0a20 2020 2020 203e 3e3e 2076  mean.      >>> v
+0000e940: 6172 6961 626c 6573 203d 2046 6f6f 2829  ariables = Foo()
+0000e950: 2e69 6e69 7428 7b27 7061 7261 6d73 273a  .init({'params':
+0000e960: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
+0000e970: 3029 2c20 2773 7461 7473 273a 206a 6178  0), 'stats': jax
+0000e980: 2e72 616e 646f 6d2e 6b65 7928 3129 7d2c  .random.key(1)},
+0000e990: 206a 6e70 2e6f 6e65 7328 2832 2c20 3329   jnp.ones((2, 3)
+0000e9a0: 2929 0a20 2020 2020 203e 3e3e 206a 6178  )).      >>> jax
+0000e9b0: 2e74 7265 655f 7574 696c 2e74 7265 655f  .tree_util.tree_
+0000e9c0: 6d61 7028 6a6e 702e 7368 6170 652c 2076  map(jnp.shape, v
+0000e9d0: 6172 6961 626c 6573 290a 2020 2020 2020  ariables).      
+0000e9e0: 7b27 7061 7261 6d73 273a 207b 2744 656e  {'params': {'Den
+0000e9f0: 7365 5f30 273a 207b 2762 6961 7327 3a20  se_0': {'bias': 
+0000ea00: 2834 2c29 2c20 276b 6572 6e65 6c27 3a20  (4,), 'kernel': 
+0000ea10: 2833 2c20 3429 7d2c 2027 6d65 616e 273a  (3, 4)}, 'mean':
+0000ea20: 2028 322c 2034 297d 7d0a 0a20 2020 2049   (2, 4)}}..    I
+0000ea30: 6e20 7468 6520 6578 616d 706c 6520 6162  n the example ab
+0000ea40: 6f76 652c 2074 6865 2066 756e 6374 696f  ove, the functio
+0000ea50: 6e20 6060 6c65 6375 6e5f 6e6f 726d 616c  n ``lecun_normal
+0000ea60: 6060 2065 7870 6563 7473 2074 776f 2061  `` expects two a
+0000ea70: 7267 756d 656e 7473 3a0a 2020 2020 6060  rguments:.    ``
+0000ea80: 6b65 7960 6020 616e 6420 6060 7368 6170  key`` and ``shap
+0000ea90: 6560 602c 2062 7574 206f 6e6c 7920 6060  e``, but only ``
+0000eaa0: 7368 6170 6560 6020 6861 7320 746f 2062  shape`` has to b
+0000eab0: 6520 7072 6f76 6964 6564 2065 7870 6c69  e provided expli
+0000eac0: 6369 746c 793b 0a20 2020 2060 606b 6579  citly;.    ``key
+0000ead0: 6060 2069 7320 7365 7420 6175 746f 6d61  `` is set automa
+0000eae0: 7469 6361 6c6c 7920 7573 696e 6720 7468  tically using th
+0000eaf0: 6520 5052 4e47 2066 6f72 2060 6070 6172  e PRNG for ``par
+0000eb00: 616d 7360 6020 7468 6174 2069 7320 7061  ams`` that is pa
+0000eb10: 7373 6564 0a20 2020 2077 6865 6e20 696e  ssed.    when in
+0000eb20: 6974 6961 6c69 7a69 6e67 2074 6865 206d  itializing the m
+0000eb30: 6f64 756c 6520 7573 696e 6720 3a6d 6574  odule using :met
+0000eb40: 683a 6069 6e69 7460 2e0a 0a20 2020 2041  h:`init`...    A
+0000eb50: 7267 733a 0a20 2020 2020 206e 616d 653a  rgs:.      name:
+0000eb60: 2054 6865 2070 6172 616d 6574 6572 206e   The parameter n
+0000eb70: 616d 652e 0a20 2020 2020 2069 6e69 745f  ame..      init_
+0000eb80: 666e 3a20 5468 6520 6675 6e63 7469 6f6e  fn: The function
+0000eb90: 2074 6861 7420 7769 6c6c 2062 6520 6361   that will be ca
+0000eba0: 6c6c 6564 2074 6f20 636f 6d70 7574 6520  lled to compute 
+0000ebb0: 7468 6520 696e 6974 6961 6c20 7661 6c75  the initial valu
+0000ebc0: 6520 6f66 0a20 2020 2020 2020 2074 6869  e of.        thi
+0000ebd0: 7320 7661 7269 6162 6c65 2e20 5468 6973  s variable. This
+0000ebe0: 2066 756e 6374 696f 6e20 7769 6c6c 206f   function will o
+0000ebf0: 6e6c 7920 6265 2063 616c 6c65 6420 7468  nly be called th
+0000ec00: 6520 6669 7273 7420 7469 6d65 2074 6869  e first time thi
+0000ec10: 730a 2020 2020 2020 2020 7061 7261 6d65  s.        parame
+0000ec20: 7465 7220 6973 2075 7365 6420 696e 2074  ter is used in t
+0000ec30: 6869 7320 6d6f 6475 6c65 2e0a 2020 2020  his module..    
+0000ec40: 2020 2a69 6e69 745f 6172 6773 3a20 5468    *init_args: Th
+0000ec50: 6520 706f 7369 7469 6f6e 616c 2061 7267  e positional arg
+0000ec60: 756d 656e 7473 2074 6f20 7061 7373 2074  uments to pass t
+0000ec70: 6f20 696e 6974 5f66 6e2e 0a20 2020 2020  o init_fn..     
+0000ec80: 2075 6e62 6f78 3a20 4966 2054 7275 652c   unbox: If True,
+0000ec90: 2060 6041 7869 734d 6574 6164 6174 6160   ``AxisMetadata`
+0000eca0: 6020 696e 7374 616e 6365 7320 6172 6520  ` instances are 
+0000ecb0: 7265 706c 6163 6564 2062 7920 7468 6569  replaced by thei
+0000ecc0: 7220 756e 626f 7865 640a 2020 2020 2020  r unboxed.      
+0000ecd0: 2020 7661 6c75 652c 2073 6565 2060 6066    value, see ``f
+0000ece0: 6c61 782e 6e6e 2e6d 6574 612e 756e 626f  lax.nn.meta.unbo
+0000ecf0: 7860 6020 2864 6566 6175 6c74 3a20 5472  x`` (default: Tr
+0000ed00: 7565 292e 0a20 2020 2020 202a 2a69 6e69  ue)..      **ini
+0000ed10: 745f 6b77 6172 6773 3a20 5468 6520 6b65  t_kwargs: The ke
+0000ed20: 792d 776f 7264 2061 7267 756d 656e 7473  y-word arguments
+0000ed30: 2074 6f20 7061 7373 2074 6f20 696e 6974   to pass to init
+0000ed40: 5f66 6e2e 0a0a 2020 2020 5265 7475 726e  _fn...    Return
+0000ed50: 733a 0a20 2020 2020 2054 6865 2076 616c  s:.      The val
+0000ed60: 7565 206f 6620 7468 6520 696e 6974 6961  ue of the initia
+0000ed70: 6c69 7a65 6420 7061 7261 6d65 7465 722e  lized parameter.
+0000ed80: 2054 6872 6f77 7320 616e 2065 7272 6f72   Throws an error
+0000ed90: 2069 6620 7468 6520 7061 7261 6d65 7465   if the paramete
+0000eda0: 720a 2020 2020 2020 6578 6973 7473 2061  r.      exists a
+0000edb0: 6c72 6561 6479 2e0a 2020 2020 2222 220a  lready..    """.
+0000edc0: 2020 2020 6966 206e 6f74 2073 656c 662e      if not self.
+0000edd0: 5f69 6e69 7469 616c 697a 6174 696f 6e5f  _initialization_
+0000ede0: 616c 6c6f 7765 643a 0a20 2020 2020 2072  allowed:.      r
+0000edf0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+0000ee00: 0a20 2020 2020 2020 2027 5061 7261 6d65  .        'Parame
+0000ee10: 7465 7273 206d 7573 7420 6265 2069 6e69  ters must be ini
+0000ee20: 7469 616c 697a 6564 2069 6e20 6073 6574  tialized in `set
+0000ee30: 7570 2829 6020 6f72 2069 6e20 6120 6d65  up()` or in a me
+0000ee40: 7468 6f64 2027 0a20 2020 2020 2020 2027  thod '.        '
+0000ee50: 7772 6170 7065 6420 696e 2060 4063 6f6d  wrapped in `@com
+0000ee60: 7061 6374 6027 0a20 2020 2020 2029 0a20  pact`'.      ). 
+0000ee70: 2020 2069 6620 7365 6c66 2e5f 6e61 6d65     if self._name
+0000ee80: 5f74 616b 656e 286e 616d 652c 2063 6f6c  _taken(name, col
+0000ee90: 6c65 6374 696f 6e3d 2770 6172 616d 7327  lection='params'
+0000eea0: 293a 0a20 2020 2020 2072 6169 7365 2065  ):.      raise e
+0000eeb0: 7272 6f72 732e 4e61 6d65 496e 5573 6545  rrors.NameInUseE
+0000eec0: 7272 6f72 2827 7061 7261 6d27 2c20 6e61  rror('param', na
+0000eed0: 6d65 2c20 7365 6c66 2e5f 5f63 6c61 7373  me, self.__class
+0000eee0: 5f5f 2e5f 5f6e 616d 655f 5f29 0a20 2020  __.__name__).   
+0000eef0: 2061 7373 6572 7420 7365 6c66 2e73 636f   assert self.sco
+0000ef00: 7065 2069 7320 6e6f 7420 4e6f 6e65 0a20  pe is not None. 
+0000ef10: 2020 2076 203d 2073 656c 662e 7363 6f70     v = self.scop
+0000ef20: 652e 7061 7261 6d28 6e61 6d65 2c20 696e  e.param(name, in
+0000ef30: 6974 5f66 6e2c 202a 696e 6974 5f61 7267  it_fn, *init_arg
+0000ef40: 732c 2075 6e62 6f78 3d75 6e62 6f78 2c20  s, unbox=unbox, 
+0000ef50: 2a2a 696e 6974 5f6b 7761 7267 7329 0a20  **init_kwargs). 
+0000ef60: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
+0000ef70: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
+0000ef80: 2770 6172 616d 7327 0a20 2020 2072 6574  'params'.    ret
+0000ef90: 7572 6e20 760a 0a20 2064 6566 2068 6173  urn v..  def has
+0000efa0: 5f76 6172 6961 626c 6528 7365 6c66 2c20  _variable(self, 
+0000efb0: 636f 6c3a 2073 7472 2c20 6e61 6d65 3a20  col: str, name: 
+0000efc0: 7374 7229 202d 3e20 626f 6f6c 3a0a 2020  str) -> bool:.  
+0000efd0: 2020 2222 2243 6865 636b 7320 6966 2061    """Checks if a
+0000efe0: 2076 6172 6961 626c 6520 6f66 2067 6976   variable of giv
+0000eff0: 656e 2063 6f6c 6c65 6374 696f 6e20 616e  en collection an
+0000f000: 6420 6e61 6d65 2065 7869 7374 7320 696e  d name exists in
+0000f010: 2074 6869 7320 4d6f 6475 6c65 2e0a 0a20   this Module... 
+0000f020: 2020 2053 6565 203a 6d6f 643a 6066 6c61     See :mod:`fla
+0000f030: 782e 636f 7265 2e76 6172 6961 626c 6573  x.core.variables
+0000f040: 6020 666f 7220 6d6f 7265 2065 7870 6c61  ` for more expla
+0000f050: 6e61 7469 6f6e 206f 6e20 7661 7269 6162  nation on variab
+0000f060: 6c65 7320 616e 640a 2020 2020 636f 6c6c  les and.    coll
+0000f070: 6563 7469 6f6e 732e 0a0a 2020 2020 4172  ections...    Ar
+0000f080: 6773 3a0a 2020 2020 2020 636f 6c3a 2054  gs:.      col: T
+0000f090: 6865 2076 6172 6961 626c 6520 636f 6c6c  he variable coll
+0000f0a0: 6563 7469 6f6e 206e 616d 652e 0a20 2020  ection name..   
+0000f0b0: 2020 206e 616d 653a 2054 6865 206e 616d     name: The nam
+0000f0c0: 6520 6f66 2074 6865 2076 6172 6961 626c  e of the variabl
+0000f0d0: 652e 0a0a 2020 2020 5265 7475 726e 733a  e...    Returns:
+0000f0e0: 0a20 2020 2020 2054 7275 6520 6966 2074  .      True if t
+0000f0f0: 6865 2076 6172 6961 626c 6520 6578 6973  he variable exis
+0000f100: 7473 2e0a 2020 2020 2222 220a 2020 2020  ts..    """.    
+0000f110: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+0000f120: 204e 6f6e 653a 0a20 2020 2020 2072 6169   None:.      rai
+0000f130: 7365 2056 616c 7565 4572 726f 7228 2243  se ValueError("C
+0000f140: 616e 2774 2061 6363 6573 7320 7661 7269  an't access vari
+0000f150: 6162 6c65 7320 6f6e 2075 6e62 6f75 6e64  ables on unbound
+0000f160: 206d 6f64 756c 6573 2229 0a20 2020 2072   modules").    r
+0000f170: 6574 7572 6e20 7365 6c66 2e73 636f 7065  eturn self.scope
+0000f180: 2e68 6173 5f76 6172 6961 626c 6528 636f  .has_variable(co
+0000f190: 6c2c 206e 616d 6529 0a0a 2020 6465 6620  l, name)..  def 
+0000f1a0: 6973 5f6d 7574 6162 6c65 5f63 6f6c 6c65  is_mutable_colle
+0000f1b0: 6374 696f 6e28 7365 6c66 2c20 636f 6c3a  ction(self, col:
+0000f1c0: 2073 7472 2920 2d3e 2062 6f6f 6c3a 0a20   str) -> bool:. 
+0000f1d0: 2020 2022 2222 5265 7475 726e 7320 7472     """Returns tr
+0000f1e0: 7565 2069 6620 7468 6520 636f 6c6c 6563  ue if the collec
+0000f1f0: 7469 6f6e 2060 6063 6f6c 6060 2069 7320  tion ``col`` is 
+0000f200: 6d75 7461 626c 652e 2222 220a 2020 2020  mutable.""".    
+0000f210: 6966 2073 656c 662e 7363 6f70 6520 6973  if self.scope is
+0000f220: 204e 6f6e 653a 0a20 2020 2020 2072 6169   None:.      rai
+0000f230: 7365 2056 616c 7565 4572 726f 7228 2243  se ValueError("C
+0000f240: 616e 2774 2063 6865 636b 206d 7574 6162  an't check mutab
+0000f250: 696c 6974 7920 6f6e 2075 6e62 6f75 6e64  ility on unbound
+0000f260: 206d 6f64 756c 6573 2229 0a20 2020 2072   modules").    r
+0000f270: 6574 7572 6e20 7365 6c66 2e73 636f 7065  eturn self.scope
+0000f280: 2e69 735f 6d75 7461 626c 655f 636f 6c6c  .is_mutable_coll
+0000f290: 6563 7469 6f6e 2863 6f6c 290a 0a20 2064  ection(col)..  d
+0000f2a0: 6566 2068 6173 5f72 6e67 2873 656c 662c  ef has_rng(self,
+0000f2b0: 206e 616d 653a 2073 7472 2920 2d3e 2062   name: str) -> b
+0000f2c0: 6f6f 6c3a 0a20 2020 2022 2222 5265 7475  ool:.    """Retu
+0000f2d0: 726e 7320 7472 7565 2069 6620 6120 5052  rns true if a PR
+0000f2e0: 4e47 5365 7175 656e 6365 2077 6974 6820  NGSequence with 
+0000f2f0: 6e61 6d65 2060 606e 616d 6560 6020 6578  name ``name`` ex
+0000f300: 6973 7473 2e22 2222 0a20 2020 2069 6620  ists.""".    if 
+0000f310: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+0000f320: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+0000f330: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+0000f340: 7420 7175 6572 7920 666f 7220 524e 4773  t query for RNGs
+0000f350: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
+0000f360: 6c65 7322 290a 2020 2020 7265 7475 726e  les").    return
+0000f370: 2073 656c 662e 7363 6f70 652e 6861 735f   self.scope.has_
+0000f380: 726e 6728 6e61 6d65 290a 0a20 2064 6566  rng(name)..  def
+0000f390: 206d 616b 655f 726e 6728 7365 6c66 2c20   make_rng(self, 
+0000f3a0: 6e61 6d65 3a20 7374 7220 3d20 2770 6172  name: str = 'par
+0000f3b0: 616d 7327 2920 2d3e 2050 524e 474b 6579  ams') -> PRNGKey
+0000f3c0: 3a0a 2020 2020 2222 2252 6574 7572 6e73  :.    """Returns
+0000f3d0: 2061 206e 6577 2052 4e47 206b 6579 2066   a new RNG key f
+0000f3e0: 726f 6d20 6120 6769 7665 6e20 524e 4720  rom a given RNG 
+0000f3f0: 7365 7175 656e 6365 2066 6f72 2074 6869  sequence for thi
+0000f400: 7320 4d6f 6475 6c65 2e0a 0a20 2020 2054  s Module...    T
+0000f410: 6865 206e 6577 2052 4e47 206b 6579 2069  he new RNG key i
+0000f420: 7320 7370 6c69 7420 6672 6f6d 2074 6865  s split from the
+0000f430: 2070 7265 7669 6f75 7320 6f6e 652e 2054   previous one. T
+0000f440: 6875 732c 2065 7665 7279 2063 616c 6c20  hus, every call 
+0000f450: 746f 0a20 2020 2060 606d 616b 655f 726e  to.    ``make_rn
+0000f460: 6760 6020 7265 7475 726e 7320 6120 6e65  g`` returns a ne
+0000f470: 7720 524e 4720 6b65 792c 2077 6869 6c65  w RNG key, while
+0000f480: 2073 7469 6c6c 2067 7561 7261 6e74 6565   still guarantee
+0000f490: 696e 6720 6675 6c6c 0a20 2020 2072 6570  ing full.    rep
+0000f4a0: 726f 6475 6369 6269 6c69 7479 2e0a 0a20  roducibility... 
+0000f4b0: 2020 202e 2e20 6e6f 7465 3a3a 0a20 2020     .. note::.   
+0000f4c0: 2020 2049 6620 616e 2069 6e76 616c 6964     If an invalid
+0000f4d0: 206e 616d 6520 6973 2070 6173 7365 6420   name is passed 
+0000f4e0: 2869 2e65 2e20 6e6f 2052 4e47 206b 6579  (i.e. no RNG key
+0000f4f0: 2077 6173 2070 6173 7365 6420 6279 0a20   was passed by. 
+0000f500: 2020 2020 2074 6865 2075 7365 7220 696e       the user in
+0000f510: 2060 602e 696e 6974 6060 206f 7220 6060   ``.init`` or ``
+0000f520: 2e61 7070 6c79 6060 2066 6f72 2074 6869  .apply`` for thi
+0000f530: 7320 6e61 6d65 292c 2074 6865 6e20 6060  s name), then ``
+0000f540: 6e61 6d65 6060 0a20 2020 2020 2077 696c  name``.      wil
+0000f550: 6c20 6465 6661 756c 7420 746f 2060 6027  l default to ``'
+0000f560: 7061 7261 6d73 2760 602e 0a0a 2020 2020  params'``...    
+0000f570: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
+0000f580: 203e 3e3e 2069 6d70 6f72 7420 6a61 780a   >>> import jax.
+0000f590: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+0000f5a0: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+0000f5b0: 6e0a 0a20 2020 2020 203e 3e3e 2063 6c61  n..      >>> cla
+0000f5c0: 7373 2050 6172 616d 734d 6f64 756c 6528  ss ParamsModule(
+0000f5d0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
+0000f5e0: 2020 2e2e 2e20 2020 6465 6620 5f5f 6361    ...   def __ca
+0000f5f0: 6c6c 5f5f 2873 656c 6629 3a0a 2020 2020  ll__(self):.    
+0000f600: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
+0000f610: 2073 656c 662e 6d61 6b65 5f72 6e67 2827   self.make_rng('
+0000f620: 7061 7261 6d73 2729 0a20 2020 2020 203e  params').      >
+0000f630: 3e3e 2063 6c61 7373 204f 7468 6572 4d6f  >> class OtherMo
+0000f640: 6475 6c65 286e 6e2e 4d6f 6475 6c65 293a  dule(nn.Module):
+0000f650: 0a20 2020 2020 202e 2e2e 2020 2064 6566  .      ...   def
+0000f660: 205f 5f63 616c 6c5f 5f28 7365 6c66 293a   __call__(self):
+0000f670: 0a20 2020 2020 202e 2e2e 2020 2020 2072  .      ...     r
+0000f680: 6574 7572 6e20 7365 6c66 2e6d 616b 655f  eturn self.make_
+0000f690: 726e 6728 276f 7468 6572 2729 0a0a 2020  rng('other')..  
+0000f6a0: 2020 2020 3e3e 3e20 6b65 7920 3d20 6a61      >>> key = ja
+0000f6b0: 782e 7261 6e64 6f6d 2e6b 6579 2830 290a  x.random.key(0).
+0000f6c0: 2020 2020 2020 3e3e 3e20 7061 7261 6d73        >>> params
+0000f6d0: 5f6f 7574 2c20 5f20 3d20 5061 7261 6d73  _out, _ = Params
+0000f6e0: 4d6f 6475 6c65 2829 2e69 6e69 745f 7769  Module().init_wi
+0000f6f0: 7468 5f6f 7574 7075 7428 7b27 7061 7261  th_output({'para
+0000f700: 6d73 273a 206b 6579 7d29 0a20 2020 2020  ms': key}).     
+0000f710: 203e 3e3e 2023 2073 656c 662e 6d61 6b65   >>> # self.make
+0000f720: 5f72 6e67 2827 6f74 6865 7227 2920 7769  _rng('other') wi
+0000f730: 6c6c 2064 6566 6175 6c74 2074 6f20 7573  ll default to us
+0000f740: 696e 6720 7468 6520 2770 6172 616d 7327  ing the 'params'
+0000f750: 2052 4e47 2073 7472 6561 6d0a 2020 2020   RNG stream.    
+0000f760: 2020 3e3e 3e20 6f74 6865 725f 6f75 742c    >>> other_out,
+0000f770: 205f 203d 204f 7468 6572 4d6f 6475 6c65   _ = OtherModule
+0000f780: 2829 2e69 6e69 745f 7769 7468 5f6f 7574  ().init_with_out
+0000f790: 7075 7428 7b27 7061 7261 6d73 273a 206b  put({'params': k
+0000f7a0: 6579 7d29 0a20 2020 2020 203e 3e3e 2061  ey}).      >>> a
+0000f7b0: 7373 6572 7420 7061 7261 6d73 5f6f 7574  ssert params_out
+0000f7c0: 203d 3d20 6f74 6865 725f 6f75 740a 0a20   == other_out.. 
+0000f7d0: 2020 204c 6561 726e 206d 6f72 6520 6162     Learn more ab
+0000f7e0: 6f75 7420 524e 4727 7320 6279 2072 6561  out RNG's by rea
+0000f7f0: 6469 6e67 2074 6865 2046 6c61 7820 524e  ding the Flax RN
+0000f800: 4720 6775 6964 653a 0a20 2020 2068 7474  G guide:.    htt
+0000f810: 7073 3a2f 2f66 6c61 782e 7265 6164 7468  ps://flax.readth
+0000f820: 6564 6f63 732e 696f 2f65 6e2f 6c61 7465  edocs.io/en/late
+0000f830: 7374 2f67 7569 6465 732f 666c 6178 5f66  st/guides/flax_f
+0000f840: 756e 6461 6d65 6e74 616c 732f 726e 675f  undamentals/rng_
+0000f850: 6775 6964 652e 6874 6d6c 0a0a 2020 2020  guide.html..    
+0000f860: 4172 6773 3a0a 2020 2020 2020 6e61 6d65  Args:.      name
+0000f870: 3a20 5468 6520 524e 4720 7365 7175 656e  : The RNG sequen
+0000f880: 6365 206e 616d 652e 0a0a 2020 2020 5265  ce name...    Re
+0000f890: 7475 726e 733a 0a20 2020 2020 2054 6865  turns:.      The
+0000f8a0: 206e 6577 6c79 2067 656e 6572 6174 6564   newly generated
+0000f8b0: 2052 4e47 206b 6579 2e0a 2020 2020 2222   RNG key..    ""
+0000f8c0: 220a 2020 2020 6966 2073 656c 662e 7363  ".    if self.sc
+0000f8d0: 6f70 6520 6973 204e 6f6e 653a 0a20 2020  ope is None:.   
+0000f8e0: 2020 2072 6169 7365 2056 616c 7565 4572     raise ValueEr
+0000f8f0: 726f 7228 2243 616e 2774 2075 7365 2052  ror("Can't use R
+0000f900: 4e47 7320 6f6e 2075 6e62 6f75 6e64 206d  NGs on unbound m
+0000f910: 6f64 756c 6573 2229 0a20 2020 2072 6574  odules").    ret
+0000f920: 7572 6e20 7365 6c66 2e73 636f 7065 2e6d  urn self.scope.m
+0000f930: 616b 655f 726e 6728 6e61 6d65 290a 0a20  ake_rng(name).. 
+0000f940: 2064 6566 2069 735f 696e 6974 6961 6c69   def is_initiali
+0000f950: 7a69 6e67 2873 656c 6629 202d 3e20 626f  zing(self) -> bo
+0000f960: 6f6c 3a0a 2020 2020 2222 2252 6574 7572  ol:.    """Retur
+0000f970: 6e73 2054 7275 6520 6966 2072 756e 6e69  ns True if runni
+0000f980: 6e67 2075 6e64 6572 2073 656c 662e 696e  ng under self.in
+0000f990: 6974 282e 2e2e 2920 6f72 206e 6e2e 696e  it(...) or nn.in
+0000f9a0: 6974 282e 2e2e 2928 292e 0a0a 2020 2020  it(...)()...    
+0000f9b0: 5468 6973 2069 7320 6120 6865 6c70 6572  This is a helper
+0000f9c0: 206d 6574 686f 6420 746f 2068 616e 646c   method to handl
+0000f9d0: 6520 7468 6520 636f 6d6d 6f6e 2063 6173  e the common cas
+0000f9e0: 6520 6f66 2073 696d 706c 6520 696e 6974  e of simple init
+0000f9f0: 6961 6c69 7a61 7469 6f6e 0a20 2020 2077  ialization.    w
+0000fa00: 6865 7265 2077 6520 7769 7368 2074 6f20  here we wish to 
+0000fa10: 6861 7665 2073 6574 7570 206c 6f67 6963  have setup logic
+0000fa20: 206f 6363 7572 2077 6865 6e20 6f6e 6c79   occur when only
+0000fa30: 2063 616c 6c65 6420 756e 6465 720a 2020   called under.  
+0000fa40: 2020 6060 6d6f 6475 6c65 2e69 6e69 7460    ``module.init`
+0000fa50: 6020 6f72 2060 606e 6e2e 696e 6974 6060  ` or ``nn.init``
+0000fa60: 2e20 2046 6f72 206d 6f72 6520 636f 6d70  .  For more comp
+0000fa70: 6c69 6361 7465 6420 6d75 6c74 692d 7068  licated multi-ph
+0000fa80: 6173 650a 2020 2020 696e 6974 6961 6c69  ase.    initiali
+0000fa90: 7a61 7469 6f6e 2073 6365 6e61 7269 6f73  zation scenarios
+0000faa0: 2069 7420 6973 2062 6574 7465 7220 746f   it is better to
+0000fab0: 2074 6573 7420 666f 7220 7468 6520 6d75   test for the mu
+0000fac0: 7461 6269 6c69 7479 206f 660a 2020 2020  tability of.    
+0000fad0: 7061 7274 6963 756c 6172 2076 6172 6961  particular varia
+0000fae0: 626c 6520 636f 6c6c 6563 7469 6f6e 7320  ble collections 
+0000faf0: 6f72 2066 6f72 2074 6865 2070 7265 7365  or for the prese
+0000fb00: 6e63 6520 6f66 2070 6172 7469 6375 6c61  nce of particula
+0000fb10: 720a 2020 2020 7661 7269 6162 6c65 7320  r.    variables 
+0000fb20: 7468 6174 2070 6f74 656e 7469 616c 6c79  that potentially
+0000fb30: 206e 6565 6420 746f 2062 6520 696e 6974   need to be init
+0000fb40: 6961 6c69 7a65 642e 0a20 2020 2022 2222  ialized..    """
+0000fb50: 0a20 2020 2069 6620 7365 6c66 2e73 636f  .    if self.sco
+0000fb60: 7065 2069 7320 4e6f 6e65 3a0a 2020 2020  pe is None:.    
+0000fb70: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+0000fb80: 6f72 2822 4361 6e27 7420 6368 6563 6b20  or("Can't check 
+0000fb90: 6966 2072 756e 6e69 6e67 2075 6e64 6572  if running under
+0000fba0: 2069 6e69 7428 2920 6f6e 2075 6e62 6f75   init() on unbou
+0000fbb0: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
+0000fbc0: 2072 6574 7572 6e20 7365 6c66 2e73 636f   return self.sco
+0000fbd0: 7065 2e67 6574 5f66 6c61 6728 2769 6e69  pe.get_flag('ini
+0000fbe0: 7469 616c 697a 696e 6727 2c20 4661 6c73  tializing', Fals
+0000fbf0: 6529 0a0a 2020 6465 6620 5f6d 6f64 756c  e)..  def _modul
+0000fc00: 655f 6368 6563 6b73 2873 656c 6629 3a0a  e_checks(self):.
+0000fc10: 2020 2020 2222 2252 756e 2073 7461 6e64      """Run stand
+0000fc20: 6172 6420 7275 6e74 696d 6520 6368 6563  ard runtime chec
+0000fc30: 6b73 2e22 2222 0a0a 2020 2020 6966 206e  ks."""..    if n
+0000fc40: 6f74 2069 7369 6e73 7461 6e63 6528 7365  ot isinstance(se
+0000fc50: 6c66 2c20 4d6f 6475 6c65 293a 0a20 2020  lf, Module):.   
+0000fc60: 2020 2072 6169 7365 2065 7272 6f72 732e     raise errors.
+0000fc70: 496e 7661 6c69 6449 6e73 7461 6e63 654d  InvalidInstanceM
+0000fc80: 6f64 756c 6545 7272 6f72 2829 0a0a 2020  oduleError()..  
+0000fc90: 2020 6f76 6572 7269 6464 656e 5f70 6f73    overridden_pos
+0000fca0: 745f 696e 6974 203d 2073 656c 662e 5f5f  t_init = self.__
+0000fcb0: 706f 7374 5f69 6e69 745f 5f20 213d 204d  post_init__ != M
+0000fcc0: 6f64 756c 652e 5f5f 706f 7374 5f69 6e69  odule.__post_ini
+0000fcd0: 745f 5f0a 2020 2020 6966 206f 7665 7272  t__.    if overr
+0000fce0: 6964 6465 6e5f 706f 7374 5f69 6e69 7420  idden_post_init 
+0000fcf0: 616e 6420 6e6f 7420 6861 7361 7474 7228  and not hasattr(
+0000fd00: 7365 6c66 2c20 275f 6964 2729 3a0a 2020  self, '_id'):.  
+0000fd10: 2020 2020 7261 6973 6520 6572 726f 7273      raise errors
+0000fd20: 2e49 6e63 6f72 7265 6374 506f 7374 496e  .IncorrectPostIn
+0000fd30: 6974 4f76 6572 7269 6465 4572 726f 7228  itOverrideError(
+0000fd40: 290a 0a20 2040 7472 6163 6562 6163 6b5f  )..  @traceback_
+0000fd50: 7574 696c 2e61 7069 5f62 6f75 6e64 6172  util.api_boundar
+0000fd60: 790a 2020 6465 6620 6269 6e64 280a 2020  y.  def bind(.  
+0000fd70: 2020 7365 6c66 3a20 4d2c 0a20 2020 2076    self: M,.    v
+0000fd80: 6172 6961 626c 6573 3a20 5661 7269 6162  ariables: Variab
+0000fd90: 6c65 4469 6374 2c0a 2020 2020 2a61 7267  leDict,.    *arg
+0000fda0: 732c 0a20 2020 2072 6e67 733a 204f 7074  s,.    rngs: Opt
+0000fdb0: 696f 6e61 6c5b 524e 4753 6571 7565 6e63  ional[RNGSequenc
+0000fdc0: 6573 5d20 3d20 4e6f 6e65 2c0a 2020 2020  es] = None,.    
+0000fdd0: 6d75 7461 626c 653a 2043 6f6c 6c65 6374  mutable: Collect
+0000fde0: 696f 6e46 696c 7465 7220 3d20 4661 6c73  ionFilter = Fals
+0000fdf0: 652c 0a20 2029 202d 3e20 4d3a 0a20 2020  e,.  ) -> M:.   
+0000fe00: 2022 2222 4372 6561 7465 7320 616e 2069   """Creates an i
+0000fe10: 6e74 6572 6163 7469 7665 204d 6f64 756c  nteractive Modul
+0000fe20: 6520 696e 7374 616e 6365 2062 7920 6269  e instance by bi
+0000fe30: 6e64 696e 6720 7661 7269 6162 6c65 7320  nding variables 
+0000fe40: 616e 6420 524e 4773 2e0a 0a20 2020 2060  and RNGs...    `
+0000fe50: 6062 696e 6460 6020 7072 6f76 6964 6573  `bind`` provides
+0000fe60: 2061 6e20 2269 6e74 6572 6163 7469 7665   an "interactive
+0000fe70: 2220 696e 7374 616e 6365 206f 6620 6120  " instance of a 
+0000fe80: 4d6f 6475 6c65 2064 6972 6563 746c 7920  Module directly 
+0000fe90: 7769 7468 6f75 740a 2020 2020 7472 616e  without.    tran
+0000fea0: 7366 6f72 6d69 6e67 2061 2066 756e 6374  sforming a funct
+0000feb0: 696f 6e20 7769 7468 2060 6061 7070 6c79  ion with ``apply
+0000fec0: 6060 2e20 5468 6973 2069 7320 7061 7274  ``. This is part
+0000fed0: 6963 756c 6172 6c79 2075 7365 6675 6c20  icularly useful 
+0000fee0: 666f 720a 2020 2020 6465 6275 6767 696e  for.    debuggin
+0000fef0: 6720 616e 6420 696e 7465 7261 6374 6976  g and interactiv
+0000ff00: 6520 7573 6520 6361 7365 7320 6c69 6b65  e use cases like
+0000ff10: 206e 6f74 6562 6f6f 6b73 2077 6865 7265   notebooks where
+0000ff20: 2061 2066 756e 6374 696f 6e20 776f 756c   a function woul
+0000ff30: 640a 2020 2020 6c69 6d69 7420 7468 6520  d.    limit the 
+0000ff40: 6162 696c 6974 7920 746f 2073 706c 6974  ability to split
+0000ff50: 2075 7020 636f 6465 2069 6e74 6f20 6469   up code into di
+0000ff60: 6666 6572 656e 7420 6365 6c6c 732e 0a0a  fferent cells...
+0000ff70: 2020 2020 4f6e 6365 2074 6865 2076 6172      Once the var
+0000ff80: 6961 626c 6573 2028 616e 6420 6f70 7469  iables (and opti
+0000ff90: 6f6e 616c 6c79 2052 4e47 7329 2061 7265  onally RNGs) are
+0000ffa0: 2062 6f75 6e64 2074 6f20 6120 6060 4d6f   bound to a ``Mo
+0000ffb0: 6475 6c65 6060 2069 740a 2020 2020 6265  dule`` it.    be
+0000ffc0: 636f 6d65 7320 6120 7374 6174 6566 756c  comes a stateful
+0000ffd0: 206f 626a 6563 742e 204e 6f74 6520 7468   object. Note th
+0000ffe0: 6174 2069 6469 6f6d 6174 6963 204a 4158  at idiomatic JAX
+0000fff0: 2069 7320 6675 6e63 7469 6f6e 616c 2061   is functional a
+00010000: 6e64 0a20 2020 2074 6865 7265 666f 7265  nd.    therefore
+00010010: 2061 6e20 696e 7465 7261 6374 6976 6520   an interactive 
+00010020: 696e 7374 616e 6365 2064 6f65 7320 6e6f  instance does no
+00010030: 7420 6d69 7820 7765 6c6c 2077 6974 6820  t mix well with 
+00010040: 7661 6e69 6c6c 6120 4a41 5820 4150 4973  vanilla JAX APIs
+00010050: 2e0a 2020 2020 6060 6269 6e64 2829 6060  ..    ``bind()``
+00010060: 2073 686f 756c 6420 6f6e 6c79 2062 6520   should only be 
+00010070: 7573 6564 2066 6f72 2069 6e74 6572 6163  used for interac
+00010080: 7469 7665 2065 7870 6572 696d 656e 7461  tive experimenta
+00010090: 7469 6f6e 2c20 616e 6420 696e 2061 6c6c  tion, and in all
+000100a0: 0a20 2020 206f 7468 6572 2063 6173 6573  .    other cases
+000100b0: 2077 6520 7374 726f 6e67 6c79 2065 6e63   we strongly enc
+000100c0: 6f75 7261 6765 2075 7365 7273 2074 6f20  ourage users to 
+000100d0: 7573 6520 6060 6170 706c 7928 2960 6020  use ``apply()`` 
+000100e0: 696e 7374 6561 642e 0a0a 2020 2020 4578  instead...    Ex
+000100f0: 616d 706c 653a 3a0a 0a20 2020 2020 203e  ample::..      >
+00010100: 3e3e 2069 6d70 6f72 7420 6a61 780a 2020  >> import jax.  
+00010110: 2020 2020 3e3e 3e20 696d 706f 7274 206a      >>> import j
+00010120: 6178 2e6e 756d 7079 2061 7320 6a6e 700a  ax.numpy as jnp.
+00010130: 2020 2020 2020 3e3e 3e20 696d 706f 7274        >>> import
+00010140: 2066 6c61 782e 6c69 6e65 6e20 6173 206e   flax.linen as n
+00010150: 6e0a 0a20 2020 2020 203e 3e3e 2063 6c61  n..      >>> cla
+00010160: 7373 2041 7574 6f45 6e63 6f64 6572 286e  ss AutoEncoder(n
+00010170: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
+00010180: 202e 2e2e 2020 2064 6566 2073 6574 7570   ...   def setup
+00010190: 2873 656c 6629 3a0a 2020 2020 2020 2e2e  (self):.      ..
+000101a0: 2e20 2020 2020 7365 6c66 2e65 6e63 6f64  .     self.encod
+000101b0: 6572 203d 206e 6e2e 4465 6e73 6528 3329  er = nn.Dense(3)
+000101c0: 0a20 2020 2020 202e 2e2e 2020 2020 2073  .      ...     s
+000101d0: 656c 662e 6465 636f 6465 7220 3d20 6e6e  elf.decoder = nn
+000101e0: 2e44 656e 7365 2835 290a 2020 2020 2020  .Dense(5).      
+000101f0: 2e2e 2e0a 2020 2020 2020 2e2e 2e20 2020  ....      ...   
+00010200: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
+00010210: 662c 2078 293a 0a20 2020 2020 202e 2e2e  f, x):.      ...
+00010220: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
+00010230: 2e64 6563 6f64 6572 2873 656c 662e 656e  .decoder(self.en
+00010240: 636f 6465 7228 7829 290a 0a20 2020 2020  coder(x))..     
+00010250: 203e 3e3e 2078 203d 206a 6e70 2e6f 6e65   >>> x = jnp.one
+00010260: 7328 2831 362c 2039 2929 0a20 2020 2020  s((16, 9)).     
+00010270: 203e 3e3e 2061 6520 3d20 4175 746f 456e   >>> ae = AutoEn
+00010280: 636f 6465 7228 290a 2020 2020 2020 3e3e  coder().      >>
+00010290: 3e20 7661 7269 6162 6c65 7320 3d20 6165  > variables = ae
+000102a0: 2e69 6e69 7428 6a61 782e 7261 6e64 6f6d  .init(jax.random
+000102b0: 2e6b 6579 2830 292c 2078 290a 2020 2020  .key(0), x).    
+000102c0: 2020 3e3e 3e20 6d6f 6465 6c20 3d20 6165    >>> model = ae
+000102d0: 2e62 696e 6428 7661 7269 6162 6c65 7329  .bind(variables)
+000102e0: 0a20 2020 2020 203e 3e3e 207a 203d 206d  .      >>> z = m
+000102f0: 6f64 656c 2e65 6e63 6f64 6572 2878 290a  odel.encoder(x).
+00010300: 2020 2020 2020 3e3e 3e20 785f 7265 636f        >>> x_reco
+00010310: 6e73 7472 7563 7465 6420 3d20 6d6f 6465  nstructed = mode
+00010320: 6c2e 6465 636f 6465 7228 7a29 0a0a 2020  l.decoder(z)..  
+00010330: 2020 4172 6773 3a0a 2020 2020 2020 7661    Args:.      va
+00010340: 7269 6162 6c65 733a 2041 2064 6963 7469  riables: A dicti
+00010350: 6f6e 6172 7920 636f 6e74 6169 6e69 6e67  onary containing
+00010360: 2076 6172 6961 626c 6573 206b 6579 6564   variables keyed
+00010370: 2062 7920 7661 7269 6162 6c65 0a20 2020   by variable.   
+00010380: 2020 2020 2063 6f6c 6c65 6374 696f 6e73       collections
+00010390: 2e20 5365 6520 3a6d 6f64 3a60 666c 6178  . See :mod:`flax
+000103a0: 2e63 6f72 652e 7661 7269 6162 6c65 7360  .core.variables`
+000103b0: 2066 6f72 206d 6f72 6520 6465 7461 696c   for more detail
+000103c0: 7320 6162 6f75 740a 2020 2020 2020 2020  s about.        
+000103d0: 7661 7269 6162 6c65 732e 0a20 2020 2020  variables..     
+000103e0: 202a 6172 6773 3a20 4e61 6d65 6420 6172   *args: Named ar
+000103f0: 6775 6d65 6e74 7320 286e 6f74 2075 7365  guments (not use
+00010400: 6429 2e0a 2020 2020 2020 726e 6773 3a20  d)..      rngs: 
+00010410: 6120 6469 6374 206f 6620 5052 4e47 4b65  a dict of PRNGKe
+00010420: 7973 2074 6f20 696e 6974 6961 6c69 7a65  ys to initialize
+00010430: 2074 6865 2050 524e 4720 7365 7175 656e   the PRNG sequen
+00010440: 6365 732e 0a20 2020 2020 206d 7574 6162  ces..      mutab
+00010450: 6c65 3a20 4361 6e20 6265 2062 6f6f 6c2c  le: Can be bool,
+00010460: 2073 7472 2c20 6f72 206c 6973 742e 2053   str, or list. S
+00010470: 7065 6369 6669 6573 2077 6869 6368 2063  pecifies which c
+00010480: 6f6c 6c65 6374 696f 6e73 2073 686f 756c  ollections shoul
+00010490: 6420 6265 0a20 2020 2020 2020 2074 7265  d be.        tre
+000104a0: 6174 6564 2061 7320 6d75 7461 626c 653a  ated as mutable:
+000104b0: 2060 6062 6f6f 6c60 603a 2061 6c6c 2f6e   ``bool``: all/n
+000104c0: 6f20 636f 6c6c 6563 7469 6f6e 7320 6172  o collections ar
+000104d0: 6520 6d75 7461 626c 652e 2060 6073 7472  e mutable. ``str
+000104e0: 6060 3a0a 2020 2020 2020 2020 5468 6520  ``:.        The 
+000104f0: 6e61 6d65 206f 6620 6120 7369 6e67 6c65  name of a single
+00010500: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+00010510: 696f 6e2e 2060 606c 6973 7460 603a 2041  ion. ``list``: A
+00010520: 206c 6973 7420 6f66 206e 616d 6573 206f   list of names o
+00010530: 660a 2020 2020 2020 2020 6d75 7461 626c  f.        mutabl
+00010540: 6520 636f 6c6c 6563 7469 6f6e 732e 0a0a  e collections...
+00010550: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   
+00010560: 2020 2041 2063 6f70 7920 6f66 2074 6869     A copy of thi
+00010570: 7320 696e 7374 616e 6365 2077 6974 6820  s instance with 
+00010580: 626f 756e 6420 7661 7269 6162 6c65 7320  bound variables 
+00010590: 616e 6420 524e 4773 2e0a 2020 2020 2222  and RNGs..    ""
+000105a0: 220a 2020 2020 4d6f 6475 6c65 2e5f 6d6f  ".    Module._mo
+000105b0: 6475 6c65 5f63 6865 636b 7328 7365 6c66  dule_checks(self
+000105c0: 290a 0a20 2020 2064 656c 2061 7267 730a  )..    del args.
+000105d0: 2020 2020 7363 6f70 6520 3d20 636f 7265      scope = core
+000105e0: 2e62 696e 6428 7661 7269 6162 6c65 732c  .bind(variables,
+000105f0: 2072 6e67 733d 726e 6773 2c20 6d75 7461   rngs=rngs, muta
+00010600: 626c 653d 6d75 7461 626c 6529 0a20 2020  ble=mutable).   
+00010610: 2072 6574 7572 6e20 7365 6c66 2e63 6c6f   return self.clo
+00010620: 6e65 2870 6172 656e 743d 7363 6f70 652c  ne(parent=scope,
+00010630: 205f 6465 6570 5f63 6c6f 6e65 3d54 7275   _deep_clone=Tru
+00010640: 6529 0a0a 2020 6465 6620 756e 6269 6e64  e)..  def unbind
+00010650: 2873 656c 663a 204d 2920 2d3e 2054 7570  (self: M) -> Tup
+00010660: 6c65 5b4d 2c20 5661 7269 6162 6c65 4469  le[M, VariableDi
+00010670: 6374 5d3a 0a20 2020 2022 2222 5265 7475  ct]:.    """Retu
+00010680: 726e 7320 616e 2075 6e62 6f75 6e64 2063  rns an unbound c
+00010690: 6f70 7920 6f66 2061 204d 6f64 756c 6520  opy of a Module 
+000106a0: 616e 6420 6974 7320 7661 7269 6162 6c65  and its variable
+000106b0: 732e 0a0a 2020 2020 6060 756e 6269 6e64  s...    ``unbind
+000106c0: 6060 2068 656c 7073 2063 7265 6174 6520  `` helps create 
+000106d0: 6120 7374 6174 656c 6573 7320 7665 7273  a stateless vers
+000106e0: 696f 6e20 6f66 2061 2062 6f75 6e64 204d  ion of a bound M
+000106f0: 6f64 756c 652e 0a0a 2020 2020 416e 2065  odule...    An e
+00010700: 7861 6d70 6c65 206f 6620 6120 636f 6d6d  xample of a comm
+00010710: 6f6e 2075 7365 2063 6173 653a 2074 6f20  on use case: to 
+00010720: 6578 7472 6163 7420 6120 7375 622d 4d6f  extract a sub-Mo
+00010730: 6475 6c65 2064 6566 696e 6564 2069 6e73  dule defined ins
+00010740: 6964 650a 2020 2020 6060 7365 7475 7028  ide.    ``setup(
+00010750: 2960 6020 616e 6420 6974 7320 636f 7272  )`` and its corr
+00010760: 6573 706f 6e64 696e 6720 7661 7269 6162  esponding variab
+00010770: 6c65 733a 2031 2920 7465 6d70 6f72 6172  les: 1) temporar
+00010780: 696c 7920 6060 6269 6e64 6060 2074 6865  ily ``bind`` the
+00010790: 0a20 2020 2070 6172 656e 7420 4d6f 6475  .    parent Modu
+000107a0: 6c65 3b20 616e 6420 7468 656e 2032 2920  le; and then 2) 
+000107b0: 6060 756e 6269 6e64 6060 2074 6865 2064  ``unbind`` the d
+000107c0: 6573 6972 6564 2073 7562 2d4d 6f64 756c  esired sub-Modul
+000107d0: 652e 2028 5265 6361 6c6c 2074 6861 740a  e. (Recall that.
+000107e0: 2020 2020 6060 7365 7475 7028 2960 6020      ``setup()`` 
+000107f0: 6973 206f 6e6c 7920 6361 6c6c 6564 2077  is only called w
+00010800: 6865 6e20 7468 6520 4d6f 6475 6c65 2069  hen the Module i
+00010810: 7320 626f 756e 642e 293a 3a0a 0a20 2020  s bound.)::..   
+00010820: 2020 203e 3e3e 2063 6c61 7373 2045 6e63     >>> class Enc
+00010830: 6f64 6572 286e 6e2e 4d6f 6475 6c65 293a  oder(nn.Module):
+00010840: 0a20 2020 2020 202e 2e2e 2020 2040 6e6e  .      ...   @nn
+00010850: 2e63 6f6d 7061 6374 0a20 2020 2020 202e  .compact.      .
+00010860: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
+00010870: 5f28 7365 6c66 2c20 7829 3a0a 2020 2020  _(self, x):.    
+00010880: 2020 2e2e 2e20 2020 2020 2e2e 2e0a 2020    ...     ....  
+00010890: 2020 2020 2e2e 2e20 2020 2020 7265 7475      ...     retu
+000108a0: 726e 206e 6e2e 4465 6e73 6528 3235 3629  rn nn.Dense(256)
+000108b0: 2878 290a 0a20 2020 2020 203e 3e3e 2063  (x)..      >>> c
+000108c0: 6c61 7373 2044 6563 6f64 6572 286e 6e2e  lass Decoder(nn.
+000108d0: 4d6f 6475 6c65 293a 0a20 2020 2020 202e  Module):.      .
+000108e0: 2e2e 2020 2040 6e6e 2e63 6f6d 7061 6374  ..   @nn.compact
+000108f0: 0a20 2020 2020 202e 2e2e 2020 2064 6566  .      ...   def
+00010900: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
+00010910: 7829 3a0a 2020 2020 2020 2e2e 2e20 2020  x):.      ...   
+00010920: 2020 2e2e 2e0a 2020 2020 2020 2e2e 2e20    ....      ... 
+00010930: 2020 2020 7265 7475 726e 206e 6e2e 4465      return nn.De
+00010940: 6e73 6528 3738 3429 2878 290a 0a20 2020  nse(784)(x)..   
+00010950: 2020 203e 3e3e 2063 6c61 7373 2041 7574     >>> class Aut
+00010960: 6f45 6e63 6f64 6572 286e 6e2e 4d6f 6475  oEncoder(nn.Modu
+00010970: 6c65 293a 0a20 2020 2020 202e 2e2e 2020  le):.      ...  
+00010980: 2064 6566 2073 6574 7570 2873 656c 6629   def setup(self)
+00010990: 3a0a 2020 2020 2020 2e2e 2e20 2020 2020  :.      ...     
+000109a0: 7365 6c66 2e65 6e63 6f64 6572 203d 2045  self.encoder = E
+000109b0: 6e63 6f64 6572 2829 0a20 2020 2020 202e  ncoder().      .
+000109c0: 2e2e 2020 2020 2073 656c 662e 6465 636f  ..     self.deco
+000109d0: 6465 7220 3d20 4465 636f 6465 7228 290a  der = Decoder().
+000109e0: 2020 2020 2020 2e2e 2e0a 2020 2020 2020        ....      
+000109f0: 2e2e 2e20 2020 6465 6620 5f5f 6361 6c6c  ...   def __call
+00010a00: 5f5f 2873 656c 662c 2078 293a 0a20 2020  __(self, x):.   
+00010a10: 2020 202e 2e2e 2020 2020 2072 6574 7572     ...     retur
+00010a20: 6e20 7365 6c66 2e64 6563 6f64 6572 2873  n self.decoder(s
+00010a30: 656c 662e 656e 636f 6465 7228 7829 290a  elf.encoder(x)).
+00010a40: 0a20 2020 2020 203e 3e3e 206d 6f64 756c  .      >>> modul
+00010a50: 6520 3d20 4175 746f 456e 636f 6465 7228  e = AutoEncoder(
+00010a60: 290a 2020 2020 2020 3e3e 3e20 7661 7269  ).      >>> vari
+00010a70: 6162 6c65 7320 3d20 6d6f 6475 6c65 2e69  ables = module.i
+00010a80: 6e69 7428 6a61 782e 7261 6e64 6f6d 2e6b  nit(jax.random.k
+00010a90: 6579 2830 292c 206a 6e70 2e6f 6e65 7328  ey(0), jnp.ones(
+00010aa0: 2831 2c20 3738 3429 2929 0a0a 2020 2020  (1, 784)))..    
+00010ab0: 2020 3e3e 3e20 2320 4578 7472 6163 7420    >>> # Extract 
+00010ac0: 7468 6520 456e 636f 6465 7220 7375 622d  the Encoder sub-
+00010ad0: 4d6f 6475 6c65 2061 6e64 2069 7473 2076  Module and its v
+00010ae0: 6172 6961 626c 6573 0a20 2020 2020 203e  ariables.      >
+00010af0: 3e3e 2065 6e63 6f64 6572 2c20 656e 636f  >> encoder, enco
+00010b00: 6465 725f 7661 7273 203d 206d 6f64 756c  der_vars = modul
+00010b10: 652e 6269 6e64 2876 6172 6961 626c 6573  e.bind(variables
+00010b20: 292e 656e 636f 6465 722e 756e 6269 6e64  ).encoder.unbind
+00010b30: 2829 0a0a 2020 2020 5265 7475 726e 733a  ()..    Returns:
+00010b40: 0a20 2020 2020 2041 2074 7570 6c65 2077  .      A tuple w
+00010b50: 6974 6820 616e 2075 6e62 6f75 6e64 2063  ith an unbound c
+00010b60: 6f70 7920 6f66 2074 6869 7320 4d6f 6475  opy of this Modu
+00010b70: 6c65 2061 6e64 2069 7473 2076 6172 6961  le and its varia
+00010b80: 626c 6573 2e0a 2020 2020 2222 220a 2020  bles..    """.  
+00010b90: 2020 4d6f 6475 6c65 2e5f 6d6f 6475 6c65    Module._module
+00010ba0: 5f63 6865 636b 7328 7365 6c66 290a 0a20  _checks(self).. 
+00010bb0: 2020 2069 6620 7365 6c66 2e73 636f 7065     if self.scope
+00010bc0: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      
+00010bd0: 7261 6973 6520 6572 726f 7273 2e43 616c  raise errors.Cal
+00010be0: 6c55 6e62 696e 644f 6e55 6e62 6f75 6e64  lUnbindOnUnbound
+00010bf0: 4d6f 6475 6c65 4572 726f 7228 290a 0a20  ModuleError().. 
+00010c00: 2020 2076 6172 6961 626c 6573 203d 2073     variables = s
+00010c10: 656c 662e 7661 7269 6162 6c65 730a 2020  elf.variables.  
+00010c20: 2020 6d6f 6475 6c65 203d 2073 656c 662e    module = self.
+00010c30: 636c 6f6e 6528 5f64 6565 705f 636c 6f6e  clone(_deep_clon
+00010c40: 653d 5472 7565 2c20 5f72 6573 6574 5f6e  e=True, _reset_n
+00010c50: 616d 6573 3d54 7275 652c 206e 616d 653d  ames=True, name=
+00010c60: 4e6f 6e65 290a 2020 2020 7265 7475 726e  None).    return
+00010c70: 206d 6f64 756c 652c 2076 6172 6961 626c   module, variabl
+00010c80: 6573 0a0a 2020 4074 7261 6365 6261 636b  es..  @traceback
+00010c90: 5f75 7469 6c2e 6170 695f 626f 756e 6461  _util.api_bounda
+00010ca0: 7279 0a20 2064 6566 2061 7070 6c79 280a  ry.  def apply(.
+00010cb0: 2020 2020 7365 6c66 2c0a 2020 2020 7661      self,.    va
+00010cc0: 7269 6162 6c65 733a 2056 6172 6961 626c  riables: Variabl
+00010cd0: 6544 6963 742c 0a20 2020 202a 6172 6773  eDict,.    *args
+00010ce0: 2c0a 2020 2020 726e 6773 3a20 4f70 7469  ,.    rngs: Opti
+00010cf0: 6f6e 616c 5b55 6e69 6f6e 5b50 524e 474b  onal[Union[PRNGK
+00010d00: 6579 2c20 524e 4753 6571 7565 6e63 6573  ey, RNGSequences
+00010d10: 5d5d 203d 204e 6f6e 652c 0a20 2020 206d  ]] = None,.    m
+00010d20: 6574 686f 643a 2055 6e69 6f6e 5b43 616c  ethod: Union[Cal
+00010d30: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d2c  lable[..., Any],
+00010d40: 2073 7472 2c20 4e6f 6e65 5d20 3d20 4e6f   str, None] = No
+00010d50: 6e65 2c0a 2020 2020 6d75 7461 626c 653a  ne,.    mutable:
+00010d60: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
+00010d70: 7220 3d20 4661 6c73 652c 0a20 2020 2063  r = False,.    c
+00010d80: 6170 7475 7265 5f69 6e74 6572 6d65 6469  apture_intermedi
+00010d90: 6174 6573 3a20 556e 696f 6e5b 626f 6f6c  ates: Union[bool
+00010da0: 2c20 4361 6c6c 6162 6c65 5b5b 274d 6f64  , Callable[['Mod
+00010db0: 756c 6527 2c20 7374 725d 2c20 626f 6f6c  ule', str], bool
+00010dc0: 5d5d 203d 2046 616c 7365 2c0a 2020 2020  ]] = False,.    
+00010dd0: 2a2a 6b77 6172 6773 2c0a 2020 2920 2d3e  **kwargs,.  ) ->
+00010de0: 2055 6e69 6f6e 5b41 6e79 2c20 5475 706c   Union[Any, Tupl
+00010df0: 655b 416e 792c 2055 6e69 6f6e 5b46 726f  e[Any, Union[Fro
+00010e00: 7a65 6e56 6172 6961 626c 6544 6963 742c  zenVariableDict,
+00010e10: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
+00010e20: 5d5d 3a0a 2020 2020 2222 2241 7070 6c69  ]]:.    """Appli
+00010e30: 6573 2061 206d 6f64 756c 6520 6d65 7468  es a module meth
+00010e40: 6f64 2074 6f20 7661 7269 6162 6c65 7320  od to variables 
+00010e50: 616e 6420 7265 7475 726e 7320 6f75 7470  and returns outp
+00010e60: 7574 2061 6e64 206d 6f64 6966 6965 6420  ut and modified 
+00010e70: 7661 7269 6162 6c65 732e 0a0a 2020 2020  variables...    
+00010e80: 4e6f 7465 2074 6861 7420 6060 6d65 7468  Note that ``meth
+00010e90: 6f64 6060 2073 686f 756c 6420 6265 2073  od`` should be s
+00010ea0: 6574 2069 6620 6f6e 6520 776f 756c 6420  et if one would 
+00010eb0: 6c69 6b65 2074 6f20 6361 6c6c 2060 6061  like to call ``a
+00010ec0: 7070 6c79 6060 206f 6e20 610a 2020 2020  pply`` on a.    
+00010ed0: 6469 6666 6572 656e 7420 636c 6173 7320  different class 
+00010ee0: 6d65 7468 6f64 2074 6861 6e20 6060 5f5f  method than ``__
+00010ef0: 6361 6c6c 5f5f 6060 2e20 466f 7220 696e  call__``. For in
+00010f00: 7374 616e 6365 2c20 7375 7070 6f73 6520  stance, suppose 
+00010f10: 610a 2020 2020 5472 616e 7366 6f72 6d65  a.    Transforme
+00010f20: 7220 6d6f 6475 6c65 7320 6861 7320 6120  r modules has a 
+00010f30: 6d65 7468 6f64 2063 616c 6c65 6420 6060  method called ``
+00010f40: 656e 636f 6465 6060 2c20 7468 656e 2074  encode``, then t
+00010f50: 6865 2066 6f6c 6c6f 7769 6e67 2063 616c  he following cal
+00010f60: 6c73 0a20 2020 2060 6061 7070 6c79 6060  ls.    ``apply``
+00010f70: 206f 6e20 7468 6174 206d 6574 686f 643a   on that method:
+00010f80: 3a0a 0a20 2020 2020 203e 3e3e 2069 6d70  :..      >>> imp
+00010f90: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
+00010fa0: 7320 6e6e 0a20 2020 2020 203e 3e3e 2069  s nn.      >>> i
+00010fb0: 6d70 6f72 7420 6a61 782c 206a 6178 2e6e  mport jax, jax.n
+00010fc0: 756d 7079 2061 7320 6a6e 700a 2020 2020  umpy as jnp.    
+00010fd0: 2020 3e3e 3e20 696d 706f 7274 206e 756d    >>> import num
+00010fe0: 7079 2061 7320 6e70 0a0a 2020 2020 2020  py as np..      
+00010ff0: 3e3e 3e20 636c 6173 7320 5472 616e 7366  >>> class Transf
+00011000: 6f72 6d65 7228 6e6e 2e4d 6f64 756c 6529  ormer(nn.Module)
+00011010: 3a0a 2020 2020 2020 2e2e 2e20 2020 6465  :.      ...   de
+00011020: 6620 656e 636f 6465 2873 656c 662c 2078  f encode(self, x
+00011030: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
+00011040: 202e 2e2e 0a0a 2020 2020 2020 3e3e 3e20   .....      >>> 
+00011050: 7820 3d20 6a6e 702e 6f6e 6573 2828 3136  x = jnp.ones((16
+00011060: 2c20 3929 290a 2020 2020 2020 3e3e 3e20  , 9)).      >>> 
+00011070: 6d6f 6465 6c20 3d20 5472 616e 7366 6f72  model = Transfor
+00011080: 6d65 7228 290a 2020 2020 2020 3e3e 3e20  mer().      >>> 
+00011090: 7661 7269 6162 6c65 7320 3d20 6d6f 6465  variables = mode
+000110a0: 6c2e 696e 6974 286a 6178 2e72 616e 646f  l.init(jax.rando
+000110b0: 6d2e 6b65 7928 3029 2c20 782c 206d 6574  m.key(0), x, met
+000110c0: 686f 643d 5472 616e 7366 6f72 6d65 722e  hod=Transformer.
+000110d0: 656e 636f 6465 290a 0a20 2020 2020 203e  encode)..      >
+000110e0: 3e3e 2065 6e63 6f64 6564 203d 206d 6f64  >> encoded = mod
+000110f0: 656c 2e61 7070 6c79 2876 6172 6961 626c  el.apply(variabl
+00011100: 6573 2c20 782c 206d 6574 686f 643d 5472  es, x, method=Tr
+00011110: 616e 7366 6f72 6d65 722e 656e 636f 6465  ansformer.encode
+00011120: 290a 0a20 2020 2049 6620 6120 6675 6e63  )..    If a func
+00011130: 7469 6f6e 2069 6e73 7461 6e63 6520 6973  tion instance is
+00011140: 2070 726f 7669 6465 642c 2074 6865 2075   provided, the u
+00011150: 6e62 6f75 6e64 2066 756e 6374 696f 6e20  nbound function 
+00011160: 6973 2075 7365 642e 2046 6f72 0a20 2020  is used. For.   
+00011170: 2069 6e73 7461 6e63 652c 2074 6865 2065   instance, the e
+00011180: 7861 6d70 6c65 2062 656c 6f77 2069 7320  xample below is 
+00011190: 6571 7569 7661 6c65 6e74 2074 6f20 7468  equivalent to th
+000111a0: 6520 6f6e 6520 6162 6f76 653a 3a0a 0a20  e one above::.. 
+000111b0: 2020 2020 203e 3e3e 2065 6e63 6f64 6564       >>> encoded
+000111c0: 203d 206d 6f64 656c 2e61 7070 6c79 2876   = model.apply(v
+000111d0: 6172 6961 626c 6573 2c20 782c 206d 6574  ariables, x, met
+000111e0: 686f 643d 6d6f 6465 6c2e 656e 636f 6465  hod=model.encode
+000111f0: 290a 0a20 2020 2059 6f75 2063 616e 2061  )..    You can a
+00011200: 6c73 6f20 7061 7373 2061 2073 7472 696e  lso pass a strin
+00011210: 6720 746f 2061 2063 616c 6c61 626c 6520  g to a callable 
+00011220: 6174 7472 6962 7574 6520 6f66 2074 6865  attribute of the
+00011230: 206d 6f64 756c 652e 2046 6f72 0a20 2020   module. For.   
+00011240: 2065 7861 6d70 6c65 2c20 7468 6520 7072   example, the pr
+00011250: 6576 696f 7573 2063 616e 2062 6520 7772  evious can be wr
+00011260: 6974 7465 6e20 6173 3a3a 0a0a 2020 2020  itten as::..    
+00011270: 2020 3e3e 3e20 656e 636f 6465 6420 3d20    >>> encoded = 
+00011280: 6d6f 6465 6c2e 6170 706c 7928 7661 7269  model.apply(vari
+00011290: 6162 6c65 732c 2078 2c20 6d65 7468 6f64  ables, x, method
+000112a0: 3d27 656e 636f 6465 2729 0a0a 2020 2020  ='encode')..    
+000112b0: 4e6f 7465 2060 606d 6574 686f 6460 6020  Note ``method`` 
+000112c0: 6361 6e20 616c 736f 2062 6520 6120 6675  can also be a fu
+000112d0: 6e63 7469 6f6e 2074 6861 7420 6973 206e  nction that is n
+000112e0: 6f74 2064 6566 696e 6564 2069 6e0a 2020  ot defined in.  
+000112f0: 2020 6060 5472 616e 7366 6f72 6d65 7260    ``Transformer`
+00011300: 602e 2049 6e20 7468 6174 2063 6173 652c  `. In that case,
+00011310: 2074 6865 2066 756e 6374 696f 6e20 7368   the function sh
+00011320: 6f75 6c64 2068 6176 6520 6174 206c 6561  ould have at lea
+00011330: 7374 206f 6e65 0a20 2020 2061 7267 756d  st one.    argum
+00011340: 656e 7420 7265 7072 6573 656e 7469 6e67  ent representing
+00011350: 2061 6e20 696e 7374 616e 6365 206f 6620   an instance of 
+00011360: 7468 6520 4d6f 6475 6c65 2063 6c61 7373  the Module class
+00011370: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6465  ::..      >>> de
+00011380: 6620 6f74 6865 725f 666e 2869 6e73 7461  f other_fn(insta
+00011390: 6e63 652c 2078 293a 0a20 2020 2020 202e  nce, x):.      .
+000113a0: 2e2e 2020 2023 2069 6e73 7461 6e63 652e  ..   # instance.
+000113b0: 736f 6d65 5f6d 6f64 756c 655f 6174 7472  some_module_attr
+000113c0: 282e 2e2e 290a 2020 2020 2020 2e2e 2e20  (...).      ... 
+000113d0: 2020 696e 7374 616e 6365 2e65 6e63 6f64    instance.encod
+000113e0: 650a 2020 2020 2020 2e2e 2e20 2020 2e2e  e.      ...   ..
+000113f0: 2e0a 0a20 2020 2020 203e 3e3e 206d 6f64  ...      >>> mod
+00011400: 656c 2e61 7070 6c79 2876 6172 6961 626c  el.apply(variabl
+00011410: 6573 2c20 782c 206d 6574 686f 643d 6f74  es, x, method=ot
+00011420: 6865 725f 666e 290a 0a20 2020 2049 6620  her_fn)..    If 
+00011430: 796f 7520 7061 7373 2061 2073 696e 676c  you pass a singl
+00011440: 6520 6060 5052 4e47 4b65 7960 602c 2046  e ``PRNGKey``, F
+00011450: 6c61 7820 7769 6c6c 2075 7365 2069 7420  lax will use it 
+00011460: 746f 2066 6565 6420 7468 6520 6060 2770  to feed the ``'p
+00011470: 6172 616d 7327 6060 0a20 2020 2052 4e47  arams'``.    RNG
+00011480: 2073 7472 6561 6d2e 2020 4966 2079 6f75   stream.  If you
+00011490: 2077 616e 7420 746f 2075 7365 2061 2064   want to use a d
+000114a0: 6966 6665 7265 6e74 2052 4e47 2073 7472  ifferent RNG str
+000114b0: 6561 6d20 6f72 206e 6565 6420 746f 2075  eam or need to u
+000114c0: 7365 0a20 2020 206d 756c 7469 706c 6520  se.    multiple 
+000114d0: 7374 7265 616d 732c 2079 6f75 2063 616e  streams, you can
+000114e0: 2070 6173 7320 6120 6469 6374 696f 6e61   pass a dictiona
+000114f0: 7279 206d 6170 7069 6e67 2065 6163 6820  ry mapping each 
+00011500: 524e 4720 7374 7265 616d 206e 616d 650a  RNG stream name.
+00011510: 2020 2020 746f 2069 7473 2063 6f72 7265      to its corre
+00011520: 7370 6f6e 6469 6e67 2060 6050 524e 474b  sponding ``PRNGK
+00011530: 6579 6060 2074 6f20 6060 6170 706c 7960  ey`` to ``apply`
+00011540: 602e 2049 6620 6060 7365 6c66 2e6d 616b  `. If ``self.mak
+00011550: 655f 726e 6728 6e61 6d65 2960 600a 2020  e_rng(name)``.  
+00011560: 2020 6973 2063 616c 6c65 6420 6f6e 2061    is called on a
+00011570: 6e20 524e 4720 7374 7265 616d 206e 616d  n RNG stream nam
+00011580: 6520 7468 6174 2069 736e 2774 2070 6173  e that isn't pas
+00011590: 7365 6420 6279 2074 6865 2075 7365 722c  sed by the user,
+000115a0: 2069 7420 7769 6c6c 0a20 2020 2064 6566   it will.    def
+000115b0: 6175 6c74 2074 6f20 7573 696e 6720 7468  ault to using th
+000115c0: 6520 6060 2770 6172 616d 7327 6060 2052  e ``'params'`` R
+000115d0: 4e47 2073 7472 6561 6d2e 0a0a 2020 2020  NG stream...    
+000115e0: 4578 616d 706c 653a 3a0a 0a20 2020 2020  Example::..     
+000115f0: 203e 3e3e 2063 6c61 7373 2046 6f6f 286e   >>> class Foo(n
+00011600: 6e2e 4d6f 6475 6c65 293a 0a20 2020 2020  n.Module):.     
+00011610: 202e 2e2e 2020 2040 6e6e 2e63 6f6d 7061   ...   @nn.compa
+00011620: 6374 0a20 2020 2020 202e 2e2e 2020 2064  ct.      ...   d
+00011630: 6566 205f 5f63 616c 6c5f 5f28 7365 6c66  ef __call__(self
+00011640: 2c20 782c 2061 6464 5f6e 6f69 7365 3d46  , x, add_noise=F
+00011650: 616c 7365 293a 0a20 2020 2020 202e 2e2e  alse):.      ...
+00011660: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
+00011670: 6528 3136 2928 7829 0a20 2020 2020 202e  e(16)(x).      .
+00011680: 2e2e 2020 2020 2078 203d 206e 6e2e 7265  ..     x = nn.re
+00011690: 6c75 2878 290a 2020 2020 2020 2e2e 2e0a  lu(x).      ....
+000116a0: 2020 2020 2020 2e2e 2e20 2020 2020 6966        ...     if
+000116b0: 2061 6464 5f6e 6f69 7365 3a0a 2020 2020   add_noise:.    
+000116c0: 2020 2e2e 2e20 2020 2020 2020 2320 4164    ...       # Ad
+000116d0: 6420 6761 7573 7369 616e 206e 6f69 7365  d gaussian noise
+000116e0: 0a20 2020 2020 202e 2e2e 2020 2020 2020  .      ...      
+000116f0: 206e 6f69 7365 5f6b 6579 203d 2073 656c   noise_key = sel
+00011700: 662e 6d61 6b65 5f72 6e67 2827 6e6f 6973  f.make_rng('nois
+00011710: 6527 290a 2020 2020 2020 2e2e 2e20 2020  e').      ...   
+00011720: 2020 2020 7820 3d20 7820 2b20 6a61 782e      x = x + jax.
+00011730: 7261 6e64 6f6d 2e6e 6f72 6d61 6c28 6e6f  random.normal(no
+00011740: 6973 655f 6b65 792c 2078 2e73 6861 7065  ise_key, x.shape
+00011750: 290a 2020 2020 2020 2e2e 2e0a 2020 2020  ).      ....    
+00011760: 2020 2e2e 2e20 2020 2020 7265 7475 726e    ...     return
+00011770: 206e 6e2e 4465 6e73 6528 3129 2878 290a   nn.Dense(1)(x).
+00011780: 0a20 2020 2020 203e 3e3e 2078 203d 206a  .      >>> x = j
+00011790: 6e70 2e65 6d70 7479 2828 312c 2037 2929  np.empty((1, 7))
+000117a0: 0a20 2020 2020 203e 3e3e 206d 6f64 756c  .      >>> modul
+000117b0: 6520 3d20 466f 6f28 290a 2020 2020 2020  e = Foo().      
+000117c0: 3e3e 3e20 726e 6773 203d 207b 2770 6172  >>> rngs = {'par
+000117d0: 616d 7327 3a20 6a61 782e 7261 6e64 6f6d  ams': jax.random
+000117e0: 2e6b 6579 2830 292c 2027 6e6f 6973 6527  .key(0), 'noise'
+000117f0: 3a20 6a61 782e 7261 6e64 6f6d 2e6b 6579  : jax.random.key
+00011800: 2831 297d 0a20 2020 2020 203e 3e3e 2076  (1)}.      >>> v
+00011810: 6172 6961 626c 6573 203d 206d 6f64 756c  ariables = modul
+00011820: 652e 696e 6974 2872 6e67 732c 2078 290a  e.init(rngs, x).
+00011830: 2020 2020 2020 3e3e 3e20 6f75 7430 203d        >>> out0 =
+00011840: 206d 6f64 756c 652e 6170 706c 7928 7661   module.apply(va
+00011850: 7269 6162 6c65 732c 2078 2c20 6164 645f  riables, x, add_
+00011860: 6e6f 6973 653d 5472 7565 2c20 726e 6773  noise=True, rngs
+00011870: 3d72 6e67 7329 0a0a 2020 2020 2020 3e3e  =rngs)..      >>
+00011880: 3e20 726e 6773 5b27 6e6f 6973 6527 5d20  > rngs['noise'] 
+00011890: 3d20 6a61 782e 7261 6e64 6f6d 2e6b 6579  = jax.random.key
+000118a0: 2830 290a 2020 2020 2020 3e3e 3e20 6f75  (0).      >>> ou
+000118b0: 7431 203d 206d 6f64 756c 652e 6170 706c  t1 = module.appl
+000118c0: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
+000118d0: 6164 645f 6e6f 6973 653d 5472 7565 2c20  add_noise=True, 
+000118e0: 726e 6773 3d72 6e67 7329 0a20 2020 2020  rngs=rngs).     
+000118f0: 203e 3e3e 2023 2064 6966 6665 7265 6e74   >>> # different
+00011900: 206f 7574 7075 7420 286b 6579 2831 2920   output (key(1) 
+00011910: 7673 206b 6579 2830 2929 0a20 2020 2020  vs key(0)).     
+00011920: 203e 3e3e 206e 702e 7465 7374 696e 672e   >>> np.testing.
+00011930: 6173 7365 7274 5f72 6169 7365 7328 4173  assert_raises(As
+00011940: 7365 7274 696f 6e45 7272 6f72 2c20 6e70  sertionError, np
+00011950: 2e74 6573 7469 6e67 2e61 7373 6572 745f  .testing.assert_
+00011960: 616c 6c63 6c6f 7365 2c20 6f75 7430 2c20  allclose, out0, 
+00011970: 6f75 7431 290a 0a20 2020 2020 203e 3e3e  out1)..      >>>
+00011980: 2064 656c 2072 6e67 735b 276e 6f69 7365   del rngs['noise
+00011990: 275d 0a20 2020 2020 203e 3e3e 2023 2073  '].      >>> # s
+000119a0: 656c 662e 6d61 6b65 5f72 6e67 2827 6e6f  elf.make_rng('no
+000119b0: 6973 6527 2920 7769 6c6c 2064 6566 6175  ise') will defau
+000119c0: 6c74 2074 6f20 7573 696e 6720 7468 6520  lt to using the 
+000119d0: 2770 6172 616d 7327 2052 4e47 2073 7472  'params' RNG str
+000119e0: 6561 6d0a 2020 2020 2020 3e3e 3e20 6f75  eam.      >>> ou
+000119f0: 7432 203d 206d 6f64 756c 652e 6170 706c  t2 = module.appl
+00011a00: 7928 7661 7269 6162 6c65 732c 2078 2c20  y(variables, x, 
+00011a10: 6164 645f 6e6f 6973 653d 5472 7565 2c20  add_noise=True, 
+00011a20: 726e 6773 3d72 6e67 7329 0a20 2020 2020  rngs=rngs).     
+00011a30: 203e 3e3e 2023 2073 616d 6520 6f75 7470   >>> # same outp
+00011a40: 7574 2028 6b65 7928 3029 290a 2020 2020  ut (key(0)).    
+00011a50: 2020 3e3e 3e20 6e70 2e74 6573 7469 6e67    >>> np.testing
+00011a60: 2e61 7373 6572 745f 616c 6c63 6c6f 7365  .assert_allclose
+00011a70: 286f 7574 312c 206f 7574 3229 0a0a 2020  (out1, out2)..  
+00011a80: 2020 2020 3e3e 3e20 2320 7061 7373 696e      >>> # passin
+00011a90: 6720 696e 2061 2073 696e 676c 6520 6b65  g in a single ke
+00011aa0: 7920 6973 2065 7175 6976 616c 656e 7420  y is equivalent 
+00011ab0: 746f 2070 6173 7369 6e67 2069 6e20 7b27  to passing in {'
+00011ac0: 7061 7261 6d73 273a 206b 6579 7d0a 2020  params': key}.  
+00011ad0: 2020 2020 3e3e 3e20 6f75 7433 203d 206d      >>> out3 = m
+00011ae0: 6f64 756c 652e 6170 706c 7928 7661 7269  odule.apply(vari
+00011af0: 6162 6c65 732c 2078 2c20 6164 645f 6e6f  ables, x, add_no
+00011b00: 6973 653d 5472 7565 2c20 726e 6773 3d6a  ise=True, rngs=j
+00011b10: 6178 2e72 616e 646f 6d2e 6b65 7928 3029  ax.random.key(0)
+00011b20: 290a 2020 2020 2020 3e3e 3e20 2320 7361  ).      >>> # sa
+00011b30: 6d65 206f 7574 7075 7420 286b 6579 2830  me output (key(0
+00011b40: 2929 0a20 2020 2020 203e 3e3e 206e 702e  )).      >>> np.
+00011b50: 7465 7374 696e 672e 6173 7365 7274 5f61  testing.assert_a
+00011b60: 6c6c 636c 6f73 6528 6f75 7432 2c20 6f75  llclose(out2, ou
+00011b70: 7433 290a 0a20 2020 2041 7267 733a 0a20  t3)..    Args:. 
+00011b80: 2020 2020 2076 6172 6961 626c 6573 3a20       variables: 
+00011b90: 4120 6469 6374 696f 6e61 7279 2063 6f6e  A dictionary con
+00011ba0: 7461 696e 696e 6720 7661 7269 6162 6c65  taining variable
+00011bb0: 7320 6b65 7965 6420 6279 2076 6172 6961  s keyed by varia
+00011bc0: 626c 650a 2020 2020 2020 2020 636f 6c6c  ble.        coll
+00011bd0: 6563 7469 6f6e 732e 2053 6565 203a 6d6f  ections. See :mo
+00011be0: 643a 6066 6c61 782e 636f 7265 2e76 6172  d:`flax.core.var
+00011bf0: 6961 626c 6573 6020 666f 7220 6d6f 7265  iables` for more
+00011c00: 2064 6574 6169 6c73 2061 626f 7574 0a20   details about. 
+00011c10: 2020 2020 2020 2076 6172 6961 626c 6573         variables
+00011c20: 2e0a 2020 2020 2020 2a61 7267 733a 204e  ..      *args: N
+00011c30: 616d 6564 2061 7267 756d 656e 7473 2070  amed arguments p
+00011c40: 6173 7365 6420 746f 2074 6865 2073 7065  assed to the spe
+00011c50: 6369 6669 6564 2061 7070 6c79 206d 6574  cified apply met
+00011c60: 686f 642e 0a20 2020 2020 2072 6e67 733a  hod..      rngs:
+00011c70: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
+00011c80: 6579 7320 746f 2069 6e69 7469 616c 697a  eys to initializ
+00011c90: 6520 7468 6520 5052 4e47 2073 6571 7565  e the PRNG seque
+00011ca0: 6e63 6573 2e20 5468 6520 2270 6172 616d  nces. The "param
+00011cb0: 7322 0a20 2020 2020 2020 2050 524e 4720  s".        PRNG 
+00011cc0: 7365 7175 656e 6365 2069 7320 7573 6564  sequence is used
+00011cd0: 2074 6f20 696e 6974 6961 6c69 7a65 2070   to initialize p
+00011ce0: 6172 616d 6574 6572 732e 0a20 2020 2020  arameters..     
+00011cf0: 206d 6574 686f 643a 2041 2066 756e 6374   method: A funct
+00011d00: 696f 6e20 746f 2063 616c 6c20 6170 706c  ion to call appl
+00011d10: 7920 6f6e 2e20 5468 6973 2069 7320 6765  y on. This is ge
+00011d20: 6e65 7261 6c6c 7920 6120 6675 6e63 7469  nerally a functi
+00011d30: 6f6e 2069 6e20 7468 650a 2020 2020 2020  on in the.      
+00011d40: 2020 6d6f 6475 6c65 2e20 4966 2070 726f    module. If pro
+00011d50: 7669 6465 642c 2061 7070 6c69 6573 2074  vided, applies t
+00011d60: 6869 7320 6d65 7468 6f64 2e20 4966 206e  his method. If n
+00011d70: 6f74 2070 726f 7669 6465 642c 2061 7070  ot provided, app
+00011d80: 6c69 6573 2074 6865 0a20 2020 2020 2020  lies the.       
+00011d90: 2060 605f 5f63 616c 6c5f 5f60 6020 6d65   ``__call__`` me
+00011da0: 7468 6f64 206f 6620 7468 6520 6d6f 6475  thod of the modu
+00011db0: 6c65 2e20 4120 7374 7269 6e67 2063 616e  le. A string can
+00011dc0: 2061 6c73 6f20 6265 2070 726f 7669 6465   also be provide
+00011dd0: 6420 746f 0a20 2020 2020 2020 2073 7065  d to.        spe
+00011de0: 6369 6679 2061 206d 6574 686f 6420 6279  cify a method by
+00011df0: 206e 616d 652e 0a20 2020 2020 206d 7574   name..      mut
+00011e00: 6162 6c65 3a20 4361 6e20 6265 2062 6f6f  able: Can be boo
+00011e10: 6c2c 2073 7472 2c20 6f72 206c 6973 742e  l, str, or list.
+00011e20: 2053 7065 6369 6669 6573 2077 6869 6368   Specifies which
+00011e30: 2063 6f6c 6c65 6374 696f 6e73 2073 686f   collections sho
+00011e40: 756c 6420 6265 0a20 2020 2020 2020 2074  uld be.        t
+00011e50: 7265 6174 6564 2061 7320 6d75 7461 626c  reated as mutabl
+00011e60: 653a 2060 6062 6f6f 6c60 603a 2061 6c6c  e: ``bool``: all
+00011e70: 2f6e 6f20 636f 6c6c 6563 7469 6f6e 7320  /no collections 
+00011e80: 6172 6520 6d75 7461 626c 652e 2060 6073  are mutable. ``s
+00011e90: 7472 6060 3a0a 2020 2020 2020 2020 5468  tr``:.        Th
+00011ea0: 6520 6e61 6d65 206f 6620 6120 7369 6e67  e name of a sing
+00011eb0: 6c65 206d 7574 6162 6c65 2063 6f6c 6c65  le mutable colle
+00011ec0: 6374 696f 6e2e 2060 606c 6973 7460 603a  ction. ``list``:
+00011ed0: 2041 206c 6973 7420 6f66 206e 616d 6573   A list of names
+00011ee0: 206f 660a 2020 2020 2020 2020 6d75 7461   of.        muta
+00011ef0: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
+00011f00: 0a20 2020 2020 2063 6170 7475 7265 5f69  .      capture_i
+00011f10: 6e74 6572 6d65 6469 6174 6573 3a20 4966  ntermediates: If
+00011f20: 2060 6054 7275 6560 602c 2063 6170 7475   ``True``, captu
+00011f30: 7265 7320 696e 7465 726d 6564 6961 7465  res intermediate
+00011f40: 2072 6574 7572 6e20 7661 6c75 6573 206f   return values o
+00011f50: 660a 2020 2020 2020 2020 616c 6c20 4d6f  f.        all Mo
+00011f60: 6475 6c65 7320 696e 7369 6465 2074 6865  dules inside the
+00011f70: 2022 696e 7465 726d 6564 6961 7465 7322   "intermediates"
+00011f80: 2063 6f6c 6c65 6374 696f 6e2e 2042 7920   collection. By 
+00011f90: 6465 6661 756c 742c 206f 6e6c 7920 7468  default, only th
+00011fa0: 650a 2020 2020 2020 2020 7265 7475 726e  e.        return
+00011fb0: 2076 616c 7565 7320 6f66 2061 6c6c 2060   values of all `
+00011fc0: 605f 5f63 616c 6c5f 5f60 6020 6d65 7468  `__call__`` meth
+00011fd0: 6f64 7320 6172 6520 7374 6f72 6564 2e20  ods are stored. 
+00011fe0: 4120 6675 6e63 7469 6f6e 2063 616e 2062  A function can b
+00011ff0: 650a 2020 2020 2020 2020 7061 7373 6564  e.        passed
+00012000: 2074 6f20 6368 616e 6765 2074 6865 2066   to change the f
+00012010: 696c 7465 7220 6265 6861 7669 6f72 2e20  ilter behavior. 
+00012020: 5468 6520 6669 6c74 6572 2066 756e 6374  The filter funct
+00012030: 696f 6e20 7461 6b65 7320 7468 650a 2020  ion takes the.  
+00012040: 2020 2020 2020 4d6f 6475 6c65 2069 6e73        Module ins
+00012050: 7461 6e63 6520 616e 6420 6d65 7468 6f64  tance and method
+00012060: 206e 616d 6520 616e 6420 7265 7475 726e   name and return
+00012070: 7320 6120 626f 6f6c 2069 6e64 6963 6174  s a bool indicat
+00012080: 696e 6720 7768 6574 6865 720a 2020 2020  ing whether.    
+00012090: 2020 2020 7468 6520 6f75 7470 7574 206f      the output o
+000120a0: 6620 7468 6174 206d 6574 686f 6420 696e  f that method in
+000120b0: 766f 6361 7469 6f6e 2073 686f 756c 6420  vocation should 
+000120c0: 6265 2073 746f 7265 642e 0a20 2020 2020  be stored..     
+000120d0: 202a 2a6b 7761 7267 733a 204b 6579 776f   **kwargs: Keywo
+000120e0: 7264 2061 7267 756d 656e 7473 2070 6173  rd arguments pas
+000120f0: 7365 6420 746f 2074 6865 2073 7065 6369  sed to the speci
+00012100: 6669 6564 2061 7070 6c79 206d 6574 686f  fied apply metho
+00012110: 642e 0a0a 2020 2020 5265 7475 726e 733a  d...    Returns:
+00012120: 0a20 2020 2020 2049 6620 6060 6d75 7461  .      If ``muta
+00012130: 626c 6560 6020 6973 2046 616c 7365 2c20  ble`` is False, 
+00012140: 7265 7475 726e 7320 6f75 7470 7574 2e20  returns output. 
+00012150: 4966 2061 6e79 2063 6f6c 6c65 6374 696f  If any collectio
+00012160: 6e73 2061 7265 0a20 2020 2020 206d 7574  ns are.      mut
+00012170: 6162 6c65 2c20 7265 7475 726e 7320 6060  able, returns ``
+00012180: 286f 7574 7075 742c 2076 6172 7329 6060  (output, vars)``
+00012190: 2c20 7768 6572 6520 6060 7661 7273 6060  , where ``vars``
+000121a0: 2061 7265 2069 7320 6120 6469 6374 0a20   are is a dict. 
+000121b0: 2020 2020 206f 6620 7468 6520 6d6f 6469       of the modi
+000121c0: 6669 6564 2063 6f6c 6c65 6374 696f 6e73  fied collections
+000121d0: 2e0a 2020 2020 2222 220a 2020 2020 4d6f  ..    """.    Mo
+000121e0: 6475 6c65 2e5f 6d6f 6475 6c65 5f63 6865  dule._module_che
+000121f0: 636b 7328 7365 6c66 290a 0a20 2020 2069  cks(self)..    i
+00012200: 6620 726e 6773 2069 7320 6e6f 7420 4e6f  f rngs is not No
+00012210: 6e65 2061 6e64 206e 6f74 2069 7369 6e73  ne and not isins
+00012220: 7461 6e63 6528 726e 6773 2c20 6469 6374  tance(rngs, dict
+00012230: 293a 0a20 2020 2020 2069 6620 6e6f 7420  ):.      if not 
+00012240: 636f 7265 2e73 636f 7065 2e5f 6973 5f76  core.scope._is_v
+00012250: 616c 6964 5f72 6e67 2872 6e67 7329 3a0a  alid_rng(rngs):.
+00012260: 2020 2020 2020 2020 7261 6973 6520 6572          raise er
+00012270: 726f 7273 2e49 6e76 616c 6964 526e 6745  rors.InvalidRngE
+00012280: 7272 6f72 280a 2020 2020 2020 2020 2020  rror(.          
+00012290: 2752 4e47 7320 7368 6f75 6c64 2062 6520  'RNGs should be 
+000122a0: 6f66 2073 6861 7065 2028 322c 2920 6f72  of shape (2,) or
+000122b0: 2050 524e 474b 6579 2069 6e20 4d6f 6475   PRNGKey in Modu
+000122c0: 6c65 2027 0a20 2020 2020 2020 2020 2066  le '.          f
+000122d0: 277b 7365 6c66 2e5f 5f63 6c61 7373 5f5f  '{self.__class__
+000122e0: 2e5f 5f6e 616d 655f 5f7d 2c20 6275 7420  .__name__}, but 
+000122f0: 726e 6773 2061 7265 3a20 7b72 6e67 737d  rngs are: {rngs}
+00012300: 270a 2020 2020 2020 2020 290a 2020 2020  '.        ).    
+00012310: 2020 726e 6773 203d 207b 2770 6172 616d    rngs = {'param
+00012320: 7327 3a20 726e 6773 7d0a 0a20 2020 2069  s': rngs}..    i
+00012330: 6620 6973 696e 7374 616e 6365 286d 6574  f isinstance(met
+00012340: 686f 642c 2073 7472 293a 0a20 2020 2020  hod, str):.     
+00012350: 2061 7474 7269 6275 7465 5f6e 616d 6520   attribute_name 
+00012360: 3d20 6d65 7468 6f64 0a20 2020 2020 206d  = method.      m
+00012370: 6574 686f 6420 3d20 6765 7461 7474 7228  ethod = getattr(
+00012380: 7365 6c66 2c20 6174 7472 6962 7574 655f  self, attribute_
+00012390: 6e61 6d65 290a 2020 2020 2020 6966 206e  name).      if n
+000123a0: 6f74 2063 616c 6c61 626c 6528 6d65 7468  ot callable(meth
+000123b0: 6f64 293a 0a20 2020 2020 2020 2063 6c61  od):.        cla
+000123c0: 7373 5f6e 616d 6520 3d20 7479 7065 2873  ss_name = type(s
+000123d0: 656c 6629 2e5f 5f6e 616d 655f 5f0a 2020  elf).__name__.  
+000123e0: 2020 2020 2020 7261 6973 6520 5479 7065        raise Type
+000123f0: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         
+00012400: 2066 2227 7b63 6c61 7373 5f6e 616d 657d   f"'{class_name}
+00012410: 2e7b 6174 7472 6962 7574 655f 6e61 6d65  .{attribute_name
+00012420: 7d27 206d 7573 7420 6265 2061 2063 616c  }' must be a cal
+00012430: 6c61 626c 652c 2067 6f74 220a 2020 2020  lable, got".    
+00012440: 2020 2020 2020 6627 207b 7479 7065 286d        f' {type(m
+00012450: 6574 686f 6429 7d2e 270a 2020 2020 2020  ethod)}.'.      
+00012460: 2020 290a 2020 2020 2020 2320 6966 2074    ).      # if t
+00012470: 6865 2060 6d65 7468 6f64 6020 7374 7269  he `method` stri
+00012480: 6e67 2069 7320 6120 7375 626d 6f64 756c  ng is a submodul
+00012490: 652c 2077 6520 6372 6561 7465 2061 206c  e, we create a l
+000124a0: 616d 6264 6120 6675 6e63 7469 6f6e 0a20  ambda function. 
+000124b0: 2020 2020 2023 2074 6861 7420 6361 6c6c       # that call
+000124c0: 7320 7468 6520 7375 626d 6f64 756c 652c  s the submodule,
+000124d0: 2066 6f72 7761 7264 696e 6720 616c 6c20   forwarding all 
+000124e0: 6172 6775 6d65 6e74 732e 0a20 2020 2020  arguments..     
+000124f0: 2069 6620 6973 696e 7374 616e 6365 286d   if isinstance(m
+00012500: 6574 686f 642c 204d 6f64 756c 6529 3a0a  ethod, Module):.
+00012510: 2020 2020 2020 2020 6d65 7468 6f64 203d          method =
+00012520: 206c 616d 6264 6120 7365 6c66 2c20 2a61   lambda self, *a
+00012530: 7267 732c 202a 2a6b 7761 7267 733a 2067  rgs, **kwargs: g
+00012540: 6574 6174 7472 2873 656c 662c 2061 7474  etattr(self, att
+00012550: 7269 6275 7465 5f6e 616d 6529 280a 2020  ribute_name)(.  
+00012560: 2020 2020 2020 2020 2a61 7267 732c 202a          *args, *
+00012570: 2a6b 7761 7267 730a 2020 2020 2020 2020  *kwargs.        
+00012580: 290a 2020 2020 656c 6966 206d 6574 686f  ).    elif metho
+00012590: 6420 6973 204e 6f6e 653a 0a20 2020 2020  d is None:.     
+000125a0: 206d 6574 686f 6420 3d20 7365 6c66 2e5f   method = self._
+000125b0: 5f63 616c 6c5f 5f0a 2020 2020 6d65 7468  _call__.    meth
+000125c0: 6f64 203d 205f 6765 745f 756e 626f 756e  od = _get_unboun
+000125d0: 645f 666e 286d 6574 686f 6429 0a20 2020  d_fn(method).   
+000125e0: 2072 6574 7572 6e20 6170 706c 7928 0a20   return apply(. 
+000125f0: 2020 2020 206d 6574 686f 642c 0a20 2020       method,.   
+00012600: 2020 2073 656c 662c 0a20 2020 2020 206d     self,.      m
+00012610: 7574 6162 6c65 3d6d 7574 6162 6c65 2c0a  utable=mutable,.
+00012620: 2020 2020 2020 6361 7074 7572 655f 696e        capture_in
+00012630: 7465 726d 6564 6961 7465 733d 6361 7074  termediates=capt
+00012640: 7572 655f 696e 7465 726d 6564 6961 7465  ure_intermediate
+00012650: 732c 0a20 2020 2029 2876 6172 6961 626c  s,.    )(variabl
+00012660: 6573 2c20 2a61 7267 732c 202a 2a6b 7761  es, *args, **kwa
+00012670: 7267 732c 2072 6e67 733d 726e 6773 290a  rgs, rngs=rngs).
+00012680: 0a20 2040 7472 6163 6562 6163 6b5f 7574  .  @traceback_ut
+00012690: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
+000126a0: 2020 6465 6620 696e 6974 5f77 6974 685f    def init_with_
+000126b0: 6f75 7470 7574 280a 2020 2020 7365 6c66  output(.    self
+000126c0: 2c0a 2020 2020 726e 6773 3a20 556e 696f  ,.    rngs: Unio
+000126d0: 6e5b 5052 4e47 4b65 792c 2052 4e47 5365  n[PRNGKey, RNGSe
+000126e0: 7175 656e 6365 735d 2c0a 2020 2020 2a61  quences],.    *a
+000126f0: 7267 732c 0a20 2020 206d 6574 686f 643a  rgs,.    method:
+00012700: 2055 6e69 6f6e 5b43 616c 6c61 626c 655b   Union[Callable[
+00012710: 2e2e 2e2c 2041 6e79 5d2c 2073 7472 2c20  ..., Any], str, 
+00012720: 4e6f 6e65 5d20 3d20 4e6f 6e65 2c0a 2020  None] = None,.  
+00012730: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+00012740: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
+00012750: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
+00012760: 6961 7465 7327 292c 0a20 2020 2063 6170  iates'),.    cap
+00012770: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+00012780: 6573 3a20 556e 696f 6e5b 626f 6f6c 2c20  es: Union[bool, 
+00012790: 4361 6c6c 6162 6c65 5b5b 274d 6f64 756c  Callable[['Modul
+000127a0: 6527 2c20 7374 725d 2c20 626f 6f6c 5d5d  e', str], bool]]
+000127b0: 203d 2046 616c 7365 2c0a 2020 2020 2a2a   = False,.    **
+000127c0: 6b77 6172 6773 2c0a 2020 2920 2d3e 2054  kwargs,.  ) -> T
+000127d0: 7570 6c65 5b41 6e79 2c20 556e 696f 6e5b  uple[Any, Union[
+000127e0: 4672 6f7a 656e 5661 7269 6162 6c65 4469  FrozenVariableDi
+000127f0: 6374 2c20 4469 6374 5b73 7472 2c20 416e  ct, Dict[str, An
+00012800: 795d 5d5d 3a0a 2020 2020 2222 2249 6e69  y]]]:.    """Ini
+00012810: 7469 616c 697a 6573 2061 206d 6f64 756c  tializes a modul
+00012820: 6520 6d65 7468 6f64 2077 6974 6820 7661  e method with va
+00012830: 7269 6162 6c65 7320 616e 6420 7265 7475  riables and retu
+00012840: 726e 7320 6f75 7470 7574 2061 6e64 206d  rns output and m
+00012850: 6f64 6966 6965 6420 7661 7269 6162 6c65  odified variable
+00012860: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  
+00012870: 2020 2020 726e 6773 3a20 5468 6520 726e      rngs: The rn
+00012880: 6773 2066 6f72 2074 6865 2076 6172 6961  gs for the varia
+00012890: 626c 6520 636f 6c6c 6563 7469 6f6e 732e  ble collections.
+000128a0: 0a20 2020 2020 202a 6172 6773 3a20 4e61  .      *args: Na
+000128b0: 6d65 6420 6172 6775 6d65 6e74 7320 7061  med arguments pa
+000128c0: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
+000128d0: 2066 756e 6374 696f 6e2e 0a20 2020 2020   function..     
+000128e0: 206d 6574 686f 643a 2041 6e20 6f70 7469   method: An opti
+000128f0: 6f6e 616c 206d 6574 686f 642e 2049 6620  onal method. If 
+00012900: 7072 6f76 6964 6564 2c20 6170 706c 6965  provided, applie
+00012910: 7320 7468 6973 206d 6574 686f 642e 2049  s this method. I
+00012920: 6620 6e6f 740a 2020 2020 2020 2020 7072  f not.        pr
+00012930: 6f76 6964 6564 2c20 6170 706c 6965 7320  ovided, applies 
+00012940: 7468 6520 6060 5f5f 6361 6c6c 5f5f 6060  the ``__call__``
+00012950: 206d 6574 686f 642e 2041 2073 7472 696e   method. A strin
+00012960: 6720 6361 6e20 616c 736f 2062 650a 2020  g can also be.  
+00012970: 2020 2020 2020 7072 6f76 6964 6564 2074        provided t
+00012980: 6f20 7370 6563 6966 7920 6120 6d65 7468  o specify a meth
+00012990: 6f64 2062 7920 6e61 6d65 2e0a 2020 2020  od by name..    
+000129a0: 2020 6d75 7461 626c 653a 2043 616e 2062    mutable: Can b
+000129b0: 6520 626f 6f6c 2c20 7374 722c 206f 7220  e bool, str, or 
+000129c0: 6c69 7374 2e20 5370 6563 6966 6965 7320  list. Specifies 
+000129d0: 7768 6963 6820 636f 6c6c 6563 7469 6f6e  which collection
+000129e0: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
+000129f0: 2020 2020 7472 6561 7465 6420 6173 206d      treated as m
+00012a00: 7574 6162 6c65 3a20 6060 626f 6f6c 6060  utable: ``bool``
+00012a10: 3a20 616c 6c2f 6e6f 2063 6f6c 6c65 6374  : all/no collect
+00012a20: 696f 6e73 2061 7265 206d 7574 6162 6c65  ions are mutable
+00012a30: 2e20 6060 7374 7260 603a 0a20 2020 2020  . ``str``:.     
+00012a40: 2020 2054 6865 206e 616d 6520 6f66 2061     The name of a
+00012a50: 2073 696e 676c 6520 6d75 7461 626c 6520   single mutable 
+00012a60: 636f 6c6c 6563 7469 6f6e 2e20 6060 6c69  collection. ``li
+00012a70: 7374 6060 3a20 4120 6c69 7374 206f 6620  st``: A list of 
+00012a80: 6e61 6d65 7320 6f66 0a20 2020 2020 2020  names of.       
+00012a90: 206d 7574 6162 6c65 2063 6f6c 6c65 6374   mutable collect
+00012aa0: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
+00012ab0: 2c20 616c 6c20 636f 6c6c 6563 7469 6f6e  , all collection
+00012ac0: 7320 6578 6365 7074 2022 696e 7465 726d  s except "interm
+00012ad0: 6564 6961 7465 7322 0a20 2020 2020 2020  ediates".       
+00012ae0: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
+00012af0: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
+00012b00: 726d 6564 6961 7465 733a 2049 6620 6060  rmediates: If ``
+00012b10: 5472 7565 6060 2c20 6361 7074 7572 6573  True``, captures
+00012b20: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
+00012b30: 7475 726e 2076 616c 7565 7320 6f66 0a20  turn values of. 
+00012b40: 2020 2020 2020 2061 6c6c 204d 6f64 756c         all Modul
+00012b50: 6573 2069 6e73 6964 6520 7468 6520 2269  es inside the "i
+00012b60: 6e74 6572 6d65 6469 6174 6573 2220 636f  ntermediates" co
+00012b70: 6c6c 6563 7469 6f6e 2e20 4279 2064 6566  llection. By def
+00012b80: 6175 6c74 206f 6e6c 7920 7468 650a 2020  ault only the.  
+00012b90: 2020 2020 2020 7265 7475 726e 2076 616c        return val
+00012ba0: 7565 7320 6f66 2061 6c6c 2060 605f 5f63  ues of all ``__c
+00012bb0: 616c 6c5f 5f60 6020 6d65 7468 6f64 7320  all__`` methods 
+00012bc0: 6172 6520 7374 6f72 6564 2e20 4120 6675  are stored. A fu
+00012bd0: 6e63 7469 6f6e 2063 616e 2062 650a 2020  nction can be.  
+00012be0: 2020 2020 2020 7061 7373 6564 2074 6f20        passed to 
+00012bf0: 6368 616e 6765 2074 6865 2066 696c 7465  change the filte
+00012c00: 7220 6265 6861 7669 6f72 2e20 5468 6520  r behavior. The 
+00012c10: 6669 6c74 6572 2066 756e 6374 696f 6e20  filter function 
+00012c20: 7461 6b65 7320 7468 650a 2020 2020 2020  takes the.      
+00012c30: 2020 4d6f 6475 6c65 2069 6e73 7461 6e63    Module instanc
+00012c40: 6520 616e 6420 6d65 7468 6f64 206e 616d  e and method nam
+00012c50: 6520 616e 6420 7265 7475 726e 7320 6120  e and returns a 
+00012c60: 626f 6f6c 2069 6e64 6963 6174 696e 6720  bool indicating 
+00012c70: 7768 6574 6865 720a 2020 2020 2020 2020  whether.        
+00012c80: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
+00012c90: 6174 206d 6574 686f 6420 696e 766f 6361  at method invoca
+00012ca0: 7469 6f6e 2073 686f 756c 6420 6265 2073  tion should be s
+00012cb0: 746f 7265 642e 0a20 2020 2020 202a 2a6b  tored..      **k
+00012cc0: 7761 7267 733a 204b 6579 776f 7264 2061  wargs: Keyword a
+00012cd0: 7267 756d 656e 7473 2070 6173 7365 6420  rguments passed 
+00012ce0: 746f 2074 6865 2069 6e69 7420 6675 6e63  to the init func
+00012cf0: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
+00012d00: 6e73 3a0a 2020 2020 2020 6060 286f 7574  ns:.      ``(out
+00012d10: 7075 742c 2076 6172 7329 6060 2c20 7768  put, vars)``, wh
+00012d20: 6572 6520 6060 7661 7273 6060 2061 7265  ere ``vars`` are
+00012d30: 2069 7320 6120 6469 6374 206f 6620 7468   is a dict of th
+00012d40: 6520 6d6f 6469 6669 6564 0a20 2020 2020  e modified.     
+00012d50: 2063 6f6c 6c65 6374 696f 6e73 2e0a 2020   collections..  
+00012d60: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
+00012d70: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
+00012d80: 7365 6c66 290a 0a20 2020 2069 6620 6e6f  self)..    if no
+00012d90: 7420 6973 696e 7374 616e 6365 2872 6e67  t isinstance(rng
+00012da0: 732c 2064 6963 7429 3a0a 2020 2020 2020  s, dict):.      
+00012db0: 6966 206e 6f74 2063 6f72 652e 7363 6f70  if not core.scop
+00012dc0: 652e 5f69 735f 7661 6c69 645f 726e 6728  e._is_valid_rng(
+00012dd0: 726e 6773 293a 0a20 2020 2020 2020 2072  rngs):.        r
+00012de0: 6169 7365 2065 7272 6f72 732e 496e 7661  aise errors.Inva
+00012df0: 6c69 6452 6e67 4572 726f 7228 0a20 2020  lidRngError(.   
+00012e00: 2020 2020 2020 2027 524e 4773 2073 686f         'RNGs sho
+00012e10: 756c 6420 6265 206f 6620 7368 6170 6520  uld be of shape 
+00012e20: 2832 2c29 206f 7220 5052 4e47 4b65 7920  (2,) or PRNGKey 
+00012e30: 696e 204d 6f64 756c 6520 270a 2020 2020  in Module '.    
+00012e40: 2020 2020 2020 6627 7b73 656c 662e 5f5f        f'{self.__
+00012e50: 636c 6173 735f 5f2e 5f5f 6e61 6d65 5f5f  class__.__name__
+00012e60: 7d2c 2062 7574 2072 6e67 7320 6172 653a  }, but rngs are:
+00012e70: 207b 726e 6773 7d27 0a20 2020 2020 2020   {rngs}'.       
+00012e80: 2029 0a20 2020 2020 2072 6e67 7320 3d20   ).      rngs = 
+00012e90: 7b27 7061 7261 6d73 273a 2072 6e67 737d  {'params': rngs}
+00012ea0: 0a0a 2020 2020 6966 2069 7369 6e73 7461  ..    if isinsta
+00012eb0: 6e63 6528 6d65 7468 6f64 2c20 7374 7229  nce(method, str)
+00012ec0: 3a0a 2020 2020 2020 6174 7472 6962 7574  :.      attribut
+00012ed0: 655f 6e61 6d65 203d 206d 6574 686f 640a  e_name = method.
+00012ee0: 2020 2020 2020 6d65 7468 6f64 203d 2067        method = g
+00012ef0: 6574 6174 7472 2873 656c 662c 2061 7474  etattr(self, att
+00012f00: 7269 6275 7465 5f6e 616d 6529 0a20 2020  ribute_name).   
+00012f10: 2020 2069 6620 6e6f 7420 6361 6c6c 6162     if not callab
+00012f20: 6c65 286d 6574 686f 6429 3a0a 2020 2020  le(method):.    
+00012f30: 2020 2020 636c 6173 735f 6e61 6d65 203d      class_name =
+00012f40: 2074 7970 6528 7365 6c66 292e 5f5f 6e61   type(self).__na
+00012f50: 6d65 5f5f 0a20 2020 2020 2020 2072 6169  me__.        rai
+00012f60: 7365 2054 7970 6545 7272 6f72 280a 2020  se TypeError(.  
+00012f70: 2020 2020 2020 2020 6622 277b 636c 6173          f"'{clas
+00012f80: 735f 6e61 6d65 7d2e 7b61 7474 7269 6275  s_name}.{attribu
+00012f90: 7465 5f6e 616d 657d 2720 6d75 7374 2062  te_name}' must b
+00012fa0: 6520 6120 6361 6c6c 6162 6c65 2c20 676f  e a callable, go
+00012fb0: 7422 0a20 2020 2020 2020 2020 2066 2720  t".          f' 
+00012fc0: 7b74 7970 6528 6d65 7468 6f64 297d 2e27  {type(method)}.'
+00012fd0: 0a20 2020 2020 2020 2029 0a20 2020 2065  .        ).    e
+00012fe0: 6c69 6620 6d65 7468 6f64 2069 7320 4e6f  lif method is No
+00012ff0: 6e65 3a0a 2020 2020 2020 6d65 7468 6f64  ne:.      method
+00013000: 203d 2073 656c 662e 5f5f 6361 6c6c 5f5f   = self.__call__
+00013010: 0a20 2020 206d 6574 686f 6420 3d20 5f67  .    method = _g
+00013020: 6574 5f75 6e62 6f75 6e64 5f66 6e28 6d65  et_unbound_fn(me
+00013030: 7468 6f64 290a 2020 2020 7265 7475 726e  thod).    return
+00013040: 2069 6e69 745f 7769 7468 5f6f 7574 7075   init_with_outpu
+00013050: 7428 0a20 2020 2020 206d 6574 686f 642c  t(.      method,
+00013060: 0a20 2020 2020 2073 656c 662c 0a20 2020  .      self,.   
+00013070: 2020 206d 7574 6162 6c65 3d6d 7574 6162     mutable=mutab
+00013080: 6c65 2c0a 2020 2020 2020 6361 7074 7572  le,.      captur
+00013090: 655f 696e 7465 726d 6564 6961 7465 733d  e_intermediates=
+000130a0: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+000130b0: 6961 7465 732c 0a20 2020 2029 2872 6e67  iates,.    )(rng
+000130c0: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
+000130d0: 6773 290a 0a20 2040 7472 6163 6562 6163  gs)..  @tracebac
+000130e0: 6b5f 7574 696c 2e61 7069 5f62 6f75 6e64  k_util.api_bound
+000130f0: 6172 790a 2020 6465 6620 696e 6974 280a  ary.  def init(.
+00013100: 2020 2020 7365 6c66 2c0a 2020 2020 726e      self,.    rn
+00013110: 6773 3a20 556e 696f 6e5b 5052 4e47 4b65  gs: Union[PRNGKe
+00013120: 792c 2052 4e47 5365 7175 656e 6365 735d  y, RNGSequences]
+00013130: 2c0a 2020 2020 2a61 7267 732c 0a20 2020  ,.    *args,.   
+00013140: 206d 6574 686f 643a 2055 6e69 6f6e 5b43   method: Union[C
+00013150: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+00013160: 5d2c 2073 7472 2c20 4e6f 6e65 5d20 3d20  ], str, None] = 
+00013170: 4e6f 6e65 2c0a 2020 2020 6d75 7461 626c  None,.    mutabl
+00013180: 653a 2043 6f6c 6c65 6374 696f 6e46 696c  e: CollectionFil
+00013190: 7465 7220 3d20 4465 6e79 4c69 7374 2827  ter = DenyList('
+000131a0: 696e 7465 726d 6564 6961 7465 7327 292c  intermediates'),
+000131b0: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
+000131c0: 6572 6d65 6469 6174 6573 3a20 556e 696f  ermediates: Unio
+000131d0: 6e5b 626f 6f6c 2c20 4361 6c6c 6162 6c65  n[bool, Callable
+000131e0: 5b5b 274d 6f64 756c 6527 2c20 7374 725d  [['Module', str]
+000131f0: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
+00013200: 2c0a 2020 2020 2a2a 6b77 6172 6773 2c0a  ,.    **kwargs,.
+00013210: 2020 2920 2d3e 2055 6e69 6f6e 5b46 726f    ) -> Union[Fro
+00013220: 7a65 6e56 6172 6961 626c 6544 6963 742c  zenVariableDict,
+00013230: 2044 6963 745b 7374 722c 2041 6e79 5d5d   Dict[str, Any]]
+00013240: 3a0a 2020 2020 2222 2249 6e69 7469 616c  :.    """Initial
+00013250: 697a 6573 2061 206d 6f64 756c 6520 6d65  izes a module me
+00013260: 7468 6f64 2077 6974 6820 7661 7269 6162  thod with variab
+00013270: 6c65 7320 616e 6420 7265 7475 726e 7320  les and returns 
+00013280: 6d6f 6469 6669 6564 2076 6172 6961 626c  modified variabl
+00013290: 6573 2e0a 0a20 2020 2060 6069 6e69 7460  es...    ``init`
+000132a0: 6020 7461 6b65 7320 6173 2066 6972 7374  ` takes as first
+000132b0: 2061 7267 756d 656e 7420 6569 7468 6572   argument either
+000132c0: 2061 2073 696e 676c 6520 6060 5052 4e47   a single ``PRNG
+000132d0: 4b65 7960 602c 206f 7220 610a 2020 2020  Key``, or a.    
+000132e0: 6469 6374 696f 6e61 7279 206d 6170 7069  dictionary mappi
+000132f0: 6e67 2076 6172 6961 626c 6520 636f 6c6c  ng variable coll
+00013300: 6563 7469 6f6e 7320 6e61 6d65 7320 746f  ections names to
+00013310: 2074 6865 6972 2060 6050 524e 474b 6579   their ``PRNGKey
+00013320: 7360 602c 2061 6e64 0a20 2020 2077 696c  s``, and.    wil
+00013330: 6c20 6361 6c6c 2060 606d 6574 686f 6460  l call ``method`
+00013340: 6020 2877 6869 6368 2069 7320 7468 6520  ` (which is the 
+00013350: 6d6f 6475 6c65 2773 2060 605f 5f63 616c  module's ``__cal
+00013360: 6c5f 5f60 6020 6675 6e63 7469 6f6e 2062  l__`` function b
+00013370: 790a 2020 2020 6465 6661 756c 7429 2070  y.    default) p
+00013380: 6173 7369 6e67 2060 602a 6172 6773 6060  assing ``*args``
+00013390: 2061 6e64 2060 602a 2a6b 7761 7267 7360   and ``**kwargs`
+000133a0: 602c 2061 6e64 2072 6574 7572 6e73 0a20  `, and returns. 
+000133b0: 2020 2061 2064 6963 7469 6f6e 6172 7920     a dictionary 
+000133c0: 6f66 2069 6e69 7469 616c 697a 6564 2076  of initialized v
+000133d0: 6172 6961 626c 6573 2e0a 0a20 2020 2045  ariables...    E
+000133e0: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
+000133f0: 3e3e 3e20 696d 706f 7274 2066 6c61 782e  >>> import flax.
+00013400: 6c69 6e65 6e20 6173 206e 6e0a 2020 2020  linen as nn.    
+00013410: 2020 3e3e 3e20 696d 706f 7274 206a 6178    >>> import jax
+00013420: 2c20 6a61 782e 6e75 6d70 7920 6173 206a  , jax.numpy as j
+00013430: 6e70 0a20 2020 2020 203e 3e3e 2069 6d70  np.      >>> imp
+00013440: 6f72 7420 6e75 6d70 7920 6173 206e 700a  ort numpy as np.
+00013450: 0a20 2020 2020 203e 3e3e 2063 6c61 7373  .      >>> class
+00013460: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
+00013470: 0a20 2020 2020 202e 2e2e 2020 2040 6e6e  .      ...   @nn
+00013480: 2e63 6f6d 7061 6374 0a20 2020 2020 202e  .compact.      .
+00013490: 2e2e 2020 2064 6566 205f 5f63 616c 6c5f  ..   def __call_
+000134a0: 5f28 7365 6c66 2c20 782c 2074 7261 696e  _(self, x, train
+000134b0: 293a 0a20 2020 2020 202e 2e2e 2020 2020  ):.      ...    
+000134c0: 2078 203d 206e 6e2e 4465 6e73 6528 3136   x = nn.Dense(16
+000134d0: 2928 7829 0a20 2020 2020 202e 2e2e 2020  )(x).      ...  
+000134e0: 2020 2078 203d 206e 6e2e 4261 7463 684e     x = nn.BatchN
+000134f0: 6f72 6d28 7573 655f 7275 6e6e 696e 675f  orm(use_running_
+00013500: 6176 6572 6167 653d 6e6f 7420 7472 6169  average=not trai
+00013510: 6e29 2878 290a 2020 2020 2020 2e2e 2e20  n)(x).      ... 
+00013520: 2020 2020 7820 3d20 6e6e 2e72 656c 7528      x = nn.relu(
+00013530: 7829 0a20 2020 2020 202e 2e2e 2020 2020  x).      ...    
+00013540: 2072 6574 7572 6e20 6e6e 2e44 656e 7365   return nn.Dense
+00013550: 2831 2928 7829 0a0a 2020 2020 2020 3e3e  (1)(x)..      >>
+00013560: 3e20 7820 3d20 6a6e 702e 656d 7074 7928  > x = jnp.empty(
+00013570: 2831 2c20 3729 290a 2020 2020 2020 3e3e  (1, 7)).      >>
+00013580: 3e20 6d6f 6475 6c65 203d 2046 6f6f 2829  > module = Foo()
+00013590: 0a20 2020 2020 203e 3e3e 206b 6579 203d  .      >>> key =
+000135a0: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
+000135b0: 3029 0a20 2020 2020 203e 3e3e 2076 6172  0).      >>> var
+000135c0: 6961 626c 6573 203d 206d 6f64 756c 652e  iables = module.
+000135d0: 696e 6974 286b 6579 2c20 782c 2074 7261  init(key, x, tra
+000135e0: 696e 3d46 616c 7365 290a 0a20 2020 2049  in=False)..    I
+000135f0: 6620 796f 7520 7061 7373 2061 2073 696e  f you pass a sin
+00013600: 676c 6520 6060 5052 4e47 4b65 7960 602c  gle ``PRNGKey``,
+00013610: 2046 6c61 7820 7769 6c6c 2075 7365 2069   Flax will use i
+00013620: 7420 746f 2066 6565 6420 7468 6520 6060  t to feed the ``
+00013630: 2770 6172 616d 7327 6060 0a20 2020 2052  'params'``.    R
+00013640: 4e47 2073 7472 6561 6d2e 2020 4966 2079  NG stream.  If y
+00013650: 6f75 2077 616e 7420 746f 2075 7365 2061  ou want to use a
+00013660: 2064 6966 6665 7265 6e74 2052 4e47 2073   different RNG s
+00013670: 7472 6561 6d20 6f72 206e 6565 6420 746f  tream or need to
+00013680: 2075 7365 0a20 2020 206d 756c 7469 706c   use.    multipl
+00013690: 6520 7374 7265 616d 732c 2079 6f75 2063  e streams, you c
+000136a0: 616e 2070 6173 7320 6120 6469 6374 696f  an pass a dictio
+000136b0: 6e61 7279 206d 6170 7069 6e67 2065 6163  nary mapping eac
+000136c0: 6820 524e 4720 7374 7265 616d 206e 616d  h RNG stream nam
+000136d0: 650a 2020 2020 746f 2069 7473 2063 6f72  e.    to its cor
+000136e0: 7265 7370 6f6e 6469 6e67 2060 6050 524e  responding ``PRN
+000136f0: 474b 6579 6060 2074 6f20 6060 696e 6974  GKey`` to ``init
+00013700: 6060 2e20 4966 2060 6073 656c 662e 6d61  ``. If ``self.ma
+00013710: 6b65 5f72 6e67 286e 616d 6529 6060 0a20  ke_rng(name)``. 
+00013720: 2020 2069 7320 6361 6c6c 6564 206f 6e20     is called on 
+00013730: 616e 2052 4e47 2073 7472 6561 6d20 6e61  an RNG stream na
+00013740: 6d65 2074 6861 7420 6973 6e27 7420 7061  me that isn't pa
+00013750: 7373 6564 2062 7920 7468 6520 7573 6572  ssed by the user
+00013760: 2c20 6974 2077 696c 6c0a 2020 2020 6465  , it will.    de
+00013770: 6661 756c 7420 746f 2075 7369 6e67 2074  fault to using t
+00013780: 6865 2060 6027 7061 7261 6d73 2760 6020  he ``'params'`` 
+00013790: 524e 4720 7374 7265 616d 2e0a 0a20 2020  RNG stream...   
+000137a0: 2045 7861 6d70 6c65 3a3a 0a0a 2020 2020   Example::..    
+000137b0: 2020 3e3e 3e20 636c 6173 7320 466f 6f28    >>> class Foo(
+000137c0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
+000137d0: 2020 2e2e 2e20 2020 406e 6e2e 636f 6d70    ...   @nn.comp
+000137e0: 6163 740a 2020 2020 2020 2e2e 2e20 2020  act.      ...   
+000137f0: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
+00013800: 662c 2078 293a 0a20 2020 2020 202e 2e2e  f, x):.      ...
+00013810: 2020 2020 2078 203d 206e 6e2e 4465 6e73       x = nn.Dens
+00013820: 6528 3136 2928 7829 0a20 2020 2020 202e  e(16)(x).      .
+00013830: 2e2e 2020 2020 2078 203d 206e 6e2e 7265  ..     x = nn.re
+00013840: 6c75 2878 290a 2020 2020 2020 2e2e 2e0a  lu(x).      ....
+00013850: 2020 2020 2020 2e2e 2e20 2020 2020 6f74        ...     ot
+00013860: 6865 725f 7661 7269 6162 6c65 203d 2073  her_variable = s
+00013870: 656c 662e 7661 7269 6162 6c65 280a 2020  elf.variable(.  
+00013880: 2020 2020 2e2e 2e20 2020 2020 2020 276f      ...       'o
+00013890: 7468 6572 5f63 6f6c 6c65 6374 696f 6e27  ther_collection'
+000138a0: 2c0a 2020 2020 2020 2e2e 2e20 2020 2020  ,.      ...     
+000138b0: 2020 276f 7468 6572 5f76 6172 6961 626c    'other_variabl
+000138c0: 6527 2c0a 2020 2020 2020 2e2e 2e20 2020  e',.      ...   
+000138d0: 2020 2020 6c61 6d62 6461 2078 3a20 6a61      lambda x: ja
+000138e0: 782e 7261 6e64 6f6d 2e6e 6f72 6d61 6c28  x.random.normal(
+000138f0: 7365 6c66 2e6d 616b 655f 726e 6728 276f  self.make_rng('o
+00013900: 7468 6572 5f72 6e67 2729 2c20 782e 7368  ther_rng'), x.sh
+00013910: 6170 6529 2c0a 2020 2020 2020 2e2e 2e20  ape),.      ... 
+00013920: 2020 2020 2020 782c 0a20 2020 2020 202e        x,.      .
+00013930: 2e2e 2020 2020 2029 0a20 2020 2020 202e  ..     ).      .
+00013940: 2e2e 2020 2020 2078 203d 2078 202b 206f  ..     x = x + o
+00013950: 7468 6572 5f76 6172 6961 626c 652e 7661  ther_variable.va
+00013960: 6c75 650a 2020 2020 2020 2e2e 2e0a 2020  lue.      ....  
+00013970: 2020 2020 2e2e 2e20 2020 2020 7265 7475      ...     retu
+00013980: 726e 206e 6e2e 4465 6e73 6528 3129 2878  rn nn.Dense(1)(x
+00013990: 290a 0a20 2020 2020 203e 3e3e 206d 6f64  )..      >>> mod
+000139a0: 756c 6520 3d20 466f 6f28 290a 2020 2020  ule = Foo().    
+000139b0: 2020 3e3e 3e20 726e 6773 203d 207b 2770    >>> rngs = {'p
+000139c0: 6172 616d 7327 3a20 6a61 782e 7261 6e64  arams': jax.rand
+000139d0: 6f6d 2e6b 6579 2830 292c 2027 6f74 6865  om.key(0), 'othe
+000139e0: 725f 726e 6727 3a20 6a61 782e 7261 6e64  r_rng': jax.rand
+000139f0: 6f6d 2e6b 6579 2831 297d 0a20 2020 2020  om.key(1)}.     
+00013a00: 203e 3e3e 2076 6172 6961 626c 6573 3020   >>> variables0 
+00013a10: 3d20 6d6f 6475 6c65 2e69 6e69 7428 726e  = module.init(rn
+00013a20: 6773 2c20 7829 0a0a 2020 2020 2020 3e3e  gs, x)..      >>
+00013a30: 3e20 726e 6773 5b27 6f74 6865 725f 726e  > rngs['other_rn
+00013a40: 6727 5d20 3d20 6a61 782e 7261 6e64 6f6d  g'] = jax.random
+00013a50: 2e6b 6579 2830 290a 2020 2020 2020 3e3e  .key(0).      >>
+00013a60: 3e20 7661 7269 6162 6c65 7331 203d 206d  > variables1 = m
+00013a70: 6f64 756c 652e 696e 6974 2872 6e67 732c  odule.init(rngs,
+00013a80: 2078 290a 2020 2020 2020 3e3e 3e20 2320   x).      >>> # 
+00013a90: 6571 7569 7661 6c65 6e74 2070 6172 616d  equivalent param
+00013aa0: 7320 286b 6579 2830 2929 0a20 2020 2020  s (key(0)).     
+00013ab0: 203e 3e3e 205f 203d 206a 6178 2e74 7265   >>> _ = jax.tre
+00013ac0: 655f 7574 696c 2e74 7265 655f 6d61 7028  e_util.tree_map(
+00013ad0: 0a20 2020 2020 202e 2e2e 2020 206e 702e  .      ...   np.
+00013ae0: 7465 7374 696e 672e 6173 7365 7274 5f61  testing.assert_a
+00013af0: 6c6c 636c 6f73 652c 2076 6172 6961 626c  llclose, variabl
+00013b00: 6573 305b 2770 6172 616d 7327 5d2c 2076  es0['params'], v
+00013b10: 6172 6961 626c 6573 315b 2770 6172 616d  ariables1['param
+00013b20: 7327 5d0a 2020 2020 2020 2e2e 2e20 290a  s'].      ... ).
+00013b30: 2020 2020 2020 3e3e 3e20 2320 6469 6666        >>> # diff
+00013b40: 6572 656e 7420 6f74 6865 725f 7661 7269  erent other_vari
+00013b50: 6162 6c65 2028 6b65 7928 3129 2076 7320  able (key(1) vs 
+00013b60: 6b65 7928 3029 290a 2020 2020 2020 3e3e  key(0)).      >>
+00013b70: 3e20 6e70 2e74 6573 7469 6e67 2e61 7373  > np.testing.ass
+00013b80: 6572 745f 7261 6973 6573 280a 2020 2020  ert_raises(.    
+00013b90: 2020 2e2e 2e20 2020 4173 7365 7274 696f    ...   Assertio
+00013ba0: 6e45 7272 6f72 2c0a 2020 2020 2020 2e2e  nError,.      ..
+00013bb0: 2e20 2020 6e70 2e74 6573 7469 6e67 2e61  .   np.testing.a
+00013bc0: 7373 6572 745f 616c 6c63 6c6f 7365 2c0a  ssert_allclose,.
+00013bd0: 2020 2020 2020 2e2e 2e20 2020 7661 7269        ...   vari
+00013be0: 6162 6c65 7330 5b27 6f74 6865 725f 636f  ables0['other_co
+00013bf0: 6c6c 6563 7469 6f6e 275d 5b27 6f74 6865  llection']['othe
+00013c00: 725f 7661 7269 6162 6c65 275d 2c0a 2020  r_variable'],.  
+00013c10: 2020 2020 2e2e 2e20 2020 7661 7269 6162      ...   variab
+00013c20: 6c65 7331 5b27 6f74 6865 725f 636f 6c6c  les1['other_coll
+00013c30: 6563 7469 6f6e 275d 5b27 6f74 6865 725f  ection']['other_
+00013c40: 7661 7269 6162 6c65 275d 2c0a 2020 2020  variable'],.    
+00013c50: 2020 2e2e 2e20 290a 0a20 2020 2020 203e    ... )..      >
+00013c60: 3e3e 2064 656c 2072 6e67 735b 276f 7468  >> del rngs['oth
+00013c70: 6572 5f72 6e67 275d 0a20 2020 2020 203e  er_rng'].      >
+00013c80: 3e3e 2023 2073 656c 662e 6d61 6b65 5f72  >> # self.make_r
+00013c90: 6e67 2827 6f74 6865 725f 726e 6727 2920  ng('other_rng') 
+00013ca0: 7769 6c6c 2064 6566 6175 6c74 2074 6f20  will default to 
+00013cb0: 7573 696e 6720 7468 6520 2770 6172 616d  using the 'param
+00013cc0: 7327 2052 4e47 2073 7472 6561 6d0a 2020  s' RNG stream.  
+00013cd0: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
+00013ce0: 7332 203d 206d 6f64 756c 652e 696e 6974  s2 = module.init
+00013cf0: 2872 6e67 732c 2078 290a 2020 2020 2020  (rngs, x).      
+00013d00: 3e3e 3e20 2320 6571 7569 7661 6c65 6e74  >>> # equivalent
+00013d10: 2070 6172 616d 7320 286b 6579 2830 2929   params (key(0))
+00013d20: 0a20 2020 2020 203e 3e3e 205f 203d 206a  .      >>> _ = j
+00013d30: 6178 2e74 7265 655f 7574 696c 2e74 7265  ax.tree_util.tre
+00013d40: 655f 6d61 7028 0a20 2020 2020 202e 2e2e  e_map(.      ...
+00013d50: 2020 206e 702e 7465 7374 696e 672e 6173     np.testing.as
+00013d60: 7365 7274 5f61 6c6c 636c 6f73 652c 2076  sert_allclose, v
+00013d70: 6172 6961 626c 6573 315b 2770 6172 616d  ariables1['param
+00013d80: 7327 5d2c 2076 6172 6961 626c 6573 325b  s'], variables2[
+00013d90: 2770 6172 616d 7327 5d0a 2020 2020 2020  'params'].      
+00013da0: 2e2e 2e20 290a 2020 2020 2020 3e3e 3e20  ... ).      >>> 
+00013db0: 2320 6571 7569 7661 6c65 6e74 206f 7468  # equivalent oth
+00013dc0: 6572 5f76 6172 6961 626c 6520 286b 6579  er_variable (key
+00013dd0: 2830 2929 0a20 2020 2020 203e 3e3e 206e  (0)).      >>> n
+00013de0: 702e 7465 7374 696e 672e 6173 7365 7274  p.testing.assert
+00013df0: 5f61 6c6c 636c 6f73 6528 0a20 2020 2020  _allclose(.     
+00013e00: 202e 2e2e 2020 2076 6172 6961 626c 6573   ...   variables
+00013e10: 315b 276f 7468 6572 5f63 6f6c 6c65 6374  1['other_collect
+00013e20: 696f 6e27 5d5b 276f 7468 6572 5f76 6172  ion']['other_var
+00013e30: 6961 626c 6527 5d2c 0a20 2020 2020 202e  iable'],.      .
+00013e40: 2e2e 2020 2076 6172 6961 626c 6573 325b  ..   variables2[
+00013e50: 276f 7468 6572 5f63 6f6c 6c65 6374 696f  'other_collectio
+00013e60: 6e27 5d5b 276f 7468 6572 5f76 6172 6961  n']['other_varia
+00013e70: 626c 6527 5d2c 0a20 2020 2020 202e 2e2e  ble'],.      ...
+00013e80: 2029 0a0a 2020 2020 2020 3e3e 3e20 2320   )..      >>> # 
+00013e90: 7061 7373 696e 6720 696e 2061 2073 696e  passing in a sin
+00013ea0: 676c 6520 6b65 7920 6973 2065 7175 6976  gle key is equiv
+00013eb0: 616c 656e 7420 746f 2070 6173 7369 6e67  alent to passing
+00013ec0: 2069 6e20 7b27 7061 7261 6d73 273a 206b   in {'params': k
+00013ed0: 6579 7d0a 2020 2020 2020 3e3e 3e20 7661  ey}.      >>> va
+00013ee0: 7269 6162 6c65 7333 203d 206d 6f64 756c  riables3 = modul
+00013ef0: 652e 696e 6974 286a 6178 2e72 616e 646f  e.init(jax.rando
+00013f00: 6d2e 6b65 7928 3029 2c20 7829 0a20 2020  m.key(0), x).   
+00013f10: 2020 203e 3e3e 2023 2065 7175 6976 616c     >>> # equival
+00013f20: 656e 7420 7061 7261 6d73 2028 6b65 7928  ent params (key(
+00013f30: 3029 290a 2020 2020 2020 3e3e 3e20 5f20  0)).      >>> _ 
+00013f40: 3d20 6a61 782e 7472 6565 5f75 7469 6c2e  = jax.tree_util.
+00013f50: 7472 6565 5f6d 6170 280a 2020 2020 2020  tree_map(.      
+00013f60: 2e2e 2e20 2020 6e70 2e74 6573 7469 6e67  ...   np.testing
+00013f70: 2e61 7373 6572 745f 616c 6c63 6c6f 7365  .assert_allclose
+00013f80: 2c20 7661 7269 6162 6c65 7332 5b27 7061  , variables2['pa
+00013f90: 7261 6d73 275d 2c20 7661 7269 6162 6c65  rams'], variable
+00013fa0: 7333 5b27 7061 7261 6d73 275d 0a20 2020  s3['params'].   
+00013fb0: 2020 202e 2e2e 2029 0a20 2020 2020 203e     ... ).      >
+00013fc0: 3e3e 2023 2065 7175 6976 616c 656e 7420  >> # equivalent 
+00013fd0: 6f74 6865 725f 7661 7269 6162 6c65 2028  other_variable (
+00013fe0: 6b65 7928 3029 290a 2020 2020 2020 3e3e  key(0)).      >>
+00013ff0: 3e20 6e70 2e74 6573 7469 6e67 2e61 7373  > np.testing.ass
+00014000: 6572 745f 616c 6c63 6c6f 7365 280a 2020  ert_allclose(.  
+00014010: 2020 2020 2e2e 2e20 2020 7661 7269 6162      ...   variab
+00014020: 6c65 7332 5b27 6f74 6865 725f 636f 6c6c  les2['other_coll
+00014030: 6563 7469 6f6e 275d 5b27 6f74 6865 725f  ection']['other_
+00014040: 7661 7269 6162 6c65 275d 2c0a 2020 2020  variable'],.    
+00014050: 2020 2e2e 2e20 2020 7661 7269 6162 6c65    ...   variable
+00014060: 7333 5b27 6f74 6865 725f 636f 6c6c 6563  s3['other_collec
+00014070: 7469 6f6e 275d 5b27 6f74 6865 725f 7661  tion']['other_va
+00014080: 7269 6162 6c65 275d 2c0a 2020 2020 2020  riable'],.      
+00014090: 2e2e 2e20 290a 0a20 2020 204a 6974 7469  ... )..    Jitti
+000140a0: 6e67 2060 6069 6e69 7460 6020 696e 6974  ng ``init`` init
+000140b0: 6961 6c69 7a65 7320 6120 6d6f 6465 6c20  ializes a model 
+000140c0: 6c61 7a69 6c79 2075 7369 6e67 206f 6e6c  lazily using onl
+000140d0: 7920 7468 6520 7368 6170 6573 206f 6620  y the shapes of 
+000140e0: 7468 650a 2020 2020 7072 6f76 6964 6564  the.    provided
+000140f0: 2061 7267 756d 656e 7473 2c20 616e 6420   arguments, and 
+00014100: 6176 6f69 6473 2063 6f6d 7075 7469 6e67  avoids computing
+00014110: 2074 6865 2066 6f72 7761 7264 2070 6173   the forward pas
+00014120: 7320 7769 7468 2061 6374 7561 6c0a 2020  s with actual.  
+00014130: 2020 7661 6c75 6573 2e20 4578 616d 706c    values. Exampl
+00014140: 653a 3a0a 0a20 2020 2020 203e 3e3e 206d  e::..      >>> m
+00014150: 6f64 756c 6520 3d20 6e6e 2e44 656e 7365  odule = nn.Dense
+00014160: 2831 290a 2020 2020 2020 3e3e 3e20 696e  (1).      >>> in
+00014170: 6974 5f6a 6974 203d 206a 6178 2e6a 6974  it_jit = jax.jit
+00014180: 286d 6f64 756c 652e 696e 6974 290a 2020  (module.init).  
+00014190: 2020 2020 3e3e 3e20 7661 7269 6162 6c65      >>> variable
+000141a0: 7320 3d20 696e 6974 5f6a 6974 286a 6178  s = init_jit(jax
+000141b0: 2e72 616e 646f 6d2e 6b65 7928 3029 2c20  .random.key(0), 
+000141c0: 7829 0a0a 2020 2020 6060 696e 6974 6060  x)..    ``init``
+000141d0: 2069 7320 6120 6c69 6768 7420 7772 6170   is a light wrap
+000141e0: 7065 7220 6f76 6572 2060 6061 7070 6c79  per over ``apply
+000141f0: 6060 2c20 736f 206f 7468 6572 2060 6061  ``, so other ``a
+00014200: 7070 6c79 6060 2061 7267 756d 656e 7473  pply`` arguments
+00014210: 0a20 2020 206c 696b 6520 6060 6d65 7468  .    like ``meth
+00014220: 6f64 6060 2c20 6060 6d75 7461 626c 6560  od``, ``mutable`
+00014230: 602c 2061 6e64 2060 6063 6170 7475 7265  `, and ``capture
+00014240: 5f69 6e74 6572 6d65 6469 6174 6573 6060  _intermediates``
+00014250: 2061 7265 2061 6c73 6f0a 2020 2020 6176   are also.    av
+00014260: 6169 6c61 626c 652e 0a0a 2020 2020 4172  ailable...    Ar
+00014270: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
+00014280: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
+00014290: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
+000142a0: 7469 6f6e 732e 0a20 2020 2020 202a 6172  tions..      *ar
+000142b0: 6773 3a20 4e61 6d65 6420 6172 6775 6d65  gs: Named argume
+000142c0: 6e74 7320 7061 7373 6564 2074 6f20 7468  nts passed to th
+000142d0: 6520 696e 6974 2066 756e 6374 696f 6e2e  e init function.
+000142e0: 0a20 2020 2020 206d 6574 686f 643a 2041  .      method: A
+000142f0: 6e20 6f70 7469 6f6e 616c 206d 6574 686f  n optional metho
+00014300: 642e 2049 6620 7072 6f76 6964 6564 2c20  d. If provided, 
+00014310: 6170 706c 6965 7320 7468 6973 206d 6574  applies this met
+00014320: 686f 642e 2049 6620 6e6f 740a 2020 2020  hod. If not.    
+00014330: 2020 2020 7072 6f76 6964 6564 2c20 6170      provided, ap
+00014340: 706c 6965 7320 7468 6520 6060 5f5f 6361  plies the ``__ca
+00014350: 6c6c 5f5f 6060 206d 6574 686f 642e 2041  ll__`` method. A
+00014360: 2073 7472 696e 6720 6361 6e20 616c 736f   string can also
+00014370: 2062 6520 7072 6f76 6964 6564 0a20 2020   be provided.   
+00014380: 2020 2020 2074 6f20 7370 6563 6966 7920       to specify 
+00014390: 6120 6d65 7468 6f64 2062 7920 6e61 6d65  a method by name
+000143a0: 2e0a 2020 2020 2020 6d75 7461 626c 653a  ..      mutable:
+000143b0: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
+000143c0: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
+000143d0: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
+000143e0: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
+000143f0: 650a 2020 2020 2020 2020 7472 6561 7465  e.        treate
+00014400: 6420 6173 206d 7574 6162 6c65 3a20 6060  d as mutable: ``
+00014410: 626f 6f6c 6060 3a20 616c 6c2f 6e6f 2063  bool``: all/no c
+00014420: 6f6c 6c65 6374 696f 6e73 2061 7265 206d  ollections are m
+00014430: 7574 6162 6c65 2e20 6060 7374 7260 603a  utable. ``str``:
+00014440: 0a20 2020 2020 2020 2054 6865 206e 616d  .        The nam
+00014450: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
+00014460: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
+00014470: 2e20 6060 6c69 7374 6060 3a20 4120 6c69  . ``list``: A li
+00014480: 7374 206f 6620 6e61 6d65 7320 6f66 0a20  st of names of. 
+00014490: 2020 2020 2020 206d 7574 6162 6c65 2063         mutable c
+000144a0: 6f6c 6c65 6374 696f 6e73 2e20 4279 2064  ollections. By d
+000144b0: 6566 6175 6c74 2061 6c6c 2063 6f6c 6c65  efault all colle
+000144c0: 6374 696f 6e73 2065 7863 6570 7420 2269  ctions except "i
+000144d0: 6e74 6572 6d65 6469 6174 6573 220a 2020  ntermediates".  
+000144e0: 2020 2020 2020 6172 6520 6d75 7461 626c        are mutabl
+000144f0: 652e 0a20 2020 2020 2063 6170 7475 7265  e..      capture
+00014500: 5f69 6e74 6572 6d65 6469 6174 6573 3a20  _intermediates: 
+00014510: 4966 2060 6054 7275 6560 602c 2063 6170  If ``True``, cap
+00014520: 7475 7265 7320 696e 7465 726d 6564 6961  tures intermedia
+00014530: 7465 2072 6574 7572 6e20 7661 6c75 6573  te return values
+00014540: 206f 660a 2020 2020 2020 2020 616c 6c20   of.        all 
+00014550: 4d6f 6475 6c65 7320 696e 7369 6465 2074  Modules inside t
+00014560: 6865 2022 696e 7465 726d 6564 6961 7465  he "intermediate
+00014570: 7322 2063 6f6c 6c65 6374 696f 6e2e 2042  s" collection. B
+00014580: 7920 6465 6661 756c 7420 6f6e 6c79 2074  y default only t
+00014590: 6865 0a20 2020 2020 2020 2072 6574 7572  he.        retur
+000145a0: 6e20 7661 6c75 6573 206f 6620 616c 6c20  n values of all 
+000145b0: 6060 5f5f 6361 6c6c 5f5f 6060 206d 6574  ``__call__`` met
+000145c0: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
+000145d0: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
+000145e0: 6265 0a20 2020 2020 2020 2070 6173 7365  be.        passe
+000145f0: 6420 746f 2063 6861 6e67 6520 7468 6520  d to change the 
+00014600: 6669 6c74 6572 2062 6568 6176 696f 722e  filter behavior.
+00014610: 2054 6865 2066 696c 7465 7220 6675 6e63   The filter func
+00014620: 7469 6f6e 2074 616b 6573 2074 6865 0a20  tion takes the. 
+00014630: 2020 2020 2020 204d 6f64 756c 6520 696e         Module in
+00014640: 7374 616e 6365 2061 6e64 206d 6574 686f  stance and metho
+00014650: 6420 6e61 6d65 2061 6e64 2072 6574 7572  d name and retur
+00014660: 6e73 2061 2062 6f6f 6c20 696e 6469 6361  ns a bool indica
+00014670: 7469 6e67 2077 6865 7468 6572 0a20 2020  ting whether.   
+00014680: 2020 2020 2074 6865 206f 7574 7075 7420       the output 
+00014690: 6f66 2074 6861 7420 6d65 7468 6f64 2069  of that method i
+000146a0: 6e76 6f63 6174 696f 6e20 7368 6f75 6c64  nvocation should
+000146b0: 2062 6520 7374 6f72 6564 2e0a 2020 2020   be stored..    
+000146c0: 2020 2a2a 6b77 6172 6773 3a20 4b65 7977    **kwargs: Keyw
+000146d0: 6f72 6420 6172 6775 6d65 6e74 7320 7061  ord arguments pa
+000146e0: 7373 6564 2074 6f20 7468 6520 696e 6974  ssed to the init
+000146f0: 2066 756e 6374 696f 6e2e 0a0a 2020 2020   function...    
+00014700: 5265 7475 726e 733a 0a20 2020 2020 2054  Returns:.      T
+00014710: 6865 2069 6e69 7469 616c 697a 6564 2076  he initialized v
+00014720: 6172 6961 626c 6520 6469 6374 2e0a 2020  ariable dict..  
+00014730: 2020 2222 220a 2020 2020 4d6f 6475 6c65    """.    Module
+00014740: 2e5f 6d6f 6475 6c65 5f63 6865 636b 7328  ._module_checks(
+00014750: 7365 6c66 290a 0a20 2020 205f 2c20 765f  self)..    _, v_
+00014760: 6f75 7420 3d20 7365 6c66 2e69 6e69 745f  out = self.init_
+00014770: 7769 7468 5f6f 7574 7075 7428 0a20 2020  with_output(.   
+00014780: 2020 2072 6e67 732c 0a20 2020 2020 202a     rngs,.      *
+00014790: 6172 6773 2c0a 2020 2020 2020 6d65 7468  args,.      meth
+000147a0: 6f64 3d6d 6574 686f 642c 0a20 2020 2020  od=method,.     
+000147b0: 206d 7574 6162 6c65 3d6d 7574 6162 6c65   mutable=mutable
+000147c0: 2c0a 2020 2020 2020 6361 7074 7572 655f  ,.      capture_
+000147d0: 696e 7465 726d 6564 6961 7465 733d 6361  intermediates=ca
+000147e0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+000147f0: 7465 732c 0a20 2020 2020 202a 2a6b 7761  tes,.      **kwa
+00014800: 7267 732c 0a20 2020 2029 0a20 2020 2072  rgs,.    ).    r
+00014810: 6574 7572 6e20 765f 6f75 740a 0a20 2040  eturn v_out..  @
+00014820: 7472 6163 6562 6163 6b5f 7574 696c 2e61  traceback_util.a
+00014830: 7069 5f62 6f75 6e64 6172 790a 2020 6465  pi_boundary.  de
+00014840: 6620 6c61 7a79 5f69 6e69 7428 0a20 2020  f lazy_init(.   
+00014850: 2073 656c 662c 0a20 2020 2072 6e67 733a   self,.    rngs:
+00014860: 2055 6e69 6f6e 5b50 524e 474b 6579 2c20   Union[PRNGKey, 
+00014870: 524e 4753 6571 7565 6e63 6573 5d2c 0a20  RNGSequences],. 
+00014880: 2020 202a 6172 6773 2c0a 2020 2020 6d65     *args,.    me
+00014890: 7468 6f64 3a20 4f70 7469 6f6e 616c 5b43  thod: Optional[C
+000148a0: 616c 6c61 626c 655b 2e2e 2e2c 2041 6e79  allable[..., Any
+000148b0: 5d5d 203d 204e 6f6e 652c 0a20 2020 206d  ]] = None,.    m
+000148c0: 7574 6162 6c65 3a20 436f 6c6c 6563 7469  utable: Collecti
+000148d0: 6f6e 4669 6c74 6572 203d 2044 656e 794c  onFilter = DenyL
+000148e0: 6973 7428 2769 6e74 6572 6d65 6469 6174  ist('intermediat
+000148f0: 6573 2729 2c0a 2020 2020 2a2a 6b77 6172  es'),.    **kwar
+00014900: 6773 2c0a 2020 2920 2d3e 2046 726f 7a65  gs,.  ) -> Froze
+00014910: 6e56 6172 6961 626c 6544 6963 743a 0a20  nVariableDict:. 
+00014920: 2020 2022 2222 496e 6974 6961 6c69 7a65     """Initialize
+00014930: 7320 6120 6d6f 6475 6c65 2077 6974 686f  s a module witho
+00014940: 7574 2063 6f6d 7075 7469 6e67 206f 6e20  ut computing on 
+00014950: 616e 2061 6374 7561 6c20 696e 7075 742e  an actual input.
+00014960: 0a0a 2020 2020 6c61 7a79 5f69 6e69 7420  ..    lazy_init 
+00014970: 7769 6c6c 2069 6e69 7469 616c 697a 6520  will initialize 
+00014980: 7468 6520 7661 7269 6162 6c65 7320 7769  the variables wi
+00014990: 7468 6f75 7420 646f 696e 6720 756e 6e65  thout doing unne
+000149a0: 6365 7373 6172 7920 636f 6d70 7574 652e  cessary compute.
+000149b0: 0a20 2020 2054 6865 2069 6e70 7574 2064  .    The input d
+000149c0: 6174 6120 7368 6f75 6c64 2062 6520 7061  ata should be pa
+000149d0: 7373 6564 2061 7320 6120 6060 6a61 782e  ssed as a ``jax.
+000149e0: 5368 6170 6544 7479 7065 5374 7275 6374  ShapeDtypeStruct
+000149f0: 6060 2077 6869 6368 0a20 2020 2073 7065  `` which.    spe
+00014a00: 6369 6669 6573 2074 6865 2073 6861 7065  cifies the shape
+00014a10: 2061 6e64 2064 7479 7065 206f 6620 7468   and dtype of th
+00014a20: 6520 696e 7075 7420 6275 7420 6e6f 2063  e input but no c
+00014a30: 6f6e 6372 6574 6520 6461 7461 2e0a 0a20  oncrete data... 
+00014a40: 2020 2045 7861 6d70 6c65 3a3a 0a0a 2020     Example::..  
+00014a50: 2020 2020 3e3e 3e20 6d6f 6465 6c20 3d20      >>> model = 
+00014a60: 6e6e 2e44 656e 7365 2866 6561 7475 7265  nn.Dense(feature
+00014a70: 733d 3235 3629 0a20 2020 2020 203e 3e3e  s=256).      >>>
+00014a80: 2076 6172 6961 626c 6573 203d 206d 6f64   variables = mod
+00014a90: 656c 2e6c 617a 795f 696e 6974 280a 2020  el.lazy_init(.  
+00014aa0: 2020 2020 2e2e 2e20 2020 2020 6a61 782e      ...     jax.
+00014ab0: 7261 6e64 6f6d 2e6b 6579 2830 292c 206a  random.key(0), j
+00014ac0: 6178 2e53 6861 7065 4474 7970 6553 7472  ax.ShapeDtypeStr
+00014ad0: 7563 7428 2831 2c20 3132 3829 2c20 6a6e  uct((1, 128), jn
+00014ae0: 702e 666c 6f61 7433 3229 290a 0a20 2020  p.float32))..   
+00014af0: 2054 6865 2061 7267 7320 616e 6420 6b77   The args and kw
+00014b00: 6172 6773 2061 7267 7320 7061 7373 6564  args args passed
+00014b10: 2074 6f20 6060 6c61 7a79 5f69 6e69 7460   to ``lazy_init`
+00014b20: 6020 6361 6e20 6265 2061 206d 6978 206f  ` can be a mix o
+00014b30: 660a 2020 2020 636f 6e63 7265 7465 2028  f.    concrete (
+00014b40: 6a61 7820 6172 7261 7973 2c20 7363 616c  jax arrays, scal
+00014b50: 6172 732c 2062 6f6f 6c73 2920 616e 6420  ars, bools) and 
+00014b60: 6162 7374 7261 6374 2028 5368 6170 6544  abstract (ShapeD
+00014b70: 7479 7065 5374 7275 6374 290a 2020 2020  typeStruct).    
+00014b80: 7661 6c75 6573 2e20 436f 6e63 7265 7465  values. Concrete
+00014b90: 2076 616c 7565 7320 6172 6520 6f6e 6c79   values are only
+00014ba0: 206e 6563 6573 7361 7279 2066 6f72 2061   necessary for a
+00014bb0: 7267 756d 656e 7473 2074 6861 7420 6166  rguments that af
+00014bc0: 6665 6374 0a20 2020 2074 6865 2069 6e69  fect.    the ini
+00014bd0: 7469 616c 697a 6174 696f 6e20 6f66 2076  tialization of v
+00014be0: 6172 6961 626c 6573 2e20 466f 7220 6578  ariables. For ex
+00014bf0: 616d 706c 652c 2074 6865 206d 6f64 656c  ample, the model
+00014c00: 206d 6967 6874 2065 7870 6563 740a 2020   might expect.  
+00014c10: 2020 6120 6b65 7977 6f72 6420 6172 6720    a keyword arg 
+00014c20: 7468 6174 2065 6e61 626c 6573 2f64 6973  that enables/dis
+00014c30: 6162 6c65 7320 6120 7375 6270 6172 7420  ables a subpart 
+00014c40: 6f66 2074 6865 206d 6f64 656c 2e0a 2020  of the model..  
+00014c50: 2020 496e 2074 6869 7320 6361 7365 2c20    In this case, 
+00014c60: 616e 2065 7870 6c69 6369 7420 7661 6c75  an explicit valu
+00014c70: 6520 2854 7275 652f 466c 6173 6529 2073  e (True/Flase) s
+00014c80: 686f 756c 6420 6265 2070 6173 7365 6420  hould be passed 
+00014c90: 6f74 6865 7277 6973 650a 2020 2020 6060  otherwise.    ``
+00014ca0: 6c61 7a79 5f69 6e69 7460 6020 6361 6e6e  lazy_init`` cann
+00014cb0: 6f74 2069 6e66 6572 2077 6869 6368 2076  ot infer which v
+00014cc0: 6172 6961 626c 6573 2073 686f 756c 6420  ariables should 
+00014cd0: 6265 2069 6e69 7469 616c 697a 6564 2e0a  be initialized..
+00014ce0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+00014cf0: 2072 6e67 733a 2054 6865 2072 6e67 7320   rngs: The rngs 
+00014d00: 666f 7220 7468 6520 7661 7269 6162 6c65  for the variable
+00014d10: 2063 6f6c 6c65 6374 696f 6e73 2e0a 2020   collections..  
+00014d20: 2020 2020 2a61 7267 733a 2061 7267 756d      *args: argum
+00014d30: 656e 7473 2070 6173 7365 6420 746f 2074  ents passed to t
+00014d40: 6865 2069 6e69 7420 6675 6e63 7469 6f6e  he init function
+00014d50: 2e0a 2020 2020 2020 6d65 7468 6f64 3a20  ..      method: 
+00014d60: 416e 206f 7074 696f 6e61 6c20 6d65 7468  An optional meth
+00014d70: 6f64 2e20 4966 2070 726f 7669 6465 642c  od. If provided,
+00014d80: 2061 7070 6c69 6573 2074 6869 7320 6d65   applies this me
+00014d90: 7468 6f64 2e20 4966 206e 6f74 0a20 2020  thod. If not.   
+00014da0: 2020 2020 2070 726f 7669 6465 642c 2061       provided, a
+00014db0: 7070 6c69 6573 2074 6865 2060 605f 5f63  pplies the ``__c
+00014dc0: 616c 6c5f 5f60 6020 6d65 7468 6f64 2e0a  all__`` method..
+00014dd0: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
+00014de0: 616e 2062 6520 626f 6f6c 2c20 7374 722c  an be bool, str,
+00014df0: 206f 7220 6c69 7374 2e20 5370 6563 6966   or list. Specif
+00014e00: 6965 7320 7768 6963 6820 636f 6c6c 6563  ies which collec
+00014e10: 7469 6f6e 7320 7368 6f75 6c64 2062 650a  tions should be.
+00014e20: 2020 2020 2020 2020 7472 6561 7465 6420          treated 
+00014e30: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
+00014e40: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
+00014e50: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
+00014e60: 6162 6c65 2e20 6060 7374 7260 603a 0a20  able. ``str``:. 
+00014e70: 2020 2020 2020 2054 6865 206e 616d 6520         The name 
+00014e80: 6f66 2061 2073 696e 676c 6520 6d75 7461  of a single muta
+00014e90: 626c 6520 636f 6c6c 6563 7469 6f6e 2e20  ble collection. 
+00014ea0: 6060 6c69 7374 6060 3a20 4120 6c69 7374  ``list``: A list
+00014eb0: 206f 6620 6e61 6d65 7320 6f66 0a20 2020   of names of.   
+00014ec0: 2020 2020 206d 7574 6162 6c65 2063 6f6c       mutable col
+00014ed0: 6c65 6374 696f 6e73 2e20 4279 2064 6566  lections. By def
+00014ee0: 6175 6c74 2061 6c6c 2063 6f6c 6c65 6374  ault all collect
+00014ef0: 696f 6e73 2065 7863 6570 7420 2269 6e74  ions except "int
+00014f00: 6572 6d65 6469 6174 6573 220a 2020 2020  ermediates".    
+00014f10: 2020 2020 6172 6520 6d75 7461 626c 652e      are mutable.
+00014f20: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
+00014f30: 204b 6579 776f 7264 2061 7267 756d 656e   Keyword argumen
+00014f40: 7473 2070 6173 7365 6420 746f 2074 6865  ts passed to the
+00014f50: 2069 6e69 7420 6675 6e63 7469 6f6e 2e0a   init function..
+00014f60: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  
+00014f70: 2020 2020 5468 6520 696e 6974 6961 6c69      The initiali
+00014f80: 7a65 6420 7661 7269 6162 6c65 2064 6963  zed variable dic
+00014f90: 742e 0a20 2020 2022 2222 0a20 2020 204d  t..    """.    M
+00014fa0: 6f64 756c 652e 5f6d 6f64 756c 655f 6368  odule._module_ch
+00014fb0: 6563 6b73 2873 656c 6629 0a0a 2020 2020  ecks(self)..    
+00014fc0: 6465 6620 6c61 7a79 5f77 7261 7070 6572  def lazy_wrapper
+00014fd0: 2872 6e67 732c 202a 6172 6773 2c20 2a2a  (rngs, *args, **
+00014fe0: 6b77 6172 6773 293a 0a20 2020 2020 2072  kwargs):.      r
+00014ff0: 6574 7572 6e20 7365 6c66 2e69 6e69 7428  eturn self.init(
+00015000: 726e 6773 2c20 2a61 7267 732c 206d 6574  rngs, *args, met
+00015010: 686f 643d 6d65 7468 6f64 2c20 6d75 7461  hod=method, muta
+00015020: 626c 653d 6d75 7461 626c 652c 202a 2a6b  ble=mutable, **k
+00015030: 7761 7267 7329 0a0a 2020 2020 7265 7475  wargs)..    retu
+00015040: 726e 2070 6172 7469 616c 5f65 7661 6c2e  rn partial_eval.
+00015050: 6c61 7a79 5f69 6e69 7428 6c61 7a79 5f77  lazy_init(lazy_w
+00015060: 7261 7070 6572 2928 726e 6773 2c20 2a61  rapper)(rngs, *a
+00015070: 7267 732c 202a 2a6b 7761 7267 7329 0a0a  rgs, **kwargs)..
+00015080: 2020 4070 726f 7065 7274 790a 2020 6465    @property.  de
+00015090: 6620 7661 7269 6162 6c65 7328 7365 6c66  f variables(self
+000150a0: 2920 2d3e 2056 6172 6961 626c 6544 6963  ) -> VariableDic
+000150b0: 743a 0a20 2020 2022 2222 5265 7475 726e  t:.    """Return
+000150c0: 7320 7468 6520 7661 7269 6162 6c65 7320  s the variables 
+000150d0: 696e 2074 6869 7320 6d6f 6475 6c65 2e22  in this module."
+000150e0: 2222 0a20 2020 2069 6620 7365 6c66 2e73  "".    if self.s
+000150f0: 636f 7065 2069 7320 4e6f 6e65 3a0a 2020  cope is None:.  
+00015100: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
+00015110: 7272 6f72 2822 4361 6e27 7420 6163 6365  rror("Can't acce
+00015120: 7373 2076 6172 6961 626c 6573 206f 6e20  ss variables on 
+00015130: 756e 626f 756e 6420 6d6f 6475 6c65 7322  unbound modules"
+00015140: 290a 2020 2020 7265 7475 726e 2073 656c  ).    return sel
+00015150: 662e 7363 6f70 652e 7661 7269 6162 6c65  f.scope.variable
+00015160: 7328 290a 0a20 2064 6566 2067 6574 5f76  s()..  def get_v
+00015170: 6172 6961 626c 6528 7365 6c66 2c20 636f  ariable(self, co
+00015180: 6c3a 2073 7472 2c20 6e61 6d65 3a20 7374  l: str, name: st
+00015190: 722c 2064 6566 6175 6c74 3a20 4f70 7469  r, default: Opti
+000151a0: 6f6e 616c 5b54 5d20 3d20 4e6f 6e65 2920  onal[T] = None) 
+000151b0: 2d3e 2054 3a0a 2020 2020 2222 2252 6574  -> T:.    """Ret
+000151c0: 7269 6576 6573 2074 6865 2076 616c 7565  rieves the value
+000151d0: 206f 6620 6120 5661 7269 6162 6c65 2e0a   of a Variable..
+000151e0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     
+000151f0: 2063 6f6c 3a20 7468 6520 7661 7269 6162   col: the variab
+00015200: 6c65 2063 6f6c 6c65 6374 696f 6e2e 0a20  le collection.. 
+00015210: 2020 2020 206e 616d 653a 2074 6865 206e       name: the n
+00015220: 616d 6520 6f66 2074 6865 2076 6172 6961  ame of the varia
+00015230: 626c 652e 0a20 2020 2020 2064 6566 6175  ble..      defau
+00015240: 6c74 3a20 7468 6520 6465 6661 756c 7420  lt: the default 
+00015250: 7661 6c75 6520 746f 2072 6574 7572 6e20  value to return 
+00015260: 6966 2074 6865 2076 6172 6961 626c 6520  if the variable 
+00015270: 646f 6573 206e 6f74 2065 7869 7374 2069  does not exist i
+00015280: 6e0a 2020 2020 2020 2020 7468 6973 2073  n.        this s
+00015290: 636f 7065 2e0a 0a20 2020 2052 6574 7572  cope...    Retur
+000152a0: 6e73 3a0a 2020 2020 2020 5468 6520 7661  ns:.      The va
+000152b0: 6c75 6520 6f66 2074 6865 2069 6e70 7574  lue of the input
+000152c0: 2076 6172 6961 626c 652c 206f 6620 7468   variable, of th
+000152d0: 6520 6465 6661 756c 7420 7661 6c75 6520  e default value 
+000152e0: 6966 2074 6865 2076 6172 6961 626c 650a  if the variable.
+000152f0: 2020 2020 2020 646f 6573 6e27 7420 6578        doesn't ex
+00015300: 6973 7420 696e 2074 6869 7320 7363 6f70  ist in this scop
+00015310: 652e 0a20 2020 2022 2222 0a20 2020 2069  e..    """.    i
+00015320: 6620 7365 6c66 2e73 636f 7065 2069 7320  f self.scope is 
+00015330: 4e6f 6e65 3a0a 2020 2020 2020 7261 6973  None:.      rais
+00015340: 6520 5661 6c75 6545 7272 6f72 2822 4361  e ValueError("Ca
+00015350: 6e27 7420 6163 6365 7373 2076 6172 6961  n't access varia
+00015360: 626c 6573 206f 6e20 756e 626f 756e 6420  bles on unbound 
+00015370: 6d6f 6475 6c65 7322 290a 2020 2020 7265  modules").    re
+00015380: 7475 726e 2073 656c 662e 7363 6f70 652e  turn self.scope.
+00015390: 6765 745f 7661 7269 6162 6c65 2863 6f6c  get_variable(col
+000153a0: 2c20 6e61 6d65 2c20 6465 6661 756c 7429  , name, default)
+000153b0: 0a0a 2020 6465 6620 7075 745f 7661 7269  ..  def put_vari
+000153c0: 6162 6c65 2873 656c 662c 2063 6f6c 3a20  able(self, col: 
+000153d0: 7374 722c 206e 616d 653a 2073 7472 2c20  str, name: str, 
+000153e0: 7661 6c75 653a 2041 6e79 293a 0a20 2020  value: Any):.   
+000153f0: 2022 2222 5570 6461 7465 7320 7468 6520   """Updates the 
+00015400: 7661 6c75 6520 6f66 2074 6865 2067 6976  value of the giv
+00015410: 656e 2076 6172 6961 626c 6520 6966 2069  en variable if i
+00015420: 7420 6973 206d 7574 6162 6c65 2c20 6f72  t is mutable, or
+00015430: 2061 6e20 6572 726f 7220 6f74 6865 7277   an error otherw
+00015440: 6973 652e 0a0a 2020 2020 4172 6773 3a0a  ise...    Args:.
+00015450: 2020 2020 2020 636f 6c3a 2074 6865 2076        col: the v
+00015460: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
+00015470: 6f6e 2e0a 2020 2020 2020 6e61 6d65 3a20  on..      name: 
+00015480: 7468 6520 6e61 6d65 206f 6620 7468 6520  the name of the 
+00015490: 7661 7269 6162 6c65 2e0a 2020 2020 2020  variable..      
+000154a0: 7661 6c75 653a 2074 6865 206e 6577 2076  value: the new v
+000154b0: 616c 7565 206f 6620 7468 6520 7661 7269  alue of the vari
+000154c0: 6162 6c65 2e0a 2020 2020 2222 220a 2020  able..    """.  
+000154d0: 2020 6966 2073 656c 662e 7363 6f70 6520    if self.scope 
+000154e0: 6973 204e 6f6e 653a 0a20 2020 2020 2072  is None:.      r
+000154f0: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00015500: 2243 616e 2774 2061 6363 6573 7320 7661  "Can't access va
+00015510: 7269 6162 6c65 7320 6f6e 2075 6e62 6f75  riables on unbou
+00015520: 6e64 206d 6f64 756c 6573 2229 0a20 2020  nd modules").   
+00015530: 2073 656c 662e 7363 6f70 652e 7075 745f   self.scope.put_
+00015540: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
+00015550: 6d65 2c20 7661 6c75 6529 0a0a 2020 406f  me, value)..  @o
+00015560: 7665 726c 6f61 640a 2020 6465 6620 736f  verload.  def so
+00015570: 7728 7365 6c66 2c20 636f 6c3a 2073 7472  w(self, col: str
+00015580: 2c20 6e61 6d65 3a20 7374 722c 2076 616c  , name: str, val
+00015590: 7565 3a20 416e 7929 202d 3e20 626f 6f6c  ue: Any) -> bool
+000155a0: 3a0a 2020 2020 2e2e 2e0a 0a20 2040 6f76  :.    .....  @ov
+000155b0: 6572 6c6f 6164 0a20 2064 6566 2073 6f77  erload.  def sow
+000155c0: 280a 2020 2020 7365 6c66 2c0a 2020 2020  (.    self,.    
+000155d0: 636f 6c3a 2073 7472 2c0a 2020 2020 6e61  col: str,.    na
+000155e0: 6d65 3a20 7374 722c 0a20 2020 2076 616c  me: str,.    val
+000155f0: 7565 3a20 542c 0a20 2020 2072 6564 7563  ue: T,.    reduc
+00015600: 655f 666e 3a20 4361 6c6c 6162 6c65 5b5b  e_fn: Callable[[
+00015610: 4b2c 2054 5d2c 204b 5d20 3d20 7475 706c  K, T], K] = tupl
+00015620: 655f 7265 6475 6365 2c0a 2020 2020 696e  e_reduce,.    in
+00015630: 6974 5f66 6e3a 2043 616c 6c61 626c 655b  it_fn: Callable[
+00015640: 5b5d 2c20 4b5d 203d 2074 7570 6c65 5f69  [], K] = tuple_i
+00015650: 6e69 742c 2020 2320 7479 7065 3a20 6967  nit,  # type: ig
+00015660: 6e6f 7265 0a20 2029 202d 3e20 626f 6f6c  nore.  ) -> bool
+00015670: 3a0a 2020 2020 2e2e 2e0a 0a20 2064 6566  :.    .....  def
+00015680: 2073 6f77 280a 2020 2020 7365 6c66 2c0a   sow(.    self,.
+00015690: 2020 2020 636f 6c3a 2073 7472 2c0a 2020      col: str,.  
+000156a0: 2020 6e61 6d65 3a20 7374 722c 0a20 2020    name: str,.   
+000156b0: 2076 616c 7565 3a20 542c 0a20 2020 2072   value: T,.    r
+000156c0: 6564 7563 655f 666e 3a20 4361 6c6c 6162  educe_fn: Callab
+000156d0: 6c65 5b5b 4b2c 2054 5d2c 204b 5d20 3d20  le[[K, T], K] = 
+000156e0: 7475 706c 655f 7265 6475 6365 2c0a 2020  tuple_reduce,.  
+000156f0: 2020 696e 6974 5f66 6e3a 2043 616c 6c61    init_fn: Calla
+00015700: 626c 655b 5b5d 2c20 4b5d 203d 2074 7570  ble[[], K] = tup
+00015710: 6c65 5f69 6e69 742c 2020 2320 7479 7065  le_init,  # type
+00015720: 3a20 6967 6e6f 7265 0a20 2029 202d 3e20  : ignore.  ) -> 
+00015730: 626f 6f6c 3a0a 2020 2020 2222 2253 746f  bool:.    """Sto
+00015740: 7265 7320 6120 7661 6c75 6520 696e 2061  res a value in a
+00015750: 2063 6f6c 6c65 6374 696f 6e2e 0a0a 2020   collection...  
+00015760: 2020 436f 6c6c 6563 7469 6f6e 7320 6361    Collections ca
+00015770: 6e20 6265 2075 7365 6420 746f 2063 6f6c  n be used to col
+00015780: 6c65 6374 2069 6e74 6572 6d65 6469 6174  lect intermediat
+00015790: 6520 7661 6c75 6573 2077 6974 686f 7574  e values without
+000157a0: 0a20 2020 2074 6865 206f 7665 7268 6561  .    the overhea
+000157b0: 6420 6f66 2065 7870 6c69 6369 746c 7920  d of explicitly 
+000157c0: 7061 7373 696e 6720 6120 636f 6e74 6169  passing a contai
+000157d0: 6e65 7220 7468 726f 7567 6820 6561 6368  ner through each
+000157e0: 204d 6f64 756c 6520 6361 6c6c 2e0a 0a20   Module call... 
+000157f0: 2020 2049 6620 7468 6520 7461 7267 6574     If the target
+00015800: 2063 6f6c 6c65 6374 696f 6e20 6973 206e   collection is n
+00015810: 6f74 206d 7574 6162 6c65 2060 6073 6f77  ot mutable ``sow
+00015820: 6060 2062 6568 6176 6573 206c 696b 6520  `` behaves like 
+00015830: 6120 6e6f 2d6f 700a 2020 2020 616e 6420  a no-op.    and 
+00015840: 7265 7475 726e 7320 6060 4661 6c73 6560  returns ``False`
+00015850: 602e 0a0a 2020 2020 4578 616d 706c 653a  `...    Example:
+00015860: 3a0a 0a20 2020 2020 203e 3e3e 2069 6d70  :..      >>> imp
+00015870: 6f72 7420 6a61 780a 2020 2020 2020 3e3e  ort jax.      >>
+00015880: 3e20 696d 706f 7274 206a 6178 2e6e 756d  > import jax.num
+00015890: 7079 2061 7320 6a6e 700a 2020 2020 2020  py as jnp.      
+000158a0: 3e3e 3e20 696d 706f 7274 2066 6c61 782e  >>> import flax.
+000158b0: 6c69 6e65 6e20 6173 206e 6e0a 0a20 2020  linen as nn..   
+000158c0: 2020 203e 3e3e 2063 6c61 7373 2046 6f6f     >>> class Foo
+000158d0: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
+000158e0: 2020 202e 2e2e 2020 2040 6e6e 2e63 6f6d     ...   @nn.com
+000158f0: 7061 6374 0a20 2020 2020 202e 2e2e 2020  pact.      ...  
+00015900: 2064 6566 205f 5f63 616c 6c5f 5f28 7365   def __call__(se
+00015910: 6c66 2c20 7829 3a0a 2020 2020 2020 2e2e  lf, x):.      ..
+00015920: 2e20 2020 2020 6820 3d20 6e6e 2e44 656e  .     h = nn.Den
+00015930: 7365 2834 2928 7829 0a20 2020 2020 202e  se(4)(x).      .
+00015940: 2e2e 2020 2020 2073 656c 662e 736f 7728  ..     self.sow(
+00015950: 2769 6e74 6572 6d65 6469 6174 6573 272c  'intermediates',
+00015960: 2027 6827 2c20 6829 0a20 2020 2020 202e   'h', h).      .
+00015970: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
+00015980: 2e44 656e 7365 2832 2928 6829 0a0a 2020  .Dense(2)(h)..  
+00015990: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
+000159a0: 6f6e 6573 2828 3136 2c20 3929 290a 2020  ones((16, 9)).  
+000159b0: 2020 2020 3e3e 3e20 6d6f 6465 6c20 3d20      >>> model = 
+000159c0: 466f 6f28 290a 2020 2020 2020 3e3e 3e20  Foo().      >>> 
+000159d0: 7661 7269 6162 6c65 7320 3d20 6d6f 6465  variables = mode
+000159e0: 6c2e 696e 6974 286a 6178 2e72 616e 646f  l.init(jax.rando
+000159f0: 6d2e 6b65 7928 3029 2c20 7829 0a20 2020  m.key(0), x).   
+00015a00: 2020 203e 3e3e 2079 2c20 7374 6174 6520     >>> y, state 
+00015a10: 3d20 6d6f 6465 6c2e 6170 706c 7928 7661  = model.apply(va
+00015a20: 7269 6162 6c65 732c 2078 2c20 6d75 7461  riables, x, muta
+00015a30: 626c 653d 5b27 696e 7465 726d 6564 6961  ble=['intermedia
+00015a40: 7465 7327 5d29 0a20 2020 2020 203e 3e3e  tes']).      >>>
+00015a50: 206a 6178 2e74 7265 652e 6d61 7028 6a6e   jax.tree.map(jn
+00015a60: 702e 7368 6170 652c 2073 7461 7465 5b27  p.shape, state['
+00015a70: 696e 7465 726d 6564 6961 7465 7327 5d29  intermediates'])
+00015a80: 0a20 2020 2020 207b 2768 273a 2028 2831  .      {'h': ((1
+00015a90: 362c 2034 292c 297d 0a0a 2020 2020 4279  6, 4),)}..    By
+00015aa0: 2064 6566 6175 6c74 2074 6865 2076 616c   default the val
+00015ab0: 7565 7320 6172 6520 7374 6f72 6564 2069  ues are stored i
+00015ac0: 6e20 6120 7475 706c 6520 616e 6420 6561  n a tuple and ea
+00015ad0: 6368 2073 746f 7265 6420 7661 6c75 650a  ch stored value.
+00015ae0: 2020 2020 6973 2061 7070 656e 6465 6420      is appended 
+00015af0: 6174 2074 6865 2065 6e64 2e20 5468 6973  at the end. This
+00015b00: 2077 6179 2061 6c6c 2069 6e74 6572 6d65   way all interme
+00015b10: 6469 6174 6573 2063 616e 2062 6520 7472  diates can be tr
+00015b20: 6163 6b65 6420 7768 656e 0a20 2020 2074  acked when.    t
+00015b30: 6865 2073 616d 6520 6d6f 6475 6c65 2069  he same module i
+00015b40: 7320 6361 6c6c 6564 206d 756c 7469 706c  s called multipl
+00015b50: 6520 7469 6d65 732e 2041 6c74 6572 6e61  e times. Alterna
+00015b60: 7469 7665 6c79 2c20 6120 6375 7374 6f6d  tively, a custom
+00015b70: 0a20 2020 2069 6e69 742f 7265 6475 6365  .    init/reduce
+00015b80: 2066 756e 6374 696f 6e20 6361 6e20 6265   function can be
+00015b90: 2070 6173 7365 643a 3a0a 0a20 2020 2020   passed::..     
+00015ba0: 203e 3e3e 2063 6c61 7373 2046 6f6f 3228   >>> class Foo2(
+00015bb0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
+00015bc0: 2020 2e2e 2e20 2020 406e 6e2e 636f 6d70    ...   @nn.comp
+00015bd0: 6163 740a 2020 2020 2020 2e2e 2e20 2020  act.      ...   
+00015be0: 6465 6620 5f5f 6361 6c6c 5f5f 2873 656c  def __call__(sel
+00015bf0: 662c 2078 293a 0a20 2020 2020 202e 2e2e  f, x):.      ...
+00015c00: 2020 2020 2069 6e69 745f 666e 203d 206c       init_fn = l
+00015c10: 616d 6264 613a 2030 0a20 2020 2020 202e  ambda: 0.      .
+00015c20: 2e2e 2020 2020 2072 6564 7563 655f 666e  ..     reduce_fn
+00015c30: 203d 206c 616d 6264 6120 612c 2062 3a20   = lambda a, b: 
+00015c40: 6120 2b20 620a 2020 2020 2020 2e2e 2e20  a + b.      ... 
+00015c50: 2020 2020 7365 6c66 2e73 6f77 2827 696e      self.sow('in
+00015c60: 7465 726d 6564 6961 7465 7327 2c20 2768  termediates', 'h
+00015c70: 272c 2078 2c0a 2020 2020 2020 2e2e 2e20  ', x,.      ... 
+00015c80: 2020 2020 2020 2020 2020 2020 2020 696e                in
+00015c90: 6974 5f66 6e3d 696e 6974 5f66 6e2c 2072  it_fn=init_fn, r
+00015ca0: 6564 7563 655f 666e 3d72 6564 7563 655f  educe_fn=reduce_
+00015cb0: 666e 290a 2020 2020 2020 2e2e 2e20 2020  fn).      ...   
+00015cc0: 2020 7365 6c66 2e73 6f77 2827 696e 7465    self.sow('inte
+00015cd0: 726d 6564 6961 7465 7327 2c20 2768 272c  rmediates', 'h',
+00015ce0: 2078 202a 2032 2c0a 2020 2020 2020 2e2e   x * 2,.      ..
+00015cf0: 2e20 2020 2020 2020 2020 2020 2020 2020  .               
+00015d00: 696e 6974 5f66 6e3d 696e 6974 5f66 6e2c  init_fn=init_fn,
+00015d10: 2072 6564 7563 655f 666e 3d72 6564 7563   reduce_fn=reduc
+00015d20: 655f 666e 290a 2020 2020 2020 2e2e 2e20  e_fn).      ... 
+00015d30: 2020 2020 7265 7475 726e 2078 0a0a 2020      return x..  
+00015d40: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
+00015d50: 6f6e 6573 2828 312c 2031 2929 0a20 2020  ones((1, 1)).   
+00015d60: 2020 203e 3e3e 206d 6f64 656c 203d 2046     >>> model = F
+00015d70: 6f6f 3228 290a 2020 2020 2020 3e3e 3e20  oo2().      >>> 
+00015d80: 7661 7269 6162 6c65 7320 3d20 6d6f 6465  variables = mode
+00015d90: 6c2e 696e 6974 286a 6178 2e72 616e 646f  l.init(jax.rando
+00015da0: 6d2e 6b65 7928 3029 2c20 7829 0a20 2020  m.key(0), x).   
+00015db0: 2020 203e 3e3e 2079 2c20 7374 6174 6520     >>> y, state 
+00015dc0: 3d20 6d6f 6465 6c2e 6170 706c 7928 0a20  = model.apply(. 
+00015dd0: 2020 2020 202e 2e2e 2020 2020 2076 6172       ...     var
+00015de0: 6961 626c 6573 2c20 782c 206d 7574 6162  iables, x, mutab
+00015df0: 6c65 3d5b 2769 6e74 6572 6d65 6469 6174  le=['intermediat
+00015e00: 6573 275d 290a 2020 2020 2020 3e3e 3e20  es']).      >>> 
+00015e10: 7072 696e 7428 7374 6174 655b 2769 6e74  print(state['int
+00015e20: 6572 6d65 6469 6174 6573 275d 290a 2020  ermediates']).  
+00015e30: 2020 2020 7b27 6827 3a20 4172 7261 7928      {'h': Array(
+00015e40: 5b5b 332e 5d5d 2c20 6474 7970 653d 666c  [[3.]], dtype=fl
+00015e50: 6f61 7433 3229 7d0a 0a20 2020 2041 7267  oat32)}..    Arg
+00015e60: 733a 0a20 2020 2020 2063 6f6c 3a20 5468  s:.      col: Th
+00015e70: 6520 6e61 6d65 206f 6620 7468 6520 7661  e name of the va
+00015e80: 7269 6162 6c65 2063 6f6c 6c65 6374 696f  riable collectio
+00015e90: 6e2e 0a20 2020 2020 206e 616d 653a 2054  n..      name: T
+00015ea0: 6865 206e 616d 6520 6f66 2074 6865 2076  he name of the v
+00015eb0: 6172 6961 626c 652e 0a20 2020 2020 2076  ariable..      v
+00015ec0: 616c 7565 3a20 5468 6520 7661 6c75 6520  alue: The value 
+00015ed0: 6f66 2074 6865 2076 6172 6961 626c 652e  of the variable.
+00015ee0: 0a20 2020 2020 2072 6564 7563 655f 666e  .      reduce_fn
+00015ef0: 3a20 5468 6520 6675 6e63 7469 6f6e 2075  : The function u
+00015f00: 7365 6420 746f 2063 6f6d 6269 6e65 2074  sed to combine t
+00015f10: 6865 2065 7869 7374 696e 6720 7661 6c75  he existing valu
+00015f20: 6520 7769 7468 2074 6865 206e 6577 0a20  e with the new. 
+00015f30: 2020 2020 2020 2076 616c 7565 2e20 5468         value. Th
+00015f40: 6520 6465 6661 756c 7420 6973 2074 6f20  e default is to 
+00015f50: 6170 7065 6e64 2074 6865 2076 616c 7565  append the value
+00015f60: 2074 6f20 6120 7475 706c 652e 0a20 2020   to a tuple..   
+00015f70: 2020 2069 6e69 745f 666e 3a20 466f 7220     init_fn: For 
+00015f80: 7468 6520 6669 7273 7420 7661 6c75 6520  the first value 
+00015f90: 7374 6f72 6564 2c20 6060 7265 6475 6365  stored, ``reduce
+00015fa0: 5f66 6e60 6020 7769 6c6c 2062 6520 7061  _fn`` will be pa
+00015fb0: 7373 6564 2074 6865 2072 6573 756c 740a  ssed the result.
+00015fc0: 2020 2020 2020 2020 6f66 2060 6069 6e69          of ``ini
+00015fd0: 745f 666e 6060 2074 6f67 6574 6865 7220  t_fn`` together 
+00015fe0: 7769 7468 2074 6865 2076 616c 7565 2074  with the value t
+00015ff0: 6f20 6265 2073 746f 7265 642e 2054 6865  o be stored. The
+00016000: 2064 6566 6175 6c74 2069 7320 616e 0a20   default is an. 
+00016010: 2020 2020 2020 2065 6d70 7479 2074 7570         empty tup
+00016020: 6c65 2e0a 0a20 2020 2052 6574 7572 6e73  le...    Returns
+00016030: 3a0a 2020 2020 2020 6060 5472 7565 6060  :.      ``True``
+00016040: 2069 6620 7468 6520 7661 6c75 6520 6861   if the value ha
+00016050: 7320 6265 656e 2073 746f 7265 6420 7375  s been stored su
+00016060: 6363 6573 7366 756c 6c79 2c20 6060 4661  ccessfully, ``Fa
+00016070: 6c73 6560 6020 6f74 6865 7277 6973 652e  lse`` otherwise.
+00016080: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    """.    if 
+00016090: 7365 6c66 2e73 636f 7065 2069 7320 4e6f  self.scope is No
+000160a0: 6e65 3a0a 2020 2020 2020 7261 6973 6520  ne:.      raise 
+000160b0: 5661 6c75 6545 7272 6f72 2822 4361 6e27  ValueError("Can'
+000160c0: 7420 7374 6f72 6520 7661 7269 6162 6c65  t store variable
+000160d0: 7320 6f6e 2075 6e62 6f75 6e64 206d 6f64  s on unbound mod
+000160e0: 756c 6573 2229 0a20 2020 2069 6620 6e6f  ules").    if no
+000160f0: 7420 7365 6c66 2e73 636f 7065 2e69 735f  t self.scope.is_
+00016100: 6d75 7461 626c 655f 636f 6c6c 6563 7469  mutable_collecti
+00016110: 6f6e 2863 6f6c 293a 0a20 2020 2020 2072  on(col):.      r
+00016120: 6574 7572 6e20 4661 6c73 650a 2020 2020  eturn False.    
+00016130: 6966 2073 656c 662e 7363 6f70 652e 6861  if self.scope.ha
+00016140: 735f 7661 7269 6162 6c65 2863 6f6c 2c20  s_variable(col, 
+00016150: 6e61 6d65 293a 0a20 2020 2020 2078 7320  name):.      xs 
+00016160: 3d20 7365 6c66 2e73 636f 7065 2e67 6574  = self.scope.get
+00016170: 5f76 6172 6961 626c 6528 636f 6c2c 206e  _variable(col, n
+00016180: 616d 6529 0a20 2020 2065 6c73 653a 0a20  ame).    else:. 
+00016190: 2020 2020 2073 656c 662e 7363 6f70 652e       self.scope.
+000161a0: 7265 7365 7276 6528 6e61 6d65 2c20 636f  reserve(name, co
+000161b0: 6c29 0a20 2020 2020 2073 656c 662e 5f73  l).      self._s
+000161c0: 7461 7465 2e63 6869 6c64 7265 6e5b 6e61  tate.children[na
+000161d0: 6d65 5d20 3d20 636f 6c0a 2020 2020 2020  me] = col.      
+000161e0: 7873 203d 2069 6e69 745f 666e 2829 0a20  xs = init_fn(). 
+000161f0: 2020 2078 7320 3d20 7265 6475 6365 5f66     xs = reduce_f
+00016200: 6e28 7873 2c20 7661 6c75 6529 0a20 2020  n(xs, value).   
+00016210: 2073 656c 662e 7363 6f70 652e 7075 745f   self.scope.put_
+00016220: 7661 7269 6162 6c65 2863 6f6c 2c20 6e61  variable(col, na
+00016230: 6d65 2c20 7873 290a 2020 2020 7265 7475  me, xs).    retu
+00016240: 726e 2054 7275 650a 0a20 2064 6566 2070  rn True..  def p
+00016250: 6572 7475 7262 280a 2020 2020 7365 6c66  erturb(.    self
+00016260: 2c20 6e61 6d65 3a20 7374 722c 2076 616c  , name: str, val
+00016270: 7565 3a20 542c 2063 6f6c 6c65 6374 696f  ue: T, collectio
+00016280: 6e3a 2073 7472 203d 2027 7065 7274 7572  n: str = 'pertur
+00016290: 6261 7469 6f6e 7327 0a20 2029 202d 3e20  bations'.  ) -> 
+000162a0: 543a 0a20 2020 2022 2222 4164 6420 616e  T:.    """Add an
+000162b0: 207a 6572 6f2d 7661 6c75 6520 7661 7269   zero-value vari
+000162c0: 6162 6c65 2028 2770 6572 7475 7262 6174  able ('perturbat
+000162d0: 696f 6e27 2920 746f 2074 6865 2069 6e74  ion') to the int
+000162e0: 6572 6d65 6469 6174 6520 7661 6c75 652e  ermediate value.
+000162f0: 0a0a 2020 2020 5468 6520 6772 6164 6965  ..    The gradie
+00016300: 6e74 206f 6620 6060 7661 6c75 6560 6020  nt of ``value`` 
+00016310: 776f 756c 6420 6265 2074 6865 2073 616d  would be the sam
+00016320: 6520 6173 2074 6865 2067 7261 6469 656e  e as the gradien
+00016330: 7420 6f66 2074 6869 730a 2020 2020 7065  t of this.    pe
+00016340: 7274 7572 6261 7469 6f6e 2076 6172 6961  rturbation varia
+00016350: 626c 652e 2054 6865 7265 666f 7265 2c20  ble. Therefore, 
+00016360: 6966 2079 6f75 2064 6566 696e 6520 796f  if you define yo
+00016370: 7572 206c 6f73 7320 6675 6e63 7469 6f6e  ur loss function
+00016380: 2077 6974 680a 2020 2020 626f 7468 2070   with.    both p
+00016390: 6172 616d 7320 616e 6420 7065 7274 7572  arams and pertur
+000163a0: 6261 7469 6f6e 7320 6173 2073 7461 6e64  bations as stand
+000163b0: 616c 6f6e 6520 6172 6775 6d65 6e74 732c  alone arguments,
+000163c0: 2079 6f75 2063 616e 2067 6574 2074 6865   you can get the
+000163d0: 0a20 2020 2069 6e74 6572 6d65 6469 6174  .    intermediat
+000163e0: 6520 6772 6164 6965 6e74 7320 6f66 2060  e gradients of `
+000163f0: 6076 616c 7565 6060 2062 7920 7275 6e6e  `value`` by runn
+00016400: 696e 6720 6060 6a61 782e 6772 6164 6060  ing ``jax.grad``
+00016410: 206f 6e20 7468 6520 7065 7274 7572 6261   on the perturba
+00016420: 7469 6f6e 0a20 2020 2061 7267 756d 656e  tion.    argumen
+00016430: 742e 0a0a 2020 2020 2e2e 206e 6f74 653a  t...    .. note:
+00016440: 3a0a 2020 2020 2020 5468 6973 2069 7320  :.      This is 
+00016450: 616e 2065 7870 6572 696d 656e 7461 6c20  an experimental 
+00016460: 4150 4920 616e 6420 6d61 7920 6265 2074  API and may be t
+00016470: 7765 616b 6564 206c 6174 6572 2066 6f72  weaked later for
+00016480: 2062 6574 7465 720a 2020 2020 2020 7065   better.      pe
+00016490: 7266 6f72 6d61 6e63 6520 616e 6420 7573  rformance and us
+000164a0: 6162 696c 6974 792e 0a20 2020 2020 2041  ability..      A
+000164b0: 7420 6974 7320 6375 7272 656e 7420 7374  t its current st
+000164c0: 6167 652c 2069 7420 6372 6561 7465 7320  age, it creates 
+000164d0: 6578 7472 6120 6475 6d6d 7920 7661 7269  extra dummy vari
+000164e0: 6162 6c65 7320 7468 6174 206f 6363 7570  ables that occup
+000164f0: 6965 7320 6578 7472 610a 2020 2020 2020  ies extra.      
+00016500: 6d65 6d6f 7279 2073 7061 6365 2e20 5573  memory space. Us
+00016510: 6520 6974 206f 6e6c 7920 746f 2064 6562  e it only to deb
+00016520: 7567 2067 7261 6469 656e 7473 2069 6e20  ug gradients in 
+00016530: 7472 6169 6e69 6e67 2e0a 0a20 2020 2045  training...    E
+00016540: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
+00016550: 3e3e 3e20 636c 6173 7320 466f 6f28 6e6e  >>> class Foo(nn
+00016560: 2e4d 6f64 756c 6529 3a0a 2020 2020 2020  .Module):.      
+00016570: 2e2e 2e20 2020 406e 6e2e 636f 6d70 6163  ...   @nn.compac
+00016580: 740a 2020 2020 2020 2e2e 2e20 2020 6465  t.      ...   de
+00016590: 6620 5f5f 6361 6c6c 5f5f 2873 656c 662c  f __call__(self,
+000165a0: 2078 293a 0a20 2020 2020 202e 2e2e 2020   x):.      ...  
+000165b0: 2020 2078 203d 206e 6e2e 4465 6e73 6528     x = nn.Dense(
+000165c0: 3329 2878 290a 2020 2020 2020 2e2e 2e20  3)(x).      ... 
+000165d0: 2020 2020 7820 3d20 7365 6c66 2e70 6572      x = self.per
+000165e0: 7475 7262 2827 6465 6e73 6533 272c 2078  turb('dense3', x
+000165f0: 290a 2020 2020 2020 2e2e 2e20 2020 2020  ).      ...     
+00016600: 7265 7475 726e 206e 6e2e 4465 6e73 6528  return nn.Dense(
+00016610: 3229 2878 290a 0a20 2020 2020 203e 3e3e  2)(x)..      >>>
+00016620: 2064 6566 206c 6f73 7328 7661 7269 6162   def loss(variab
+00016630: 6c65 732c 2069 6e70 7574 732c 2074 6172  les, inputs, tar
+00016640: 6765 7473 293a 0a20 2020 2020 202e 2e2e  gets):.      ...
+00016650: 2020 2070 7265 6473 203d 206d 6f64 656c     preds = model
+00016660: 2e61 7070 6c79 2876 6172 6961 626c 6573  .apply(variables
+00016670: 2c20 696e 7075 7473 290a 2020 2020 2020  , inputs).      
+00016680: 2e2e 2e20 2020 7265 7475 726e 206a 6e70  ...   return jnp
+00016690: 2e73 7175 6172 6528 7072 6564 7320 2d20  .square(preds - 
+000166a0: 7461 7267 6574 7329 2e6d 6561 6e28 290a  targets).mean().
+000166b0: 0a20 2020 2020 203e 3e3e 2078 203d 206a  .      >>> x = j
+000166c0: 6e70 2e6f 6e65 7328 2832 2c20 3929 290a  np.ones((2, 9)).
+000166d0: 2020 2020 2020 3e3e 3e20 7920 3d20 6a6e        >>> y = jn
+000166e0: 702e 6f6e 6573 2828 322c 2032 2929 0a20  p.ones((2, 2)). 
+000166f0: 2020 2020 203e 3e3e 206d 6f64 656c 203d       >>> model =
+00016700: 2046 6f6f 2829 0a20 2020 2020 203e 3e3e   Foo().      >>>
+00016710: 2076 6172 6961 626c 6573 203d 206d 6f64   variables = mod
+00016720: 656c 2e69 6e69 7428 6a61 782e 7261 6e64  el.init(jax.rand
+00016730: 6f6d 2e6b 6579 2830 292c 2078 290a 2020  om.key(0), x).  
+00016740: 2020 2020 3e3e 3e20 696e 746d 5f67 7261      >>> intm_gra
+00016750: 6473 203d 206a 6178 2e67 7261 6428 6c6f  ds = jax.grad(lo
+00016760: 7373 2c20 6172 676e 756d 733d 3029 2876  ss, argnums=0)(v
+00016770: 6172 6961 626c 6573 2c20 782c 2079 290a  ariables, x, y).
+00016780: 2020 2020 2020 3e3e 3e20 7072 696e 7428        >>> print(
+00016790: 696e 746d 5f67 7261 6473 5b27 7065 7274  intm_grads['pert
+000167a0: 7572 6261 7469 6f6e 7327 5d5b 2764 656e  urbations']['den
+000167b0: 7365 3327 5d29 0a20 2020 2020 205b 5b2d  se3']).      [[-
+000167c0: 312e 3435 3639 3234 2020 202d 302e 3434  1.456924   -0.44
+000167d0: 3333 3235 3337 2020 302e 3032 3432 3238  332537  0.024228
+000167e0: 3437 5d0a 2020 2020 2020 205b 2d31 2e34  47].       [-1.4
+000167f0: 3536 3932 3420 2020 2d30 2e34 3433 3332  56924   -0.44332
+00016800: 3533 3720 2030 2e30 3234 3232 3834 375d  537  0.02422847]
+00016810: 5d0a 0a20 2020 2049 6620 7065 7274 7572  ]..    If pertur
+00016820: 6261 7469 6f6e 7320 6172 6520 6e6f 7420  bations are not 
+00016830: 7061 7373 6564 2074 6f20 6060 6170 706c  passed to ``appl
+00016840: 7960 602c 2060 6070 6572 7475 7262 6060  y``, ``perturb``
+00016850: 2062 6568 6176 6573 206c 696b 6520 6120   behaves like a 
+00016860: 6e6f 2d6f 700a 2020 2020 736f 2079 6f75  no-op.    so you
+00016870: 2063 616e 2065 6173 696c 7920 6469 7361   can easily disa
+00016880: 626c 6520 7468 6520 6265 6861 7669 6f72  ble the behavior
+00016890: 2077 6865 6e20 6e6f 7420 6e65 6564 6564   when not needed
+000168a0: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 6d6f  ::..      >>> mo
+000168b0: 6465 6c2e 6170 706c 7928 7661 7269 6162  del.apply(variab
+000168c0: 6c65 732c 2078 2920 2320 776f 726b 7320  les, x) # works 
+000168d0: 6173 2065 7870 6563 7465 640a 2020 2020  as expected.    
+000168e0: 2020 4172 7261 7928 5b5b 2d31 2e30 3938    Array([[-1.098
+000168f0: 3031 3238 202c 202d 302e 3637 3936 3137  0128 , -0.679617
+00016900: 3335 5d2c 0a20 2020 2020 2020 2020 2020  35],.           
+00016910: 2020 5b2d 312e 3039 3830 3132 3820 2c20    [-1.0980128 , 
+00016920: 2d30 2e36 3739 3631 3733 355d 5d2c 2064  -0.67961735]], d
+00016930: 7479 7065 3d66 6c6f 6174 3332 290a 2020  type=float32).  
+00016940: 2020 2020 3e3e 3e20 6d6f 6465 6c2e 6170      >>> model.ap
+00016950: 706c 7928 7b27 7061 7261 6d73 273a 2076  ply({'params': v
+00016960: 6172 6961 626c 6573 5b27 7061 7261 6d73  ariables['params
+00016970: 275d 7d2c 2078 2920 2320 6265 6861 7665  ']}, x) # behave
+00016980: 7320 6c69 6b65 2061 206e 6f2d 6f70 0a20  s like a no-op. 
+00016990: 2020 2020 2041 7272 6179 285b 5b2d 312e       Array([[-1.
+000169a0: 3039 3830 3132 3820 2c20 2d30 2e36 3739  0980128 , -0.679
+000169b0: 3631 3733 355d 2c0a 2020 2020 2020 2020  61735],.        
+000169c0: 2020 2020 205b 2d31 2e30 3938 3031 3238       [-1.0980128
+000169d0: 202c 202d 302e 3637 3936 3137 3335 5d5d   , -0.67961735]]
+000169e0: 2c20 6474 7970 653d 666c 6f61 7433 3229  , dtype=float32)
+000169f0: 0a20 2020 2020 203e 3e3e 2069 6e74 6d5f  .      >>> intm_
+00016a00: 6772 6164 7320 3d20 6a61 782e 6772 6164  grads = jax.grad
+00016a10: 286c 6f73 732c 2061 7267 6e75 6d73 3d30  (loss, argnums=0
+00016a20: 2928 7b27 7061 7261 6d73 273a 2076 6172  )({'params': var
+00016a30: 6961 626c 6573 5b27 7061 7261 6d73 275d  iables['params']
+00016a40: 7d2c 2078 2c20 7929 0a20 2020 2020 203e  }, x, y).      >
+00016a50: 3e3e 2027 7065 7274 7572 6261 7469 6f6e  >> 'perturbation
+00016a60: 7327 206e 6f74 2069 6e20 696e 746d 5f67  s' not in intm_g
+00016a70: 7261 6473 0a20 2020 2020 2054 7275 650a  rads.      True.
+00016a80: 2020 2020 2222 220a 2020 2020 6966 2073      """.    if s
+00016a90: 656c 662e 7363 6f70 6520 6973 204e 6f6e  elf.scope is Non
+00016aa0: 653a 0a20 2020 2020 2072 6169 7365 2056  e:.      raise V
+00016ab0: 616c 7565 4572 726f 7228 2243 616e 2774  alueError("Can't
+00016ac0: 2073 746f 7265 2076 6172 6961 626c 6573   store variables
+00016ad0: 206f 6e20 756e 626f 756e 6420 6d6f 6475   on unbound modu
+00016ae0: 6c65 7322 290a 0a20 2020 2069 6620 7365  les")..    if se
+00016af0: 6c66 2e69 735f 6d75 7461 626c 655f 636f  lf.is_mutable_co
+00016b00: 6c6c 6563 7469 6f6e 2863 6f6c 6c65 6374  llection(collect
+00016b10: 696f 6e29 3a0a 2020 2020 2020 6966 206e  ion):.      if n
+00016b20: 6f74 2073 656c 662e 7363 6f70 652e 6861  ot self.scope.ha
+00016b30: 735f 7661 7269 6162 6c65 2863 6f6c 6c65  s_variable(colle
+00016b40: 6374 696f 6e2c 206e 616d 6529 3a0a 2020  ction, name):.  
+00016b50: 2020 2020 2020 7365 6c66 2e73 636f 7065        self.scope
+00016b60: 2e72 6573 6572 7665 286e 616d 652c 2063  .reserve(name, c
+00016b70: 6f6c 6c65 6374 696f 6e29 0a20 2020 2020  ollection).     
+00016b80: 2020 2073 656c 662e 5f73 7461 7465 2e63     self._state.c
+00016b90: 6869 6c64 7265 6e5b 6e61 6d65 5d20 3d20  hildren[name] = 
+00016ba0: 636f 6c6c 6563 7469 6f6e 0a20 2020 2020  collection.     
+00016bb0: 2020 2073 656c 662e 7363 6f70 652e 7075     self.scope.pu
+00016bc0: 745f 7661 7269 6162 6c65 2863 6f6c 6c65  t_variable(colle
+00016bd0: 6374 696f 6e2c 206e 616d 652c 206a 6e70  ction, name, jnp
+00016be0: 2e7a 6572 6f73 5f6c 696b 6528 7661 6c75  .zeros_like(valu
+00016bf0: 6529 2920 2023 2074 7970 653a 2069 676e  e))  # type: ign
+00016c00: 6f72 650a 0a20 2020 2069 6620 636f 6c6c  ore..    if coll
+00016c10: 6563 7469 6f6e 2069 6e20 7365 6c66 2e73  ection in self.s
+00016c20: 636f 7065 2e72 6f6f 742e 5f76 6172 6961  cope.root._varia
+00016c30: 626c 6573 3a0a 2020 2020 2020 6966 2073  bles:.      if s
+00016c40: 656c 662e 7363 6f70 652e 6861 735f 7661  elf.scope.has_va
+00016c50: 7269 6162 6c65 2863 6f6c 6c65 6374 696f  riable(collectio
+00016c60: 6e2c 206e 616d 6529 3a0a 2020 2020 2020  n, name):.      
+00016c70: 2020 7661 6c75 6520 2b3d 2073 656c 662e    value += self.
+00016c80: 7363 6f70 652e 6765 745f 7661 7269 6162  scope.get_variab
+00016c90: 6c65 2863 6f6c 6c65 6374 696f 6e2c 206e  le(collection, n
+00016ca0: 616d 6529 2020 2320 7479 7065 3a20 6967  ame)  # type: ig
+00016cb0: 6e6f 7265 0a20 2020 2020 2065 6c73 653a  nore.      else:
+00016cc0: 0a20 2020 2020 2020 2072 6169 7365 2056  .        raise V
+00016cd0: 616c 7565 4572 726f 7228 6622 5065 7274  alueError(f"Pert
+00016ce0: 7572 6261 7469 6f6e 2063 6f6c 6c65 6374  urbation collect
+00016cf0: 696f 6e20 7b63 6f6c 6c65 6374 696f 6e7d  ion {collection}
+00016d00: 2070 7265 7365 6e74 2c20 6275 7420 220a   present, but ".
+00016d10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00016d20: 2020 2020 2020 2020 2066 226d 6973 7369           f"missi
+00016d30: 6e67 2070 6572 7475 7262 6174 696f 6e20  ng perturbation 
+00016d40: 7661 7269 6162 6c65 207b 6e61 6d65 7d22  variable {name}"
+00016d50: 290a 0a20 2020 2072 6574 7572 6e20 7661  )..    return va
+00016d60: 6c75 650a 0a20 2064 6566 2074 6162 756c  lue..  def tabul
+00016d70: 6174 6528 0a20 2020 2073 656c 662c 0a20  ate(.    self,. 
+00016d80: 2020 2072 6e67 733a 2055 6e69 6f6e 5b50     rngs: Union[P
+00016d90: 524e 474b 6579 2c20 524e 4753 6571 7565  RNGKey, RNGSeque
+00016da0: 6e63 6573 5d2c 0a20 2020 202a 6172 6773  nces],.    *args
+00016db0: 2c0a 2020 2020 6465 7074 683a 204f 7074  ,.    depth: Opt
+00016dc0: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non
+00016dd0: 652c 0a20 2020 2073 686f 775f 7265 7065  e,.    show_repe
+00016de0: 6174 6564 3a20 626f 6f6c 203d 2046 616c  ated: bool = Fal
+00016df0: 7365 2c0a 2020 2020 6d75 7461 626c 653a  se,.    mutable:
+00016e00: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
+00016e10: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
+00016e20: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
+00016e30: 2020 2063 6f6e 736f 6c65 5f6b 7761 7267     console_kwarg
+00016e40: 733a 204f 7074 696f 6e61 6c5b 4d61 7070  s: Optional[Mapp
+00016e50: 696e 675b 7374 722c 2041 6e79 5d5d 203d  ing[str, Any]] =
+00016e60: 204e 6f6e 652c 0a20 2020 2074 6162 6c65   None,.    table
+00016e70: 5f6b 7761 7267 733a 204d 6170 7069 6e67  _kwargs: Mapping
+00016e80: 5b73 7472 2c20 416e 795d 203d 204d 6170  [str, Any] = Map
+00016e90: 7069 6e67 5072 6f78 7954 7970 6528 7b7d  pingProxyType({}
+00016ea0: 292c 0a20 2020 2063 6f6c 756d 6e5f 6b77  ),.    column_kw
+00016eb0: 6172 6773 3a20 4d61 7070 696e 675b 7374  args: Mapping[st
+00016ec0: 722c 2041 6e79 5d20 3d20 4d61 7070 696e  r, Any] = Mappin
+00016ed0: 6750 726f 7879 5479 7065 287b 7d29 2c0a  gProxyType({}),.
+00016ee0: 2020 2020 636f 6d70 7574 655f 666c 6f70      compute_flop
+00016ef0: 733a 2062 6f6f 6c20 3d20 4661 6c73 652c  s: bool = False,
+00016f00: 0a20 2020 2063 6f6d 7075 7465 5f76 6a70  .    compute_vjp
+00016f10: 5f66 6c6f 7073 3a20 626f 6f6c 203d 2046  _flops: bool = F
+00016f20: 616c 7365 2c0a 2020 2020 2a2a 6b77 6172  alse,.    **kwar
+00016f30: 6773 2c0a 2020 2920 2d3e 2073 7472 3a0a  gs,.  ) -> str:.
+00016f40: 2020 2020 2222 2243 7265 6174 6573 2061      """Creates a
+00016f50: 2073 756d 6d61 7279 206f 6620 7468 6520   summary of the 
+00016f60: 4d6f 6475 6c65 2072 6570 7265 7365 6e74  Module represent
+00016f70: 6564 2061 7320 6120 7461 626c 652e 0a0a  ed as a table...
+00016f80: 2020 2020 5468 6973 206d 6574 686f 6420      This method 
+00016f90: 6861 7320 7468 6520 7361 6d65 2073 6967  has the same sig
+00016fa0: 6e61 7475 7265 2061 6e64 2069 6e74 6572  nature and inter
+00016fb0: 6e61 6c6c 7920 6361 6c6c 7320 6060 4d6f  nally calls ``Mo
+00016fc0: 6475 6c65 2e69 6e69 7460 602c 0a20 2020  dule.init``,.   
+00016fd0: 2062 7574 2069 6e73 7465 6164 206f 6620   but instead of 
+00016fe0: 7265 7475 726e 696e 6720 7468 6520 7661  returning the va
+00016ff0: 7269 6162 6c65 732c 2069 7420 7265 7475  riables, it retu
+00017000: 726e 7320 7468 6520 7374 7269 6e67 2073  rns the string s
+00017010: 756d 6d61 7269 7a69 6e67 0a20 2020 2074  ummarizing.    t
+00017020: 6865 204d 6f64 756c 6520 696e 2061 2074  he Module in a t
+00017030: 6162 6c65 2e20 6060 7461 6275 6c61 7465  able. ``tabulate
+00017040: 6060 2075 7365 7320 6060 6a61 782e 6576  `` uses ``jax.ev
+00017050: 616c 5f73 6861 7065 6060 2074 6f20 7275  al_shape`` to ru
+00017060: 6e20 7468 6520 666f 7277 6172 640a 2020  n the forward.  
+00017070: 2020 636f 6d70 7574 6174 696f 6e20 7769    computation wi
+00017080: 7468 6f75 7420 636f 6e73 756d 696e 6720  thout consuming 
+00017090: 616e 7920 464c 4f50 7320 6f72 2061 6c6c  any FLOPs or all
+000170a0: 6f63 6174 696e 6720 6d65 6d6f 7279 2e0a  ocating memory..
+000170b0: 0a20 2020 2041 6464 6974 696f 6e61 6c20  .    Additional 
+000170c0: 6172 6775 6d65 6e74 7320 6361 6e20 6265  arguments can be
+000170d0: 2070 6173 7365 6420 696e 746f 2074 6865   passed into the
+000170e0: 2060 6063 6f6e 736f 6c65 5f6b 7761 7267   ``console_kwarg
+000170f0: 7360 6020 6172 6775 6d65 6e74 2c20 666f  s`` argument, fo
+00017100: 720a 2020 2020 6578 616d 706c 652c 2060  r.    example, `
+00017110: 607b 2777 6964 7468 273a 2031 3230 7d60  `{'width': 120}`
+00017120: 602e 2046 6f72 2061 2066 756c 6c20 6c69  `. For a full li
+00017130: 7374 206f 6620 6060 636f 6e73 6f6c 655f  st of ``console_
+00017140: 6b77 6172 6773 6060 2061 7267 756d 656e  kwargs`` argumen
+00017150: 7473 2c0a 2020 2020 7365 653a 0a20 2020  ts,.    see:.   
+00017160: 2068 7474 7073 3a2f 2f72 6963 682e 7265   https://rich.re
+00017170: 6164 7468 6564 6f63 732e 696f 2f65 6e2f  adthedocs.io/en/
+00017180: 7374 6162 6c65 2f72 6566 6572 656e 6365  stable/reference
+00017190: 2f63 6f6e 736f 6c65 2e68 746d 6c23 7269  /console.html#ri
+000171a0: 6368 2e63 6f6e 736f 6c65 2e43 6f6e 736f  ch.console.Conso
+000171b0: 6c65 0a0a 2020 2020 4578 616d 706c 653a  le..    Example:
+000171c0: 3a0a 0a20 2020 2020 203e 3e3e 2069 6d70  :..      >>> imp
+000171d0: 6f72 7420 666c 6178 2e6c 696e 656e 2061  ort flax.linen a
+000171e0: 7320 6e6e 0a20 2020 2020 203e 3e3e 2069  s nn.      >>> i
+000171f0: 6d70 6f72 7420 6a61 782c 206a 6178 2e6e  mport jax, jax.n
+00017200: 756d 7079 2061 7320 6a6e 700a 0a20 2020  umpy as jnp..   
+00017210: 2020 203e 3e3e 2063 6c61 7373 2046 6f6f     >>> class Foo
+00017220: 286e 6e2e 4d6f 6475 6c65 293a 0a20 2020  (nn.Module):.   
+00017230: 2020 202e 2e2e 2020 2040 6e6e 2e63 6f6d     ...   @nn.com
+00017240: 7061 6374 0a20 2020 2020 202e 2e2e 2020  pact.      ...  
+00017250: 2064 6566 205f 5f63 616c 6c5f 5f28 7365   def __call__(se
+00017260: 6c66 2c20 7829 3a0a 2020 2020 2020 2e2e  lf, x):.      ..
+00017270: 2e20 2020 2020 6820 3d20 6e6e 2e44 656e  .     h = nn.Den
+00017280: 7365 2834 2928 7829 0a20 2020 2020 202e  se(4)(x).      .
+00017290: 2e2e 2020 2020 2072 6574 7572 6e20 6e6e  ..     return nn
+000172a0: 2e44 656e 7365 2832 2928 6829 0a0a 2020  .Dense(2)(h)..  
+000172b0: 2020 2020 3e3e 3e20 7820 3d20 6a6e 702e      >>> x = jnp.
+000172c0: 6f6e 6573 2828 3136 2c20 3929 290a 0a20  ones((16, 9)).. 
+000172d0: 2020 2020 203e 3e3e 2023 2070 7269 6e74       >>> # print
+000172e0: 2846 6f6f 2829 2e74 6162 756c 6174 6528  (Foo().tabulate(
+000172f0: 0a20 2020 2020 203e 3e3e 2023 2020 2020  .      >>> #    
+00017300: 206a 6178 2e72 616e 646f 6d2e 6b65 7928   jax.random.key(
+00017310: 3029 2c20 782c 2063 6f6d 7075 7465 5f66  0), x, compute_f
+00017320: 6c6f 7073 3d54 7275 652c 2063 6f6d 7075  lops=True, compu
+00017330: 7465 5f76 6a70 5f66 6c6f 7073 3d54 7275  te_vjp_flops=Tru
+00017340: 6529 290a 0a20 2020 2054 6869 7320 6769  e))..    This gi
+00017350: 7665 7320 7468 6520 666f 6c6c 6f77 696e  ves the followin
+00017360: 6720 6f75 7470 7574 3a3a 0a0a 2020 2020  g output::..    
+00017370: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017380: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017390: 2020 2020 2020 2020 466f 6f20 5375 6d6d          Foo Summ
+000173a0: 6172 790a 2020 2020 2020 e294 8fe2 9481  ary.      ......
+000173b0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000173c0: 9481 e294 81e2 9481 e294 b3e2 9481 e294  ................
+000173d0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000173e0: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
+000173f0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017400: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017410: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
+00017420: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017430: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017440: e294 81e2 94b3 e294 81e2 9481 e294 81e2  ................
+00017450: 9481 e294 81e2 9481 e294 81e2 94b3 e294  ................
+00017460: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017470: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017480: 94b3 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017490: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000174a0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+000174b0: 9481 e294 81e2 9493 0a20 2020 2020 20e2  .........      .
+000174c0: 9483 2070 6174 6820 2020 20e2 9483 206d  .. path    ... m
+000174d0: 6f64 756c 6520 e294 8320 696e 7075 7473  odule ... inputs
+000174e0: 2020 2020 2020 2020 e294 8320 6f75 7470          ... outp
+000174f0: 7574 7320 2020 2020 2020 e294 8320 666c  uts       ... fl
+00017500: 6f70 7320 e294 8320 766a 705f 666c 6f70  ops ... vjp_flop
+00017510: 7320 e294 8320 7061 7261 6d73 2020 2020  s ... params    
+00017520: 2020 2020 2020 e294 830a 2020 2020 2020        ....      
+00017530: e294 a1e2 9481 e294 81e2 9481 e294 81e2  ................
+00017540: 9481 e294 81e2 9481 e294 81e2 9481 e295  ................
+00017550: 87e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017560: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
+00017570: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+00017580: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017590: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
+000175a0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000175b0: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+000175c0: e294 81e2 9481 e294 81e2 9587 e294 81e2  ................
+000175d0: 9481 e294 81e2 9481 e294 81e2 9481 e294  ................
+000175e0: 81e2 9587 e294 81e2 9481 e294 81e2 9481  ................
+000175f0: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017600: 9481 e294 81e2 9587 e294 81e2 9481 e294  ................
+00017610: 81e2 9481 e294 81e2 9481 e294 81e2 9481  ................
+00017620: e294 81e2 9481 e294 81e2 9481 e294 81e2  ................
+00017630: 9481 e294 81e2 9481 e294 81e2 94a9 0a20  ............... 
+00017640: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017650: 20e2 9482 2046 6f6f 2020 2020 e294 8220   ... Foo    ... 
+00017660: 666c 6f61 7433 325b 3136 2c39 5d20 e294  float32[16,9] ..
+00017670: 8220 666c 6f61 7433 325b 3136 2c32 5d20  . float32[16,2] 
+00017680: e294 8220 3135 3034 2020 e294 8220 3434  ... 1504  ... 44
+00017690: 3630 2020 2020 2020 e294 8220 2020 2020  60      ...     
+000176a0: 2020 2020 2020 2020 2020 2020 e294 820a              ....
+000176b0: 2020 2020 2020 e294 9ce2 9480 e294 80e2        ..........
+000176c0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000176d0: 80e2 9480 e294 bce2 9480 e294 80e2 9480  ................
+000176e0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000176f0: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
+00017700: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017710: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017720: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
+00017730: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017740: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017750: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
+00017760: 80e2 9480 e294 80e2 94bc e294 80e2 9480  ................
 00017770: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017780: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017780: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
 00017790: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000177a0: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
+000177a0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
 000177b0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000177c0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-000177d0: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
-000177e0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-000177f0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017800: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
-00017810: 9480 e294 80e2 9480 e294 bce2 9480 e294  ................
-00017820: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017830: e294 80e2 9480 e294 80e2 9480 e294 bce2  ................
-00017840: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017850: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017860: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017870: 9480 e294 a40a 2020 2020 2020 e294 8220  ......      ... 
-00017880: 4465 6e73 655f 3120 e294 8220 4465 6e73  Dense_1 ... Dens
-00017890: 6520 20e2 9482 2066 6c6f 6174 3332 5b31  e  ... float32[1
-000178a0: 362c 345d 20e2 9482 2066 6c6f 6174 3332  6,4] ... float32
-000178b0: 5b31 362c 325d 20e2 9482 2032 3838 2020  [16,2] ... 288  
-000178c0: 20e2 9482 2038 3430 2020 2020 2020 20e2   ... 840       .
-000178d0: 9482 2062 6961 733a 2020 2020 2020 2020  .. bias:        
-000178e0: 2020 20e2 9482 0a20 2020 2020 20e2 9482     ....      ...
-000178f0: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-00017900: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-00017910: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00017920: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00017930: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
-00017940: e294 8220 666c 6f61 7433 325b 325d 2020  ... float32[2]  
-00017950: 2020 2020 e294 820a 2020 2020 2020 e294      ....      ..
-00017960: 8220 2020 2020 2020 2020 e294 8220 2020  .         ...   
-00017970: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00017980: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00017990: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-000179a0: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-000179b0: 20e2 9482 206b 6572 6e65 6c3a 2020 2020   ... kernel:    
-000179c0: 2020 2020 20e2 9482 0a20 2020 2020 20e2       ....      .
-000179d0: 9482 2020 2020 2020 2020 20e2 9482 2020  ..         ...  
-000179e0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-000179f0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00017a00: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-00017a10: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
-00017a20: 2020 e294 8220 666c 6f61 7433 325b 342c    ... float32[4,
-00017a30: 325d 2020 2020 e294 820a 2020 2020 2020  2]    ....      
-00017a40: e294 8220 2020 2020 2020 2020 e294 8220  ...         ... 
-00017a50: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
-00017a60: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
-00017a70: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
-00017a80: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
-00017a90: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
-00017aa0: 2020 2020 2020 20e2 9482 0a20 2020 2020         ....     
-00017ab0: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
-00017ac0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
-00017ad0: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
-00017ae0: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
-00017af0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
-00017b00: 2020 2020 e294 8220 3130 2028 3430 2042      ... 10 (40 B
-00017b10: 2920 2020 2020 2020 e294 820a 2020 2020  )       ....    
-00017b20: 2020 e294 9ce2 9480 e294 80e2 9480 e294    ..............
+000177c0: 80e2 94a4 0a20 2020 2020 20e2 9482 2044  .....      ... D
+000177d0: 656e 7365 5f30 20e2 9482 2044 656e 7365  ense_0 ... Dense
+000177e0: 2020 e294 8220 666c 6f61 7433 325b 3136    ... float32[16
+000177f0: 2c39 5d20 e294 8220 666c 6f61 7433 325b  ,9] ... float32[
+00017800: 3136 2c34 5d20 e294 8220 3132 3136 2020  16,4] ... 1216  
+00017810: e294 8220 3336 3230 2020 2020 2020 e294  ... 3620      ..
+00017820: 8220 6269 6173 3a20 2020 2020 2020 2020  . bias:         
+00017830: 2020 e294 820a 2020 2020 2020 e294 8220    ....      ... 
+00017840: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00017850: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00017860: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017870: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017880: 20e2 9482 2020 2020 2020 2020 2020 20e2   ...           .
+00017890: 9482 2066 6c6f 6174 3332 5b34 5d20 2020  .. float32[4]   
+000178a0: 2020 20e2 9482 0a20 2020 2020 20e2 9482     ....      ...
+000178b0: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+000178c0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+000178d0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+000178e0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+000178f0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
+00017900: e294 8220 6b65 726e 656c 3a20 2020 2020  ... kernel:     
+00017910: 2020 2020 e294 820a 2020 2020 2020 e294      ....      ..
+00017920: 8220 2020 2020 2020 2020 e294 8220 2020  .         ...   
+00017930: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017940: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017950: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+00017960: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00017970: 20e2 9482 2066 6c6f 6174 3332 5b39 2c34   ... float32[9,4
+00017980: 5d20 2020 20e2 9482 0a20 2020 2020 20e2  ]    ....      .
+00017990: 9482 2020 2020 2020 2020 20e2 9482 2020  ..         ...  
+000179a0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+000179b0: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+000179c0: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
+000179d0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+000179e0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
+000179f0: 2020 2020 2020 e294 820a 2020 2020 2020        ....      
+00017a00: e294 8220 2020 2020 2020 2020 e294 8220  ...         ... 
+00017a10: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017a20: 2020 2020 2020 2020 20e2 9482 2020 2020           ...    
+00017a30: 2020 2020 2020 2020 2020 20e2 9482 2020             ...  
+00017a40: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017a50: 2020 20e2 9482 2034 3020 2831 3630 2042     ... 40 (160 B
+00017a60: 2920 2020 2020 20e2 9482 0a20 2020 2020  )      ....     
+00017a70: 20e2 949c e294 80e2 9480 e294 80e2 9480   ...............
+00017a80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017a90: 94bc e294 80e2 9480 e294 80e2 9480 e294  ................
+00017aa0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
+00017ab0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017ac0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017ad0: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
+00017ae0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017af0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017b00: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
+00017b10: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017b20: 9480 e294 bce2 9480 e294 80e2 9480 e294  ................
 00017b30: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017b40: e294 bce2 9480 e294 80e2 9480 e294 80e2  ................
-00017b50: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
+00017b40: e294 80e2 9480 e294 bce2 9480 e294 80e2  ................
+00017b50: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
 00017b60: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017b70: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017b80: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
-00017b90: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017ba0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017bb0: 9480 e294 80e2 9480 e294 80e2 94bc e294  ................
-00017bc0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017bd0: e294 80e2 94bc e294 80e2 9480 e294 80e2  ................
-00017be0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017bf0: 80e2 9480 e294 80e2 94bc e294 80e2 9480  ................
-00017c00: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017c10: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017c20: 80e2 9480 e294 80e2 9480 e294 80e2 94a4  ................
-00017c30: 0a20 2020 2020 20e2 9482 2020 2020 2020  .      ...      
-00017c40: 2020 20e2 9482 2020 2020 2020 2020 e294     ...        ..
-00017c50: 8220 2020 2020 2020 2020 2020 2020 2020  .               
-00017c60: e294 8220 2020 2020 2020 2020 2020 2020  ...             
-00017c70: 2020 e294 8220 2020 2020 2020 e294 8220    ...       ... 
-00017c80: 2020 2020 546f 7461 6c20 e294 8220 3530      Total ... 50
-00017c90: 2028 3230 3020 4229 2020 2020 2020 e294   (200 B)      ..
-00017ca0: 820a 2020 2020 2020 e294 94e2 9480 e294  ..      ........
-00017cb0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017cc0: e294 80e2 9480 e294 b4e2 9480 e294 80e2  ................
-00017cd0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017ce0: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
-00017cf0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017d00: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017d10: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
-00017d20: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017d30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017d40: 80e2 94b4 e294 80e2 9480 e294 80e2 9480  ................
-00017d50: e294 80e2 9480 e294 80e2 94b4 e294 80e2  ................
-00017d60: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017d70: 80e2 9480 e294 80e2 9480 e294 80e2 94b4  ................
-00017d80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
-00017d90: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
-00017da0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
-00017db0: e294 80e2 9498 0a0a 2020 2020 2020 2020  ........        
-00017dc0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00017dd0: 2020 2020 2020 2020 2020 2020 546f 7461              Tota
-00017de0: 6c20 5061 7261 6d65 7465 7273 3a20 3530  l Parameters: 50
-00017df0: 2028 3230 3020 4229 0a0a 2020 2020 2a2a   (200 B)..    **
-00017e00: 4e6f 7465 2a2a 3a20 726f 7773 206f 7264  Note**: rows ord
-00017e10: 6572 2069 6e20 7468 6520 7461 626c 6520  er in the table 
-00017e20: 646f 6573 206e 6f74 2072 6570 7265 7365  does not represe
-00017e30: 6e74 2065 7865 6375 7469 6f6e 206f 7264  nt execution ord
-00017e40: 6572 2c0a 2020 2020 696e 7374 6561 6420  er,.    instead 
-00017e50: 6974 2061 6c69 676e 7320 7769 7468 2074  it aligns with t
-00017e60: 6865 206f 7264 6572 206f 6620 6b65 7973  he order of keys
-00017e70: 2069 6e20 6060 7661 7269 6162 6c65 7360   in ``variables`
-00017e80: 6020 7768 6963 6820 6172 6520 736f 7274  ` which are sort
-00017e90: 6564 0a20 2020 2061 6c70 6861 6265 7469  ed.    alphabeti
-00017ea0: 6361 6c6c 792e 0a0a 2020 2020 2a2a 4e6f  cally...    **No
-00017eb0: 7465 2a2a 3a20 6060 766a 705f 666c 6f70  te**: ``vjp_flop
-00017ec0: 7360 6020 7265 7475 726e 7320 6060 3060  s`` returns ``0`
-00017ed0: 6020 6966 2074 6865 206d 6f64 756c 6520  ` if the module 
-00017ee0: 6973 206e 6f74 2064 6966 6665 7265 6e74  is not different
-00017ef0: 6961 626c 652e 0a0a 2020 2020 4172 6773  iable...    Args
-00017f00: 3a0a 2020 2020 2020 726e 6773 3a20 5468  :.      rngs: Th
-00017f10: 6520 726e 6773 2066 6f72 2074 6865 2076  e rngs for the v
-00017f20: 6172 6961 626c 6520 636f 6c6c 6563 7469  ariable collecti
-00017f30: 6f6e 7320 6173 2070 6173 7365 6420 746f  ons as passed to
-00017f40: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
-00017f50: 2e0a 2020 2020 2020 2a61 7267 733a 2054  ..      *args: T
-00017f60: 6865 2061 7267 756d 656e 7473 2074 6f20  he arguments to 
-00017f70: 7468 6520 666f 7277 6172 6420 636f 6d70  the forward comp
-00017f80: 7574 6174 696f 6e2e 0a20 2020 2020 2064  utation..      d
-00017f90: 6570 7468 3a20 636f 6e74 726f 6c73 2068  epth: controls h
-00017fa0: 6f77 206d 616e 7920 7375 626d 6f64 756c  ow many submodul
-00017fb0: 6520 6465 6570 2074 6865 2073 756d 6d61  e deep the summa
-00017fc0: 7279 2063 616e 2067 6f2e 2042 7920 6465  ry can go. By de
-00017fd0: 6661 756c 742c 0a20 2020 2020 2020 2069  fault,.        i
-00017fe0: 7473 2060 604e 6f6e 6560 6020 7768 6963  ts ``None`` whic
-00017ff0: 6820 6d65 616e 7320 6e6f 206c 696d 6974  h means no limit
-00018000: 2e20 4966 2061 2073 7562 6d6f 6475 6c65  . If a submodule
-00018010: 2069 7320 6e6f 7420 7368 6f77 6e20 6265   is not shown be
-00018020: 6361 7573 6520 6f66 0a20 2020 2020 2020  cause of.       
-00018030: 2074 6865 2064 6570 7468 206c 696d 6974   the depth limit
-00018040: 2c20 6974 7320 7061 7261 6d65 7465 7220  , its parameter 
-00018050: 636f 756e 7420 616e 6420 6279 7465 7320  count and bytes 
-00018060: 7769 6c6c 2062 6520 6164 6465 6420 746f  will be added to
-00018070: 2074 6865 2072 6f77 0a20 2020 2020 2020   the row.       
-00018080: 206f 6620 6974 7320 6669 7273 7420 7368   of its first sh
-00018090: 6f77 6e20 616e 6365 7374 6f72 2073 7563  own ancestor suc
-000180a0: 6820 7468 6174 2074 6865 2073 756d 206f  h that the sum o
-000180b0: 6620 616c 6c20 726f 7773 2061 6c77 6179  f all rows alway
-000180c0: 7320 6164 6473 0a20 2020 2020 2020 2075  s adds.        u
-000180d0: 7020 746f 2074 6865 2074 6f74 616c 206e  p to the total n
-000180e0: 756d 6265 7220 6f66 2070 6172 616d 6574  umber of paramet
-000180f0: 6572 7320 6f66 2074 6865 204d 6f64 756c  ers of the Modul
-00018100: 652e 0a20 2020 2020 2073 686f 775f 7265  e..      show_re
-00018110: 7065 6174 6564 3a20 4966 2060 6054 7275  peated: If ``Tru
-00018120: 6560 602c 2072 6570 6561 7465 6420 6361  e``, repeated ca
-00018130: 6c6c 7320 746f 2074 6865 2073 616d 6520  lls to the same 
-00018140: 6d6f 6475 6c65 2077 696c 6c20 6265 2073  module will be s
-00018150: 686f 776e 0a20 2020 2020 2020 2069 6e20  hown.        in 
-00018160: 7468 6520 7461 626c 652c 206f 7468 6572  the table, other
-00018170: 7769 7365 206f 6e6c 7920 7468 6520 6669  wise only the fi
-00018180: 7273 7420 6361 6c6c 2077 696c 6c20 6265  rst call will be
-00018190: 2073 686f 776e 2e20 4465 6661 756c 7420   shown. Default 
-000181a0: 6973 0a20 2020 2020 2020 2060 6046 616c  is.        ``Fal
-000181b0: 7365 6060 2e0a 2020 2020 2020 6d75 7461  se``..      muta
-000181c0: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
-000181d0: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
-000181e0: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
-000181f0: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
-00018200: 6c64 2062 650a 2020 2020 2020 2020 7472  ld be.        tr
-00018210: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
-00018220: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
-00018230: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
-00018240: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
-00018250: 7260 603a 0a20 2020 2020 2020 2054 6865  r``:.        The
-00018260: 206e 616d 6520 6f66 2061 2073 696e 676c   name of a singl
-00018270: 6520 6d75 7461 626c 6520 636f 6c6c 6563  e mutable collec
-00018280: 7469 6f6e 2e20 6060 6c69 7374 6060 3a20  tion. ``list``: 
-00018290: 4120 6c69 7374 206f 6620 6e61 6d65 7320  A list of names 
-000182a0: 6f66 0a20 2020 2020 2020 206d 7574 6162  of.        mutab
-000182b0: 6c65 2063 6f6c 6c65 6374 696f 6e73 2e20  le collections. 
-000182c0: 4279 2064 6566 6175 6c74 2c20 616c 6c20  By default, all 
-000182d0: 636f 6c6c 6563 7469 6f6e 7320 6578 6365  collections exce
-000182e0: 7074 2027 696e 7465 726d 6564 6961 7465  pt 'intermediate
-000182f0: 7327 0a20 2020 2020 2020 2061 7265 206d  s'.        are m
-00018300: 7574 6162 6c65 2e0a 2020 2020 2020 636f  utable..      co
-00018310: 6e73 6f6c 655f 6b77 6172 6773 3a20 416e  nsole_kwargs: An
-00018320: 206f 7074 696f 6e61 6c20 6469 6374 696f   optional dictio
-00018330: 6e61 7279 2077 6974 6820 6164 6469 7469  nary with additi
-00018340: 6f6e 616c 206b 6579 776f 7264 2061 7267  onal keyword arg
-00018350: 756d 656e 7473 0a20 2020 2020 2020 2074  uments.        t
-00018360: 6861 7420 6172 6520 7061 7373 6564 2074  hat are passed t
-00018370: 6f20 6060 7269 6368 2e63 6f6e 736f 6c65  o ``rich.console
-00018380: 2e43 6f6e 736f 6c65 6060 2077 6865 6e20  .Console`` when 
-00018390: 7265 6e64 6572 696e 6720 7468 6520 7461  rendering the ta
-000183a0: 626c 652e 0a20 2020 2020 2020 2044 6566  ble..        Def
-000183b0: 6175 6c74 2061 7267 756d 656e 7473 2061  ault arguments a
-000183c0: 7265 2060 607b 2766 6f72 6365 5f74 6572  re ``{'force_ter
-000183d0: 6d69 6e61 6c27 3a20 5472 7565 2c20 2766  minal': True, 'f
-000183e0: 6f72 6365 5f6a 7570 7974 6572 273a 0a20  orce_jupyter':. 
-000183f0: 2020 2020 2020 2046 616c 7365 7d60 602e         False}``.
-00018400: 0a20 2020 2020 2074 6162 6c65 5f6b 7761  .      table_kwa
-00018410: 7267 733a 2041 6e20 6f70 7469 6f6e 616c  rgs: An optional
-00018420: 2064 6963 7469 6f6e 6172 7920 7769 7468   dictionary with
-00018430: 2061 6464 6974 696f 6e61 6c20 6b65 7977   additional keyw
-00018440: 6f72 6420 6172 6775 6d65 6e74 730a 2020  ord arguments.  
-00018450: 2020 2020 2020 7468 6174 2061 7265 2070        that are p
-00018460: 6173 7365 6420 746f 2060 6072 6963 682e  assed to ``rich.
-00018470: 7461 626c 652e 5461 626c 6560 6020 636f  table.Table`` co
-00018480: 6e73 7472 7563 746f 722e 0a20 2020 2020  nstructor..     
-00018490: 2063 6f6c 756d 6e5f 6b77 6172 6773 3a20   column_kwargs: 
-000184a0: 416e 206f 7074 696f 6e61 6c20 6469 6374  An optional dict
-000184b0: 696f 6e61 7279 2077 6974 6820 6164 6469  ionary with addi
-000184c0: 7469 6f6e 616c 206b 6579 776f 7264 2061  tional keyword a
-000184d0: 7267 756d 656e 7473 0a20 2020 2020 2020  rguments.       
-000184e0: 2074 6861 7420 6172 6520 7061 7373 6564   that are passed
-000184f0: 2074 6f20 6060 7269 6368 2e74 6162 6c65   to ``rich.table
-00018500: 2e54 6162 6c65 2e61 6464 5f63 6f6c 756d  .Table.add_colum
-00018510: 6e60 6020 7768 656e 2061 6464 696e 6720  n`` when adding 
-00018520: 636f 6c75 6d6e 7320 746f 0a20 2020 2020  columns to.     
-00018530: 2020 2074 6865 2074 6162 6c65 2e0a 2020     the table..  
-00018540: 2020 2020 636f 6d70 7574 655f 666c 6f70      compute_flop
-00018550: 733a 2077 6865 7468 6572 2074 6f20 696e  s: whether to in
-00018560: 636c 7564 6520 6120 6060 666c 6f70 7360  clude a ``flops`
-00018570: 6020 636f 6c75 6d6e 2069 6e20 7468 6520  ` column in the 
-00018580: 7461 626c 6520 6c69 7374 696e 670a 2020  table listing.  
-00018590: 2020 2020 2020 7468 6520 6573 7469 6d61        the estima
-000185a0: 7465 6420 464c 4f50 7320 636f 7374 206f  ted FLOPs cost o
-000185b0: 6620 6561 6368 206d 6f64 756c 6520 666f  f each module fo
-000185c0: 7277 6172 6420 7061 7373 2e20 446f 6573  rward pass. Does
-000185d0: 2069 6e63 7572 2061 6374 7561 6c0a 2020   incur actual.  
-000185e0: 2020 2020 2020 6f6e 2d64 6576 6963 6520        on-device 
-000185f0: 636f 6d70 7574 6174 696f 6e20 2f20 636f  computation / co
-00018600: 6d70 696c 6174 696f 6e20 2f20 6d65 6d6f  mpilation / memo
-00018610: 7279 2061 6c6c 6f63 6174 696f 6e2c 2062  ry allocation, b
-00018620: 7574 2073 7469 6c6c 0a20 2020 2020 2020  ut still.       
-00018630: 2069 6e74 726f 6475 6365 7320 6f76 6572   introduces over
-00018640: 6865 6164 2066 6f72 206c 6172 6765 206d  head for large m
-00018650: 6f64 756c 6573 2028 652e 672e 2065 7874  odules (e.g. ext
-00018660: 7261 2032 3020 7365 636f 6e64 7320 666f  ra 20 seconds fo
-00018670: 7220 610a 2020 2020 2020 2020 5374 6162  r a.        Stab
-00018680: 6c65 2044 6966 6675 7369 6f6e 2773 2055  le Diffusion's U
-00018690: 4e65 742c 2077 6865 7265 6173 206f 7468  Net, whereas oth
-000186a0: 6572 7769 7365 2074 6162 756c 6174 696f  erwise tabulatio
-000186b0: 6e20 776f 756c 6420 6669 6e69 7368 2069  n would finish i
-000186c0: 6e20 350a 2020 2020 2020 2020 7365 636f  n 5.        seco
-000186d0: 6e64 7329 2e0a 2020 2020 2020 636f 6d70  nds)..      comp
-000186e0: 7574 655f 766a 705f 666c 6f70 733a 2077  ute_vjp_flops: w
-000186f0: 6865 7468 6572 2074 6f20 696e 636c 7564  hether to includ
-00018700: 6520 6120 6060 766a 705f 666c 6f70 7360  e a ``vjp_flops`
-00018710: 6020 636f 6c75 6d6e 2069 6e20 7468 6520  ` column in the 
-00018720: 7461 626c 650a 2020 2020 2020 2020 6c69  table.        li
-00018730: 7374 696e 6720 7468 6520 6573 7469 6d61  sting the estima
-00018740: 7465 6420 464c 4f50 7320 636f 7374 206f  ted FLOPs cost o
-00018750: 6620 6561 6368 206d 6f64 756c 6520 6261  f each module ba
-00018760: 636b 7761 7264 2070 6173 732e 0a20 2020  ckward pass..   
-00018770: 2020 2020 2049 6e74 726f 6475 6365 7320       Introduces 
-00018780: 6120 636f 6d70 7574 6520 6f76 6572 6865  a compute overhe
-00018790: 6164 206f 6620 6162 6f75 7420 322d 3358  ad of about 2-3X
-000187a0: 206f 6620 6060 636f 6d70 7574 655f 666c   of ``compute_fl
-000187b0: 6f70 7360 602e 0a20 2020 2020 202a 2a6b  ops``..      **k
-000187c0: 7761 7267 733a 206b 6579 776f 7264 2061  wargs: keyword a
-000187d0: 7267 756d 656e 7473 2074 6f20 7061 7373  rguments to pass
-000187e0: 2074 6f20 7468 6520 666f 7277 6172 6420   to the forward 
-000187f0: 636f 6d70 7574 6174 696f 6e2e 0a0a 2020  computation...  
-00018800: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     
-00018810: 2041 2073 7472 696e 6720 7375 6d6d 6172   A string summar
-00018820: 697a 696e 6720 7468 6520 4d6f 6475 6c65  izing the Module
-00018830: 2e0a 2020 2020 2222 220a 2020 2020 6672  ..    """.    fr
-00018840: 6f6d 2066 6c61 782e 6c69 6e65 6e20 696d  om flax.linen im
-00018850: 706f 7274 2073 756d 6d61 7279 0a0a 2020  port summary..  
-00018860: 2020 7461 6275 6c61 7465 5f66 6e20 3d20    tabulate_fn = 
-00018870: 7375 6d6d 6172 792e 7461 6275 6c61 7465  summary.tabulate
-00018880: 280a 2020 2020 2020 7365 6c66 2c0a 2020  (.      self,.  
-00018890: 2020 2020 726e 6773 2c0a 2020 2020 2020      rngs,.      
-000188a0: 6465 7074 683d 6465 7074 682c 0a20 2020  depth=depth,.   
-000188b0: 2020 2073 686f 775f 7265 7065 6174 6564     show_repeated
-000188c0: 3d73 686f 775f 7265 7065 6174 6564 2c0a  =show_repeated,.
-000188d0: 2020 2020 2020 6d75 7461 626c 653d 6d75        mutable=mu
-000188e0: 7461 626c 652c 0a20 2020 2020 2063 6f6e  table,.      con
-000188f0: 736f 6c65 5f6b 7761 7267 733d 636f 6e73  sole_kwargs=cons
-00018900: 6f6c 655f 6b77 6172 6773 2c0a 2020 2020  ole_kwargs,.    
-00018910: 2020 7461 626c 655f 6b77 6172 6773 3d74    table_kwargs=t
-00018920: 6162 6c65 5f6b 7761 7267 732c 0a20 2020  able_kwargs,.   
-00018930: 2020 2063 6f6c 756d 6e5f 6b77 6172 6773     column_kwargs
-00018940: 3d63 6f6c 756d 6e5f 6b77 6172 6773 2c0a  =column_kwargs,.
-00018950: 2020 2020 2020 636f 6d70 7574 655f 666c        compute_fl
-00018960: 6f70 733d 636f 6d70 7574 655f 666c 6f70  ops=compute_flop
-00018970: 732c 0a20 2020 2020 2063 6f6d 7075 7465  s,.      compute
-00018980: 5f76 6a70 5f66 6c6f 7073 3d63 6f6d 7075  _vjp_flops=compu
-00018990: 7465 5f76 6a70 5f66 6c6f 7073 2c0a 2020  te_vjp_flops,.  
-000189a0: 2020 290a 2020 2020 7265 7475 726e 2074    ).    return t
-000189b0: 6162 756c 6174 655f 666e 282a 6172 6773  abulate_fn(*args
-000189c0: 2c20 2a2a 6b77 6172 6773 290a 0a20 2064  , **kwargs)..  d
-000189d0: 6566 206d 6f64 756c 655f 7061 7468 7328  ef module_paths(
-000189e0: 0a20 2020 2073 656c 662c 0a20 2020 2072  .    self,.    r
-000189f0: 6e67 733a 2055 6e69 6f6e 5b50 524e 474b  ngs: Union[PRNGK
-00018a00: 6579 2c20 524e 4753 6571 7565 6e63 6573  ey, RNGSequences
-00018a10: 5d2c 0a20 2020 202a 6172 6773 2c0a 2020  ],.    *args,.  
-00018a20: 2020 7368 6f77 5f72 6570 6561 7465 643a    show_repeated:
-00018a30: 2062 6f6f 6c20 3d20 4661 6c73 652c 0a20   bool = False,. 
-00018a40: 2020 206d 7574 6162 6c65 3a20 436f 6c6c     mutable: Coll
-00018a50: 6563 7469 6f6e 4669 6c74 6572 203d 2044  ectionFilter = D
-00018a60: 656e 794c 6973 7428 2769 6e74 6572 6d65  enyList('interme
-00018a70: 6469 6174 6573 2729 2c0a 2020 2020 2a2a  diates'),.    **
-00018a80: 6b77 6172 6773 2c0a 2020 2920 2d3e 2064  kwargs,.  ) -> d
-00018a90: 6963 745b 7374 722c 2027 4d6f 6475 6c65  ict[str, 'Module
-00018aa0: 275d 3a0a 2020 2020 2222 2252 6574 7572  ']:.    """Retur
-00018ab0: 6e73 2061 2064 6963 7469 6f6e 6172 7920  ns a dictionary 
-00018ac0: 6d61 7070 696e 6720 6d6f 6475 6c65 2070  mapping module p
-00018ad0: 6174 6873 2074 6f20 6d6f 6475 6c65 2069  aths to module i
-00018ae0: 6e73 7461 6e63 6573 2e0a 0a20 2020 2054  nstances...    T
-00018af0: 6869 7320 6d65 7468 6f64 2068 6173 2074  his method has t
-00018b00: 6865 2073 616d 6520 7369 676e 6174 7572  he same signatur
-00018b10: 6520 616e 6420 696e 7465 726e 616c 6c79  e and internally
-00018b20: 2063 616c 6c73 2060 604d 6f64 756c 652e   calls ``Module.
-00018b30: 696e 6974 6060 2c0a 2020 2020 6275 7420  init``,.    but 
-00018b40: 696e 7374 6561 6420 6f66 2072 6574 7572  instead of retur
-00018b50: 6e69 6e67 2074 6865 2076 6172 6961 626c  ning the variabl
-00018b60: 6573 2c20 6974 2072 6574 7572 6e73 2061  es, it returns a
-00018b70: 2064 6963 7469 6f6e 6172 7920 6d61 7070   dictionary mapp
-00018b80: 696e 670a 2020 2020 6d6f 6475 6c65 2070  ing.    module p
-00018b90: 6174 6873 2074 6f20 756e 626f 756e 6465  aths to unbounde
-00018ba0: 6420 636f 7069 6573 206f 6620 6d6f 6475  d copies of modu
-00018bb0: 6c65 2069 6e73 7461 6e63 6573 2074 6861  le instances tha
-00018bc0: 7420 7765 7265 2075 7365 640a 2020 2020  t were used.    
-00018bd0: 6174 2072 756e 7469 6d65 2e20 6060 6d6f  at runtime. ``mo
-00018be0: 6475 6c65 5f70 6174 6873 6060 2075 7365  dule_paths`` use
-00018bf0: 7320 6060 6a61 782e 6576 616c 5f73 6861  s ``jax.eval_sha
-00018c00: 7065 6060 2074 6f20 7275 6e20 7468 6520  pe`` to run the 
-00018c10: 666f 7277 6172 640a 2020 2020 636f 6d70  forward.    comp
-00018c20: 7574 6174 696f 6e20 7769 7468 6f75 7420  utation without 
-00018c30: 636f 6e73 756d 696e 6720 616e 7920 464c  consuming any FL
-00018c40: 4f50 7320 6f72 2061 6c6c 6f63 6174 696e  OPs or allocatin
-00018c50: 6720 6d65 6d6f 7279 2e0a 0a20 2020 2045  g memory...    E
-00018c60: 7861 6d70 6c65 3a3a 0a0a 2020 2020 2020  xample::..      
-00018c70: 3e3e 3e20 696d 706f 7274 2066 6c61 782e  >>> import flax.
-00018c80: 6c69 6e65 6e20 6173 206e 6e0a 2020 2020  linen as nn.    
-00018c90: 2020 3e3e 3e20 696d 706f 7274 206a 6178    >>> import jax
-00018ca0: 2c20 6a61 782e 6e75 6d70 7920 6173 206a  , jax.numpy as j
-00018cb0: 6e70 0a0a 2020 2020 2020 3e3e 3e20 636c  np..      >>> cl
-00018cc0: 6173 7320 466f 6f28 6e6e 2e4d 6f64 756c  ass Foo(nn.Modul
-00018cd0: 6529 3a0a 2020 2020 2020 2e2e 2e20 2020  e):.      ...   
-00018ce0: 406e 6e2e 636f 6d70 6163 740a 2020 2020  @nn.compact.    
-00018cf0: 2020 2e2e 2e20 2020 6465 6620 5f5f 6361    ...   def __ca
-00018d00: 6c6c 5f5f 2873 656c 662c 2078 293a 0a20  ll__(self, x):. 
-00018d10: 2020 2020 202e 2e2e 2020 2020 2068 203d       ...     h =
-00018d20: 206e 6e2e 4465 6e73 6528 3429 2878 290a   nn.Dense(4)(x).
-00018d30: 2020 2020 2020 2e2e 2e20 2020 2020 7265        ...     re
-00018d40: 7475 726e 206e 6e2e 4465 6e73 6528 3229  turn nn.Dense(2)
-00018d50: 2868 290a 0a20 2020 2020 203e 3e3e 2078  (h)..      >>> x
-00018d60: 203d 206a 6e70 2e6f 6e65 7328 2831 362c   = jnp.ones((16,
-00018d70: 2039 2929 0a20 2020 2020 203e 3e3e 206d   9)).      >>> m
-00018d80: 6f64 756c 6573 203d 2046 6f6f 2829 2e6d  odules = Foo().m
-00018d90: 6f64 756c 655f 7061 7468 7328 6a61 782e  odule_paths(jax.
-00018da0: 7261 6e64 6f6d 2e6b 6579 2830 292c 2078  random.key(0), x
-00018db0: 290a 2020 2020 2020 3e3e 3e20 7072 696e  ).      >>> prin
-00018dc0: 7428 7b0a 2020 2020 2020 2e2e 2e20 2020  t({.      ...   
-00018dd0: 2020 703a 2074 7970 6528 6d29 2e5f 5f6e    p: type(m).__n
-00018de0: 616d 655f 5f20 666f 7220 702c 206d 2069  ame__ for p, m i
-00018df0: 6e20 6d6f 6475 6c65 732e 6974 656d 7328  n modules.items(
-00018e00: 290a 2020 2020 2020 2e2e 2e20 7d29 0a20  ).      ... }). 
-00018e10: 2020 2020 207b 2727 3a20 2746 6f6f 272c       {'': 'Foo',
-00018e20: 2027 4465 6e73 655f 3027 3a20 2744 656e   'Dense_0': 'Den
-00018e30: 7365 272c 2027 4465 6e73 655f 3127 3a20  se', 'Dense_1': 
-00018e40: 2744 656e 7365 277d 0a0a 2020 2020 4172  'Dense'}..    Ar
-00018e50: 6773 3a0a 2020 2020 2020 726e 6773 3a20  gs:.      rngs: 
-00018e60: 5468 6520 726e 6773 2066 6f72 2074 6865  The rngs for the
-00018e70: 2076 6172 6961 626c 6520 636f 6c6c 6563   variable collec
-00018e80: 7469 6f6e 7320 6173 2070 6173 7365 6420  tions as passed 
-00018e90: 746f 2060 604d 6f64 756c 652e 696e 6974  to ``Module.init
-00018ea0: 6060 2e0a 2020 2020 2020 2a61 7267 733a  ``..      *args:
-00018eb0: 2054 6865 2061 7267 756d 656e 7473 2074   The arguments t
-00018ec0: 6f20 7468 6520 666f 7277 6172 6420 636f  o the forward co
-00018ed0: 6d70 7574 6174 696f 6e2e 0a20 2020 2020  mputation..     
-00018ee0: 2073 686f 775f 7265 7065 6174 6564 3a20   show_repeated: 
-00018ef0: 4966 2060 6054 7275 6560 602c 2072 6570  If ``True``, rep
-00018f00: 6561 7465 6420 6361 6c6c 7320 746f 2074  eated calls to t
-00018f10: 6865 2073 616d 6520 6d6f 6475 6c65 2077  he same module w
-00018f20: 696c 6c20 6265 0a20 2020 2020 2020 2073  ill be.        s
-00018f30: 686f 776e 2069 6e20 7468 6520 7461 626c  hown in the tabl
-00018f40: 652c 206f 7468 6572 7769 7365 206f 6e6c  e, otherwise onl
-00018f50: 7920 7468 6520 6669 7273 7420 6361 6c6c  y the first call
-00018f60: 2077 696c 6c20 6265 2073 686f 776e 2e0a   will be shown..
-00018f70: 2020 2020 2020 2020 4465 6661 756c 7420          Default 
-00018f80: 6973 2060 6046 616c 7365 6060 2e0a 2020  is ``False``..  
-00018f90: 2020 2020 6d75 7461 626c 653a 2043 616e      mutable: Can
-00018fa0: 2062 6520 626f 6f6c 2c20 7374 722c 206f   be bool, str, o
-00018fb0: 7220 6c69 7374 2e20 5370 6563 6966 6965  r list. Specifie
-00018fc0: 7320 7768 6963 6820 636f 6c6c 6563 7469  s which collecti
-00018fd0: 6f6e 7320 7368 6f75 6c64 0a20 2020 2020  ons should.     
-00018fe0: 2020 2062 6520 7472 6561 7465 6420 6173     be treated as
-00018ff0: 206d 7574 6162 6c65 3a20 6060 626f 6f6c   mutable: ``bool
-00019000: 6060 3a20 616c 6c2f 6e6f 2063 6f6c 6c65  ``: all/no colle
-00019010: 6374 696f 6e73 2061 7265 206d 7574 6162  ctions are mutab
-00019020: 6c65 2e0a 2020 2020 2020 2020 6060 7374  le..        ``st
-00019030: 7260 603a 2054 6865 206e 616d 6520 6f66  r``: The name of
-00019040: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
-00019050: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
-00019060: 6c69 7374 6060 3a20 4120 6c69 7374 206f  list``: A list o
-00019070: 660a 2020 2020 2020 2020 6e61 6d65 7320  f.        names 
-00019080: 6f66 206d 7574 6162 6c65 2063 6f6c 6c65  of mutable colle
-00019090: 6374 696f 6e73 2e20 4279 2064 6566 6175  ctions. By defau
-000190a0: 6c74 2c20 616c 6c20 636f 6c6c 6563 7469  lt, all collecti
-000190b0: 6f6e 7320 6578 6365 7074 0a20 2020 2020  ons except.     
-000190c0: 2020 2027 696e 7465 726d 6564 6961 7465     'intermediate
-000190d0: 7327 2061 7265 206d 7574 6162 6c65 2e0a  s' are mutable..
-000190e0: 2020 2020 2020 2a2a 6b77 6172 6773 3a20        **kwargs: 
-000190f0: 6b65 7977 6f72 6420 6172 6775 6d65 6e74  keyword argument
-00019100: 7320 746f 2070 6173 7320 746f 2074 6865  s to pass to the
-00019110: 2066 6f72 7761 7264 2063 6f6d 7075 7461   forward computa
-00019120: 7469 6f6e 2e0a 0a20 2020 2052 6574 7572  tion...    Retur
-00019130: 6e73 3a0a 2020 2020 2020 4120 6469 6374  ns:.      A dict
-00019140: 6069 6f6e 6172 7920 6d61 7070 696e 6720  `ionary mapping 
-00019150: 6d6f 6475 6c65 2070 6174 6873 2074 6f20  module paths to 
-00019160: 6d6f 6475 6c65 2069 6e73 7461 6e63 6573  module instances
-00019170: 2e0a 2020 2020 2222 220a 2020 2020 6672  ..    """.    fr
-00019180: 6f6d 2066 6c61 782e 6c69 6e65 6e20 696d  om flax.linen im
-00019190: 706f 7274 2073 756d 6d61 7279 0a0a 2020  port summary..  
-000191a0: 2020 7461 626c 6520 3d20 7375 6d6d 6172    table = summar
-000191b0: 792e 5f67 6574 5f6d 6f64 756c 655f 7461  y._get_module_ta
-000191c0: 626c 6528 0a20 2020 2020 206d 6f64 756c  ble(.      modul
-000191d0: 653d 7365 6c66 2c0a 2020 2020 2020 6465  e=self,.      de
-000191e0: 7074 683d 4e6f 6e65 2c0a 2020 2020 2020  pth=None,.      
-000191f0: 7368 6f77 5f72 6570 6561 7465 643d 7368  show_repeated=sh
-00019200: 6f77 5f72 6570 6561 7465 642c 0a20 2020  ow_repeated,.   
-00019210: 2020 2063 6f6d 7075 7465 5f66 6c6f 7073     compute_flops
-00019220: 3d46 616c 7365 2c0a 2020 2020 2020 636f  =False,.      co
-00019230: 6d70 7574 655f 766a 705f 666c 6f70 733d  mpute_vjp_flops=
-00019240: 4661 6c73 652c 0a20 2020 2029 2872 6e67  False,.    )(rng
-00019250: 732c 202a 6172 6773 2c20 2a2a 6b77 6172  s, *args, **kwar
-00019260: 6773 2c20 6d75 7461 626c 653d 6d75 7461  gs, mutable=muta
-00019270: 626c 6529 0a0a 2020 2020 7265 7475 726e  ble)..    return
-00019280: 207b 272f 272e 6a6f 696e 2872 6f77 2e70   {'/'.join(row.p
-00019290: 6174 6829 3a20 726f 772e 6d6f 6475 6c65  ath): row.module
-000192a0: 5f63 6f70 7920 666f 7220 726f 7720 696e  _copy for row in
-000192b0: 2074 6162 6c65 7d0a 0a0a 5f50 6172 656e   table}..._Paren
-000192c0: 7454 7970 6520 3d20 556e 696f 6e5b 5479  tType = Union[Ty
-000192d0: 7065 5b4d 6f64 756c 655d 2c20 5363 6f70  pe[Module], Scop
-000192e0: 652c 2054 7970 655b 5f53 656e 7469 6e65  e, Type[_Sentine
-000192f0: 6c5d 2c20 4e6f 6e65 5d0a 0a0a 6465 6620  l], None]...def 
-00019300: 6d65 7267 655f 7061 7261 6d28 6e61 6d65  merge_param(name
-00019310: 3a20 7374 722c 2061 3a20 4f70 7469 6f6e  : str, a: Option
-00019320: 616c 5b54 5d2c 2062 3a20 4f70 7469 6f6e  al[T], b: Option
-00019330: 616c 5b54 5d29 202d 3e20 543a 0a20 2022  al[T]) -> T:.  "
-00019340: 2222 4d65 7267 6573 2063 6f6e 7374 7275  ""Merges constru
-00019350: 6374 696f 6e2d 2061 6e64 2063 616c 6c2d  ction- and call-
-00019360: 7469 6d65 2061 7267 756d 656e 742e 0a0a  time argument...
-00019370: 2020 5468 6973 2069 7320 6120 7574 696c    This is a util
-00019380: 6974 7920 666f 7220 7375 7070 6f72 7469  ity for supporti
-00019390: 6e67 2061 2070 6174 7465 726e 2077 6865  ng a pattern whe
-000193a0: 7265 2061 204d 6f64 756c 6520 6879 7065  re a Module hype
-000193b0: 7270 6172 616d 6574 6572 0a20 2063 616e  rparameter.  can
-000193c0: 2062 6520 7061 7373 6564 2065 6974 6865   be passed eithe
-000193d0: 7220 746f 2060 605f 5f69 6e69 745f 5f60  r to ``__init__`
-000193e0: 6020 6f72 2060 605f 5f63 616c 6c5f 5f60  ` or ``__call__`
-000193f0: 602c 2061 6e64 2074 6865 2076 616c 7565  `, and the value
-00019400: 2074 6861 7420 6973 0a20 206e 6f74 2060   that is.  not `
-00019410: 604e 6f6e 6560 6020 7769 6c6c 2062 6520  `None`` will be 
-00019420: 7573 6564 2e0a 0a20 2045 7861 6d70 6c65  used...  Example
-00019430: 3a3a 0a0a 2020 2020 3e3e 3e20 696d 706f  ::..    >>> impo
-00019440: 7274 2066 6c61 782e 6c69 6e65 6e20 6173  rt flax.linen as
-00019450: 206e 6e0a 2020 2020 3e3e 3e20 6672 6f6d   nn.    >>> from
-00019460: 2074 7970 696e 6720 696d 706f 7274 204f   typing import O
-00019470: 7074 696f 6e61 6c0a 0a20 2020 203e 3e3e  ptional..    >>>
-00019480: 2063 6c61 7373 2046 6f6f 286e 6e2e 4d6f   class Foo(nn.Mo
-00019490: 6475 6c65 293a 0a20 2020 202e 2e2e 2020  dule):.    ...  
-000194a0: 2074 7261 696e 3a20 4f70 7469 6f6e 616c   train: Optional
-000194b0: 5b62 6f6f 6c5d 203d 204e 6f6e 650a 0a20  [bool] = None.. 
-000194c0: 2020 202e 2e2e 2020 2064 6566 205f 5f63     ...   def __c
-000194d0: 616c 6c5f 5f28 7365 6c66 2c20 7472 6169  all__(self, trai
-000194e0: 6e3a 204f 7074 696f 6e61 6c5b 626f 6f6c  n: Optional[bool
-000194f0: 5d20 3d20 4e6f 6e65 293a 0a20 2020 202e  ] = None):.    .
-00019500: 2e2e 2020 2020 2074 7261 696e 203d 206e  ..     train = n
-00019510: 6e2e 6d65 7267 655f 7061 7261 6d28 2774  n.merge_param('t
-00019520: 7261 696e 272c 2073 656c 662e 7472 6169  rain', self.trai
-00019530: 6e2c 2074 7261 696e 290a 0a20 2041 6e20  n, train)..  An 
-00019540: 6572 726f 7220 6973 2074 6872 6f77 6e20  error is thrown 
-00019550: 7768 656e 2062 6f74 6820 6172 6775 6d65  when both argume
-00019560: 6e74 7320 6172 6520 6060 4e6f 6e65 6060  nts are ``None``
-00019570: 206f 7220 626f 7468 2076 616c 7565 7320   or both values 
-00019580: 6172 6520 6e6f 740a 2020 6060 4e6f 6e65  are not.  ``None
-00019590: 6060 2e0a 0a20 2041 7267 733a 0a20 2020  ``...  Args:.   
-000195a0: 206e 616d 653a 2074 6865 206e 616d 6520   name: the name 
-000195b0: 6f66 2074 6865 2070 6172 616d 6574 6572  of the parameter
-000195c0: 2e20 5573 6564 2066 6f72 2065 7272 6f72  . Used for error
-000195d0: 206d 6573 7361 6765 732e 0a20 2020 2061   messages..    a
-000195e0: 3a20 6f70 7469 6f6e 2061 0a20 2020 2062  : option a.    b
-000195f0: 3a20 6f70 7469 6f6e 2062 0a0a 2020 5265  : option b..  Re
-00019600: 7475 726e 733a 0a20 2020 2061 206f 7220  turns:.    a or 
-00019610: 6220 7768 6963 6865 7665 7220 6973 206e  b whichever is n
-00019620: 6f74 2060 604e 6f6e 6560 602e 0a20 2022  ot ``None``..  "
-00019630: 2222 0a20 2069 6620 6120 6973 204e 6f6e  "".  if a is Non
-00019640: 6520 616e 6420 6220 6973 204e 6f6e 653a  e and b is None:
-00019650: 0a20 2020 2072 6169 7365 2056 616c 7565  .    raise Value
-00019660: 4572 726f 7228 0a20 2020 2020 2066 2750  Error(.      f'P
-00019670: 6172 616d 6574 6572 2022 7b6e 616d 657d  arameter "{name}
-00019680: 2220 6d75 7374 2062 6520 7061 7373 6564  " must be passed
-00019690: 2074 6f20 7468 6520 636f 6e73 7472 7563   to the construc
-000196a0: 746f 7220 6f72 2061 7420 6361 6c6c 2074  tor or at call t
-000196b0: 696d 652e 270a 2020 2020 290a 2020 6966  ime.'.    ).  if
-000196c0: 2061 2069 7320 6e6f 7420 4e6f 6e65 2061   a is not None a
-000196d0: 6e64 2062 2069 7320 6e6f 7420 4e6f 6e65  nd b is not None
-000196e0: 3a0a 2020 2020 7261 6973 6520 5661 6c75  :.    raise Valu
-000196f0: 6545 7272 6f72 280a 2020 2020 2020 6627  eError(.      f'
-00019700: 5061 7261 6d65 7465 7220 227b 6e61 6d65  Parameter "{name
-00019710: 7d22 2077 6173 2070 6173 7365 6420 746f  }" was passed to
-00019720: 2074 6865 2063 6f6e 7374 7275 6374 6f72   the constructor
-00019730: 2061 6e64 2061 7420 6361 6c6c 2074 696d   and at call tim
-00019740: 652e 270a 2020 2020 2020 2720 5368 6f75  e.'.      ' Shou
-00019750: 6c64 2062 6520 7061 7373 6564 206a 7573  ld be passed jus
-00019760: 7420 6f6e 6365 2e27 0a20 2020 2029 0a20  t once.'.    ). 
-00019770: 2069 6620 6120 6973 204e 6f6e 653a 0a20   if a is None:. 
-00019780: 2020 2061 7373 6572 7420 6220 6973 206e     assert b is n
-00019790: 6f74 204e 6f6e 650a 2020 2020 7265 7475  ot None.    retu
-000197a0: 726e 2062 0a20 2072 6574 7572 6e20 610a  rn b.  return a.
-000197b0: 0a0a 4074 7261 6365 6261 636b 5f75 7469  ..@traceback_uti
-000197c0: 6c2e 6170 695f 626f 756e 6461 7279 0a64  l.api_boundary.d
-000197d0: 6566 2061 7070 6c79 280a 2020 666e 3a20  ef apply(.  fn: 
-000197e0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-000197f0: 795d 2c0a 2020 6d6f 6475 6c65 3a20 4d6f  y],.  module: Mo
-00019800: 6475 6c65 2c0a 2020 6d75 7461 626c 653a  dule,.  mutable:
-00019810: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
-00019820: 7220 3d20 4661 6c73 652c 0a20 2063 6170  r = False,.  cap
-00019830: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-00019840: 6573 3a20 556e 696f 6e5b 626f 6f6c 2c20  es: Union[bool, 
-00019850: 4361 6c6c 6162 6c65 5b5b 4d6f 6475 6c65  Callable[[Module
-00019860: 2c20 7374 725d 2c20 626f 6f6c 5d5d 203d  , str], bool]] =
-00019870: 2046 616c 7365 2c0a 2920 2d3e 2043 616c   False,.) -> Cal
-00019880: 6c61 626c 655b 2e2e 2e2c 2041 6e79 5d3a  lable[..., Any]:
-00019890: 0a20 2022 2222 4372 6561 7465 7320 616e  .  """Creates an
-000198a0: 2061 7070 6c79 2066 756e 6374 696f 6e20   apply function 
-000198b0: 746f 2063 616c 6c20 6060 666e 6060 2077  to call ``fn`` w
-000198c0: 6974 6820 6120 626f 756e 6420 6d6f 6475  ith a bound modu
-000198d0: 6c65 2e0a 0a20 2055 6e6c 696b 6520 6060  le...  Unlike ``
-000198e0: 4d6f 6475 6c65 2e61 7070 6c79 6060 2074  Module.apply`` t
-000198f0: 6869 7320 6675 6e63 7469 6f6e 2072 6574  his function ret
-00019900: 7572 6e73 2061 206e 6577 2066 756e 6374  urns a new funct
-00019910: 696f 6e20 7769 7468 2074 6865 0a20 2073  ion with the.  s
-00019920: 6967 6e61 7475 7265 2060 6028 7661 7269  ignature ``(vari
-00019930: 6162 6c65 732c 202a 6172 6773 2c20 726e  ables, *args, rn
-00019940: 6773 3d4e 6f6e 652c 202a 2a6b 7761 7267  gs=None, **kwarg
-00019950: 7329 202d 3e20 5460 6020 7768 6572 6520  s) -> T`` where 
-00019960: 6060 5460 6020 6973 2074 6865 0a20 2072  ``T`` is the.  r
-00019970: 6574 7572 6e20 7479 7065 206f 6620 6060  eturn type of ``
-00019980: 666e 6060 2e20 4966 2060 606d 7574 6162  fn``. If ``mutab
-00019990: 6c65 6060 2069 7320 6e6f 7420 6060 4661  le`` is not ``Fa
-000199a0: 6c73 6560 6020 7468 6520 7265 7475 726e  lse`` the return
-000199b0: 2074 7970 6520 6973 2061 0a20 2074 7570   type is a.  tup
-000199c0: 6c65 2077 6865 7265 2074 6865 2073 6563  le where the sec
-000199d0: 6f6e 6420 6974 656d 2069 7320 6120 6060  ond item is a ``
-000199e0: 4672 6f7a 656e 4469 6374 6060 2077 6974  FrozenDict`` wit
-000199f0: 6820 7468 6520 6d75 7461 7465 6420 7661  h the mutated va
-00019a00: 7269 6162 6c65 732e 0a0a 2020 5468 6520  riables...  The 
-00019a10: 6170 706c 7920 6675 6e63 7469 6f6e 2074  apply function t
-00019a20: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
-00019a30: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
-00019a40: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
-00019a50: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
-00019a60: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
-00019a70: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
-00019a80: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
-00019a90: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
-00019aa0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
-00019ab0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
-00019ac0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
-00019ad0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
-00019ae0: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
-00019af0: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
-00019b00: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
-00019b10: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
-00019b20: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
-00019b30: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
-00019b40: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
-00019b50: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
-00019b60: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
-00019b70: 7661 7269 6162 6c65 7320 3d20 7b7d 0a20  variables = {}. 
-00019b80: 2020 203e 3e3e 2066 6f6f 203d 2046 6f6f     >>> foo = Foo
-00019b90: 2829 0a20 2020 203e 3e3e 2066 5f6a 6974  ().    >>> f_jit
-00019ba0: 7465 6420 3d20 6a61 782e 6a69 7428 6e6e  ted = jax.jit(nn
-00019bb0: 2e61 7070 6c79 2866 2c20 666f 6f29 290a  .apply(f, foo)).
-00019bc0: 2020 2020 3e3e 3e20 665f 6a69 7474 6564      >>> f_jitted
-00019bd0: 2876 6172 6961 626c 6573 2c20 6a6e 702e  (variables, jnp.
-00019be0: 6f6e 6573 2828 312c 2033 2929 290a 0a20  ones((1, 3))).. 
-00019bf0: 2041 7267 733a 0a20 2020 2066 6e3a 2054   Args:.    fn: T
-00019c00: 6865 2066 756e 6374 696f 6e20 7468 6174  he function that
-00019c10: 2073 686f 756c 6420 6265 2061 7070 6c69   should be appli
-00019c20: 6564 2e20 5468 6520 6669 7273 7420 6172  ed. The first ar
-00019c30: 6775 6d65 6e74 2070 6173 7365 6420 7769  gument passed wi
-00019c40: 6c6c 2062 650a 2020 2020 2020 6120 6d6f  ll be.      a mo
-00019c50: 6475 6c65 2069 6e73 7461 6e63 6520 6f66  dule instance of
-00019c60: 2074 6865 2060 606d 6f64 756c 6560 6020   the ``module`` 
-00019c70: 7769 7468 2076 6172 6961 626c 6573 2061  with variables a
-00019c80: 6e64 2052 4e47 7320 626f 756e 6420 746f  nd RNGs bound to
-00019c90: 2069 742e 0a20 2020 206d 6f64 756c 653a   it..    module:
-00019ca0: 2054 6865 2060 604d 6f64 756c 6560 6020   The ``Module`` 
-00019cb0: 7468 6174 2077 696c 6c20 6265 2075 7365  that will be use
-00019cc0: 6420 746f 2062 696e 6420 7661 7269 6162  d to bind variab
-00019cd0: 6c65 7320 616e 6420 524e 4773 2074 6f2e  les and RNGs to.
-00019ce0: 2054 6865 0a20 2020 2020 2060 604d 6f64   The.      ``Mod
-00019cf0: 756c 6560 6020 7061 7373 6564 2061 7320  ule`` passed as 
-00019d00: 7468 6520 6669 7273 7420 6172 6775 6d65  the first argume
-00019d10: 6e74 2074 6f20 6060 666e 6060 2077 696c  nt to ``fn`` wil
-00019d20: 6c20 6265 2061 2063 6c6f 6e65 206f 660a  l be a clone of.
-00019d30: 2020 2020 2020 6d6f 6475 6c65 2e0a 2020        module..  
-00019d40: 2020 6d75 7461 626c 653a 2043 616e 2062    mutable: Can b
-00019d50: 6520 626f 6f6c 2c20 7374 722c 206f 7220  e bool, str, or 
-00019d60: 6c69 7374 2e20 5370 6563 6966 6965 7320  list. Specifies 
-00019d70: 7768 6963 6820 636f 6c6c 6563 7469 6f6e  which collection
-00019d80: 7320 7368 6f75 6c64 2062 650a 2020 2020  s should be.    
-00019d90: 2020 7472 6561 7465 6420 6173 206d 7574    treated as mut
-00019da0: 6162 6c65 3a20 6060 626f 6f6c 6060 3a20  able: ``bool``: 
-00019db0: 616c 6c2f 6e6f 2063 6f6c 6c65 6374 696f  all/no collectio
-00019dc0: 6e73 2061 7265 206d 7574 6162 6c65 2e20  ns are mutable. 
-00019dd0: 6060 7374 7260 603a 2054 6865 0a20 2020  ``str``: The.   
-00019de0: 2020 206e 616d 6520 6f66 2061 2073 696e     name of a sin
-00019df0: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
-00019e00: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
-00019e10: 3a20 4120 6c69 7374 206f 6620 6e61 6d65  : A list of name
-00019e20: 7320 6f66 206d 7574 6162 6c65 0a20 2020  s of mutable.   
-00019e30: 2020 2063 6f6c 6c65 6374 696f 6e73 2e0a     collections..
-00019e40: 2020 2020 6361 7074 7572 655f 696e 7465      capture_inte
-00019e50: 726d 6564 6961 7465 733a 2049 6620 6060  rmediates: If ``
-00019e60: 5472 7565 6060 2c20 6361 7074 7572 6573  True``, captures
-00019e70: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-00019e80: 7475 726e 2076 616c 7565 7320 6f66 2061  turn values of a
-00019e90: 6c6c 0a20 2020 2020 204d 6f64 756c 6573  ll.      Modules
-00019ea0: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
-00019eb0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
-00019ec0: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
-00019ed0: 6c74 2c20 6f6e 6c79 2074 6865 2072 6574  lt, only the ret
-00019ee0: 7572 6e0a 2020 2020 2020 7661 6c75 6573  urn.      values
-00019ef0: 206f 6620 616c 6c20 605f 5f63 616c 6c5f   of all `__call_
-00019f00: 5f60 206d 6574 686f 6473 2061 7265 2073  _` methods are s
-00019f10: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
-00019f20: 6e20 6361 6e20 6265 2070 6173 7365 6420  n can be passed 
-00019f30: 746f 0a20 2020 2020 2063 6861 6e67 6520  to.      change 
-00019f40: 7468 6520 6669 6c74 6572 2062 6568 6176  the filter behav
-00019f50: 696f 722e 2054 6865 2066 696c 7465 7220  ior. The filter 
-00019f60: 6675 6e63 7469 6f6e 2074 616b 6573 2074  function takes t
-00019f70: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
-00019f80: 6365 0a20 2020 2020 2061 6e64 206d 6574  ce.      and met
-00019f90: 686f 6420 6e61 6d65 2061 6e64 2072 6574  hod name and ret
-00019fa0: 7572 6e73 2061 2062 6f6f 6c20 696e 6469  urns a bool indi
-00019fb0: 6361 7469 6e67 2077 6865 7468 6572 2074  cating whether t
-00019fc0: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
-00019fd0: 740a 2020 2020 2020 6d65 7468 6f64 2069  t.      method i
-00019fe0: 6e76 6f63 6174 696f 6e20 7368 6f75 6c64  nvocation should
-00019ff0: 2062 6520 7374 6f72 6564 2e0a 0a20 2052   be stored...  R
-0001a000: 6574 7572 6e73 3a0a 2020 2020 5468 6520  eturns:.    The 
-0001a010: 6170 706c 7920 6675 6e63 7469 6f6e 2077  apply function w
-0001a020: 7261 7070 696e 6720 6060 666e 6060 2e0a  rapping ``fn``..
-0001a030: 2020 2222 220a 0a20 2040 6675 6e63 746f    """..  @functo
-0001a040: 6f6c 732e 7772 6170 7328 666e 290a 2020  ols.wraps(fn).  
-0001a050: 6465 6620 7363 6f70 655f 666e 2873 636f  def scope_fn(sco
-0001a060: 7065 2c20 2a61 7267 732c 202a 2a6b 7761  pe, *args, **kwa
-0001a070: 7267 7329 3a0a 2020 2020 5f63 6f6e 7465  rgs):.    _conte
-0001a080: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
-0001a090: 2e61 7070 656e 6428 6361 7074 7572 655f  .append(capture_
-0001a0a0: 696e 7465 726d 6564 6961 7465 7329 0a20  intermediates). 
-0001a0b0: 2020 2074 7279 3a0a 2020 2020 2020 7265     try:.      re
-0001a0c0: 7475 726e 2066 6e28 6d6f 6475 6c65 2e63  turn fn(module.c
-0001a0d0: 6c6f 6e65 2870 6172 656e 743d 7363 6f70  lone(parent=scop
-0001a0e0: 652c 205f 6465 6570 5f63 6c6f 6e65 3d54  e, _deep_clone=T
-0001a0f0: 7275 6529 2c20 2a61 7267 732c 202a 2a6b  rue), *args, **k
-0001a100: 7761 7267 7329 0a20 2020 2066 696e 616c  wargs).    final
-0001a110: 6c79 3a0a 2020 2020 2020 5f63 6f6e 7465  ly:.      _conte
-0001a120: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
-0001a130: 2e70 6f70 2829 0a0a 2020 6966 2063 6170  .pop()..  if cap
-0001a140: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
-0001a150: 6573 2069 7320 5472 7565 3a20 2023 2070  es is True:  # p
-0001a160: 796c 696e 743a 2064 6973 6162 6c65 3d67  ylint: disable=g
-0001a170: 2d62 6f6f 6c2d 6964 2d63 6f6d 7061 7269  -bool-id-compari
-0001a180: 736f 6e0a 2020 2020 6361 7074 7572 655f  son.    capture_
-0001a190: 696e 7465 726d 6564 6961 7465 7320 3d20  intermediates = 
-0001a1a0: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
-0001a1b0: 6572 6d65 6469 6174 6573 0a20 2069 6620  ermediates.  if 
-0001a1c0: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
-0001a1d0: 6961 7465 733a 0a20 2020 206d 7574 6162  iates:.    mutab
-0001a1e0: 6c65 203d 2075 6e69 6f6e 5f66 696c 7465  le = union_filte
-0001a1f0: 7273 286d 7574 6162 6c65 2c20 2769 6e74  rs(mutable, 'int
-0001a200: 6572 6d65 6469 6174 6573 2729 0a20 2072  ermediates').  r
-0001a210: 6574 7572 6e20 636f 7265 2e61 7070 6c79  eturn core.apply
-0001a220: 2873 636f 7065 5f66 6e2c 206d 7574 6162  (scope_fn, mutab
-0001a230: 6c65 3d6d 7574 6162 6c65 290a 0a0a 4074  le=mutable)...@t
-0001a240: 7261 6365 6261 636b 5f75 7469 6c2e 6170  raceback_util.ap
-0001a250: 695f 626f 756e 6461 7279 0a64 6566 2069  i_boundary.def i
-0001a260: 6e69 745f 7769 7468 5f6f 7574 7075 7428  nit_with_output(
-0001a270: 0a20 2066 6e3a 2043 616c 6c61 626c 655b  .  fn: Callable[
-0001a280: 2e2e 2e2c 2041 6e79 5d2c 0a20 206d 6f64  ..., Any],.  mod
-0001a290: 756c 653a 204d 6f64 756c 652c 0a20 206d  ule: Module,.  m
-0001a2a0: 7574 6162 6c65 3a20 436f 6c6c 6563 7469  utable: Collecti
-0001a2b0: 6f6e 4669 6c74 6572 203d 2044 656e 794c  onFilter = DenyL
-0001a2c0: 6973 7428 2769 6e74 6572 6d65 6469 6174  ist('intermediat
-0001a2d0: 6573 2729 2c0a 2020 6361 7074 7572 655f  es'),.  capture_
-0001a2e0: 696e 7465 726d 6564 6961 7465 733a 2055  intermediates: U
-0001a2f0: 6e69 6f6e 5b62 6f6f 6c2c 2043 616c 6c61  nion[bool, Calla
-0001a300: 626c 655b 5b4d 6f64 756c 652c 2073 7472  ble[[Module, str
-0001a310: 5d2c 2062 6f6f 6c5d 5d20 3d20 4661 6c73  ], bool]] = Fals
-0001a320: 652c 0a29 202d 3e20 4361 6c6c 6162 6c65  e,.) -> Callable
-0001a330: 5b2e 2e2e 2c20 5475 706c 655b 416e 792c  [..., Tuple[Any,
-0001a340: 2055 6e69 6f6e 5b46 726f 7a65 6e56 6172   Union[FrozenVar
-0001a350: 6961 626c 6544 6963 742c 2044 6963 745b  iableDict, Dict[
-0001a360: 7374 722c 2041 6e79 5d5d 5d5d 3a0a 2020  str, Any]]]]:.  
-0001a370: 2222 2243 7265 6174 6573 2061 6e20 696e  """Creates an in
-0001a380: 6974 2066 756e 6374 696f 6e20 746f 2063  it function to c
-0001a390: 616c 6c20 6060 666e 6060 2077 6974 6820  all ``fn`` with 
-0001a3a0: 6120 626f 756e 6420 6d6f 6475 6c65 2074  a bound module t
-0001a3b0: 6861 7420 616c 736f 2072 6574 7572 6e73  hat also returns
-0001a3c0: 2074 6865 2066 756e 6374 696f 6e20 6f75   the function ou
-0001a3d0: 7470 7574 732e 0a0a 2020 556e 6c69 6b65  tputs...  Unlike
-0001a3e0: 2060 604d 6f64 756c 652e 696e 6974 5f77   ``Module.init_w
-0001a3f0: 6974 685f 6f75 7470 7574 6060 2074 6869  ith_output`` thi
-0001a400: 7320 6675 6e63 7469 6f6e 2072 6574 7572  s function retur
-0001a410: 6e73 2061 206e 6577 2066 756e 6374 696f  ns a new functio
-0001a420: 6e20 7769 7468 0a20 2074 6865 2073 6967  n with.  the sig
-0001a430: 6e61 7475 7265 2060 6028 726e 6773 2c20  nature ``(rngs, 
-0001a440: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-0001a450: 202d 3e20 2854 2c20 7661 7269 6162 6c65   -> (T, variable
-0001a460: 7329 6060 2077 6865 7265 2060 6054 6060  s)`` where ``T``
-0001a470: 2069 7320 7468 650a 2020 7265 7475 726e   is the.  return
-0001a480: 2074 7970 6520 6f66 2060 6066 6e60 602e   type of ``fn``.
-0001a490: 2054 6865 2072 6e67 7320 6361 6e20 6265   The rngs can be
-0001a4a0: 2061 2064 6963 7420 6f66 2050 524e 474b   a dict of PRNGK
-0001a4b0: 6579 7320 6f72 2061 2073 696e 676c 650a  eys or a single.
-0001a4c0: 2020 6060 6050 524e 474b 6579 6060 2077    ```PRNGKey`` w
-0001a4d0: 6869 6368 2069 7320 6571 7569 7661 6c65  hich is equivale
-0001a4e0: 6e74 2074 6f20 7061 7373 696e 6720 6120  nt to passing a 
-0001a4f0: 6469 6374 2077 6974 6820 6f6e 6520 5052  dict with one PR
-0001a500: 4e47 4b65 7920 7769 7468 2074 6865 0a20  NGKey with the. 
-0001a510: 206e 616d 6520 2270 6172 616d 7322 2e0a   name "params"..
-0001a520: 0a20 2054 6865 2069 6e69 7420 6675 6e63  .  The init func
-0001a530: 7469 6f6e 2074 6861 7420 6973 2072 6574  tion that is ret
-0001a540: 7572 6e65 6420 6361 6e20 6265 2064 6972  urned can be dir
-0001a550: 6563 746c 7920 636f 6d70 6f73 6564 2077  ectly composed w
-0001a560: 6974 680a 2020 4a41 5820 7472 616e 7366  ith.  JAX transf
-0001a570: 6f72 6d61 7469 6f6e 7320 6c69 6b65 2060  ormations like `
-0001a580: 606a 6178 2e6a 6974 6060 3a3a 0a0a 2020  `jax.jit``::..  
-0001a590: 2020 3e3e 3e20 636c 6173 7320 466f 6f28    >>> class Foo(
-0001a5a0: 6e6e 2e4d 6f64 756c 6529 3a0a 2020 2020  nn.Module):.    
-0001a5b0: 2e2e 2e20 2020 6465 6620 656e 636f 6465  ...   def encode
-0001a5c0: 2873 656c 662c 2078 293a 0a20 2020 202e  (self, x):.    .
-0001a5d0: 2e2e 2020 2020 202e 2e2e 0a20 2020 202e  ..     ....    .
-0001a5e0: 2e2e 2020 2064 6566 2064 6563 6f64 6528  ..   def decode(
-0001a5f0: 7365 6c66 2c20 7829 3a0a 2020 2020 2e2e  self, x):.    ..
-0001a600: 2e20 2020 2020 2e2e 2e0a 0a20 2020 203e  .     .....    >
-0001a610: 3e3e 2064 6566 2066 2866 6f6f 2c20 7829  >> def f(foo, x)
-0001a620: 3a0a 2020 2020 2e2e 2e20 2020 7a20 3d20  :.    ...   z = 
-0001a630: 666f 6f2e 656e 636f 6465 2878 290a 2020  foo.encode(x).  
-0001a640: 2020 2e2e 2e20 2020 7920 3d20 666f 6f2e    ...   y = foo.
-0001a650: 6465 636f 6465 287a 290a 2020 2020 2e2e  decode(z).    ..
-0001a660: 2e20 2020 2320 2e2e 2e0a 2020 2020 2e2e  .   # ....    ..
-0001a670: 2e20 2020 7265 7475 726e 2079 0a0a 2020  .   return y..  
-0001a680: 2020 3e3e 3e20 666f 6f20 3d20 466f 6f28    >>> foo = Foo(
-0001a690: 290a 2020 2020 3e3e 3e20 665f 6a69 7474  ).    >>> f_jitt
-0001a6a0: 6564 203d 206a 6178 2e6a 6974 286e 6e2e  ed = jax.jit(nn.
-0001a6b0: 696e 6974 5f77 6974 685f 6f75 7470 7574  init_with_output
-0001a6c0: 2866 2c20 666f 6f29 290a 2020 2020 3e3e  (f, foo)).    >>
-0001a6d0: 3e20 792c 2076 6172 6961 626c 6573 203d  > y, variables =
-0001a6e0: 2066 5f6a 6974 7465 6428 6a61 782e 7261   f_jitted(jax.ra
-0001a6f0: 6e64 6f6d 2e6b 6579 2830 292c 206a 6e70  ndom.key(0), jnp
-0001a700: 2e6f 6e65 7328 2831 2c20 3329 2929 0a0a  .ones((1, 3)))..
-0001a710: 2020 4172 6773 3a0a 2020 2020 666e 3a20    Args:.    fn: 
-0001a720: 5468 6520 6675 6e63 7469 6f6e 2074 6861  The function tha
-0001a730: 7420 7368 6f75 6c64 2062 6520 6170 706c  t should be appl
-0001a740: 6965 642e 2054 6865 2066 6972 7374 2061  ied. The first a
-0001a750: 7267 756d 656e 7420 7061 7373 6564 2077  rgument passed w
-0001a760: 696c 6c20 6265 0a20 2020 2020 2061 206d  ill be.      a m
-0001a770: 6f64 756c 6520 696e 7374 616e 6365 206f  odule instance o
-0001a780: 6620 7468 6520 6060 6d6f 6475 6c65 6060  f the ``module``
-0001a790: 2077 6974 6820 7661 7269 6162 6c65 7320   with variables 
-0001a7a0: 616e 6420 524e 4773 2062 6f75 6e64 2074  and RNGs bound t
-0001a7b0: 6f20 6974 2e0a 2020 2020 6d6f 6475 6c65  o it..    module
-0001a7c0: 3a20 5468 6520 6060 4d6f 6475 6c65 6060  : The ``Module``
-0001a7d0: 2074 6861 7420 7769 6c6c 2062 6520 7573   that will be us
-0001a7e0: 6564 2074 6f20 6269 6e64 2076 6172 6961  ed to bind varia
-0001a7f0: 626c 6573 2061 6e64 2052 4e47 7320 746f  bles and RNGs to
-0001a800: 2e20 5468 650a 2020 2020 2020 6060 4d6f  . The.      ``Mo
-0001a810: 6475 6c65 6060 2070 6173 7365 6420 6173  dule`` passed as
-0001a820: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
-0001a830: 656e 7420 746f 2060 6066 6e60 6020 7769  ent to ``fn`` wi
-0001a840: 6c6c 2062 6520 6120 636c 6f6e 6520 6f66  ll be a clone of
-0001a850: 0a20 2020 2020 206d 6f64 756c 652e 0a20  .      module.. 
-0001a860: 2020 206d 7574 6162 6c65 3a20 4361 6e20     mutable: Can 
-0001a870: 6265 2062 6f6f 6c2c 2073 7472 2c20 6f72  be bool, str, or
-0001a880: 206c 6973 742e 2053 7065 6369 6669 6573   list. Specifies
-0001a890: 2077 6869 6368 2063 6f6c 6c65 6374 696f   which collectio
-0001a8a0: 6e73 2073 686f 756c 6420 6265 0a20 2020  ns should be.   
-0001a8b0: 2020 2074 7265 6174 6564 2061 7320 6d75     treated as mu
-0001a8c0: 7461 626c 653a 2060 6062 6f6f 6c60 603a  table: ``bool``:
-0001a8d0: 2061 6c6c 2f6e 6f20 636f 6c6c 6563 7469   all/no collecti
-0001a8e0: 6f6e 7320 6172 6520 6d75 7461 626c 652e  ons are mutable.
-0001a8f0: 2060 6073 7472 6060 3a20 5468 650a 2020   ``str``: The.  
-0001a900: 2020 2020 6e61 6d65 206f 6620 6120 7369      name of a si
-0001a910: 6e67 6c65 206d 7574 6162 6c65 2063 6f6c  ngle mutable col
-0001a920: 6c65 6374 696f 6e2e 2060 606c 6973 7460  lection. ``list`
-0001a930: 603a 2041 206c 6973 7420 6f66 206e 616d  `: A list of nam
-0001a940: 6573 206f 6620 6d75 7461 626c 650a 2020  es of mutable.  
-0001a950: 2020 2020 636f 6c6c 6563 7469 6f6e 732e      collections.
-0001a960: 2042 7920 6465 6661 756c 742c 2061 6c6c   By default, all
-0001a970: 2063 6f6c 6c65 6374 696f 6e73 2065 7863   collections exc
-0001a980: 6570 7420 2269 6e74 6572 6d65 6469 6174  ept "intermediat
-0001a990: 6573 2220 6172 650a 2020 2020 2020 6d75  es" are.      mu
-0001a9a0: 7461 626c 652e 0a20 2020 2063 6170 7475  table..    captu
-0001a9b0: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
-0001a9c0: 3a20 4966 2060 6054 7275 6560 602c 2063  : If ``True``, c
-0001a9d0: 6170 7475 7265 7320 696e 7465 726d 6564  aptures intermed
-0001a9e0: 6961 7465 2072 6574 7572 6e20 7661 6c75  iate return valu
-0001a9f0: 6573 206f 6620 616c 6c0a 2020 2020 2020  es of all.      
-0001aa00: 4d6f 6475 6c65 7320 696e 7369 6465 2074  Modules inside t
-0001aa10: 6865 2022 696e 7465 726d 6564 6961 7465  he "intermediate
-0001aa20: 7322 2063 6f6c 6c65 6374 696f 6e2e 2042  s" collection. B
-0001aa30: 7920 6465 6661 756c 742c 206f 6e6c 7920  y default, only 
-0001aa40: 7468 6520 7265 7475 726e 0a20 2020 2020  the return.     
-0001aa50: 2076 616c 7565 7320 6f66 2061 6c6c 2060   values of all `
-0001aa60: 5f5f 6361 6c6c 5f5f 6020 6d65 7468 6f64  __call__` method
-0001aa70: 7320 6172 6520 7374 6f72 6564 2e20 4120  s are stored. A 
-0001aa80: 6675 6e63 7469 6f6e 2063 616e 2062 6520  function can be 
-0001aa90: 7061 7373 6564 2074 6f0a 2020 2020 2020  passed to.      
-0001aaa0: 6368 616e 6765 2074 6865 2066 696c 7465  change the filte
-0001aab0: 7220 6265 6861 7669 6f72 2e20 5468 6520  r behavior. The 
-0001aac0: 6669 6c74 6572 2066 756e 6374 696f 6e20  filter function 
-0001aad0: 7461 6b65 7320 7468 6520 4d6f 6475 6c65  takes the Module
-0001aae0: 2069 6e73 7461 6e63 650a 2020 2020 2020   instance.      
-0001aaf0: 616e 6420 6d65 7468 6f64 206e 616d 6520  and method name 
-0001ab00: 616e 6420 7265 7475 726e 7320 6120 626f  and returns a bo
-0001ab10: 6f6c 2069 6e64 6963 6174 696e 6720 7768  ol indicating wh
-0001ab20: 6574 6865 7220 7468 6520 6f75 7470 7574  ether the output
-0001ab30: 206f 6620 7468 6174 0a20 2020 2020 206d   of that.      m
-0001ab40: 6574 686f 6420 696e 766f 6361 7469 6f6e  ethod invocation
-0001ab50: 2073 686f 756c 6420 6265 2073 746f 7265   should be store
-0001ab60: 642e 0a0a 2020 5265 7475 726e 733a 0a20  d...  Returns:. 
-0001ab70: 2020 2054 6865 2069 6e69 7420 6675 6e63     The init func
-0001ab80: 7469 6f6e 2077 7261 7070 696e 6720 6060  tion wrapping ``
-0001ab90: 666e 6060 2e0a 2020 2222 220a 0a20 2040  fn``..  """..  @
-0001aba0: 6675 6e63 746f 6f6c 732e 7772 6170 7328  functools.wraps(
-0001abb0: 666e 290a 2020 6465 6620 7363 6f70 655f  fn).  def scope_
-0001abc0: 666e 2873 636f 7065 2c20 2a61 7267 732c  fn(scope, *args,
-0001abd0: 202a 2a6b 7761 7267 7329 3a0a 2020 2020   **kwargs):.    
-0001abe0: 5f63 6f6e 7465 7874 2e63 6170 7475 7265  _context.capture
-0001abf0: 5f73 7461 636b 2e61 7070 656e 6428 6361  _stack.append(ca
-0001ac00: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-0001ac10: 7465 7329 0a20 2020 2074 7279 3a0a 2020  tes).    try:.  
-0001ac20: 2020 2020 7265 7475 726e 2066 6e28 6d6f      return fn(mo
-0001ac30: 6475 6c65 2e63 6c6f 6e65 2870 6172 656e  dule.clone(paren
-0001ac40: 743d 7363 6f70 652c 205f 6465 6570 5f63  t=scope, _deep_c
-0001ac50: 6c6f 6e65 3d54 7275 6529 2c20 2a61 7267  lone=True), *arg
-0001ac60: 732c 202a 2a6b 7761 7267 7329 0a20 2020  s, **kwargs).   
-0001ac70: 2066 696e 616c 6c79 3a0a 2020 2020 2020   finally:.      
-0001ac80: 5f63 6f6e 7465 7874 2e63 6170 7475 7265  _context.capture
-0001ac90: 5f73 7461 636b 2e70 6f70 2829 0a0a 2020  _stack.pop()..  
-0001aca0: 6966 2063 6170 7475 7265 5f69 6e74 6572  if capture_inter
-0001acb0: 6d65 6469 6174 6573 2069 7320 5472 7565  mediates is True
-0001acc0: 3a20 2023 2070 796c 696e 743a 2064 6973  :  # pylint: dis
-0001acd0: 6162 6c65 3d67 2d62 6f6f 6c2d 6964 2d63  able=g-bool-id-c
-0001ace0: 6f6d 7061 7269 736f 6e0a 2020 2020 6361  omparison.    ca
-0001acf0: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
-0001ad00: 7465 7320 3d20 6361 7074 7572 655f 6361  tes = capture_ca
-0001ad10: 6c6c 5f69 6e74 6572 6d65 6469 6174 6573  ll_intermediates
-0001ad20: 0a20 2069 6620 6361 7074 7572 655f 696e  .  if capture_in
-0001ad30: 7465 726d 6564 6961 7465 733a 0a20 2020  termediates:.   
-0001ad40: 206d 7574 6162 6c65 203d 2075 6e69 6f6e   mutable = union
-0001ad50: 5f66 696c 7465 7273 286d 7574 6162 6c65  _filters(mutable
-0001ad60: 2c20 2769 6e74 6572 6d65 6469 6174 6573  , 'intermediates
-0001ad70: 2729 0a20 2072 6574 7572 6e20 636f 7265  ').  return core
-0001ad80: 2e69 6e69 7428 7363 6f70 655f 666e 2c20  .init(scope_fn, 
-0001ad90: 6d75 7461 626c 653d 6d75 7461 626c 6529  mutable=mutable)
-0001ada0: 0a0a 0a40 7472 6163 6562 6163 6b5f 7574  ...@traceback_ut
-0001adb0: 696c 2e61 7069 5f62 6f75 6e64 6172 790a  il.api_boundary.
-0001adc0: 6465 6620 696e 6974 280a 2020 666e 3a20  def init(.  fn: 
-0001add0: 4361 6c6c 6162 6c65 5b2e 2e2e 2c20 416e  Callable[..., An
-0001ade0: 795d 2c0a 2020 6d6f 6475 6c65 3a20 4d6f  y],.  module: Mo
-0001adf0: 6475 6c65 2c0a 2020 6d75 7461 626c 653a  dule,.  mutable:
-0001ae00: 2043 6f6c 6c65 6374 696f 6e46 696c 7465   CollectionFilte
-0001ae10: 7220 3d20 4465 6e79 4c69 7374 2827 696e  r = DenyList('in
-0001ae20: 7465 726d 6564 6961 7465 7327 292c 0a20  termediates'),. 
-0001ae30: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
-0001ae40: 6469 6174 6573 3a20 556e 696f 6e5b 626f  diates: Union[bo
-0001ae50: 6f6c 2c20 4361 6c6c 6162 6c65 5b5b 4d6f  ol, Callable[[Mo
-0001ae60: 6475 6c65 2c20 7374 725d 2c20 626f 6f6c  dule, str], bool
-0001ae70: 5d5d 203d 2046 616c 7365 2c0a 2920 2d3e  ]] = False,.) ->
-0001ae80: 2043 616c 6c61 626c 655b 2e2e 2e2c 2055   Callable[..., U
-0001ae90: 6e69 6f6e 5b46 726f 7a65 6e56 6172 6961  nion[FrozenVaria
-0001aea0: 626c 6544 6963 742c 2044 6963 745b 7374  bleDict, Dict[st
-0001aeb0: 722c 2041 6e79 5d5d 5d3a 0a20 2022 2222  r, Any]]]:.  """
-0001aec0: 4372 6561 7465 7320 616e 2069 6e69 7420  Creates an init 
-0001aed0: 6675 6e63 7469 6f6e 2074 6f20 6361 6c6c  function to call
-0001aee0: 2060 6066 6e60 6020 7769 7468 2061 2062   ``fn`` with a b
-0001aef0: 6f75 6e64 206d 6f64 756c 652e 0a0a 2020  ound module...  
-0001af00: 556e 6c69 6b65 2060 604d 6f64 756c 652e  Unlike ``Module.
-0001af10: 696e 6974 6060 2074 6869 7320 6675 6e63  init`` this func
-0001af20: 7469 6f6e 2072 6574 7572 6e73 2061 206e  tion returns a n
-0001af30: 6577 2066 756e 6374 696f 6e20 7769 7468  ew function with
-0001af40: 2074 6865 2073 6967 6e61 7475 7265 0a20   the signature. 
-0001af50: 2060 6028 726e 6773 2c20 2a61 7267 732c   ``(rngs, *args,
-0001af60: 202a 2a6b 7761 7267 7329 202d 3e20 7661   **kwargs) -> va
-0001af70: 7269 6162 6c65 7360 602e 0a20 2054 6865  riables``..  The
-0001af80: 2072 6e67 7320 6361 6e20 6265 2061 2064   rngs can be a d
-0001af90: 6963 7420 6f66 2050 524e 474b 6579 7320  ict of PRNGKeys 
-0001afa0: 6f72 2061 2073 696e 676c 6520 6060 6050  or a single ```P
-0001afb0: 524e 474b 6579 6060 2077 6869 6368 2069  RNGKey`` which i
-0001afc0: 730a 2020 6571 7569 7661 6c65 6e74 2074  s.  equivalent t
-0001afd0: 6f20 7061 7373 696e 6720 6120 6469 6374  o passing a dict
-0001afe0: 2077 6974 6820 6f6e 6520 5052 4e47 4b65   with one PRNGKe
-0001aff0: 7920 7769 7468 2074 6865 206e 616d 6520  y with the name 
-0001b000: 2270 6172 616d 7322 2e0a 0a20 2054 6865  "params"...  The
-0001b010: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
-0001b020: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
-0001b030: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
-0001b040: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
-0001b050: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
-0001b060: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
-0001b070: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
-0001b080: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
-0001b090: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
-0001b0a0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
-0001b0b0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
-0001b0c0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
-0001b0d0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
-0001b0e0: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
-0001b0f0: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
-0001b100: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
-0001b110: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
-0001b120: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
-0001b130: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
-0001b140: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
-0001b150: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
-0001b160: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
-0001b170: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
-0001b180: 3e3e 3e20 665f 6a69 7474 6564 203d 206a  >>> f_jitted = j
-0001b190: 6178 2e6a 6974 286e 6e2e 696e 6974 2866  ax.jit(nn.init(f
-0001b1a0: 2c20 666f 6f29 290a 2020 2020 3e3e 3e20  , foo)).    >>> 
-0001b1b0: 7661 7269 6162 6c65 7320 3d20 665f 6a69  variables = f_ji
-0001b1c0: 7474 6564 286a 6178 2e72 616e 646f 6d2e  tted(jax.random.
-0001b1d0: 6b65 7928 3029 2c20 6a6e 702e 6f6e 6573  key(0), jnp.ones
-0001b1e0: 2828 312c 2033 2929 290a 0a20 2041 7267  ((1, 3)))..  Arg
-0001b1f0: 733a 0a20 2020 2066 6e3a 2054 6865 2066  s:.    fn: The f
-0001b200: 756e 6374 696f 6e20 7468 6174 2073 686f  unction that sho
-0001b210: 756c 6420 6265 2061 7070 6c69 6564 2e20  uld be applied. 
-0001b220: 5468 6520 6669 7273 7420 6172 6775 6d65  The first argume
-0001b230: 6e74 2070 6173 7365 6420 7769 6c6c 2062  nt passed will b
-0001b240: 650a 2020 2020 2020 6120 6d6f 6475 6c65  e.      a module
-0001b250: 2069 6e73 7461 6e63 6520 6f66 2074 6865   instance of the
-0001b260: 2060 606d 6f64 756c 6560 6020 7769 7468   ``module`` with
-0001b270: 2076 6172 6961 626c 6573 2061 6e64 2052   variables and R
-0001b280: 4e47 7320 626f 756e 6420 746f 2069 742e  NGs bound to it.
-0001b290: 0a20 2020 206d 6f64 756c 653a 2054 6865  .    module: The
-0001b2a0: 2060 604d 6f64 756c 6560 6020 7468 6174   ``Module`` that
-0001b2b0: 2077 696c 6c20 6265 2075 7365 6420 746f   will be used to
-0001b2c0: 2062 696e 6420 7661 7269 6162 6c65 7320   bind variables 
-0001b2d0: 616e 6420 524e 4773 2074 6f2e 2054 6865  and RNGs to. The
-0001b2e0: 0a20 2020 2020 2060 604d 6f64 756c 6560  .      ``Module`
-0001b2f0: 6020 7061 7373 6564 2061 7320 7468 6520  ` passed as the 
-0001b300: 6669 7273 7420 6172 6775 6d65 6e74 2074  first argument t
-0001b310: 6f20 6060 666e 6060 2077 696c 6c20 6265  o ``fn`` will be
-0001b320: 2061 2063 6c6f 6e65 206f 660a 2020 2020   a clone of.    
-0001b330: 2020 6d6f 6475 6c65 2e0a 2020 2020 6d75    module..    mu
-0001b340: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
-0001b350: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
-0001b360: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
-0001b370: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
-0001b380: 6f75 6c64 2062 650a 2020 2020 2020 7472  ould be.      tr
-0001b390: 6561 7465 6420 6173 206d 7574 6162 6c65  eated as mutable
-0001b3a0: 3a20 6060 626f 6f6c 6060 3a20 616c 6c2f  : ``bool``: all/
-0001b3b0: 6e6f 2063 6f6c 6c65 6374 696f 6e73 2061  no collections a
-0001b3c0: 7265 206d 7574 6162 6c65 2e20 6060 7374  re mutable. ``st
-0001b3d0: 7260 603a 2054 6865 0a20 2020 2020 206e  r``: The.      n
-0001b3e0: 616d 6520 6f66 2061 2073 696e 676c 6520  ame of a single 
-0001b3f0: 6d75 7461 626c 6520 636f 6c6c 6563 7469  mutable collecti
-0001b400: 6f6e 2e20 6060 6c69 7374 6060 3a20 4120  on. ``list``: A 
-0001b410: 6c69 7374 206f 6620 6e61 6d65 7320 6f66  list of names of
-0001b420: 206d 7574 6162 6c65 0a20 2020 2020 2063   mutable.      c
-0001b430: 6f6c 6c65 6374 696f 6e73 2e20 4279 2064  ollections. By d
-0001b440: 6566 6175 6c74 2c20 616c 6c20 636f 6c6c  efault, all coll
-0001b450: 6563 7469 6f6e 7320 6578 6365 7074 2022  ections except "
-0001b460: 696e 7465 726d 6564 6961 7465 7322 2061  intermediates" a
-0001b470: 7265 0a20 2020 2020 206d 7574 6162 6c65  re.      mutable
-0001b480: 2e0a 2020 2020 6361 7074 7572 655f 696e  ..    capture_in
-0001b490: 7465 726d 6564 6961 7465 733a 2049 6620  termediates: If 
-0001b4a0: 6054 7275 6560 2c20 6361 7074 7572 6573  `True`, captures
-0001b4b0: 2069 6e74 6572 6d65 6469 6174 6520 7265   intermediate re
-0001b4c0: 7475 726e 2076 616c 7565 7320 6f66 2061  turn values of a
-0001b4d0: 6c6c 0a20 2020 2020 204d 6f64 756c 6573  ll.      Modules
-0001b4e0: 2069 6e73 6964 6520 7468 6520 2269 6e74   inside the "int
-0001b4f0: 6572 6d65 6469 6174 6573 2220 636f 6c6c  ermediates" coll
-0001b500: 6563 7469 6f6e 2e20 4279 2064 6566 6175  ection. By defau
-0001b510: 6c74 2c20 6f6e 6c79 2074 6865 2072 6574  lt, only the ret
-0001b520: 7572 6e0a 2020 2020 2020 7661 6c75 6573  urn.      values
-0001b530: 206f 6620 616c 6c20 605f 5f63 616c 6c5f   of all `__call_
-0001b540: 5f60 206d 6574 686f 6473 2061 7265 2073  _` methods are s
-0001b550: 746f 7265 642e 2041 2066 756e 6374 696f  tored. A functio
-0001b560: 6e20 6361 6e20 6265 2070 6173 7365 6420  n can be passed 
-0001b570: 746f 0a20 2020 2020 2063 6861 6e67 6520  to.      change 
-0001b580: 7468 6520 6669 6c74 6572 2062 6568 6176  the filter behav
-0001b590: 696f 722e 2054 6865 2066 696c 7465 7220  ior. The filter 
-0001b5a0: 6675 6e63 7469 6f6e 2074 616b 6573 2074  function takes t
-0001b5b0: 6865 204d 6f64 756c 6520 696e 7374 616e  he Module instan
-0001b5c0: 6365 0a20 2020 2020 2061 6e64 206d 6574  ce.      and met
-0001b5d0: 686f 6420 6e61 6d65 2061 6e64 2072 6574  hod name and ret
-0001b5e0: 7572 6e73 2061 2062 6f6f 6c20 696e 6469  urns a bool indi
-0001b5f0: 6361 7469 6e67 2077 6865 7468 6572 2074  cating whether t
-0001b600: 6865 206f 7574 7075 7420 6f66 2074 6861  he output of tha
-0001b610: 740a 2020 2020 2020 6d65 7468 6f64 2069  t.      method i
-0001b620: 6e76 6f63 6174 696f 6e20 7368 6f75 6c64  nvocation should
-0001b630: 2062 6520 7374 6f72 6564 2e0a 0a20 2052   be stored...  R
-0001b640: 6574 7572 6e73 3a0a 2020 2020 5468 6520  eturns:.    The 
-0001b650: 696e 6974 2066 756e 6374 696f 6e20 7772  init function wr
-0001b660: 6170 7069 6e67 2060 6066 6e60 602e 0a20  apping ``fn``.. 
-0001b670: 2022 2222 0a20 2069 6e69 745f 666e 203d   """.  init_fn =
-0001b680: 2069 6e69 745f 7769 7468 5f6f 7574 7075   init_with_outpu
-0001b690: 7428 666e 2c20 6d6f 6475 6c65 2c20 6d75  t(fn, module, mu
-0001b6a0: 7461 626c 652c 2063 6170 7475 7265 5f69  table, capture_i
-0001b6b0: 6e74 6572 6d65 6469 6174 6573 290a 0a20  ntermediates).. 
-0001b6c0: 2040 6675 6e63 746f 6f6c 732e 7772 6170   @functools.wrap
-0001b6d0: 7328 696e 6974 5f66 6e29 0a20 2064 6566  s(init_fn).  def
-0001b6e0: 2069 6e69 745f 7772 6170 7065 7228 2a61   init_wrapper(*a
-0001b6f0: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
-0001b700: 2020 2020 7265 7475 726e 2069 6e69 745f      return init_
-0001b710: 666e 282a 6172 6773 2c20 2a2a 6b77 6172  fn(*args, **kwar
-0001b720: 6773 295b 315d 0a0a 2020 7265 7475 726e  gs)[1]..  return
-0001b730: 2069 6e69 745f 7772 6170 7065 720a 0a0a   init_wrapper...
-0001b740: 2320 544f 444f 2863 6761 7263 6961 6529  # TODO(cgarciae)
-0001b750: 3a20 7765 2061 7265 2064 6566 696e 696e  : we are definin
-0001b760: 6720 436f 6d70 6163 744e 616d 6553 636f  g CompactNameSco
-0001b770: 7065 206a 7573 7420 746f 0a23 2061 766f  pe just to.# avo
-0001b780: 6964 2061 2070 7974 7970 6520 6275 6720  id a pytype bug 
-0001b790: 7769 7468 2074 6865 2046 6c61 7820 6f76  with the Flax ov
-0001b7a0: 6572 6c61 792e 2057 6520 7368 6f75 6c64  erlay. We should
-0001b7b0: 2061 696d 2074 6f0a 2320 7265 6d6f 7665   aim to.# remove
-0001b7c0: 2069 6e20 7468 6520 6174 2073 6f6d 6520   in the at some 
-0001b7d0: 706f 696e 7420 6173 2069 7473 206e 6f74  point as its not
-0001b7e0: 2065 7267 6f6e 6f6d 6963 2e0a 6966 206e   ergonomic..if n
-0001b7f0: 6f74 2074 7970 696e 672e 5459 5045 5f43  ot typing.TYPE_C
-0001b800: 4845 434b 494e 473a 0a0a 2020 636c 6173  HECKING:..  clas
-0001b810: 7320 436f 6d70 6163 744e 616d 6553 636f  s CompactNameSco
-0001b820: 7065 284d 6f64 756c 6529 3a0a 2020 2020  pe(Module):.    
-0001b830: 666e 3a20 4361 6c6c 6162 6c65 0a20 2020  fn: Callable.   
-0001b840: 206d 6f64 756c 655f 666e 3a20 4361 6c6c   module_fn: Call
-0001b850: 6162 6c65 5b5b 5d2c 204d 6f64 756c 655d  able[[], Module]
-0001b860: 0a0a 2020 2020 4063 6f6d 7061 6374 0a20  ..    @compact. 
-0001b870: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
-0001b880: 7365 6c66 2c20 2a61 7267 732c 202a 2a6b  self, *args, **k
-0001b890: 7761 7267 7329 202d 3e20 416e 793a 0a20  wargs) -> Any:. 
-0001b8a0: 2020 2020 2072 6574 7572 6e20 7365 6c66       return self
-0001b8b0: 2e66 6e28 7365 6c66 2e6d 6f64 756c 655f  .fn(self.module_
-0001b8c0: 666e 2829 2c20 2a61 7267 732c 202a 2a6b  fn(), *args, **k
-0001b8d0: 7761 7267 7329 0a65 6c73 653a 0a0a 2020  wargs).else:..  
-0001b8e0: 4064 6174 6163 6c61 7373 6573 2e64 6174  @dataclasses.dat
-0001b8f0: 6163 6c61 7373 0a20 2063 6c61 7373 2043  aclass.  class C
-0001b900: 6f6d 7061 6374 4e61 6d65 5363 6f70 653a  ompactNameScope:
-0001b910: 0a20 2020 2066 6e3a 2043 616c 6c61 626c  .    fn: Callabl
-0001b920: 650a 2020 2020 6d6f 6475 6c65 5f66 6e3a  e.    module_fn:
-0001b930: 2043 616c 6c61 626c 650a 2020 2020 6e61   Callable.    na
-0001b940: 6d65 3a20 7374 720a 0a20 2020 2064 6566  me: str..    def
-0001b950: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
-0001b960: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
-0001b970: 202d 3e20 416e 793a 0a20 2020 2020 202e   -> Any:.      .
-0001b980: 2e2e 0a                                  ...
+00017b70: e294 80e2 9480 e294 80e2 9480 e294 a40a  ................
+00017b80: 2020 2020 2020 e294 8220 4465 6e73 655f        ... Dense_
+00017b90: 3120 e294 8220 4465 6e73 6520 20e2 9482  1 ... Dense  ...
+00017ba0: 2066 6c6f 6174 3332 5b31 362c 345d 20e2   float32[16,4] .
+00017bb0: 9482 2066 6c6f 6174 3332 5b31 362c 325d  .. float32[16,2]
+00017bc0: 20e2 9482 2032 3838 2020 20e2 9482 2038   ... 288   ... 8
+00017bd0: 3430 2020 2020 2020 20e2 9482 2062 6961  40       ... bia
+00017be0: 733a 2020 2020 2020 2020 2020 20e2 9482  s:           ...
+00017bf0: 0a20 2020 2020 20e2 9482 2020 2020 2020  .      ...      
+00017c00: 2020 20e2 9482 2020 2020 2020 2020 e294     ...        ..
+00017c10: 8220 2020 2020 2020 2020 2020 2020 2020  .               
+00017c20: e294 8220 2020 2020 2020 2020 2020 2020  ...             
+00017c30: 2020 e294 8220 2020 2020 2020 e294 8220    ...       ... 
+00017c40: 2020 2020 2020 2020 2020 e294 8220 666c            ... fl
+00017c50: 6f61 7433 325b 325d 2020 2020 2020 e294  oat32[2]      ..
+00017c60: 820a 2020 2020 2020 e294 8220 2020 2020  ..      ...     
+00017c70: 2020 2020 e294 8220 2020 2020 2020 20e2      ...        .
+00017c80: 9482 2020 2020 2020 2020 2020 2020 2020  ..              
+00017c90: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00017ca0: 2020 20e2 9482 2020 2020 2020 20e2 9482     ...       ...
+00017cb0: 2020 2020 2020 2020 2020 20e2 9482 206b             ... k
+00017cc0: 6572 6e65 6c3a 2020 2020 2020 2020 20e2  ernel:         .
+00017cd0: 9482 0a20 2020 2020 20e2 9482 2020 2020  ...      ...    
+00017ce0: 2020 2020 20e2 9482 2020 2020 2020 2020       ...        
+00017cf0: e294 8220 2020 2020 2020 2020 2020 2020  ...             
+00017d00: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
+00017d10: 2020 2020 e294 8220 2020 2020 2020 e294      ...       ..
+00017d20: 8220 2020 2020 2020 2020 2020 e294 8220  .           ... 
+00017d30: 666c 6f61 7433 325b 342c 325d 2020 2020  float32[4,2]    
+00017d40: e294 820a 2020 2020 2020 e294 8220 2020  ....      ...   
+00017d50: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00017d60: 20e2 9482 2020 2020 2020 2020 2020 2020   ...            
+00017d70: 2020 20e2 9482 2020 2020 2020 2020 2020     ...          
+00017d80: 2020 2020 20e2 9482 2020 2020 2020 20e2       ...       .
+00017d90: 9482 2020 2020 2020 2020 2020 20e2 9482  ..           ...
+00017da0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00017db0: 20e2 9482 0a20 2020 2020 20e2 9482 2020   ....      ...  
+00017dc0: 2020 2020 2020 20e2 9482 2020 2020 2020         ...      
+00017dd0: 2020 e294 8220 2020 2020 2020 2020 2020    ...           
+00017de0: 2020 2020 e294 8220 2020 2020 2020 2020      ...         
+00017df0: 2020 2020 2020 e294 8220 2020 2020 2020        ...       
+00017e00: e294 8220 2020 2020 2020 2020 2020 e294  ...           ..
+00017e10: 8220 3130 2028 3430 2042 2920 2020 2020  . 10 (40 B)     
+00017e20: 2020 e294 820a 2020 2020 2020 e294 9ce2    ....      ....
+00017e30: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017e40: 80e2 9480 e294 80e2 9480 e294 bce2 9480  ................
+00017e50: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017e60: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017e70: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017e80: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017e90: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017ea0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017eb0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017ec0: 9480 e294 80e2 94bc e294 80e2 9480 e294  ................
+00017ed0: 80e2 9480 e294 80e2 9480 e294 80e2 94bc  ................
+00017ee0: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017ef0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017f00: 80e2 94bc e294 80e2 9480 e294 80e2 9480  ................
+00017f10: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00017f20: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00017f30: 80e2 9480 e294 80e2 94a4 0a20 2020 2020  ...........     
+00017f40: 20e2 9482 2020 2020 2020 2020 20e2 9482   ...         ...
+00017f50: 2020 2020 2020 2020 e294 8220 2020 2020          ...     
+00017f60: 2020 2020 2020 2020 2020 e294 8220 2020            ...   
+00017f70: 2020 2020 2020 2020 2020 2020 e294 8220              ... 
+00017f80: 2020 2020 2020 e294 8220 2020 2020 546f        ...     To
+00017f90: 7461 6c20 e294 8220 3530 2028 3230 3020  tal ... 50 (200 
+00017fa0: 4229 2020 2020 2020 e294 820a 2020 2020  B)      ....    
+00017fb0: 2020 e294 94e2 9480 e294 80e2 9480 e294    ..............
+00017fc0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00017fd0: e294 b4e2 9480 e294 80e2 9480 e294 80e2  ................
+00017fe0: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
+00017ff0: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00018000: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00018010: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
+00018020: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00018030: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+00018040: 9480 e294 80e2 9480 e294 80e2 94b4 e294  ................
+00018050: 80e2 9480 e294 80e2 9480 e294 80e2 9480  ................
+00018060: e294 80e2 94b4 e294 80e2 9480 e294 80e2  ................
+00018070: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+00018080: 80e2 9480 e294 80e2 94b4 e294 80e2 9480  ................
+00018090: e294 80e2 9480 e294 80e2 9480 e294 80e2  ................
+000180a0: 9480 e294 80e2 9480 e294 80e2 9480 e294  ................
+000180b0: 80e2 9480 e294 80e2 9480 e294 80e2 9498  ................
+000180c0: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              
+000180d0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000180e0: 2020 2020 2020 546f 7461 6c20 5061 7261        Total Para
+000180f0: 6d65 7465 7273 3a20 3530 2028 3230 3020  meters: 50 (200 
+00018100: 4229 0a0a 2020 2020 2a2a 4e6f 7465 2a2a  B)..    **Note**
+00018110: 3a20 726f 7773 206f 7264 6572 2069 6e20  : rows order in 
+00018120: 7468 6520 7461 626c 6520 646f 6573 206e  the table does n
+00018130: 6f74 2072 6570 7265 7365 6e74 2065 7865  ot represent exe
+00018140: 6375 7469 6f6e 206f 7264 6572 2c0a 2020  cution order,.  
+00018150: 2020 696e 7374 6561 6420 6974 2061 6c69    instead it ali
+00018160: 676e 7320 7769 7468 2074 6865 206f 7264  gns with the ord
+00018170: 6572 206f 6620 6b65 7973 2069 6e20 6060  er of keys in ``
+00018180: 7661 7269 6162 6c65 7360 6020 7768 6963  variables`` whic
+00018190: 6820 6172 6520 736f 7274 6564 0a20 2020  h are sorted.   
+000181a0: 2061 6c70 6861 6265 7469 6361 6c6c 792e   alphabetically.
+000181b0: 0a0a 2020 2020 2a2a 4e6f 7465 2a2a 3a20  ..    **Note**: 
+000181c0: 6060 766a 705f 666c 6f70 7360 6020 7265  ``vjp_flops`` re
+000181d0: 7475 726e 7320 6060 3060 6020 6966 2074  turns ``0`` if t
+000181e0: 6865 206d 6f64 756c 6520 6973 206e 6f74  he module is not
+000181f0: 2064 6966 6665 7265 6e74 6961 626c 652e   differentiable.
+00018200: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    
+00018210: 2020 726e 6773 3a20 5468 6520 726e 6773    rngs: The rngs
+00018220: 2066 6f72 2074 6865 2076 6172 6961 626c   for the variabl
+00018230: 6520 636f 6c6c 6563 7469 6f6e 7320 6173  e collections as
+00018240: 2070 6173 7365 6420 746f 2060 604d 6f64   passed to ``Mod
+00018250: 756c 652e 696e 6974 6060 2e0a 2020 2020  ule.init``..    
+00018260: 2020 2a61 7267 733a 2054 6865 2061 7267    *args: The arg
+00018270: 756d 656e 7473 2074 6f20 7468 6520 666f  uments to the fo
+00018280: 7277 6172 6420 636f 6d70 7574 6174 696f  rward computatio
+00018290: 6e2e 0a20 2020 2020 2064 6570 7468 3a20  n..      depth: 
+000182a0: 636f 6e74 726f 6c73 2068 6f77 206d 616e  controls how man
+000182b0: 7920 7375 626d 6f64 756c 6520 6465 6570  y submodule deep
+000182c0: 2074 6865 2073 756d 6d61 7279 2063 616e   the summary can
+000182d0: 2067 6f2e 2042 7920 6465 6661 756c 742c   go. By default,
+000182e0: 0a20 2020 2020 2020 2069 7473 2060 604e  .        its ``N
+000182f0: 6f6e 6560 6020 7768 6963 6820 6d65 616e  one`` which mean
+00018300: 7320 6e6f 206c 696d 6974 2e20 4966 2061  s no limit. If a
+00018310: 2073 7562 6d6f 6475 6c65 2069 7320 6e6f   submodule is no
+00018320: 7420 7368 6f77 6e20 6265 6361 7573 6520  t shown because 
+00018330: 6f66 0a20 2020 2020 2020 2074 6865 2064  of.        the d
+00018340: 6570 7468 206c 696d 6974 2c20 6974 7320  epth limit, its 
+00018350: 7061 7261 6d65 7465 7220 636f 756e 7420  parameter count 
+00018360: 616e 6420 6279 7465 7320 7769 6c6c 2062  and bytes will b
+00018370: 6520 6164 6465 6420 746f 2074 6865 2072  e added to the r
+00018380: 6f77 0a20 2020 2020 2020 206f 6620 6974  ow.        of it
+00018390: 7320 6669 7273 7420 7368 6f77 6e20 616e  s first shown an
+000183a0: 6365 7374 6f72 2073 7563 6820 7468 6174  cestor such that
+000183b0: 2074 6865 2073 756d 206f 6620 616c 6c20   the sum of all 
+000183c0: 726f 7773 2061 6c77 6179 7320 6164 6473  rows always adds
+000183d0: 0a20 2020 2020 2020 2075 7020 746f 2074  .        up to t
+000183e0: 6865 2074 6f74 616c 206e 756d 6265 7220  he total number 
+000183f0: 6f66 2070 6172 616d 6574 6572 7320 6f66  of parameters of
+00018400: 2074 6865 204d 6f64 756c 652e 0a20 2020   the Module..   
+00018410: 2020 2073 686f 775f 7265 7065 6174 6564     show_repeated
+00018420: 3a20 4966 2060 6054 7275 6560 602c 2072  : If ``True``, r
+00018430: 6570 6561 7465 6420 6361 6c6c 7320 746f  epeated calls to
+00018440: 2074 6865 2073 616d 6520 6d6f 6475 6c65   the same module
+00018450: 2077 696c 6c20 6265 2073 686f 776e 0a20   will be shown. 
+00018460: 2020 2020 2020 2069 6e20 7468 6520 7461         in the ta
+00018470: 626c 652c 206f 7468 6572 7769 7365 206f  ble, otherwise o
+00018480: 6e6c 7920 7468 6520 6669 7273 7420 6361  nly the first ca
+00018490: 6c6c 2077 696c 6c20 6265 2073 686f 776e  ll will be shown
+000184a0: 2e20 4465 6661 756c 7420 6973 0a20 2020  . Default is.   
+000184b0: 2020 2020 2060 6046 616c 7365 6060 2e0a       ``False``..
+000184c0: 2020 2020 2020 6d75 7461 626c 653a 2043        mutable: C
+000184d0: 616e 2062 6520 626f 6f6c 2c20 7374 722c  an be bool, str,
+000184e0: 206f 7220 6c69 7374 2e20 5370 6563 6966   or list. Specif
+000184f0: 6965 7320 7768 6963 6820 636f 6c6c 6563  ies which collec
+00018500: 7469 6f6e 7320 7368 6f75 6c64 2062 650a  tions should be.
+00018510: 2020 2020 2020 2020 7472 6561 7465 6420          treated 
+00018520: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
+00018530: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
+00018540: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
+00018550: 6162 6c65 2e20 6060 7374 7260 603a 0a20  able. ``str``:. 
+00018560: 2020 2020 2020 2054 6865 206e 616d 6520         The name 
+00018570: 6f66 2061 2073 696e 676c 6520 6d75 7461  of a single muta
+00018580: 626c 6520 636f 6c6c 6563 7469 6f6e 2e20  ble collection. 
+00018590: 6060 6c69 7374 6060 3a20 4120 6c69 7374  ``list``: A list
+000185a0: 206f 6620 6e61 6d65 7320 6f66 0a20 2020   of names of.   
+000185b0: 2020 2020 206d 7574 6162 6c65 2063 6f6c       mutable col
+000185c0: 6c65 6374 696f 6e73 2e20 4279 2064 6566  lections. By def
+000185d0: 6175 6c74 2c20 616c 6c20 636f 6c6c 6563  ault, all collec
+000185e0: 7469 6f6e 7320 6578 6365 7074 2027 696e  tions except 'in
+000185f0: 7465 726d 6564 6961 7465 7327 0a20 2020  termediates'.   
+00018600: 2020 2020 2061 7265 206d 7574 6162 6c65       are mutable
+00018610: 2e0a 2020 2020 2020 636f 6e73 6f6c 655f  ..      console_
+00018620: 6b77 6172 6773 3a20 416e 206f 7074 696f  kwargs: An optio
+00018630: 6e61 6c20 6469 6374 696f 6e61 7279 2077  nal dictionary w
+00018640: 6974 6820 6164 6469 7469 6f6e 616c 206b  ith additional k
+00018650: 6579 776f 7264 2061 7267 756d 656e 7473  eyword arguments
+00018660: 0a20 2020 2020 2020 2074 6861 7420 6172  .        that ar
+00018670: 6520 7061 7373 6564 2074 6f20 6060 7269  e passed to ``ri
+00018680: 6368 2e63 6f6e 736f 6c65 2e43 6f6e 736f  ch.console.Conso
+00018690: 6c65 6060 2077 6865 6e20 7265 6e64 6572  le`` when render
+000186a0: 696e 6720 7468 6520 7461 626c 652e 0a20  ing the table.. 
+000186b0: 2020 2020 2020 2044 6566 6175 6c74 2061         Default a
+000186c0: 7267 756d 656e 7473 2061 7265 2060 607b  rguments are ``{
+000186d0: 2766 6f72 6365 5f74 6572 6d69 6e61 6c27  'force_terminal'
+000186e0: 3a20 5472 7565 2c20 2766 6f72 6365 5f6a  : True, 'force_j
+000186f0: 7570 7974 6572 273a 0a20 2020 2020 2020  upyter':.       
+00018700: 2046 616c 7365 7d60 602e 0a20 2020 2020   False}``..     
+00018710: 2074 6162 6c65 5f6b 7761 7267 733a 2041   table_kwargs: A
+00018720: 6e20 6f70 7469 6f6e 616c 2064 6963 7469  n optional dicti
+00018730: 6f6e 6172 7920 7769 7468 2061 6464 6974  onary with addit
+00018740: 696f 6e61 6c20 6b65 7977 6f72 6420 6172  ional keyword ar
+00018750: 6775 6d65 6e74 730a 2020 2020 2020 2020  guments.        
+00018760: 7468 6174 2061 7265 2070 6173 7365 6420  that are passed 
+00018770: 746f 2060 6072 6963 682e 7461 626c 652e  to ``rich.table.
+00018780: 5461 626c 6560 6020 636f 6e73 7472 7563  Table`` construc
+00018790: 746f 722e 0a20 2020 2020 2063 6f6c 756d  tor..      colum
+000187a0: 6e5f 6b77 6172 6773 3a20 416e 206f 7074  n_kwargs: An opt
+000187b0: 696f 6e61 6c20 6469 6374 696f 6e61 7279  ional dictionary
+000187c0: 2077 6974 6820 6164 6469 7469 6f6e 616c   with additional
+000187d0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
+000187e0: 7473 0a20 2020 2020 2020 2074 6861 7420  ts.        that 
+000187f0: 6172 6520 7061 7373 6564 2074 6f20 6060  are passed to ``
+00018800: 7269 6368 2e74 6162 6c65 2e54 6162 6c65  rich.table.Table
+00018810: 2e61 6464 5f63 6f6c 756d 6e60 6020 7768  .add_column`` wh
+00018820: 656e 2061 6464 696e 6720 636f 6c75 6d6e  en adding column
+00018830: 7320 746f 0a20 2020 2020 2020 2074 6865  s to.        the
+00018840: 2074 6162 6c65 2e0a 2020 2020 2020 636f   table..      co
+00018850: 6d70 7574 655f 666c 6f70 733a 2077 6865  mpute_flops: whe
+00018860: 7468 6572 2074 6f20 696e 636c 7564 6520  ther to include 
+00018870: 6120 6060 666c 6f70 7360 6020 636f 6c75  a ``flops`` colu
+00018880: 6d6e 2069 6e20 7468 6520 7461 626c 6520  mn in the table 
+00018890: 6c69 7374 696e 670a 2020 2020 2020 2020  listing.        
+000188a0: 7468 6520 6573 7469 6d61 7465 6420 464c  the estimated FL
+000188b0: 4f50 7320 636f 7374 206f 6620 6561 6368  OPs cost of each
+000188c0: 206d 6f64 756c 6520 666f 7277 6172 6420   module forward 
+000188d0: 7061 7373 2e20 446f 6573 2069 6e63 7572  pass. Does incur
+000188e0: 2061 6374 7561 6c0a 2020 2020 2020 2020   actual.        
+000188f0: 6f6e 2d64 6576 6963 6520 636f 6d70 7574  on-device comput
+00018900: 6174 696f 6e20 2f20 636f 6d70 696c 6174  ation / compilat
+00018910: 696f 6e20 2f20 6d65 6d6f 7279 2061 6c6c  ion / memory all
+00018920: 6f63 6174 696f 6e2c 2062 7574 2073 7469  ocation, but sti
+00018930: 6c6c 0a20 2020 2020 2020 2069 6e74 726f  ll.        intro
+00018940: 6475 6365 7320 6f76 6572 6865 6164 2066  duces overhead f
+00018950: 6f72 206c 6172 6765 206d 6f64 756c 6573  or large modules
+00018960: 2028 652e 672e 2065 7874 7261 2032 3020   (e.g. extra 20 
+00018970: 7365 636f 6e64 7320 666f 7220 610a 2020  seconds for a.  
+00018980: 2020 2020 2020 5374 6162 6c65 2044 6966        Stable Dif
+00018990: 6675 7369 6f6e 2773 2055 4e65 742c 2077  fusion's UNet, w
+000189a0: 6865 7265 6173 206f 7468 6572 7769 7365  hereas otherwise
+000189b0: 2074 6162 756c 6174 696f 6e20 776f 756c   tabulation woul
+000189c0: 6420 6669 6e69 7368 2069 6e20 350a 2020  d finish in 5.  
+000189d0: 2020 2020 2020 7365 636f 6e64 7329 2e0a        seconds)..
+000189e0: 2020 2020 2020 636f 6d70 7574 655f 766a        compute_vj
+000189f0: 705f 666c 6f70 733a 2077 6865 7468 6572  p_flops: whether
+00018a00: 2074 6f20 696e 636c 7564 6520 6120 6060   to include a ``
+00018a10: 766a 705f 666c 6f70 7360 6020 636f 6c75  vjp_flops`` colu
+00018a20: 6d6e 2069 6e20 7468 6520 7461 626c 650a  mn in the table.
+00018a30: 2020 2020 2020 2020 6c69 7374 696e 6720          listing 
+00018a40: 7468 6520 6573 7469 6d61 7465 6420 464c  the estimated FL
+00018a50: 4f50 7320 636f 7374 206f 6620 6561 6368  OPs cost of each
+00018a60: 206d 6f64 756c 6520 6261 636b 7761 7264   module backward
+00018a70: 2070 6173 732e 0a20 2020 2020 2020 2049   pass..        I
+00018a80: 6e74 726f 6475 6365 7320 6120 636f 6d70  ntroduces a comp
+00018a90: 7574 6520 6f76 6572 6865 6164 206f 6620  ute overhead of 
+00018aa0: 6162 6f75 7420 322d 3358 206f 6620 6060  about 2-3X of ``
+00018ab0: 636f 6d70 7574 655f 666c 6f70 7360 602e  compute_flops``.
+00018ac0: 0a20 2020 2020 202a 2a6b 7761 7267 733a  .      **kwargs:
+00018ad0: 206b 6579 776f 7264 2061 7267 756d 656e   keyword argumen
+00018ae0: 7473 2074 6f20 7061 7373 2074 6f20 7468  ts to pass to th
+00018af0: 6520 666f 7277 6172 6420 636f 6d70 7574  e forward comput
+00018b00: 6174 696f 6e2e 0a0a 2020 2020 5265 7475  ation...    Retu
+00018b10: 726e 733a 0a20 2020 2020 2041 2073 7472  rns:.      A str
+00018b20: 696e 6720 7375 6d6d 6172 697a 696e 6720  ing summarizing 
+00018b30: 7468 6520 4d6f 6475 6c65 2e0a 2020 2020  the Module..    
+00018b40: 2222 220a 2020 2020 6672 6f6d 2066 6c61  """.    from fla
+00018b50: 782e 6c69 6e65 6e20 696d 706f 7274 2073  x.linen import s
+00018b60: 756d 6d61 7279 0a0a 2020 2020 7461 6275  ummary..    tabu
+00018b70: 6c61 7465 5f66 6e20 3d20 7375 6d6d 6172  late_fn = summar
+00018b80: 792e 7461 6275 6c61 7465 280a 2020 2020  y.tabulate(.    
+00018b90: 2020 7365 6c66 2c0a 2020 2020 2020 726e    self,.      rn
+00018ba0: 6773 2c0a 2020 2020 2020 6465 7074 683d  gs,.      depth=
+00018bb0: 6465 7074 682c 0a20 2020 2020 2073 686f  depth,.      sho
+00018bc0: 775f 7265 7065 6174 6564 3d73 686f 775f  w_repeated=show_
+00018bd0: 7265 7065 6174 6564 2c0a 2020 2020 2020  repeated,.      
+00018be0: 6d75 7461 626c 653d 6d75 7461 626c 652c  mutable=mutable,
+00018bf0: 0a20 2020 2020 2063 6f6e 736f 6c65 5f6b  .      console_k
+00018c00: 7761 7267 733d 636f 6e73 6f6c 655f 6b77  wargs=console_kw
+00018c10: 6172 6773 2c0a 2020 2020 2020 7461 626c  args,.      tabl
+00018c20: 655f 6b77 6172 6773 3d74 6162 6c65 5f6b  e_kwargs=table_k
+00018c30: 7761 7267 732c 0a20 2020 2020 2063 6f6c  wargs,.      col
+00018c40: 756d 6e5f 6b77 6172 6773 3d63 6f6c 756d  umn_kwargs=colum
+00018c50: 6e5f 6b77 6172 6773 2c0a 2020 2020 2020  n_kwargs,.      
+00018c60: 636f 6d70 7574 655f 666c 6f70 733d 636f  compute_flops=co
+00018c70: 6d70 7574 655f 666c 6f70 732c 0a20 2020  mpute_flops,.   
+00018c80: 2020 2063 6f6d 7075 7465 5f76 6a70 5f66     compute_vjp_f
+00018c90: 6c6f 7073 3d63 6f6d 7075 7465 5f76 6a70  lops=compute_vjp
+00018ca0: 5f66 6c6f 7073 2c0a 2020 2020 290a 2020  _flops,.    ).  
+00018cb0: 2020 7265 7475 726e 2074 6162 756c 6174    return tabulat
+00018cc0: 655f 666e 282a 6172 6773 2c20 2a2a 6b77  e_fn(*args, **kw
+00018cd0: 6172 6773 290a 0a20 2064 6566 206d 6f64  args)..  def mod
+00018ce0: 756c 655f 7061 7468 7328 0a20 2020 2073  ule_paths(.    s
+00018cf0: 656c 662c 0a20 2020 2072 6e67 733a 2055  elf,.    rngs: U
+00018d00: 6e69 6f6e 5b50 524e 474b 6579 2c20 524e  nion[PRNGKey, RN
+00018d10: 4753 6571 7565 6e63 6573 5d2c 0a20 2020  GSequences],.   
+00018d20: 202a 6172 6773 2c0a 2020 2020 7368 6f77   *args,.    show
+00018d30: 5f72 6570 6561 7465 643a 2062 6f6f 6c20  _repeated: bool 
+00018d40: 3d20 4661 6c73 652c 0a20 2020 206d 7574  = False,.    mut
+00018d50: 6162 6c65 3a20 436f 6c6c 6563 7469 6f6e  able: Collection
+00018d60: 4669 6c74 6572 203d 2044 656e 794c 6973  Filter = DenyLis
+00018d70: 7428 2769 6e74 6572 6d65 6469 6174 6573  t('intermediates
+00018d80: 2729 2c0a 2020 2020 2a2a 6b77 6172 6773  '),.    **kwargs
+00018d90: 2c0a 2020 2920 2d3e 2064 6963 745b 7374  ,.  ) -> dict[st
+00018da0: 722c 2027 4d6f 6475 6c65 275d 3a0a 2020  r, 'Module']:.  
+00018db0: 2020 2222 2252 6574 7572 6e73 2061 2064    """Returns a d
+00018dc0: 6963 7469 6f6e 6172 7920 6d61 7070 696e  ictionary mappin
+00018dd0: 6720 6d6f 6475 6c65 2070 6174 6873 2074  g module paths t
+00018de0: 6f20 6d6f 6475 6c65 2069 6e73 7461 6e63  o module instanc
+00018df0: 6573 2e0a 0a20 2020 2054 6869 7320 6d65  es...    This me
+00018e00: 7468 6f64 2068 6173 2074 6865 2073 616d  thod has the sam
+00018e10: 6520 7369 676e 6174 7572 6520 616e 6420  e signature and 
+00018e20: 696e 7465 726e 616c 6c79 2063 616c 6c73  internally calls
+00018e30: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
+00018e40: 2c0a 2020 2020 6275 7420 696e 7374 6561  ,.    but instea
+00018e50: 6420 6f66 2072 6574 7572 6e69 6e67 2074  d of returning t
+00018e60: 6865 2076 6172 6961 626c 6573 2c20 6974  he variables, it
+00018e70: 2072 6574 7572 6e73 2061 2064 6963 7469   returns a dicti
+00018e80: 6f6e 6172 7920 6d61 7070 696e 670a 2020  onary mapping.  
+00018e90: 2020 6d6f 6475 6c65 2070 6174 6873 2074    module paths t
+00018ea0: 6f20 756e 626f 756e 6465 6420 636f 7069  o unbounded copi
+00018eb0: 6573 206f 6620 6d6f 6475 6c65 2069 6e73  es of module ins
+00018ec0: 7461 6e63 6573 2074 6861 7420 7765 7265  tances that were
+00018ed0: 2075 7365 640a 2020 2020 6174 2072 756e   used.    at run
+00018ee0: 7469 6d65 2e20 6060 6d6f 6475 6c65 5f70  time. ``module_p
+00018ef0: 6174 6873 6060 2075 7365 7320 6060 6a61  aths`` uses ``ja
+00018f00: 782e 6576 616c 5f73 6861 7065 6060 2074  x.eval_shape`` t
+00018f10: 6f20 7275 6e20 7468 6520 666f 7277 6172  o run the forwar
+00018f20: 640a 2020 2020 636f 6d70 7574 6174 696f  d.    computatio
+00018f30: 6e20 7769 7468 6f75 7420 636f 6e73 756d  n without consum
+00018f40: 696e 6720 616e 7920 464c 4f50 7320 6f72  ing any FLOPs or
+00018f50: 2061 6c6c 6f63 6174 696e 6720 6d65 6d6f   allocating memo
+00018f60: 7279 2e0a 0a20 2020 2045 7861 6d70 6c65  ry...    Example
+00018f70: 3a3a 0a0a 2020 2020 2020 3e3e 3e20 696d  ::..      >>> im
+00018f80: 706f 7274 2066 6c61 782e 6c69 6e65 6e20  port flax.linen 
+00018f90: 6173 206e 6e0a 2020 2020 2020 3e3e 3e20  as nn.      >>> 
+00018fa0: 696d 706f 7274 206a 6178 2c20 6a61 782e  import jax, jax.
+00018fb0: 6e75 6d70 7920 6173 206a 6e70 0a0a 2020  numpy as jnp..  
+00018fc0: 2020 2020 3e3e 3e20 636c 6173 7320 466f      >>> class Fo
+00018fd0: 6f28 6e6e 2e4d 6f64 756c 6529 3a0a 2020  o(nn.Module):.  
+00018fe0: 2020 2020 2e2e 2e20 2020 406e 6e2e 636f      ...   @nn.co
+00018ff0: 6d70 6163 740a 2020 2020 2020 2e2e 2e20  mpact.      ... 
+00019000: 2020 6465 6620 5f5f 6361 6c6c 5f5f 2873    def __call__(s
+00019010: 656c 662c 2078 293a 0a20 2020 2020 202e  elf, x):.      .
+00019020: 2e2e 2020 2020 2068 203d 206e 6e2e 4465  ..     h = nn.De
+00019030: 6e73 6528 3429 2878 290a 2020 2020 2020  nse(4)(x).      
+00019040: 2e2e 2e20 2020 2020 7265 7475 726e 206e  ...     return n
+00019050: 6e2e 4465 6e73 6528 3229 2868 290a 0a20  n.Dense(2)(h).. 
+00019060: 2020 2020 203e 3e3e 2078 203d 206a 6e70       >>> x = jnp
+00019070: 2e6f 6e65 7328 2831 362c 2039 2929 0a20  .ones((16, 9)). 
+00019080: 2020 2020 203e 3e3e 206d 6f64 756c 6573       >>> modules
+00019090: 203d 2046 6f6f 2829 2e6d 6f64 756c 655f   = Foo().module_
+000190a0: 7061 7468 7328 6a61 782e 7261 6e64 6f6d  paths(jax.random
+000190b0: 2e6b 6579 2830 292c 2078 290a 2020 2020  .key(0), x).    
+000190c0: 2020 3e3e 3e20 7072 696e 7428 7b0a 2020    >>> print({.  
+000190d0: 2020 2020 2e2e 2e20 2020 2020 703a 2074      ...     p: t
+000190e0: 7970 6528 6d29 2e5f 5f6e 616d 655f 5f20  ype(m).__name__ 
+000190f0: 666f 7220 702c 206d 2069 6e20 6d6f 6475  for p, m in modu
+00019100: 6c65 732e 6974 656d 7328 290a 2020 2020  les.items().    
+00019110: 2020 2e2e 2e20 7d29 0a20 2020 2020 207b    ... }).      {
+00019120: 2727 3a20 2746 6f6f 272c 2027 4465 6e73  '': 'Foo', 'Dens
+00019130: 655f 3027 3a20 2744 656e 7365 272c 2027  e_0': 'Dense', '
+00019140: 4465 6e73 655f 3127 3a20 2744 656e 7365  Dense_1': 'Dense
+00019150: 277d 0a0a 2020 2020 4172 6773 3a0a 2020  '}..    Args:.  
+00019160: 2020 2020 726e 6773 3a20 5468 6520 726e      rngs: The rn
+00019170: 6773 2066 6f72 2074 6865 2076 6172 6961  gs for the varia
+00019180: 626c 6520 636f 6c6c 6563 7469 6f6e 7320  ble collections 
+00019190: 6173 2070 6173 7365 6420 746f 2060 604d  as passed to ``M
+000191a0: 6f64 756c 652e 696e 6974 6060 2e0a 2020  odule.init``..  
+000191b0: 2020 2020 2a61 7267 733a 2054 6865 2061      *args: The a
+000191c0: 7267 756d 656e 7473 2074 6f20 7468 6520  rguments to the 
+000191d0: 666f 7277 6172 6420 636f 6d70 7574 6174  forward computat
+000191e0: 696f 6e2e 0a20 2020 2020 2073 686f 775f  ion..      show_
+000191f0: 7265 7065 6174 6564 3a20 4966 2060 6054  repeated: If ``T
+00019200: 7275 6560 602c 2072 6570 6561 7465 6420  rue``, repeated 
+00019210: 6361 6c6c 7320 746f 2074 6865 2073 616d  calls to the sam
+00019220: 6520 6d6f 6475 6c65 2077 696c 6c20 6265  e module will be
+00019230: 0a20 2020 2020 2020 2073 686f 776e 2069  .        shown i
+00019240: 6e20 7468 6520 7461 626c 652c 206f 7468  n the table, oth
+00019250: 6572 7769 7365 206f 6e6c 7920 7468 6520  erwise only the 
+00019260: 6669 7273 7420 6361 6c6c 2077 696c 6c20  first call will 
+00019270: 6265 2073 686f 776e 2e0a 2020 2020 2020  be shown..      
+00019280: 2020 4465 6661 756c 7420 6973 2060 6046    Default is ``F
+00019290: 616c 7365 6060 2e0a 2020 2020 2020 6d75  alse``..      mu
+000192a0: 7461 626c 653a 2043 616e 2062 6520 626f  table: Can be bo
+000192b0: 6f6c 2c20 7374 722c 206f 7220 6c69 7374  ol, str, or list
+000192c0: 2e20 5370 6563 6966 6965 7320 7768 6963  . Specifies whic
+000192d0: 6820 636f 6c6c 6563 7469 6f6e 7320 7368  h collections sh
+000192e0: 6f75 6c64 0a20 2020 2020 2020 2062 6520  ould.        be 
+000192f0: 7472 6561 7465 6420 6173 206d 7574 6162  treated as mutab
+00019300: 6c65 3a20 6060 626f 6f6c 6060 3a20 616c  le: ``bool``: al
+00019310: 6c2f 6e6f 2063 6f6c 6c65 6374 696f 6e73  l/no collections
+00019320: 2061 7265 206d 7574 6162 6c65 2e0a 2020   are mutable..  
+00019330: 2020 2020 2020 6060 7374 7260 603a 2054        ``str``: T
+00019340: 6865 206e 616d 6520 6f66 2061 2073 696e  he name of a sin
+00019350: 676c 6520 6d75 7461 626c 6520 636f 6c6c  gle mutable coll
+00019360: 6563 7469 6f6e 2e20 6060 6c69 7374 6060  ection. ``list``
+00019370: 3a20 4120 6c69 7374 206f 660a 2020 2020  : A list of.    
+00019380: 2020 2020 6e61 6d65 7320 6f66 206d 7574      names of mut
+00019390: 6162 6c65 2063 6f6c 6c65 6374 696f 6e73  able collections
+000193a0: 2e20 4279 2064 6566 6175 6c74 2c20 616c  . By default, al
+000193b0: 6c20 636f 6c6c 6563 7469 6f6e 7320 6578  l collections ex
+000193c0: 6365 7074 0a20 2020 2020 2020 2027 696e  cept.        'in
+000193d0: 7465 726d 6564 6961 7465 7327 2061 7265  termediates' are
+000193e0: 206d 7574 6162 6c65 2e0a 2020 2020 2020   mutable..      
+000193f0: 2a2a 6b77 6172 6773 3a20 6b65 7977 6f72  **kwargs: keywor
+00019400: 6420 6172 6775 6d65 6e74 7320 746f 2070  d arguments to p
+00019410: 6173 7320 746f 2074 6865 2066 6f72 7761  ass to the forwa
+00019420: 7264 2063 6f6d 7075 7461 7469 6f6e 2e0a  rd computation..
+00019430: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  
+00019440: 2020 2020 4120 6469 6374 6069 6f6e 6172      A dict`ionar
+00019450: 7920 6d61 7070 696e 6720 6d6f 6475 6c65  y mapping module
+00019460: 2070 6174 6873 2074 6f20 6d6f 6475 6c65   paths to module
+00019470: 2069 6e73 7461 6e63 6573 2e0a 2020 2020   instances..    
+00019480: 2222 220a 2020 2020 6672 6f6d 2066 6c61  """.    from fla
+00019490: 782e 6c69 6e65 6e20 696d 706f 7274 2073  x.linen import s
+000194a0: 756d 6d61 7279 0a0a 2020 2020 7461 626c  ummary..    tabl
+000194b0: 6520 3d20 7375 6d6d 6172 792e 5f67 6574  e = summary._get
+000194c0: 5f6d 6f64 756c 655f 7461 626c 6528 0a20  _module_table(. 
+000194d0: 2020 2020 206d 6f64 756c 653d 7365 6c66       module=self
+000194e0: 2c0a 2020 2020 2020 6465 7074 683d 4e6f  ,.      depth=No
+000194f0: 6e65 2c0a 2020 2020 2020 7368 6f77 5f72  ne,.      show_r
+00019500: 6570 6561 7465 643d 7368 6f77 5f72 6570  epeated=show_rep
+00019510: 6561 7465 642c 0a20 2020 2020 2063 6f6d  eated,.      com
+00019520: 7075 7465 5f66 6c6f 7073 3d46 616c 7365  pute_flops=False
+00019530: 2c0a 2020 2020 2020 636f 6d70 7574 655f  ,.      compute_
+00019540: 766a 705f 666c 6f70 733d 4661 6c73 652c  vjp_flops=False,
+00019550: 0a20 2020 2029 2872 6e67 732c 202a 6172  .    )(rngs, *ar
+00019560: 6773 2c20 2a2a 6b77 6172 6773 2c20 6d75  gs, **kwargs, mu
+00019570: 7461 626c 653d 6d75 7461 626c 6529 0a0a  table=mutable)..
+00019580: 2020 2020 7265 7475 726e 207b 272f 272e      return {'/'.
+00019590: 6a6f 696e 2872 6f77 2e70 6174 6829 3a20  join(row.path): 
+000195a0: 726f 772e 6d6f 6475 6c65 5f63 6f70 7920  row.module_copy 
+000195b0: 666f 7220 726f 7720 696e 2074 6162 6c65  for row in table
+000195c0: 7d0a 0a0a 5f50 6172 656e 7454 7970 6520  }..._ParentType 
+000195d0: 3d20 556e 696f 6e5b 5479 7065 5b4d 6f64  = Union[Type[Mod
+000195e0: 756c 655d 2c20 5363 6f70 652c 2054 7970  ule], Scope, Typ
+000195f0: 655b 5f53 656e 7469 6e65 6c5d 2c20 4e6f  e[_Sentinel], No
+00019600: 6e65 5d0a 0a0a 6465 6620 6d65 7267 655f  ne]...def merge_
+00019610: 7061 7261 6d28 6e61 6d65 3a20 7374 722c  param(name: str,
+00019620: 2061 3a20 4f70 7469 6f6e 616c 5b54 5d2c   a: Optional[T],
+00019630: 2062 3a20 4f70 7469 6f6e 616c 5b54 5d29   b: Optional[T])
+00019640: 202d 3e20 543a 0a20 2022 2222 4d65 7267   -> T:.  """Merg
+00019650: 6573 2063 6f6e 7374 7275 6374 696f 6e2d  es construction-
+00019660: 2061 6e64 2063 616c 6c2d 7469 6d65 2061   and call-time a
+00019670: 7267 756d 656e 742e 0a0a 2020 5468 6973  rgument...  This
+00019680: 2069 7320 6120 7574 696c 6974 7920 666f   is a utility fo
+00019690: 7220 7375 7070 6f72 7469 6e67 2061 2070  r supporting a p
+000196a0: 6174 7465 726e 2077 6865 7265 2061 204d  attern where a M
+000196b0: 6f64 756c 6520 6879 7065 7270 6172 616d  odule hyperparam
+000196c0: 6574 6572 0a20 2063 616e 2062 6520 7061  eter.  can be pa
+000196d0: 7373 6564 2065 6974 6865 7220 746f 2060  ssed either to `
+000196e0: 605f 5f69 6e69 745f 5f60 6020 6f72 2060  `__init__`` or `
+000196f0: 605f 5f63 616c 6c5f 5f60 602c 2061 6e64  `__call__``, and
+00019700: 2074 6865 2076 616c 7565 2074 6861 7420   the value that 
+00019710: 6973 0a20 206e 6f74 2060 604e 6f6e 6560  is.  not ``None`
+00019720: 6020 7769 6c6c 2062 6520 7573 6564 2e0a  ` will be used..
+00019730: 0a20 2045 7861 6d70 6c65 3a3a 0a0a 2020  .  Example::..  
+00019740: 2020 3e3e 3e20 696d 706f 7274 2066 6c61    >>> import fla
+00019750: 782e 6c69 6e65 6e20 6173 206e 6e0a 2020  x.linen as nn.  
+00019760: 2020 3e3e 3e20 6672 6f6d 2074 7970 696e    >>> from typin
+00019770: 6720 696d 706f 7274 204f 7074 696f 6e61  g import Optiona
+00019780: 6c0a 0a20 2020 203e 3e3e 2063 6c61 7373  l..    >>> class
+00019790: 2046 6f6f 286e 6e2e 4d6f 6475 6c65 293a   Foo(nn.Module):
+000197a0: 0a20 2020 202e 2e2e 2020 2074 7261 696e  .    ...   train
+000197b0: 3a20 4f70 7469 6f6e 616c 5b62 6f6f 6c5d  : Optional[bool]
+000197c0: 203d 204e 6f6e 650a 0a20 2020 202e 2e2e   = None..    ...
+000197d0: 2020 2064 6566 205f 5f63 616c 6c5f 5f28     def __call__(
+000197e0: 7365 6c66 2c20 7472 6169 6e3a 204f 7074  self, train: Opt
+000197f0: 696f 6e61 6c5b 626f 6f6c 5d20 3d20 4e6f  ional[bool] = No
+00019800: 6e65 293a 0a20 2020 202e 2e2e 2020 2020  ne):.    ...    
+00019810: 2074 7261 696e 203d 206e 6e2e 6d65 7267   train = nn.merg
+00019820: 655f 7061 7261 6d28 2774 7261 696e 272c  e_param('train',
+00019830: 2073 656c 662e 7472 6169 6e2c 2074 7261   self.train, tra
+00019840: 696e 290a 0a20 2041 6e20 6572 726f 7220  in)..  An error 
+00019850: 6973 2074 6872 6f77 6e20 7768 656e 2062  is thrown when b
+00019860: 6f74 6820 6172 6775 6d65 6e74 7320 6172  oth arguments ar
+00019870: 6520 6060 4e6f 6e65 6060 206f 7220 626f  e ``None`` or bo
+00019880: 7468 2076 616c 7565 7320 6172 6520 6e6f  th values are no
+00019890: 740a 2020 6060 4e6f 6e65 6060 2e0a 0a20  t.  ``None``... 
+000198a0: 2041 7267 733a 0a20 2020 206e 616d 653a   Args:.    name:
+000198b0: 2074 6865 206e 616d 6520 6f66 2074 6865   the name of the
+000198c0: 2070 6172 616d 6574 6572 2e20 5573 6564   parameter. Used
+000198d0: 2066 6f72 2065 7272 6f72 206d 6573 7361   for error messa
+000198e0: 6765 732e 0a20 2020 2061 3a20 6f70 7469  ges..    a: opti
+000198f0: 6f6e 2061 0a20 2020 2062 3a20 6f70 7469  on a.    b: opti
+00019900: 6f6e 2062 0a0a 2020 5265 7475 726e 733a  on b..  Returns:
+00019910: 0a20 2020 2061 206f 7220 6220 7768 6963  .    a or b whic
+00019920: 6865 7665 7220 6973 206e 6f74 2060 604e  hever is not ``N
+00019930: 6f6e 6560 602e 0a20 2022 2222 0a20 2069  one``..  """.  i
+00019940: 6620 6120 6973 204e 6f6e 6520 616e 6420  f a is None and 
+00019950: 6220 6973 204e 6f6e 653a 0a20 2020 2072  b is None:.    r
+00019960: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00019970: 0a20 2020 2020 2066 2750 6172 616d 6574  .      f'Paramet
+00019980: 6572 2022 7b6e 616d 657d 2220 6d75 7374  er "{name}" must
+00019990: 2062 6520 7061 7373 6564 2074 6f20 7468   be passed to th
+000199a0: 6520 636f 6e73 7472 7563 746f 7220 6f72  e constructor or
+000199b0: 2061 7420 6361 6c6c 2074 696d 652e 270a   at call time.'.
+000199c0: 2020 2020 290a 2020 6966 2061 2069 7320      ).  if a is 
+000199d0: 6e6f 7420 4e6f 6e65 2061 6e64 2062 2069  not None and b i
+000199e0: 7320 6e6f 7420 4e6f 6e65 3a0a 2020 2020  s not None:.    
+000199f0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
+00019a00: 280a 2020 2020 2020 6627 5061 7261 6d65  (.      f'Parame
+00019a10: 7465 7220 227b 6e61 6d65 7d22 2077 6173  ter "{name}" was
+00019a20: 2070 6173 7365 6420 746f 2074 6865 2063   passed to the c
+00019a30: 6f6e 7374 7275 6374 6f72 2061 6e64 2061  onstructor and a
+00019a40: 7420 6361 6c6c 2074 696d 652e 270a 2020  t call time.'.  
+00019a50: 2020 2020 2720 5368 6f75 6c64 2062 6520      ' Should be 
+00019a60: 7061 7373 6564 206a 7573 7420 6f6e 6365  passed just once
+00019a70: 2e27 0a20 2020 2029 0a20 2069 6620 6120  .'.    ).  if a 
+00019a80: 6973 204e 6f6e 653a 0a20 2020 2061 7373  is None:.    ass
+00019a90: 6572 7420 6220 6973 206e 6f74 204e 6f6e  ert b is not Non
+00019aa0: 650a 2020 2020 7265 7475 726e 2062 0a20  e.    return b. 
+00019ab0: 2072 6574 7572 6e20 610a 0a0a 4074 7261   return a...@tra
+00019ac0: 6365 6261 636b 5f75 7469 6c2e 6170 695f  ceback_util.api_
+00019ad0: 626f 756e 6461 7279 0a64 6566 2061 7070  boundary.def app
+00019ae0: 6c79 280a 2020 666e 3a20 4361 6c6c 6162  ly(.  fn: Callab
+00019af0: 6c65 5b2e 2e2e 2c20 416e 795d 2c0a 2020  le[..., Any],.  
+00019b00: 6d6f 6475 6c65 3a20 4d6f 6475 6c65 2c0a  module: Module,.
+00019b10: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+00019b20: 6374 696f 6e46 696c 7465 7220 3d20 4661  ctionFilter = Fa
+00019b30: 6c73 652c 0a20 2063 6170 7475 7265 5f69  lse,.  capture_i
+00019b40: 6e74 6572 6d65 6469 6174 6573 3a20 556e  ntermediates: Un
+00019b50: 696f 6e5b 626f 6f6c 2c20 4361 6c6c 6162  ion[bool, Callab
+00019b60: 6c65 5b5b 4d6f 6475 6c65 2c20 7374 725d  le[[Module, str]
+00019b70: 2c20 626f 6f6c 5d5d 203d 2046 616c 7365  , bool]] = False
+00019b80: 2c0a 2920 2d3e 2043 616c 6c61 626c 655b  ,.) -> Callable[
+00019b90: 2e2e 2e2c 2041 6e79 5d3a 0a20 2022 2222  ..., Any]:.  """
+00019ba0: 4372 6561 7465 7320 616e 2061 7070 6c79  Creates an apply
+00019bb0: 2066 756e 6374 696f 6e20 746f 2063 616c   function to cal
+00019bc0: 6c20 6060 666e 6060 2077 6974 6820 6120  l ``fn`` with a 
+00019bd0: 626f 756e 6420 6d6f 6475 6c65 2e0a 0a20  bound module... 
+00019be0: 2055 6e6c 696b 6520 6060 4d6f 6475 6c65   Unlike ``Module
+00019bf0: 2e61 7070 6c79 6060 2074 6869 7320 6675  .apply`` this fu
+00019c00: 6e63 7469 6f6e 2072 6574 7572 6e73 2061  nction returns a
+00019c10: 206e 6577 2066 756e 6374 696f 6e20 7769   new function wi
+00019c20: 7468 2074 6865 0a20 2073 6967 6e61 7475  th the.  signatu
+00019c30: 7265 2060 6028 7661 7269 6162 6c65 732c  re ``(variables,
+00019c40: 202a 6172 6773 2c20 726e 6773 3d4e 6f6e   *args, rngs=Non
+00019c50: 652c 202a 2a6b 7761 7267 7329 202d 3e20  e, **kwargs) -> 
+00019c60: 5460 6020 7768 6572 6520 6060 5460 6020  T`` where ``T`` 
+00019c70: 6973 2074 6865 0a20 2072 6574 7572 6e20  is the.  return 
+00019c80: 7479 7065 206f 6620 6060 666e 6060 2e20  type of ``fn``. 
+00019c90: 4966 2060 606d 7574 6162 6c65 6060 2069  If ``mutable`` i
+00019ca0: 7320 6e6f 7420 6060 4661 6c73 6560 6020  s not ``False`` 
+00019cb0: 7468 6520 7265 7475 726e 2074 7970 6520  the return type 
+00019cc0: 6973 2061 0a20 2074 7570 6c65 2077 6865  is a.  tuple whe
+00019cd0: 7265 2074 6865 2073 6563 6f6e 6420 6974  re the second it
+00019ce0: 656d 2069 7320 6120 6060 4672 6f7a 656e  em is a ``Frozen
+00019cf0: 4469 6374 6060 2077 6974 6820 7468 6520  Dict`` with the 
+00019d00: 6d75 7461 7465 6420 7661 7269 6162 6c65  mutated variable
+00019d10: 732e 0a0a 2020 5468 6520 6170 706c 7920  s...  The apply 
+00019d20: 6675 6e63 7469 6f6e 2074 6861 7420 6973  function that is
+00019d30: 2072 6574 7572 6e65 6420 6361 6e20 6265   returned can be
+00019d40: 2064 6972 6563 746c 7920 636f 6d70 6f73   directly compos
+00019d50: 6564 2077 6974 680a 2020 4a41 5820 7472  ed with.  JAX tr
+00019d60: 616e 7366 6f72 6d61 7469 6f6e 7320 6c69  ansformations li
+00019d70: 6b65 2060 606a 6178 2e6a 6974 6060 3a3a  ke ``jax.jit``::
+00019d80: 0a0a 2020 2020 3e3e 3e20 636c 6173 7320  ..    >>> class 
+00019d90: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
+00019da0: 2020 2020 2e2e 2e20 2020 6465 6620 656e      ...   def en
+00019db0: 636f 6465 2873 656c 662c 2078 293a 0a20  code(self, x):. 
+00019dc0: 2020 202e 2e2e 2020 2020 202e 2e2e 0a20     ...     .... 
+00019dd0: 2020 202e 2e2e 2020 2064 6566 2064 6563     ...   def dec
+00019de0: 6f64 6528 7365 6c66 2c20 7829 3a0a 2020  ode(self, x):.  
+00019df0: 2020 2e2e 2e20 2020 2020 2e2e 2e0a 0a20    ...     ..... 
+00019e00: 2020 203e 3e3e 2064 6566 2066 2866 6f6f     >>> def f(foo
+00019e10: 2c20 7829 3a0a 2020 2020 2e2e 2e20 2020  , x):.    ...   
+00019e20: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
+00019e30: 290a 2020 2020 2e2e 2e20 2020 7920 3d20  ).    ...   y = 
+00019e40: 666f 6f2e 6465 636f 6465 287a 290a 2020  foo.decode(z).  
+00019e50: 2020 2e2e 2e20 2020 2320 2e2e 2e0a 2020    ...   # ....  
+00019e60: 2020 2e2e 2e20 2020 7265 7475 726e 2079    ...   return y
+00019e70: 0a0a 2020 2020 3e3e 3e20 7661 7269 6162  ..    >>> variab
+00019e80: 6c65 7320 3d20 7b7d 0a20 2020 203e 3e3e  les = {}.    >>>
+00019e90: 2066 6f6f 203d 2046 6f6f 2829 0a20 2020   foo = Foo().   
+00019ea0: 203e 3e3e 2066 5f6a 6974 7465 6420 3d20   >>> f_jitted = 
+00019eb0: 6a61 782e 6a69 7428 6e6e 2e61 7070 6c79  jax.jit(nn.apply
+00019ec0: 2866 2c20 666f 6f29 290a 2020 2020 3e3e  (f, foo)).    >>
+00019ed0: 3e20 665f 6a69 7474 6564 2876 6172 6961  > f_jitted(varia
+00019ee0: 626c 6573 2c20 6a6e 702e 6f6e 6573 2828  bles, jnp.ones((
+00019ef0: 312c 2033 2929 290a 0a20 2041 7267 733a  1, 3)))..  Args:
+00019f00: 0a20 2020 2066 6e3a 2054 6865 2066 756e  .    fn: The fun
+00019f10: 6374 696f 6e20 7468 6174 2073 686f 756c  ction that shoul
+00019f20: 6420 6265 2061 7070 6c69 6564 2e20 5468  d be applied. Th
+00019f30: 6520 6669 7273 7420 6172 6775 6d65 6e74  e first argument
+00019f40: 2070 6173 7365 6420 7769 6c6c 2062 650a   passed will be.
+00019f50: 2020 2020 2020 6120 6d6f 6475 6c65 2069        a module i
+00019f60: 6e73 7461 6e63 6520 6f66 2074 6865 2060  nstance of the `
+00019f70: 606d 6f64 756c 6560 6020 7769 7468 2076  `module`` with v
+00019f80: 6172 6961 626c 6573 2061 6e64 2052 4e47  ariables and RNG
+00019f90: 7320 626f 756e 6420 746f 2069 742e 0a20  s bound to it.. 
+00019fa0: 2020 206d 6f64 756c 653a 2054 6865 2060     module: The `
+00019fb0: 604d 6f64 756c 6560 6020 7468 6174 2077  `Module`` that w
+00019fc0: 696c 6c20 6265 2075 7365 6420 746f 2062  ill be used to b
+00019fd0: 696e 6420 7661 7269 6162 6c65 7320 616e  ind variables an
+00019fe0: 6420 524e 4773 2074 6f2e 2054 6865 0a20  d RNGs to. The. 
+00019ff0: 2020 2020 2060 604d 6f64 756c 6560 6020       ``Module`` 
+0001a000: 7061 7373 6564 2061 7320 7468 6520 6669  passed as the fi
+0001a010: 7273 7420 6172 6775 6d65 6e74 2074 6f20  rst argument to 
+0001a020: 6060 666e 6060 2077 696c 6c20 6265 2061  ``fn`` will be a
+0001a030: 2063 6c6f 6e65 206f 660a 2020 2020 2020   clone of.      
+0001a040: 6d6f 6475 6c65 2e0a 2020 2020 6d75 7461  module..    muta
+0001a050: 626c 653a 2043 616e 2062 6520 626f 6f6c  ble: Can be bool
+0001a060: 2c20 7374 722c 206f 7220 6c69 7374 2e20  , str, or list. 
+0001a070: 5370 6563 6966 6965 7320 7768 6963 6820  Specifies which 
+0001a080: 636f 6c6c 6563 7469 6f6e 7320 7368 6f75  collections shou
+0001a090: 6c64 2062 650a 2020 2020 2020 7472 6561  ld be.      trea
+0001a0a0: 7465 6420 6173 206d 7574 6162 6c65 3a20  ted as mutable: 
+0001a0b0: 6060 626f 6f6c 6060 3a20 616c 6c2f 6e6f  ``bool``: all/no
+0001a0c0: 2063 6f6c 6c65 6374 696f 6e73 2061 7265   collections are
+0001a0d0: 206d 7574 6162 6c65 2e20 6060 7374 7260   mutable. ``str`
+0001a0e0: 603a 2054 6865 0a20 2020 2020 206e 616d  `: The.      nam
+0001a0f0: 6520 6f66 2061 2073 696e 676c 6520 6d75  e of a single mu
+0001a100: 7461 626c 6520 636f 6c6c 6563 7469 6f6e  table collection
+0001a110: 2e20 6060 6c69 7374 6060 3a20 4120 6c69  . ``list``: A li
+0001a120: 7374 206f 6620 6e61 6d65 7320 6f66 206d  st of names of m
+0001a130: 7574 6162 6c65 0a20 2020 2020 2063 6f6c  utable.      col
+0001a140: 6c65 6374 696f 6e73 2e0a 2020 2020 6361  lections..    ca
+0001a150: 7074 7572 655f 696e 7465 726d 6564 6961  pture_intermedia
+0001a160: 7465 733a 2049 6620 6060 5472 7565 6060  tes: If ``True``
+0001a170: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
+0001a180: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
+0001a190: 616c 7565 7320 6f66 2061 6c6c 0a20 2020  alues of all.   
+0001a1a0: 2020 204d 6f64 756c 6573 2069 6e73 6964     Modules insid
+0001a1b0: 6520 7468 6520 2269 6e74 6572 6d65 6469  e the "intermedi
+0001a1c0: 6174 6573 2220 636f 6c6c 6563 7469 6f6e  ates" collection
+0001a1d0: 2e20 4279 2064 6566 6175 6c74 2c20 6f6e  . By default, on
+0001a1e0: 6c79 2074 6865 2072 6574 7572 6e0a 2020  ly the return.  
+0001a1f0: 2020 2020 7661 6c75 6573 206f 6620 616c      values of al
+0001a200: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
+0001a210: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
+0001a220: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
+0001a230: 6265 2070 6173 7365 6420 746f 0a20 2020  be passed to.   
+0001a240: 2020 2063 6861 6e67 6520 7468 6520 6669     change the fi
+0001a250: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
+0001a260: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
+0001a270: 6f6e 2074 616b 6573 2074 6865 204d 6f64  on takes the Mod
+0001a280: 756c 6520 696e 7374 616e 6365 0a20 2020  ule instance.   
+0001a290: 2020 2061 6e64 206d 6574 686f 6420 6e61     and method na
+0001a2a0: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
+0001a2b0: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
+0001a2c0: 2077 6865 7468 6572 2074 6865 206f 7574   whether the out
+0001a2d0: 7075 7420 6f66 2074 6861 740a 2020 2020  put of that.    
+0001a2e0: 2020 6d65 7468 6f64 2069 6e76 6f63 6174    method invocat
+0001a2f0: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
+0001a300: 6f72 6564 2e0a 0a20 2052 6574 7572 6e73  ored...  Returns
+0001a310: 3a0a 2020 2020 5468 6520 6170 706c 7920  :.    The apply 
+0001a320: 6675 6e63 7469 6f6e 2077 7261 7070 696e  function wrappin
+0001a330: 6720 6060 666e 6060 2e0a 2020 2222 220a  g ``fn``..  """.
+0001a340: 0a20 2040 6675 6e63 746f 6f6c 732e 7772  .  @functools.wr
+0001a350: 6170 7328 666e 290a 2020 6465 6620 7363  aps(fn).  def sc
+0001a360: 6f70 655f 666e 2873 636f 7065 2c20 2a61  ope_fn(scope, *a
+0001a370: 7267 732c 202a 2a6b 7761 7267 7329 3a0a  rgs, **kwargs):.
+0001a380: 2020 2020 5f63 6f6e 7465 7874 2e63 6170      _context.cap
+0001a390: 7475 7265 5f73 7461 636b 2e61 7070 656e  ture_stack.appen
+0001a3a0: 6428 6361 7074 7572 655f 696e 7465 726d  d(capture_interm
+0001a3b0: 6564 6961 7465 7329 0a20 2020 2074 7279  ediates).    try
+0001a3c0: 3a0a 2020 2020 2020 7265 7475 726e 2066  :.      return f
+0001a3d0: 6e28 6d6f 6475 6c65 2e63 6c6f 6e65 2870  n(module.clone(p
+0001a3e0: 6172 656e 743d 7363 6f70 652c 205f 6465  arent=scope, _de
+0001a3f0: 6570 5f63 6c6f 6e65 3d54 7275 6529 2c20  ep_clone=True), 
+0001a400: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0001a410: 0a20 2020 2066 696e 616c 6c79 3a0a 2020  .    finally:.  
+0001a420: 2020 2020 5f63 6f6e 7465 7874 2e63 6170      _context.cap
+0001a430: 7475 7265 5f73 7461 636b 2e70 6f70 2829  ture_stack.pop()
+0001a440: 0a0a 2020 6966 2063 6170 7475 7265 5f69  ..  if capture_i
+0001a450: 6e74 6572 6d65 6469 6174 6573 2069 7320  ntermediates is 
+0001a460: 5472 7565 3a20 2023 2070 796c 696e 743a  True:  # pylint:
+0001a470: 2064 6973 6162 6c65 3d67 2d62 6f6f 6c2d   disable=g-bool-
+0001a480: 6964 2d63 6f6d 7061 7269 736f 6e0a 2020  id-comparison.  
+0001a490: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
+0001a4a0: 6564 6961 7465 7320 3d20 6361 7074 7572  ediates = captur
+0001a4b0: 655f 6361 6c6c 5f69 6e74 6572 6d65 6469  e_call_intermedi
+0001a4c0: 6174 6573 0a20 2069 6620 6361 7074 7572  ates.  if captur
+0001a4d0: 655f 696e 7465 726d 6564 6961 7465 733a  e_intermediates:
+0001a4e0: 0a20 2020 206d 7574 6162 6c65 203d 2075  .    mutable = u
+0001a4f0: 6e69 6f6e 5f66 696c 7465 7273 286d 7574  nion_filters(mut
+0001a500: 6162 6c65 2c20 2769 6e74 6572 6d65 6469  able, 'intermedi
+0001a510: 6174 6573 2729 0a20 2072 6574 7572 6e20  ates').  return 
+0001a520: 636f 7265 2e61 7070 6c79 2873 636f 7065  core.apply(scope
+0001a530: 5f66 6e2c 206d 7574 6162 6c65 3d6d 7574  _fn, mutable=mut
+0001a540: 6162 6c65 290a 0a0a 4074 7261 6365 6261  able)...@traceba
+0001a550: 636b 5f75 7469 6c2e 6170 695f 626f 756e  ck_util.api_boun
+0001a560: 6461 7279 0a64 6566 2069 6e69 745f 7769  dary.def init_wi
+0001a570: 7468 5f6f 7574 7075 7428 0a20 2066 6e3a  th_output(.  fn:
+0001a580: 2043 616c 6c61 626c 655b 2e2e 2e2c 2041   Callable[..., A
+0001a590: 6e79 5d2c 0a20 206d 6f64 756c 653a 204d  ny],.  module: M
+0001a5a0: 6f64 756c 652c 0a20 206d 7574 6162 6c65  odule,.  mutable
+0001a5b0: 3a20 436f 6c6c 6563 7469 6f6e 4669 6c74  : CollectionFilt
+0001a5c0: 6572 203d 2044 656e 794c 6973 7428 2769  er = DenyList('i
+0001a5d0: 6e74 6572 6d65 6469 6174 6573 2729 2c0a  ntermediates'),.
+0001a5e0: 2020 6361 7074 7572 655f 696e 7465 726d    capture_interm
+0001a5f0: 6564 6961 7465 733a 2055 6e69 6f6e 5b62  ediates: Union[b
+0001a600: 6f6f 6c2c 2043 616c 6c61 626c 655b 5b4d  ool, Callable[[M
+0001a610: 6f64 756c 652c 2073 7472 5d2c 2062 6f6f  odule, str], boo
+0001a620: 6c5d 5d20 3d20 4661 6c73 652c 0a29 202d  l]] = False,.) -
+0001a630: 3e20 4361 6c6c 6162 6c65 5b2e 2e2e 2c20  > Callable[..., 
+0001a640: 5475 706c 655b 416e 792c 2055 6e69 6f6e  Tuple[Any, Union
+0001a650: 5b46 726f 7a65 6e56 6172 6961 626c 6544  [FrozenVariableD
+0001a660: 6963 742c 2044 6963 745b 7374 722c 2041  ict, Dict[str, A
+0001a670: 6e79 5d5d 5d5d 3a0a 2020 2222 2243 7265  ny]]]]:.  """Cre
+0001a680: 6174 6573 2061 6e20 696e 6974 2066 756e  ates an init fun
+0001a690: 6374 696f 6e20 746f 2063 616c 6c20 6060  ction to call ``
+0001a6a0: 666e 6060 2077 6974 6820 6120 626f 756e  fn`` with a boun
+0001a6b0: 6420 6d6f 6475 6c65 2074 6861 7420 616c  d module that al
+0001a6c0: 736f 2072 6574 7572 6e73 2074 6865 2066  so returns the f
+0001a6d0: 756e 6374 696f 6e20 6f75 7470 7574 732e  unction outputs.
+0001a6e0: 0a0a 2020 556e 6c69 6b65 2060 604d 6f64  ..  Unlike ``Mod
+0001a6f0: 756c 652e 696e 6974 5f77 6974 685f 6f75  ule.init_with_ou
+0001a700: 7470 7574 6060 2074 6869 7320 6675 6e63  tput`` this func
+0001a710: 7469 6f6e 2072 6574 7572 6e73 2061 206e  tion returns a n
+0001a720: 6577 2066 756e 6374 696f 6e20 7769 7468  ew function with
+0001a730: 0a20 2074 6865 2073 6967 6e61 7475 7265  .  the signature
+0001a740: 2060 6028 726e 6773 2c20 2a61 7267 732c   ``(rngs, *args,
+0001a750: 202a 2a6b 7761 7267 7329 202d 3e20 2854   **kwargs) -> (T
+0001a760: 2c20 7661 7269 6162 6c65 7329 6060 2077  , variables)`` w
+0001a770: 6865 7265 2060 6054 6060 2069 7320 7468  here ``T`` is th
+0001a780: 650a 2020 7265 7475 726e 2074 7970 6520  e.  return type 
+0001a790: 6f66 2060 6066 6e60 602e 2054 6865 2072  of ``fn``. The r
+0001a7a0: 6e67 7320 6361 6e20 6265 2061 2064 6963  ngs can be a dic
+0001a7b0: 7420 6f66 2050 524e 474b 6579 7320 6f72  t of PRNGKeys or
+0001a7c0: 2061 2073 696e 676c 650a 2020 6060 6050   a single.  ```P
+0001a7d0: 524e 474b 6579 6060 2077 6869 6368 2069  RNGKey`` which i
+0001a7e0: 7320 6571 7569 7661 6c65 6e74 2074 6f20  s equivalent to 
+0001a7f0: 7061 7373 696e 6720 6120 6469 6374 2077  passing a dict w
+0001a800: 6974 6820 6f6e 6520 5052 4e47 4b65 7920  ith one PRNGKey 
+0001a810: 7769 7468 2074 6865 0a20 206e 616d 6520  with the.  name 
+0001a820: 2270 6172 616d 7322 2e0a 0a20 2054 6865  "params"...  The
+0001a830: 2069 6e69 7420 6675 6e63 7469 6f6e 2074   init function t
+0001a840: 6861 7420 6973 2072 6574 7572 6e65 6420  hat is returned 
+0001a850: 6361 6e20 6265 2064 6972 6563 746c 7920  can be directly 
+0001a860: 636f 6d70 6f73 6564 2077 6974 680a 2020  composed with.  
+0001a870: 4a41 5820 7472 616e 7366 6f72 6d61 7469  JAX transformati
+0001a880: 6f6e 7320 6c69 6b65 2060 606a 6178 2e6a  ons like ``jax.j
+0001a890: 6974 6060 3a3a 0a0a 2020 2020 3e3e 3e20  it``::..    >>> 
+0001a8a0: 636c 6173 7320 466f 6f28 6e6e 2e4d 6f64  class Foo(nn.Mod
+0001a8b0: 756c 6529 3a0a 2020 2020 2e2e 2e20 2020  ule):.    ...   
+0001a8c0: 6465 6620 656e 636f 6465 2873 656c 662c  def encode(self,
+0001a8d0: 2078 293a 0a20 2020 202e 2e2e 2020 2020   x):.    ...    
+0001a8e0: 202e 2e2e 0a20 2020 202e 2e2e 2020 2064   ....    ...   d
+0001a8f0: 6566 2064 6563 6f64 6528 7365 6c66 2c20  ef decode(self, 
+0001a900: 7829 3a0a 2020 2020 2e2e 2e20 2020 2020  x):.    ...     
+0001a910: 2e2e 2e0a 0a20 2020 203e 3e3e 2064 6566  .....    >>> def
+0001a920: 2066 2866 6f6f 2c20 7829 3a0a 2020 2020   f(foo, x):.    
+0001a930: 2e2e 2e20 2020 7a20 3d20 666f 6f2e 656e  ...   z = foo.en
+0001a940: 636f 6465 2878 290a 2020 2020 2e2e 2e20  code(x).    ... 
+0001a950: 2020 7920 3d20 666f 6f2e 6465 636f 6465    y = foo.decode
+0001a960: 287a 290a 2020 2020 2e2e 2e20 2020 2320  (z).    ...   # 
+0001a970: 2e2e 2e0a 2020 2020 2e2e 2e20 2020 7265  ....    ...   re
+0001a980: 7475 726e 2079 0a0a 2020 2020 3e3e 3e20  turn y..    >>> 
+0001a990: 666f 6f20 3d20 466f 6f28 290a 2020 2020  foo = Foo().    
+0001a9a0: 3e3e 3e20 665f 6a69 7474 6564 203d 206a  >>> f_jitted = j
+0001a9b0: 6178 2e6a 6974 286e 6e2e 696e 6974 5f77  ax.jit(nn.init_w
+0001a9c0: 6974 685f 6f75 7470 7574 2866 2c20 666f  ith_output(f, fo
+0001a9d0: 6f29 290a 2020 2020 3e3e 3e20 792c 2076  o)).    >>> y, v
+0001a9e0: 6172 6961 626c 6573 203d 2066 5f6a 6974  ariables = f_jit
+0001a9f0: 7465 6428 6a61 782e 7261 6e64 6f6d 2e6b  ted(jax.random.k
+0001aa00: 6579 2830 292c 206a 6e70 2e6f 6e65 7328  ey(0), jnp.ones(
+0001aa10: 2831 2c20 3329 2929 0a0a 2020 4172 6773  (1, 3)))..  Args
+0001aa20: 3a0a 2020 2020 666e 3a20 5468 6520 6675  :.    fn: The fu
+0001aa30: 6e63 7469 6f6e 2074 6861 7420 7368 6f75  nction that shou
+0001aa40: 6c64 2062 6520 6170 706c 6965 642e 2054  ld be applied. T
+0001aa50: 6865 2066 6972 7374 2061 7267 756d 656e  he first argumen
+0001aa60: 7420 7061 7373 6564 2077 696c 6c20 6265  t passed will be
+0001aa70: 0a20 2020 2020 2061 206d 6f64 756c 6520  .      a module 
+0001aa80: 696e 7374 616e 6365 206f 6620 7468 6520  instance of the 
+0001aa90: 6060 6d6f 6475 6c65 6060 2077 6974 6820  ``module`` with 
+0001aaa0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
+0001aab0: 4773 2062 6f75 6e64 2074 6f20 6974 2e0a  Gs bound to it..
+0001aac0: 2020 2020 6d6f 6475 6c65 3a20 5468 6520      module: The 
+0001aad0: 6060 4d6f 6475 6c65 6060 2074 6861 7420  ``Module`` that 
+0001aae0: 7769 6c6c 2062 6520 7573 6564 2074 6f20  will be used to 
+0001aaf0: 6269 6e64 2076 6172 6961 626c 6573 2061  bind variables a
+0001ab00: 6e64 2052 4e47 7320 746f 2e20 5468 650a  nd RNGs to. The.
+0001ab10: 2020 2020 2020 6060 4d6f 6475 6c65 6060        ``Module``
+0001ab20: 2070 6173 7365 6420 6173 2074 6865 2066   passed as the f
+0001ab30: 6972 7374 2061 7267 756d 656e 7420 746f  irst argument to
+0001ab40: 2060 6066 6e60 6020 7769 6c6c 2062 6520   ``fn`` will be 
+0001ab50: 6120 636c 6f6e 6520 6f66 0a20 2020 2020  a clone of.     
+0001ab60: 206d 6f64 756c 652e 0a20 2020 206d 7574   module..    mut
+0001ab70: 6162 6c65 3a20 4361 6e20 6265 2062 6f6f  able: Can be boo
+0001ab80: 6c2c 2073 7472 2c20 6f72 206c 6973 742e  l, str, or list.
+0001ab90: 2053 7065 6369 6669 6573 2077 6869 6368   Specifies which
+0001aba0: 2063 6f6c 6c65 6374 696f 6e73 2073 686f   collections sho
+0001abb0: 756c 6420 6265 0a20 2020 2020 2074 7265  uld be.      tre
+0001abc0: 6174 6564 2061 7320 6d75 7461 626c 653a  ated as mutable:
+0001abd0: 2060 6062 6f6f 6c60 603a 2061 6c6c 2f6e   ``bool``: all/n
+0001abe0: 6f20 636f 6c6c 6563 7469 6f6e 7320 6172  o collections ar
+0001abf0: 6520 6d75 7461 626c 652e 2060 6073 7472  e mutable. ``str
+0001ac00: 6060 3a20 5468 650a 2020 2020 2020 6e61  ``: The.      na
+0001ac10: 6d65 206f 6620 6120 7369 6e67 6c65 206d  me of a single m
+0001ac20: 7574 6162 6c65 2063 6f6c 6c65 6374 696f  utable collectio
+0001ac30: 6e2e 2060 606c 6973 7460 603a 2041 206c  n. ``list``: A l
+0001ac40: 6973 7420 6f66 206e 616d 6573 206f 6620  ist of names of 
+0001ac50: 6d75 7461 626c 650a 2020 2020 2020 636f  mutable.      co
+0001ac60: 6c6c 6563 7469 6f6e 732e 2042 7920 6465  llections. By de
+0001ac70: 6661 756c 742c 2061 6c6c 2063 6f6c 6c65  fault, all colle
+0001ac80: 6374 696f 6e73 2065 7863 6570 7420 2269  ctions except "i
+0001ac90: 6e74 6572 6d65 6469 6174 6573 2220 6172  ntermediates" ar
+0001aca0: 650a 2020 2020 2020 6d75 7461 626c 652e  e.      mutable.
+0001acb0: 0a20 2020 2063 6170 7475 7265 5f69 6e74  .    capture_int
+0001acc0: 6572 6d65 6469 6174 6573 3a20 4966 2060  ermediates: If `
+0001acd0: 6054 7275 6560 602c 2063 6170 7475 7265  `True``, capture
+0001ace0: 7320 696e 7465 726d 6564 6961 7465 2072  s intermediate r
+0001acf0: 6574 7572 6e20 7661 6c75 6573 206f 6620  eturn values of 
+0001ad00: 616c 6c0a 2020 2020 2020 4d6f 6475 6c65  all.      Module
+0001ad10: 7320 696e 7369 6465 2074 6865 2022 696e  s inside the "in
+0001ad20: 7465 726d 6564 6961 7465 7322 2063 6f6c  termediates" col
+0001ad30: 6c65 6374 696f 6e2e 2042 7920 6465 6661  lection. By defa
+0001ad40: 756c 742c 206f 6e6c 7920 7468 6520 7265  ult, only the re
+0001ad50: 7475 726e 0a20 2020 2020 2076 616c 7565  turn.      value
+0001ad60: 7320 6f66 2061 6c6c 2060 5f5f 6361 6c6c  s of all `__call
+0001ad70: 5f5f 6020 6d65 7468 6f64 7320 6172 6520  __` methods are 
+0001ad80: 7374 6f72 6564 2e20 4120 6675 6e63 7469  stored. A functi
+0001ad90: 6f6e 2063 616e 2062 6520 7061 7373 6564  on can be passed
+0001ada0: 2074 6f0a 2020 2020 2020 6368 616e 6765   to.      change
+0001adb0: 2074 6865 2066 696c 7465 7220 6265 6861   the filter beha
+0001adc0: 7669 6f72 2e20 5468 6520 6669 6c74 6572  vior. The filter
+0001add0: 2066 756e 6374 696f 6e20 7461 6b65 7320   function takes 
+0001ade0: 7468 6520 4d6f 6475 6c65 2069 6e73 7461  the Module insta
+0001adf0: 6e63 650a 2020 2020 2020 616e 6420 6d65  nce.      and me
+0001ae00: 7468 6f64 206e 616d 6520 616e 6420 7265  thod name and re
+0001ae10: 7475 726e 7320 6120 626f 6f6c 2069 6e64  turns a bool ind
+0001ae20: 6963 6174 696e 6720 7768 6574 6865 7220  icating whether 
+0001ae30: 7468 6520 6f75 7470 7574 206f 6620 7468  the output of th
+0001ae40: 6174 0a20 2020 2020 206d 6574 686f 6420  at.      method 
+0001ae50: 696e 766f 6361 7469 6f6e 2073 686f 756c  invocation shoul
+0001ae60: 6420 6265 2073 746f 7265 642e 0a0a 2020  d be stored...  
+0001ae70: 5265 7475 726e 733a 0a20 2020 2054 6865  Returns:.    The
+0001ae80: 2069 6e69 7420 6675 6e63 7469 6f6e 2077   init function w
+0001ae90: 7261 7070 696e 6720 6060 666e 6060 2e0a  rapping ``fn``..
+0001aea0: 2020 2222 220a 0a20 2040 6675 6e63 746f    """..  @functo
+0001aeb0: 6f6c 732e 7772 6170 7328 666e 290a 2020  ols.wraps(fn).  
+0001aec0: 6465 6620 7363 6f70 655f 666e 2873 636f  def scope_fn(sco
+0001aed0: 7065 2c20 2a61 7267 732c 202a 2a6b 7761  pe, *args, **kwa
+0001aee0: 7267 7329 3a0a 2020 2020 5f63 6f6e 7465  rgs):.    _conte
+0001aef0: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
+0001af00: 2e61 7070 656e 6428 6361 7074 7572 655f  .append(capture_
+0001af10: 696e 7465 726d 6564 6961 7465 7329 0a20  intermediates). 
+0001af20: 2020 2074 7279 3a0a 2020 2020 2020 7265     try:.      re
+0001af30: 7475 726e 2066 6e28 6d6f 6475 6c65 2e63  turn fn(module.c
+0001af40: 6c6f 6e65 2870 6172 656e 743d 7363 6f70  lone(parent=scop
+0001af50: 652c 205f 6465 6570 5f63 6c6f 6e65 3d54  e, _deep_clone=T
+0001af60: 7275 6529 2c20 2a61 7267 732c 202a 2a6b  rue), *args, **k
+0001af70: 7761 7267 7329 0a20 2020 2066 696e 616c  wargs).    final
+0001af80: 6c79 3a0a 2020 2020 2020 5f63 6f6e 7465  ly:.      _conte
+0001af90: 7874 2e63 6170 7475 7265 5f73 7461 636b  xt.capture_stack
+0001afa0: 2e70 6f70 2829 0a0a 2020 6966 2063 6170  .pop()..  if cap
+0001afb0: 7475 7265 5f69 6e74 6572 6d65 6469 6174  ture_intermediat
+0001afc0: 6573 2069 7320 5472 7565 3a20 2023 2070  es is True:  # p
+0001afd0: 796c 696e 743a 2064 6973 6162 6c65 3d67  ylint: disable=g
+0001afe0: 2d62 6f6f 6c2d 6964 2d63 6f6d 7061 7269  -bool-id-compari
+0001aff0: 736f 6e0a 2020 2020 6361 7074 7572 655f  son.    capture_
+0001b000: 696e 7465 726d 6564 6961 7465 7320 3d20  intermediates = 
+0001b010: 6361 7074 7572 655f 6361 6c6c 5f69 6e74  capture_call_int
+0001b020: 6572 6d65 6469 6174 6573 0a20 2069 6620  ermediates.  if 
+0001b030: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+0001b040: 6961 7465 733a 0a20 2020 206d 7574 6162  iates:.    mutab
+0001b050: 6c65 203d 2075 6e69 6f6e 5f66 696c 7465  le = union_filte
+0001b060: 7273 286d 7574 6162 6c65 2c20 2769 6e74  rs(mutable, 'int
+0001b070: 6572 6d65 6469 6174 6573 2729 0a20 2072  ermediates').  r
+0001b080: 6574 7572 6e20 636f 7265 2e69 6e69 7428  eturn core.init(
+0001b090: 7363 6f70 655f 666e 2c20 6d75 7461 626c  scope_fn, mutabl
+0001b0a0: 653d 6d75 7461 626c 6529 0a0a 0a40 7472  e=mutable)...@tr
+0001b0b0: 6163 6562 6163 6b5f 7574 696c 2e61 7069  aceback_util.api
+0001b0c0: 5f62 6f75 6e64 6172 790a 6465 6620 696e  _boundary.def in
+0001b0d0: 6974 280a 2020 666e 3a20 4361 6c6c 6162  it(.  fn: Callab
+0001b0e0: 6c65 5b2e 2e2e 2c20 416e 795d 2c0a 2020  le[..., Any],.  
+0001b0f0: 6d6f 6475 6c65 3a20 4d6f 6475 6c65 2c0a  module: Module,.
+0001b100: 2020 6d75 7461 626c 653a 2043 6f6c 6c65    mutable: Colle
+0001b110: 6374 696f 6e46 696c 7465 7220 3d20 4465  ctionFilter = De
+0001b120: 6e79 4c69 7374 2827 696e 7465 726d 6564  nyList('intermed
+0001b130: 6961 7465 7327 292c 0a20 2063 6170 7475  iates'),.  captu
+0001b140: 7265 5f69 6e74 6572 6d65 6469 6174 6573  re_intermediates
+0001b150: 3a20 556e 696f 6e5b 626f 6f6c 2c20 4361  : Union[bool, Ca
+0001b160: 6c6c 6162 6c65 5b5b 4d6f 6475 6c65 2c20  llable[[Module, 
+0001b170: 7374 725d 2c20 626f 6f6c 5d5d 203d 2046  str], bool]] = F
+0001b180: 616c 7365 2c0a 2920 2d3e 2043 616c 6c61  alse,.) -> Calla
+0001b190: 626c 655b 2e2e 2e2c 2055 6e69 6f6e 5b46  ble[..., Union[F
+0001b1a0: 726f 7a65 6e56 6172 6961 626c 6544 6963  rozenVariableDic
+0001b1b0: 742c 2044 6963 745b 7374 722c 2041 6e79  t, Dict[str, Any
+0001b1c0: 5d5d 5d3a 0a20 2022 2222 4372 6561 7465  ]]]:.  """Create
+0001b1d0: 7320 616e 2069 6e69 7420 6675 6e63 7469  s an init functi
+0001b1e0: 6f6e 2074 6f20 6361 6c6c 2060 6066 6e60  on to call ``fn`
+0001b1f0: 6020 7769 7468 2061 2062 6f75 6e64 206d  ` with a bound m
+0001b200: 6f64 756c 652e 0a0a 2020 556e 6c69 6b65  odule...  Unlike
+0001b210: 2060 604d 6f64 756c 652e 696e 6974 6060   ``Module.init``
+0001b220: 2074 6869 7320 6675 6e63 7469 6f6e 2072   this function r
+0001b230: 6574 7572 6e73 2061 206e 6577 2066 756e  eturns a new fun
+0001b240: 6374 696f 6e20 7769 7468 2074 6865 2073  ction with the s
+0001b250: 6967 6e61 7475 7265 0a20 2060 6028 726e  ignature.  ``(rn
+0001b260: 6773 2c20 2a61 7267 732c 202a 2a6b 7761  gs, *args, **kwa
+0001b270: 7267 7329 202d 3e20 7661 7269 6162 6c65  rgs) -> variable
+0001b280: 7360 602e 0a20 2054 6865 2072 6e67 7320  s``..  The rngs 
+0001b290: 6361 6e20 6265 2061 2064 6963 7420 6f66  can be a dict of
+0001b2a0: 2050 524e 474b 6579 7320 6f72 2061 2073   PRNGKeys or a s
+0001b2b0: 696e 676c 6520 6060 6050 524e 474b 6579  ingle ```PRNGKey
+0001b2c0: 6060 2077 6869 6368 2069 730a 2020 6571  `` which is.  eq
+0001b2d0: 7569 7661 6c65 6e74 2074 6f20 7061 7373  uivalent to pass
+0001b2e0: 696e 6720 6120 6469 6374 2077 6974 6820  ing a dict with 
+0001b2f0: 6f6e 6520 5052 4e47 4b65 7920 7769 7468  one PRNGKey with
+0001b300: 2074 6865 206e 616d 6520 2270 6172 616d   the name "param
+0001b310: 7322 2e0a 0a20 2054 6865 2069 6e69 7420  s"...  The init 
+0001b320: 6675 6e63 7469 6f6e 2074 6861 7420 6973  function that is
+0001b330: 2072 6574 7572 6e65 6420 6361 6e20 6265   returned can be
+0001b340: 2064 6972 6563 746c 7920 636f 6d70 6f73   directly compos
+0001b350: 6564 2077 6974 680a 2020 4a41 5820 7472  ed with.  JAX tr
+0001b360: 616e 7366 6f72 6d61 7469 6f6e 7320 6c69  ansformations li
+0001b370: 6b65 2060 606a 6178 2e6a 6974 6060 3a3a  ke ``jax.jit``::
+0001b380: 0a0a 2020 2020 3e3e 3e20 636c 6173 7320  ..    >>> class 
+0001b390: 466f 6f28 6e6e 2e4d 6f64 756c 6529 3a0a  Foo(nn.Module):.
+0001b3a0: 2020 2020 2e2e 2e20 2020 6465 6620 656e      ...   def en
+0001b3b0: 636f 6465 2873 656c 662c 2078 293a 0a20  code(self, x):. 
+0001b3c0: 2020 202e 2e2e 2020 2020 202e 2e2e 0a20     ...     .... 
+0001b3d0: 2020 202e 2e2e 2020 2064 6566 2064 6563     ...   def dec
+0001b3e0: 6f64 6528 7365 6c66 2c20 7829 3a0a 2020  ode(self, x):.  
+0001b3f0: 2020 2e2e 2e20 2020 2020 2e2e 2e0a 0a20    ...     ..... 
+0001b400: 2020 203e 3e3e 2064 6566 2066 2866 6f6f     >>> def f(foo
+0001b410: 2c20 7829 3a0a 2020 2020 2e2e 2e20 2020  , x):.    ...   
+0001b420: 7a20 3d20 666f 6f2e 656e 636f 6465 2878  z = foo.encode(x
+0001b430: 290a 2020 2020 2e2e 2e20 2020 7920 3d20  ).    ...   y = 
+0001b440: 666f 6f2e 6465 636f 6465 287a 290a 2020  foo.decode(z).  
+0001b450: 2020 2e2e 2e20 2020 2320 2e2e 2e0a 2020    ...   # ....  
+0001b460: 2020 2e2e 2e20 2020 7265 7475 726e 2079    ...   return y
+0001b470: 0a0a 2020 2020 3e3e 3e20 666f 6f20 3d20  ..    >>> foo = 
+0001b480: 466f 6f28 290a 2020 2020 3e3e 3e20 665f  Foo().    >>> f_
+0001b490: 6a69 7474 6564 203d 206a 6178 2e6a 6974  jitted = jax.jit
+0001b4a0: 286e 6e2e 696e 6974 2866 2c20 666f 6f29  (nn.init(f, foo)
+0001b4b0: 290a 2020 2020 3e3e 3e20 7661 7269 6162  ).    >>> variab
+0001b4c0: 6c65 7320 3d20 665f 6a69 7474 6564 286a  les = f_jitted(j
+0001b4d0: 6178 2e72 616e 646f 6d2e 6b65 7928 3029  ax.random.key(0)
+0001b4e0: 2c20 6a6e 702e 6f6e 6573 2828 312c 2033  , jnp.ones((1, 3
+0001b4f0: 2929 290a 0a20 2041 7267 733a 0a20 2020  )))..  Args:.   
+0001b500: 2066 6e3a 2054 6865 2066 756e 6374 696f   fn: The functio
+0001b510: 6e20 7468 6174 2073 686f 756c 6420 6265  n that should be
+0001b520: 2061 7070 6c69 6564 2e20 5468 6520 6669   applied. The fi
+0001b530: 7273 7420 6172 6775 6d65 6e74 2070 6173  rst argument pas
+0001b540: 7365 6420 7769 6c6c 2062 650a 2020 2020  sed will be.    
+0001b550: 2020 6120 6d6f 6475 6c65 2069 6e73 7461    a module insta
+0001b560: 6e63 6520 6f66 2074 6865 2060 606d 6f64  nce of the ``mod
+0001b570: 756c 6560 6020 7769 7468 2076 6172 6961  ule`` with varia
+0001b580: 626c 6573 2061 6e64 2052 4e47 7320 626f  bles and RNGs bo
+0001b590: 756e 6420 746f 2069 742e 0a20 2020 206d  und to it..    m
+0001b5a0: 6f64 756c 653a 2054 6865 2060 604d 6f64  odule: The ``Mod
+0001b5b0: 756c 6560 6020 7468 6174 2077 696c 6c20  ule`` that will 
+0001b5c0: 6265 2075 7365 6420 746f 2062 696e 6420  be used to bind 
+0001b5d0: 7661 7269 6162 6c65 7320 616e 6420 524e  variables and RN
+0001b5e0: 4773 2074 6f2e 2054 6865 0a20 2020 2020  Gs to. The.     
+0001b5f0: 2060 604d 6f64 756c 6560 6020 7061 7373   ``Module`` pass
+0001b600: 6564 2061 7320 7468 6520 6669 7273 7420  ed as the first 
+0001b610: 6172 6775 6d65 6e74 2074 6f20 6060 666e  argument to ``fn
+0001b620: 6060 2077 696c 6c20 6265 2061 2063 6c6f  `` will be a clo
+0001b630: 6e65 206f 660a 2020 2020 2020 6d6f 6475  ne of.      modu
+0001b640: 6c65 2e0a 2020 2020 6d75 7461 626c 653a  le..    mutable:
+0001b650: 2043 616e 2062 6520 626f 6f6c 2c20 7374   Can be bool, st
+0001b660: 722c 206f 7220 6c69 7374 2e20 5370 6563  r, or list. Spec
+0001b670: 6966 6965 7320 7768 6963 6820 636f 6c6c  ifies which coll
+0001b680: 6563 7469 6f6e 7320 7368 6f75 6c64 2062  ections should b
+0001b690: 650a 2020 2020 2020 7472 6561 7465 6420  e.      treated 
+0001b6a0: 6173 206d 7574 6162 6c65 3a20 6060 626f  as mutable: ``bo
+0001b6b0: 6f6c 6060 3a20 616c 6c2f 6e6f 2063 6f6c  ol``: all/no col
+0001b6c0: 6c65 6374 696f 6e73 2061 7265 206d 7574  lections are mut
+0001b6d0: 6162 6c65 2e20 6060 7374 7260 603a 2054  able. ``str``: T
+0001b6e0: 6865 0a20 2020 2020 206e 616d 6520 6f66  he.      name of
+0001b6f0: 2061 2073 696e 676c 6520 6d75 7461 626c   a single mutabl
+0001b700: 6520 636f 6c6c 6563 7469 6f6e 2e20 6060  e collection. ``
+0001b710: 6c69 7374 6060 3a20 4120 6c69 7374 206f  list``: A list o
+0001b720: 6620 6e61 6d65 7320 6f66 206d 7574 6162  f names of mutab
+0001b730: 6c65 0a20 2020 2020 2063 6f6c 6c65 6374  le.      collect
+0001b740: 696f 6e73 2e20 4279 2064 6566 6175 6c74  ions. By default
+0001b750: 2c20 616c 6c20 636f 6c6c 6563 7469 6f6e  , all collection
+0001b760: 7320 6578 6365 7074 2022 696e 7465 726d  s except "interm
+0001b770: 6564 6961 7465 7322 2061 7265 0a20 2020  ediates" are.   
+0001b780: 2020 206d 7574 6162 6c65 2e0a 2020 2020     mutable..    
+0001b790: 6361 7074 7572 655f 696e 7465 726d 6564  capture_intermed
+0001b7a0: 6961 7465 733a 2049 6620 6054 7275 6560  iates: If `True`
+0001b7b0: 2c20 6361 7074 7572 6573 2069 6e74 6572  , captures inter
+0001b7c0: 6d65 6469 6174 6520 7265 7475 726e 2076  mediate return v
+0001b7d0: 616c 7565 7320 6f66 2061 6c6c 0a20 2020  alues of all.   
+0001b7e0: 2020 204d 6f64 756c 6573 2069 6e73 6964     Modules insid
+0001b7f0: 6520 7468 6520 2269 6e74 6572 6d65 6469  e the "intermedi
+0001b800: 6174 6573 2220 636f 6c6c 6563 7469 6f6e  ates" collection
+0001b810: 2e20 4279 2064 6566 6175 6c74 2c20 6f6e  . By default, on
+0001b820: 6c79 2074 6865 2072 6574 7572 6e0a 2020  ly the return.  
+0001b830: 2020 2020 7661 6c75 6573 206f 6620 616c      values of al
+0001b840: 6c20 605f 5f63 616c 6c5f 5f60 206d 6574  l `__call__` met
+0001b850: 686f 6473 2061 7265 2073 746f 7265 642e  hods are stored.
+0001b860: 2041 2066 756e 6374 696f 6e20 6361 6e20   A function can 
+0001b870: 6265 2070 6173 7365 6420 746f 0a20 2020  be passed to.   
+0001b880: 2020 2063 6861 6e67 6520 7468 6520 6669     change the fi
+0001b890: 6c74 6572 2062 6568 6176 696f 722e 2054  lter behavior. T
+0001b8a0: 6865 2066 696c 7465 7220 6675 6e63 7469  he filter functi
+0001b8b0: 6f6e 2074 616b 6573 2074 6865 204d 6f64  on takes the Mod
+0001b8c0: 756c 6520 696e 7374 616e 6365 0a20 2020  ule instance.   
+0001b8d0: 2020 2061 6e64 206d 6574 686f 6420 6e61     and method na
+0001b8e0: 6d65 2061 6e64 2072 6574 7572 6e73 2061  me and returns a
+0001b8f0: 2062 6f6f 6c20 696e 6469 6361 7469 6e67   bool indicating
+0001b900: 2077 6865 7468 6572 2074 6865 206f 7574   whether the out
+0001b910: 7075 7420 6f66 2074 6861 740a 2020 2020  put of that.    
+0001b920: 2020 6d65 7468 6f64 2069 6e76 6f63 6174    method invocat
+0001b930: 696f 6e20 7368 6f75 6c64 2062 6520 7374  ion should be st
+0001b940: 6f72 6564 2e0a 0a20 2052 6574 7572 6e73  ored...  Returns
+0001b950: 3a0a 2020 2020 5468 6520 696e 6974 2066  :.    The init f
+0001b960: 756e 6374 696f 6e20 7772 6170 7069 6e67  unction wrapping
+0001b970: 2060 6066 6e60 602e 0a20 2022 2222 0a20   ``fn``..  """. 
+0001b980: 2069 6e69 745f 666e 203d 2069 6e69 745f   init_fn = init_
+0001b990: 7769 7468 5f6f 7574 7075 7428 666e 2c20  with_output(fn, 
+0001b9a0: 6d6f 6475 6c65 2c20 6d75 7461 626c 652c  module, mutable,
+0001b9b0: 2063 6170 7475 7265 5f69 6e74 6572 6d65   capture_interme
+0001b9c0: 6469 6174 6573 290a 0a20 2040 6675 6e63  diates)..  @func
+0001b9d0: 746f 6f6c 732e 7772 6170 7328 696e 6974  tools.wraps(init
+0001b9e0: 5f66 6e29 0a20 2064 6566 2069 6e69 745f  _fn).  def init_
+0001b9f0: 7772 6170 7065 7228 2a61 7267 732c 202a  wrapper(*args, *
+0001ba00: 2a6b 7761 7267 7329 3a0a 2020 2020 7265  *kwargs):.    re
+0001ba10: 7475 726e 2069 6e69 745f 666e 282a 6172  turn init_fn(*ar
+0001ba20: 6773 2c20 2a2a 6b77 6172 6773 295b 315d  gs, **kwargs)[1]
+0001ba30: 0a0a 2020 7265 7475 726e 2069 6e69 745f  ..  return init_
+0001ba40: 7772 6170 7065 720a 0a0a 2320 544f 444f  wrapper...# TODO
+0001ba50: 2863 6761 7263 6961 6529 3a20 7765 2061  (cgarciae): we a
+0001ba60: 7265 2064 6566 696e 696e 6720 436f 6d70  re defining Comp
+0001ba70: 6163 744e 616d 6553 636f 7065 206a 7573  actNameScope jus
+0001ba80: 7420 746f 0a23 2061 766f 6964 2061 2070  t to.# avoid a p
+0001ba90: 7974 7970 6520 6275 6720 7769 7468 2074  ytype bug with t
+0001baa0: 6865 2046 6c61 7820 6f76 6572 6c61 792e  he Flax overlay.
+0001bab0: 2057 6520 7368 6f75 6c64 2061 696d 2074   We should aim t
+0001bac0: 6f0a 2320 7265 6d6f 7665 2069 6e20 7468  o.# remove in th
+0001bad0: 6520 6174 2073 6f6d 6520 706f 696e 7420  e at some point 
+0001bae0: 6173 2069 7473 206e 6f74 2065 7267 6f6e  as its not ergon
+0001baf0: 6f6d 6963 2e0a 6966 206e 6f74 2074 7970  omic..if not typ
+0001bb00: 696e 672e 5459 5045 5f43 4845 434b 494e  ing.TYPE_CHECKIN
+0001bb10: 473a 0a0a 2020 636c 6173 7320 436f 6d70  G:..  class Comp
+0001bb20: 6163 744e 616d 6553 636f 7065 284d 6f64  actNameScope(Mod
+0001bb30: 756c 6529 3a0a 2020 2020 666e 3a20 4361  ule):.    fn: Ca
+0001bb40: 6c6c 6162 6c65 0a20 2020 206d 6f64 756c  llable.    modul
+0001bb50: 655f 666e 3a20 4361 6c6c 6162 6c65 5b5b  e_fn: Callable[[
+0001bb60: 5d2c 204d 6f64 756c 655d 0a0a 2020 2020  ], Module]..    
+0001bb70: 4063 6f6d 7061 6374 0a20 2020 2064 6566  @compact.    def
+0001bb80: 205f 5f63 616c 6c5f 5f28 7365 6c66 2c20   __call__(self, 
+0001bb90: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0001bba0: 202d 3e20 416e 793a 0a20 2020 2020 2072   -> Any:.      r
+0001bbb0: 6574 7572 6e20 7365 6c66 2e66 6e28 7365  eturn self.fn(se
+0001bbc0: 6c66 2e6d 6f64 756c 655f 666e 2829 2c20  lf.module_fn(), 
+0001bbd0: 2a61 7267 732c 202a 2a6b 7761 7267 7329  *args, **kwargs)
+0001bbe0: 0a65 6c73 653a 0a0a 2020 4064 6174 6163  .else:..  @datac
+0001bbf0: 6c61 7373 6573 2e64 6174 6163 6c61 7373  lasses.dataclass
+0001bc00: 0a20 2063 6c61 7373 2043 6f6d 7061 6374  .  class Compact
+0001bc10: 4e61 6d65 5363 6f70 653a 0a20 2020 2066  NameScope:.    f
+0001bc20: 6e3a 2043 616c 6c61 626c 650a 2020 2020  n: Callable.    
+0001bc30: 6d6f 6475 6c65 5f66 6e3a 2043 616c 6c61  module_fn: Calla
+0001bc40: 626c 650a 2020 2020 6e61 6d65 3a20 7374  ble.    name: st
+0001bc50: 720a 0a20 2020 2064 6566 205f 5f63 616c  r..    def __cal
+0001bc60: 6c5f 5f28 7365 6c66 2c20 2a61 7267 732c  l__(self, *args,
+0001bc70: 202a 2a6b 7761 7267 7329 202d 3e20 416e   **kwargs) -> An
+0001bc80: 793a 0a20 2020 2020 202e 2e2e 0a         y:.      ....
```

### Comparing `flax-0.8.3/flax/linen/normalization.py` & `flax-0.8.4/flax/linen/normalization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/partitioning.py` & `flax-0.8.4/flax/linen/partitioning.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/pooling.py` & `flax-0.8.4/flax/linen/pooling.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/recurrent.py` & `flax-0.8.4/flax/linen/recurrent.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/spmd.py` & `flax-0.8.4/flax/linen/spmd.py`

 * *Files 5% similar despite different names*

```diff
@@ -123,15 +123,18 @@
     raise ValueError(
       f'Unsupported: Dimensions {dups} occur more than once in array names.'
     )
   if not isinstance(rules, (tuple, list)):
     raise ValueError('Unknown axis rule specification type.')
   # We assign mesh axes using a priority based ruleset over logical axis names.
   result: List[Union[_UnassignedAxis, None, str, Tuple[str, ...]]]
-  result = [_unassigned_axis] * len(array_dim_names)
+  result = [
+      (_unassigned_axis if isinstance(name, str) else name)
+      for name in array_dim_names
+  ]
   for rule_model_name, rule_mesh_names in rules:
     if rule_model_name in array_dim_names:
       pos = array_dim_names.index(rule_model_name)
       if (
         _mesh_assignment_free(rule_mesh_names, result)
         and result[pos] == _unassigned_axis
       ):
@@ -260,17 +263,25 @@
       else:
         return x
   return _with_sharding_constraint(
     x, jax.sharding.PartitionSpec(*mesh_axes), mesh=mesh
   )
 
 
+def _is_axis_spec(x):
+  return (
+      isinstance(x, str)
+      or x is jax.sharding.PartitionSpec.UNCONSTRAINED
+      or x is None
+  )
+
+
 def _is_logical_spec(x):
   return x is None or (
-    isinstance(x, tuple) and all(isinstance(e, str) or e is None for e in x)
+      isinstance(x, tuple) and all(_is_axis_spec(e) for e in x)
   )
 
 
 def with_logical_constraint(
   x: ArrayPytree,
   logical_axis_resources: LogicalPartitionSpecPytree,
   rules: Optional[LogicalRules] = None,
```

### Comparing `flax-0.8.3/flax/linen/stochastic.py` & `flax-0.8.4/flax/linen/stochastic.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/summary.py` & `flax-0.8.4/flax/linen/summary.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/linen/transforms.py` & `flax-0.8.4/flax/linen/transforms.py`

 * *Files 0% similar despite different names*

```diff
@@ -493,14 +493,15 @@
     return obj
   elif isinstance(obj, Module):
     fingerprint: Any
     if obj._id in seen_modules:
       # if we have already seen the module we just use the index
       # as its static component
       fingerprint = seen_modules[obj._id]
+      return type(obj), fingerprint
     else:
       # if its a new module we add it to the cache and give it
       # a new index
       seen_modules[obj._id] = len(seen_modules)
       # TODO(cgarciae): define a way for the user of nn.jit to define
       # what fields it wants to ignore per Module instance.
       fingerprints = []
```

### Comparing `flax-0.8.3/flax/metrics/__init__.py` & `flax-0.8.4/flax/nnx/tests/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/metrics/tensorboard.py` & `flax-0.8.4/flax/metrics/tensorboard.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/serialization.py` & `flax-0.8.4/flax/serialization.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/struct.py` & `flax-0.8.4/flax/struct.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/testing/__init__.py` & `flax-0.8.4/flax/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/testing/benchmark.py` & `flax-0.8.4/flax/testing/benchmark.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/traceback_util.py` & `flax-0.8.4/flax/traceback_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/__init__.py` & `flax-0.8.4/flax/training/__init__.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/checkpoints.py` & `flax-0.8.4/flax/training/checkpoints.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/common_utils.py` & `flax-0.8.4/flax/training/common_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/dynamic_scale.py` & `flax-0.8.4/flax/training/dynamic_scale.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/early_stopping.py` & `flax-0.8.4/flax/training/early_stopping.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/lr_schedule.py` & `flax-0.8.4/flax/training/lr_schedule.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/orbax_utils.py` & `flax-0.8.4/flax/training/orbax_utils.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/prefetch_iterator.py` & `flax-0.8.4/flax/training/prefetch_iterator.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/training/train_state.py` & `flax-0.8.4/flax/training/train_state.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/traverse_util.py` & `flax-0.8.4/flax/traverse_util.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/flax/typing.py` & `flax-0.8.4/flax/typing.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,14 +21,15 @@
   Mapping,
   Optional,
   Protocol,
   Sequence,
   Tuple,
   TypeVar,
   Union,
+  runtime_checkable,
 )
 
 import jax
 from flax.core import FrozenDict
 
 import dataclasses
 
@@ -38,15 +39,15 @@
 Array = Union[jax.Array, Any]
 PRNGKey = jax.Array
 RNGSequences = Dict[str, PRNGKey]
 Dtype = Union[jax.typing.DTypeLike, Any]
 Shape = Sequence[int]
 K = TypeVar('K')
 
-
+@runtime_checkable
 class Key(Hashable, Protocol):
   def __lt__(self: K, value: K, /) -> bool:
     ...
 
 
 Path = str
 PathParts = Tuple[Key, ...]
```

### Comparing `flax-0.8.3/flax/version.py` & `flax-0.8.4/flax/version.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,8 +9,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Current Flax version at head on Github."""
-__version__ = '0.8.3'
+__version__ = '0.8.4'
```

### Comparing `flax-0.8.3/flax.egg-info/PKG-INFO` & `flax-0.8.4/flax.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flax
-Version: 0.8.3
+Version: 0.8.4
 Summary: Flax: A neural network library for JAX designed for flexibility
 Author-email: Flax team <flax-dev@google.com>
 Project-URL: homepage, https://github.com/google/flax
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
@@ -63,14 +63,16 @@
 
 
 [**Overview**](#overview)
 | [**Quick install**](#quick-install)
 | [**What does Flax look like?**](#what-does-flax-look-like)
 | [**Documentation**](https://flax.readthedocs.io/)
 
+** NEW**: Check out the [**NNX**](https://flax.readthedocs.io/en/latest/nnx/index.html) API!
+
 This README is a very short intro. **To learn everything you need to know about Flax, refer to our [full documentation](https://flax.readthedocs.io/).**
 
 Flax was originally started by engineers and researchers within the Brain Team in Google Research (in close collaboration with the JAX team), and is now developed jointly with the open source community.
 
 Flax is being used by a growing
 community of hundreds of folks in various Alphabet research departments
 for their daily work, as well as a [growing community
@@ -248,15 +250,15 @@
 To cite this repository:
 
 ```
 @software{flax2020github,
   author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
   title = {{F}lax: A neural network library and ecosystem for {JAX}},
   url = {http://github.com/google/flax},
-  version = {0.8.2},
+  version = {0.8.4},
   year = {2023},
 }
 ```
 
 In the above bibtex entry, names are in alphabetical order, the version number
 is intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.
```

### Comparing `flax-0.8.3/flax.egg-info/SOURCES.txt` & `flax-0.8.4/flax.egg-info/SOURCES.txt`

 * *Files 12% similar despite different names*

```diff
@@ -51,59 +51,53 @@
 docs/api_reference/flax.jax_utils.rst
 docs/api_reference/flax.serialization.rst
 docs/api_reference/flax.struct.rst
 docs/api_reference/flax.traceback_util.rst
 docs/api_reference/flax.training.rst
 docs/api_reference/flax.traverse_util.rst
 docs/api_reference/index.rst
-docs/api_reference/flax.experimental.nnx/helpers.rst
-docs/api_reference/flax.experimental.nnx/index.rst
-docs/api_reference/flax.experimental.nnx/module.rst
-docs/api_reference/flax.experimental.nnx/rnglib.rst
-docs/api_reference/flax.experimental.nnx/spmd.rst
-docs/api_reference/flax.experimental.nnx/transforms.rst
-docs/api_reference/flax.experimental.nnx/variables.rst
-docs/api_reference/flax.experimental.nnx/visualization.rst
-docs/api_reference/flax.experimental.nnx/nn/activations.rst
-docs/api_reference/flax.experimental.nnx/nn/attention.rst
-docs/api_reference/flax.experimental.nnx/nn/index.rst
-docs/api_reference/flax.experimental.nnx/nn/initializers.rst
-docs/api_reference/flax.experimental.nnx/nn/linear.rst
-docs/api_reference/flax.experimental.nnx/nn/normalization.rst
-docs/api_reference/flax.experimental.nnx/nn/stochastic.rst
-docs/api_reference/flax.experimental.nnx/training/index.rst
-docs/api_reference/flax.experimental.nnx/training/metrics.rst
-docs/api_reference/flax.experimental.nnx/training/optimizer.rst
 docs/api_reference/flax.linen/activation_functions.rst
 docs/api_reference/flax.linen/decorators.rst
 docs/api_reference/flax.linen/index.rst
 docs/api_reference/flax.linen/init_apply.rst
 docs/api_reference/flax.linen/initializers.rst
 docs/api_reference/flax.linen/inspection.rst
 docs/api_reference/flax.linen/layers.rst
 docs/api_reference/flax.linen/module.rst
 docs/api_reference/flax.linen/profiling.rst
 docs/api_reference/flax.linen/spmd.rst
 docs/api_reference/flax.linen/transformations.rst
 docs/api_reference/flax.linen/variable.rst
+docs/api_reference/flax.nnx/graph.rst
+docs/api_reference/flax.nnx/helpers.rst
+docs/api_reference/flax.nnx/index.rst
+docs/api_reference/flax.nnx/module.rst
+docs/api_reference/flax.nnx/rnglib.rst
+docs/api_reference/flax.nnx/spmd.rst
+docs/api_reference/flax.nnx/transforms.rst
+docs/api_reference/flax.nnx/variables.rst
+docs/api_reference/flax.nnx/visualization.rst
+docs/api_reference/flax.nnx/nn/activations.rst
+docs/api_reference/flax.nnx/nn/attention.rst
+docs/api_reference/flax.nnx/nn/index.rst
+docs/api_reference/flax.nnx/nn/initializers.rst
+docs/api_reference/flax.nnx/nn/linear.rst
+docs/api_reference/flax.nnx/nn/normalization.rst
+docs/api_reference/flax.nnx/nn/stochastic.rst
+docs/api_reference/flax.nnx/training/index.rst
+docs/api_reference/flax.nnx/training/metrics.rst
+docs/api_reference/flax.nnx/training/optimizer.rst
 docs/developer_notes/index.rst
 docs/developer_notes/lift.md
 docs/developer_notes/module_lifecycle.rst
 docs/examples/community_examples.rst
 docs/examples/core_examples.rst
 docs/examples/google_research_examples.rst
 docs/examples/index.rst
 docs/examples/repositories_that_use_flax.rst
-docs/experimental/index.rst
-docs/experimental/nnx/index.rst
-docs/experimental/nnx/mnist_tutorial.ipynb
-docs/experimental/nnx/mnist_tutorial.md
-docs/experimental/nnx/nnx_basics.ipynb
-docs/experimental/nnx/nnx_basics.md
-docs/experimental/nnx/transforms.rst
 docs/flip/0000-template.md
 docs/flip/1009-optimizer-api.md
 docs/flip/1777-default-dtype.md
 docs/flip/2396-rnn.md
 docs/flip/2434-general-metadata.md
 docs/flip/2974-kw-only-dataclasses.md
 docs/flip/3099-rnnbase-refactor.md
@@ -143,14 +137,20 @@
 docs/guides/training_techniques/dropout.rst
 docs/guides/training_techniques/index.rst
 docs/guides/training_techniques/lr_schedule.rst
 docs/guides/training_techniques/transfer_learning.ipynb
 docs/guides/training_techniques/transfer_learning.md
 docs/guides/training_techniques/use_checkpointing.ipynb
 docs/guides/training_techniques/use_checkpointing.md
+docs/nnx/index.rst
+docs/nnx/mnist_tutorial.ipynb
+docs/nnx/mnist_tutorial.md
+docs/nnx/nnx_basics.ipynb
+docs/nnx/nnx_basics.md
+docs/nnx/transforms.rst
 examples/README.md
 examples/__init__.py
 examples/cloud/README.md
 examples/cloud/launch_gce.py
 examples/cloud/startup_script.sh
 examples/imagenet/README.md
 examples/imagenet/imagenet.ipynb
@@ -303,97 +303,15 @@
 flax/core/variables.py
 flax/core/nn/__init__.py
 flax/core/nn/attention.py
 flax/core/nn/linear.py
 flax/core/nn/normalization.py
 flax/core/nn/stochastic.py
 flax/experimental/__init__.py
-flax/experimental/nnx/.gitignore
-flax/experimental/nnx/README.md
-flax/experimental/nnx/__init__.py
-flax/experimental/nnx/docs/blog.md
-flax/experimental/nnx/docs/demo.ipynb
-flax/experimental/nnx/docs/demo.md
-flax/experimental/nnx/docs/quick_start.ipynb
-flax/experimental/nnx/docs/tiny_nnx.ipynb
-flax/experimental/nnx/docs/why.ipynb
-flax/experimental/nnx/docs/why.md
-flax/experimental/nnx/docs/images/stateful-transforms.png
-flax/experimental/nnx/examples/lm1b/README.md
-flax/experimental/nnx/examples/lm1b/input_pipeline.py
-flax/experimental/nnx/examples/lm1b/input_pipeline_test.py
-flax/experimental/nnx/examples/lm1b/main.py
-flax/experimental/nnx/examples/lm1b/models.py
-flax/experimental/nnx/examples/lm1b/models_test.py
-flax/experimental/nnx/examples/lm1b/requirements.txt
-flax/experimental/nnx/examples/lm1b/temperature_sampler.py
-flax/experimental/nnx/examples/lm1b/temperature_sampler_test.py
-flax/experimental/nnx/examples/lm1b/tokenizer.py
-flax/experimental/nnx/examples/lm1b/train.py
-flax/experimental/nnx/examples/lm1b/train_test.py
-flax/experimental/nnx/examples/lm1b/utils.py
-flax/experimental/nnx/examples/lm1b/configs/default.py
-flax/experimental/nnx/examples/toy_examples/01_functional_api.py
-flax/experimental/nnx/examples/toy_examples/02_lifted_transforms.py
-flax/experimental/nnx/examples/toy_examples/05_vae.py
-flax/experimental/nnx/examples/toy_examples/06_scan_over_layers.py
-flax/experimental/nnx/examples/toy_examples/08_save_load_checkpoints.py
-flax/experimental/nnx/examples/toy_examples/09_parameter_surgery.py
-flax/experimental/nnx/examples/toy_examples/requirements.txt
-flax/experimental/nnx/nnx/__init__.py
-flax/experimental/nnx/nnx/compatibility.py
-flax/experimental/nnx/nnx/errors.py
-flax/experimental/nnx/nnx/filterlib.py
-flax/experimental/nnx/nnx/graph.py
-flax/experimental/nnx/nnx/helpers.py
-flax/experimental/nnx/nnx/ids.py
-flax/experimental/nnx/nnx/module.py
-flax/experimental/nnx/nnx/proxy_caller.py
-flax/experimental/nnx/nnx/reprlib.py
-flax/experimental/nnx/nnx/rnglib.py
-flax/experimental/nnx/nnx/spmd.py
-flax/experimental/nnx/nnx/state.py
-flax/experimental/nnx/nnx/tracers.py
-flax/experimental/nnx/nnx/transforms.py
-flax/experimental/nnx/nnx/variables.py
-flax/experimental/nnx/nnx/visualization.py
-flax/experimental/nnx/nnx/nn/__init__.py
-flax/experimental/nnx/nnx/nn/activations.py
-flax/experimental/nnx/nnx/nn/attention.py
-flax/experimental/nnx/nnx/nn/dtypes.py
-flax/experimental/nnx/nnx/nn/initializers.py
-flax/experimental/nnx/nnx/nn/linear.py
-flax/experimental/nnx/nnx/nn/normalization.py
-flax/experimental/nnx/nnx/nn/stochastic.py
-flax/experimental/nnx/nnx/training/metrics.py
-flax/experimental/nnx/nnx/training/optimizer.py
-flax/experimental/nnx/scripts/requirements.txt
-flax/experimental/nnx/scripts/run-all-examples.bash
-flax/experimental/nnx/tests/__init__.py
-flax/experimental/nnx/tests/test_compatibility.py
-flax/experimental/nnx/tests/test_containers.py
-flax/experimental/nnx/tests/test_graph_utils.py
-flax/experimental/nnx/tests/test_helpers.py
-flax/experimental/nnx/tests/test_ids.py
-flax/experimental/nnx/tests/test_integration.py
-flax/experimental/nnx/tests/test_metrics.py
-flax/experimental/nnx/tests/test_module.py
-flax/experimental/nnx/tests/test_optimizer.py
-flax/experimental/nnx/tests/test_partitioning.py
-flax/experimental/nnx/tests/test_rngs.py
-flax/experimental/nnx/tests/test_spmd.py
-flax/experimental/nnx/tests/test_state.py
-flax/experimental/nnx/tests/test_transforms.py
-flax/experimental/nnx/tests/test_variable.py
-flax/experimental/nnx/tests/nn/test_attention.py
-flax/experimental/nnx/tests/nn/test_conv.py
-flax/experimental/nnx/tests/nn/test_embed.py
-flax/experimental/nnx/tests/nn/test_linear.py
-flax/experimental/nnx/tests/nn/test_normalization.py
-flax/experimental/nnx/tests/nn/test_stochastic.py
+flax/experimental/nnx.py
 flax/linen/README.md
 flax/linen/__init__.py
 flax/linen/activation.py
 flax/linen/attention.py
 flax/linen/batch_apply.py
 flax/linen/combinators.py
 flax/linen/dtypes.py
@@ -409,14 +327,104 @@
 flax/linen/spmd.py
 flax/linen/stochastic.py
 flax/linen/summary.py
 flax/linen/transforms.py
 flax/linen/experimental/layers_with_named_axes.py
 flax/metrics/__init__.py
 flax/metrics/tensorboard.py
+flax/nnx/.gitignore
+flax/nnx/README.md
+flax/nnx/__init__.py
+flax/nnx/docs/blog.md
+flax/nnx/docs/demo.ipynb
+flax/nnx/docs/demo.md
+flax/nnx/docs/quick_start.ipynb
+flax/nnx/docs/tiny_nnx.ipynb
+flax/nnx/docs/why.ipynb
+flax/nnx/docs/why.md
+flax/nnx/docs/images/stateful-transforms.png
+flax/nnx/examples/lm1b/README.md
+flax/nnx/examples/lm1b/input_pipeline.py
+flax/nnx/examples/lm1b/input_pipeline_test.py
+flax/nnx/examples/lm1b/main.py
+flax/nnx/examples/lm1b/models.py
+flax/nnx/examples/lm1b/models_test.py
+flax/nnx/examples/lm1b/requirements.txt
+flax/nnx/examples/lm1b/temperature_sampler.py
+flax/nnx/examples/lm1b/temperature_sampler_test.py
+flax/nnx/examples/lm1b/tokenizer.py
+flax/nnx/examples/lm1b/train.py
+flax/nnx/examples/lm1b/train_test.py
+flax/nnx/examples/lm1b/utils.py
+flax/nnx/examples/lm1b/configs/default.py
+flax/nnx/examples/toy_examples/01_functional_api.py
+flax/nnx/examples/toy_examples/02_lifted_transforms.py
+flax/nnx/examples/toy_examples/05_vae.py
+flax/nnx/examples/toy_examples/06_scan_over_layers.py
+flax/nnx/examples/toy_examples/08_save_load_checkpoints.py
+flax/nnx/examples/toy_examples/09_parameter_surgery.py
+flax/nnx/examples/toy_examples/requirements.txt
+flax/nnx/nnx/__init__.py
+flax/nnx/nnx/errors.py
+flax/nnx/nnx/filterlib.py
+flax/nnx/nnx/graph.py
+flax/nnx/nnx/helpers.py
+flax/nnx/nnx/ids.py
+flax/nnx/nnx/module.py
+flax/nnx/nnx/object.py
+flax/nnx/nnx/proxy_caller.py
+flax/nnx/nnx/reprlib.py
+flax/nnx/nnx/rnglib.py
+flax/nnx/nnx/spmd.py
+flax/nnx/nnx/state.py
+flax/nnx/nnx/tracers.py
+flax/nnx/nnx/transforms.py
+flax/nnx/nnx/variables.py
+flax/nnx/nnx/visualization.py
+flax/nnx/nnx/compat/__init__.py
+flax/nnx/nnx/compat/module.py
+flax/nnx/nnx/compat/wrappers.py
+flax/nnx/nnx/nn/__init__.py
+flax/nnx/nnx/nn/activations.py
+flax/nnx/nnx/nn/attention.py
+flax/nnx/nnx/nn/dtypes.py
+flax/nnx/nnx/nn/initializers.py
+flax/nnx/nnx/nn/linear.py
+flax/nnx/nnx/nn/lora.py
+flax/nnx/nnx/nn/normalization.py
+flax/nnx/nnx/nn/stochastic.py
+flax/nnx/nnx/training/__init__.py
+flax/nnx/nnx/training/metrics.py
+flax/nnx/nnx/training/optimizer.py
+flax/nnx/scripts/requirements.txt
+flax/nnx/scripts/run-all-examples.bash
+flax/nnx/tests/__init__.py
+flax/nnx/tests/test_containers.py
+flax/nnx/tests/test_graph_utils.py
+flax/nnx/tests/test_helpers.py
+flax/nnx/tests/test_ids.py
+flax/nnx/tests/test_integration.py
+flax/nnx/tests/test_metrics.py
+flax/nnx/tests/test_module.py
+flax/nnx/tests/test_optimizer.py
+flax/nnx/tests/test_partitioning.py
+flax/nnx/tests/test_rngs.py
+flax/nnx/tests/test_spmd.py
+flax/nnx/tests/test_state.py
+flax/nnx/tests/test_transforms.py
+flax/nnx/tests/test_variable.py
+flax/nnx/tests/compat/test_module.py
+flax/nnx/tests/compat/test_wrappers.py
+flax/nnx/tests/nn/test_attention.py
+flax/nnx/tests/nn/test_conv.py
+flax/nnx/tests/nn/test_embed.py
+flax/nnx/tests/nn/test_linear.py
+flax/nnx/tests/nn/test_lora.py
+flax/nnx/tests/nn/test_normalization.py
+flax/nnx/tests/nn/test_stochastic.py
 flax/oss/ .git-blame-ignore-revs
 flax/testing/__init__.py
 flax/testing/benchmark.py
 flax/training/__init__.py
 flax/training/checkpoints.py
 flax/training/common_utils.py
 flax/training/dynamic_scale.py
```

### Comparing `flax-0.8.3/flax.egg-info/requires.txt` & `flax-0.8.4/flax.egg-info/requires.txt`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/images/flax_logo.png` & `flax-0.8.4/images/flax_logo.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/images/flax_logo.svg` & `flax-0.8.4/images/flax_logo.svg`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/images/flax_logo_250px.png` & `flax-0.8.4/images/flax_logo_250px.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/images/flax_logo_500px.png` & `flax-0.8.4/images/flax_logo_500px.png`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/pylintrc` & `flax-0.8.4/pylintrc`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/pyproject.toml` & `flax-0.8.4/pyproject.toml`

 * *Files 2% similar despite different names*

```diff
@@ -108,17 +108,18 @@
     "opt_einsum.*",
     "scipy.*",
     "libtpu.*",
     "jaxlib.mlir.*",
     "yaml",
 ]
 ignore_missing_imports = true
-# exclude nnx
+disable_error_code = "annotation-unchecked"
+# exclude nnx examples
 [[tool.mypy.overrides]]
-module = "flax.experimental.nnx.*"
+module = "flax.nnx.examples.*"
 ignore_errors = true
 
 [tool.pytest.ini_options]
 filterwarnings = [
     # By default error out on any warnings.
     "error",
     # Jax warning when no gpu/tpu found.
```

### Comparing `flax-0.8.3/tests/checkpoints_test.py` & `flax-0.8.4/tests/checkpoints_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/colab_tpu_jax_version.ipynb` & `flax-0.8.4/tests/colab_tpu_jax_version.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/configurations_test.py` & `flax-0.8.4/tests/configurations_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/core_frozen_dict_test.py` & `flax-0.8.4/tests/core/core_frozen_dict_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/core_lift_test.py` & `flax-0.8.4/tests/core/core_lift_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/core_meta_test.py` & `flax-0.8.4/tests/core/core_meta_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/core_scope_test.py` & `flax-0.8.4/tests/core/core_scope_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_auto_encoder_test.py` & `flax-0.8.4/tests/core/design/core_auto_encoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_big_resnets_test.py` & `flax-0.8.4/tests/core/design/core_big_resnets_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_custom_vjp_test.py` & `flax-0.8.4/tests/core/design/core_custom_vjp_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_dense_test.py` & `flax-0.8.4/tests/core/design/core_dense_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_flow_test.py` & `flax-0.8.4/tests/core/design/core_flow_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_resnet_test.py` & `flax-0.8.4/tests/core/design/core_resnet_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_scan_test.py` & `flax-0.8.4/tests/core/design/core_scan_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_tied_autoencoder_test.py` & `flax-0.8.4/tests/core/design/core_tied_autoencoder_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_vmap_test.py` & `flax-0.8.4/tests/core/design/core_vmap_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/core/design/core_weight_std_test.py` & `flax-0.8.4/tests/core/design/core_weight_std_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/cursor_test.py` & `flax-0.8.4/tests/cursor_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/download_dataset_metadata.sh` & `flax-0.8.4/tests/download_dataset_metadata.sh`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/early_stopping_test.py` & `flax-0.8.4/tests/early_stopping_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/import_test.ipynb` & `flax-0.8.4/tests/import_test.ipynb`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/io_test.py` & `flax-0.8.4/tests/io_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/jax_utils_test.py` & `flax-0.8.4/tests/jax_utils_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/initializers_test.py` & `flax-0.8.4/tests/linen/initializers_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/kw_only_dataclasses_test.py` & `flax-0.8.4/tests/linen/kw_only_dataclasses_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_activation_test.py` & `flax-0.8.4/tests/linen/linen_activation_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_attention_test.py` & `flax-0.8.4/tests/linen/linen_attention_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -67,14 +67,39 @@
       kernel_init=initializers.ones,
       bias_init=initializers.zeros,
       deterministic=False,
     )
     y, _ = sa_module.init_with_output(rng, q)
     self.assertEqual(y.shape, q.shape)
 
+  def test_mha_out_initializers(self):
+    rng = random.key(0)
+    q = jnp.ones((4, 2, 3, 5))
+    sa_module = nn.MultiHeadDotProductAttention(
+      num_heads=8,
+      qkv_features=16,
+      kernel_init=initializers.ones,
+      out_kernel_init=initializers.zeros,
+      bias_init=initializers.zeros,
+      out_bias_init=initializers.ones,
+      deterministic=False,
+    )
+    variables = sa_module.init(rng, q)
+    params = variables['params']
+    # test kernels
+    np.testing.assert_allclose(params['query']['kernel'], 1.0)
+    np.testing.assert_allclose(params['key']['kernel'], 1.0)
+    np.testing.assert_allclose(params['value']['kernel'], 1.0)
+    np.testing.assert_allclose(params['out']['kernel'], 0.0)
+    # test biases
+    np.testing.assert_allclose(params['query']['bias'], 0.0)
+    np.testing.assert_allclose(params['key']['bias'], 0.0)
+    np.testing.assert_allclose(params['value']['bias'], 0.0)
+    np.testing.assert_allclose(params['out']['bias'], 1.0)
+
   def test_multihead_self_attention_w_dropout(self):
     rng = random.key(0)
     x = jnp.ones((4, 2, 3, 5))
     sa_module = nn.MultiHeadDotProductAttention(
       num_heads=8,
       qkv_features=16,
       kernel_init=initializers.ones,
```

### Comparing `flax-0.8.3/tests/linen/linen_batch_apply_test.py` & `flax-0.8.4/tests/linen/linen_batch_apply_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_combinators_test.py` & `flax-0.8.4/tests/linen/linen_combinators_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_dtypes_test.py` & `flax-0.8.4/tests/linen/linen_dtypes_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_linear_test.py` & `flax-0.8.4/tests/linen/linen_linear_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_meta_test.py` & `flax-0.8.4/tests/linen/linen_meta_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_module_test.py` & `flax-0.8.4/tests/linen/linen_module_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_recurrent_test.py` & `flax-0.8.4/tests/linen/linen_recurrent_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_test.py` & `flax-0.8.4/tests/linen/linen_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/linen/linen_transforms_test.py` & `flax-0.8.4/tests/linen/linen_transforms_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -1901,14 +1901,62 @@
 
     x = jnp.array(1.0)
     self.assertEqual(n, 0)
 
     y = Parent().apply({}, x)
 
   @parameterized.named_parameters(('class', True), ('method', False))
+  def test_jit_stateful_submodules(self, jit_class: bool):
+    n = 0
+
+    class Foo(nn.Module):
+      key: int
+
+      @nn.compact
+      def __call__(self, x):
+        nonlocal n
+        n += 1
+        count = self.variable('counts', 'count', lambda: 0)
+        if not self.is_initializing():
+          count.value += 1
+        return x
+
+    if jit_class:
+      Foo = nn.jit(Foo)
+    else:  # jit method
+      Foo.__call__ = nn.jit(Foo.__call__)
+
+    class Parent(nn.Module):
+      @nn.compact
+      def __call__(self, x):
+        for _ in range(3):
+          m = Foo(0)
+          x = m(x)
+        return x
+
+    m = Parent()
+    x = jnp.array(1.0)
+    counts = m.init({}, x)['counts']
+    self.assertEqual(n, 1)
+
+    y, updates = m.apply({'counts': counts}, x, mutable=['counts'])
+    counts = updates['counts']
+    self.assertEqual(n, 2)
+
+    for count in jax.tree.leaves(counts):
+      self.assertEqual(count, 1)
+
+    y, updates = m.apply({'counts': counts}, x, mutable=['counts'])
+    counts = updates['counts']
+    self.assertEqual(n, 2)
+
+    for count in jax.tree.leaves(counts):
+      self.assertEqual(count, 2)
+
+  @parameterized.named_parameters(('class', True), ('method', False))
   def test_jit_reuse_nested_submodules(self, jit_class: bool):
     test = self
     n = 0
 
     class Foo(nn.Module):
       key: int
```

### Comparing `flax-0.8.3/tests/linen/partitioning_test.py` & `flax-0.8.4/tests/linen/partitioning_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -88,69 +88,83 @@
       )
       self.assertEqual(
         partitioning.logical_to_mesh_axes(('baz', 'bar', 'foo', 'unassigned')),
         ('data', None, 'model', None),
       )
 
   @parameterized.parameters(
-    dict(
-      rules=(('a', ('model', 'data')), ('b', 'data')),
-      axes=('a', 'b'),
-      expected=(('model', 'data'), None),
-    ),
-    dict(
-      rules=(('a', ('model', 'replica')), ('b', 'data')),
-      axes=('a', 'b'),
-      expected=(('model', 'replica'), 'data'),
-    ),
-    dict(
-      rules=(('a', ('model', 'replica')), ('b', ('data', 'model'))),
-      axes=('a', 'b'),
-      expected=(('model', 'replica'), None),
-    ),
-    dict(
-      rules=(('a', ('model', 'replica')), ('b', 'model')),
-      axes=('a', 'b', 'c'),
-      expected=(('model', 'replica'), None, None),
-    ),
-    dict(rules=(), axes=('a', 'b', 'c'), expected=(None, None, None)),
-    dict(
-      rules=(('a', None), ('a', 'model')),
-      axes=('a', 'b'),
-      expected=(None, None),
-    ),
-    dict(
-      rules=(
-        ('baz', 'data'),
-        ('bar', None),
-        ('foo', 'model'),
-        ('foo', 'data'),
-      ),
-      axes=('baz', 'bar', 'foo'),
-      expected=('data', None, 'model'),
-    ),
+      dict(
+          rules=(('a', ('model', 'data')), ('b', 'data')),
+          axes=('a', 'b'),
+          expected=(('model', 'data'), None),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', 'data')),
+          axes=('a', 'b'),
+          expected=(('model', 'replica'), 'data'),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', ('data', 'model'))),
+          axes=('a', 'b'),
+          expected=(('model', 'replica'), None),
+      ),
+      dict(
+          rules=(('a', ('model', 'replica')), ('b', 'model')),
+          axes=('a', 'b', 'c'),
+          expected=(('model', 'replica'), None, None),
+      ),
+      dict(rules=(), axes=('a', 'b', 'c'), expected=(None, None, None)),
+      dict(
+          rules=(('a', None), ('a', 'model')),
+          axes=('a', 'b'),
+          expected=(None, None),
+      ),
+      dict(
+          rules=(
+              ('baz', 'data'),
+              ('bar', None),
+              ('foo', 'model'),
+              ('foo', 'data'),
+          ),
+          axes=('baz', 'bar', 'foo'),
+          expected=('data', None, 'model'),
+      ),
+      dict(
+          rules=(('baz', 'data'), ('foo', ('model', 'emb'))),
+          axes=('baz', jax.sharding.PartitionSpec.UNCONSTRAINED, 'foo'),
+          expected=(
+              'data',
+              jax.sharding.PartitionSpec.UNCONSTRAINED,
+              ('model', 'emb'),
+          ),
+      ),
   )
   def test_logical_to_mesh_axes_cases(self, rules, axes, expected):
     with partitioning.axis_rules(rules):
       result = partitioning.logical_to_mesh_axes(axes)
     self.assertEqual(result, expected)
 
   @mock.patch('flax.linen.spmd._with_sharding_constraint')
   def test_with_sharding_constraint(self, wsc_fn):
+    unconstrained = jax.sharding.PartitionSpec.UNCONSTRAINED
     arr = jnp.ones((2, 2))
     axes = ('foo', 'bar')
     partitioning.set_axis_rules(())
     _ = partitioning.with_sharding_constraint(arr, axes)
     wsc_fn.assert_not_called()
     with partitioning.axis_rules(AXIS_RULES_1):
       _ = partitioning.with_sharding_constraint(arr, None)
       wsc_fn.assert_not_called()
       _ = partitioning.with_sharding_constraint(arr, axes)
       wsc_fn.assert_called_with(
-        arr, jax.sharding.PartitionSpec('data', 'model'), mesh=None
+          arr, jax.sharding.PartitionSpec('data', 'model'), mesh=None
+      )
+      _ = partitioning.with_sharding_constraint(arr, ('foo', unconstrained))
+      wsc_fn.assert_called_with(
+          arr, jax.sharding.PartitionSpec('data', unconstrained), mesh=None
       )
 
   @mock.patch('flax.linen.spmd._with_sharding_constraint')
   def test_with_sharding_constraint_fallback(self, wsc_fn):
     arr = jnp.ones((2, 2))
     with partitioning.axis_rules(AXIS_RULES_1):
       _ = partitioning.with_sharding_constraint(arr, ('foo', 'not_recognized'))
```

### Comparing `flax-0.8.3/tests/linen/summary_test.py` & `flax-0.8.4/tests/linen/summary_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/run_all_tests.sh` & `flax-0.8.4/tests/run_all_tests.sh`

 * *Files 12% similar despite different names*

```diff
@@ -81,15 +81,15 @@
   sphinx-build -M doctest docs docs/_build -T
   # test build html
   sphinx-build -M html docs docs/_build -T
   # test docstrings
   pytest -n auto flax \
     --doctest-modules \
     --suppress-no-test-exit-code \
-    --ignore=flax/experimental/nnx/examples
+    --ignore=flax/nnx/examples
 fi
 
 # check that flax is running on editable mode
 # (i.e. no notebook installed flax from pypi)
 echo "=== CHECKING FLAX IS EDITABLE ==="
 assert_error="flax is not running on editable mode."
 (cd docs; python -c "import flax; assert 'site-packages' not in flax.__file__, \"$assert_error\"")
@@ -108,50 +108,62 @@
       pytest -n auto $file $PYTEST_OPTS
       PYTEST_IGNORE+=" --ignore=$file"
   done
   # Run battery of core FLAX API tests.
   echo "pytest -n auto tests $PYTEST_OPTS $PYTEST_IGNORE"
   pytest -n auto tests $PYTEST_OPTS $PYTEST_IGNORE
   # Run nnx tests
-  pytest -n auto flax/experimental/nnx/tests $PYTEST_OPTS $PYTEST_IGNORE
+  pytest -n auto flax/nnx/tests $PYTEST_OPTS $PYTEST_IGNORE
+  pytest -n auto docs/_ext/codediff_test.py $PYTEST_OPTS $PYTEST_IGNORE
 
   # Per-example tests.
   #
   # we apply pytest within each example to avoid pytest's annoying test-filename collision.
   # In pytest foo/bar/baz_test.py and baz/bleep/baz_test.py will collide and error out when
   # /foo/bar and /baz/bleep aren't set up as packages.
   for egd in $(find examples -maxdepth 1 -mindepth 1 -type d); do
     # skip if folder starts with "_"
     if [[ $egd == *"_"* ]]; then
       continue
     fi
     pytest $egd
   done
 
-  for egd in $(find flax/experimental/nnx/examples -maxdepth 1 -mindepth 1 -type d); do
+  for egd in $(find flax/nnx/examples -maxdepth 1 -mindepth 1 -type d); do
     # skip if folder starts with "_" or is "toy_examples"
     if [[ $egd == *"_"* ]] || [[ $egd == *"toy_examples"* ]]; then
       continue
     fi
     pytest $egd
   done
-
 fi
 
 if $RUN_PYTYPE; then
   echo "=== RUNNING PYTYPE ==="
+  # Validate types in NNX examples.
+  for egd in $(find flax/nnx/examples -maxdepth 1 -mindepth 1 -type d); do
+    # skip if folder starts with "_" or is "toy_examples"
+    if [[ $egd == *"_"* ]] || [[ $egd == *"toy_examples"* ]]; then
+      continue
+    fi
+    # use cd to make sure pytype cache lives in example dir and doesn't name clash
+    # use *.py to avoid importing configs as a top-level import which leads to import errors
+    # because config files use relative imports (e.g. from config import ...).
+    (cd $egd ; pytype "*.py" --jobs auto --config ../../../../pyproject.toml)
+  done
   # Validate types in library code.
-  pytype --jobs auto --config pyproject.toml flax/ --exclude flax/experimental/nnx
+  pytype --jobs auto --config pyproject.toml flax/ \
+    --exclude flax/nnx/examples
 
   # Validate types in examples.
   for egd in $(find examples -maxdepth 1 -mindepth 1 -type d); do
       # use cd to make sure pytype cache lives in example dir and doesn't name clash
       # use *.py to avoid importing configs as a top-level import which leads to import errors
       # because config files use relative imports (e.g. from config import ...).
-      (cd $egd ; pytype --jobs auto --exclude flax/experimental/nnx --config ../../pyproject.toml "*.py")
+      (cd $egd ; pytype "*.py" --jobs auto --config ../../pyproject.toml)
   done
 fi
 
 if $RUN_MYPY; then
   echo "=== RUNNING MYPY ==="
   # Validate types in library code.
   mypy --config pyproject.toml flax/ --show-error-codes
```

### Comparing `flax-0.8.3/tests/serialization_test.py` & `flax-0.8.4/tests/serialization_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/struct_test.py` & `flax-0.8.4/tests/struct_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/tensorboard_test.py` & `flax-0.8.4/tests/tensorboard_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/traceback_util_test.py` & `flax-0.8.4/tests/traceback_util_test.py`

 * *Files identical despite different names*

### Comparing `flax-0.8.3/tests/traverse_util_test.py` & `flax-0.8.4/tests/traverse_util_test.py`

 * *Files identical despite different names*

