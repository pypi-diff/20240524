# Comparing `tmp/tensorpc-0.10.7.tar.gz` & `tmp/tensorpc-0.11.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "tensorpc-0.10.7.tar", last modified: Sat May 11 05:40:24 2024, max compression
+gzip compressed data, was "tensorpc-0.11.0.tar", last modified: Fri May 24 02:13:15 2024, max compression
```

## Comparing `tensorpc-0.10.7.tar` & `tensorpc-0.11.0.tar`

### file list

```diff
@@ -1,360 +1,369 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/
--rw-r--r--   0 runner    (1001) docker     (127)    11336 2024-05-11 05:39:55.000000 tensorpc-0.10.7/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-11 05:39:55.000000 tensorpc-0.10.7/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)     2105 2024-05-11 05:40:24.321910 tensorpc-0.10.7/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)      642 2024-05-11 05:39:55.000000 tensorpc-0.10.7/README.md
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-11 05:39:55.000000 tensorpc-0.10.7/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-11 05:40:24.321910 tensorpc-0.10.7/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     4406 2024-05-11 05:39:55.000000 tensorpc-0.10.7/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.277910 tensorpc-0.10.7/tensorpc/
--rw-r--r--   0 runner    (1001) docker     (127)      755 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)       23 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.277910 tensorpc-0.10.7/tensorpc/apps/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/apps/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.277910 tensorpc-0.10.7/tensorpc/apps/cbvc/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/apps/cbvc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.277910 tensorpc-0.10.7/tensorpc/apps/file/
--rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/apps/file/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.277910 tensorpc-0.10.7/tensorpc/autossh/
--rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      656 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    52830 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/core.py
--rw-r--r--   0 runner    (1001) docker     (127)      818 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/coretypes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/autossh/media/
--rw-r--r--   0 runner    (1001) docker     (127)     5929 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/media/hooks-bash-legacy.sh
--rw-r--r--   0 runner    (1001) docker     (127)     8475 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/media/hooks-bash.sh
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/autossh/scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7446 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/client.py
--rw-r--r--   0 runner    (1001) docker     (127)      764 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     3209 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/core.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/autossh/scheduler/init_scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/init_scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      210 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/init_scheduler/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/autossh/scheduler/runtask/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/runtask/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2788 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/runtask/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2609 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/task_client.py
--rw-r--r--   0 runner    (1001) docker     (127)      492 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/test_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     5518 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/scheduler/tmux.py
--rw-r--r--   0 runner    (1001) docker     (127)     3571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/serv_names.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/autossh/services/
--rw-r--r--   0 runner    (1001) docker     (127)      576 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12875 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/autossh/services/scheduler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/cppls/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/cppls/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      207 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/cppls/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/cpuusage/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/cpuusage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1085 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/cpuusage/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/createmsg/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/createmsg/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1318 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/createmsg/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/download_file/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/download_file/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/download_file/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.281910 tensorpc-0.10.7/tensorpc/cli/free_port/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/free_port/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      216 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/free_port/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/gpuusage/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/gpuusage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1426 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/gpuusage/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/iperf/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/iperf/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      908 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/iperf/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/ping/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/ping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      373 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/ping/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/proto_files/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/proto_files/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      964 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/proto_files/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/proto_root/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/proto_root/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      788 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/proto_root/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/pyls/
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/pyls/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      243 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/pyls/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/pyright_launch/
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/pyright_launch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      138 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/pyright_launch/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.285910 tensorpc-0.10.7/tensorpc/cli/start_worker/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/start_worker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1462 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/cli/start_worker/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1725 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/compat.py
--rw-r--r--   0 runner    (1001) docker     (127)      696 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.289910 tensorpc-0.10.7/tensorpc/core/
--rw-r--r--   0 runner    (1001) docker     (127)     1393 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17225 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/asyncclient.py
--rw-r--r--   0 runner    (1001) docker     (127)    13543 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/asyncserver.py
--rw-r--r--   0 runner    (1001) docker     (127)      397 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/asynctools.py
--rw-r--r--   0 runner    (1001) docker     (127)     2549 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/bgserver.py
--rw-r--r--   0 runner    (1001) docker     (127)    16973 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/client.py
--rw-r--r--   0 runner    (1001) docker     (127)      147 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    35849 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/core_io.py
--rw-r--r--   0 runner    (1001) docker     (127)      283 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/dataclass_dispatch.py
--rw-r--r--   0 runner    (1001) docker     (127)     1792 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/defs.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.289910 tensorpc-0.10.7/tensorpc/core/event_emitter/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/event_emitter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/event_emitter/aio.py
--rw-r--r--   0 runner    (1001) docker     (127)    12964 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/event_emitter/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     7113 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/funcid.py
--rw-r--r--   0 runner    (1001) docker     (127)    14601 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpclient.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.289910 tensorpc-0.10.7/tensorpc/core/httpservers/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpservers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13393 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpservers/aiohttp_impl.py
--rw-r--r--   0 runner    (1001) docker     (127)      576 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpservers/all.py
--rw-r--r--   0 runner    (1001) docker     (127)    10324 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpservers/blacksheep_impl.py
--rw-r--r--   0 runner    (1001) docker     (127)    35329 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/httpservers/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     4511 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/inspecttools.py
--rw-r--r--   0 runner    (1001) docker     (127)     2036 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/marker.py
--rw-r--r--   0 runner    (1001) docker     (127)     8940 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/moduleid.py
--rw-r--r--   0 runner    (1001) docker     (127)     1884 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/prim.py
--rw-r--r--   0 runner    (1001) docker     (127)      336 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/rprint_dispatch.py
--rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/server.py
--rw-r--r--   0 runner    (1001) docker     (127)    28998 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/server_core.py
--rw-r--r--   0 runner    (1001) docker     (127)    52173 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/serviceunit.py
--rw-r--r--   0 runner    (1001) docker     (127)    10714 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/tracer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4755 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/core/tree_id.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.289910 tensorpc-0.10.7/tensorpc/examples/
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.289910 tensorpc-0.10.7/tensorpc/examples/ai/
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/ai/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/ai/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.273910 tensorpc-0.10.7/tensorpc/examples/tutorials/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.293910 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/
--rw-r--r--   0 runner    (1001) docker     (127)      407 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.1-Hello World.md
--rw-r--r--   0 runner    (1001) docker     (127)     7599 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.10-Template Component.md
--rw-r--r--   0 runner    (1001) docker     (127)      958 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.11-Inheritance.md
--rw-r--r--   0 runner    (1001) docker     (127)     3465 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.12-App Context.md
--rw-r--r--   0 runner    (1001) docker     (127)     1350 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.13-Component Context.md
--rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.14-Host Resource.md
--rw-r--r--   0 runner    (1001) docker     (127)     5957 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.2-App Basic Architecture.md
--rw-r--r--   0 runner    (1001) docker     (127)     9827 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.3-Flex Box Basic.md
--rw-r--r--   0 runner    (1001) docker     (127)     1869 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.4-Containers.md
--rw-r--r--   0 runner    (1001) docker     (127)     3592 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.5-Advanced Events.md
--rw-r--r--   0 runner    (1001) docker     (127)     1655 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.6-Composite Component.md
--rw-r--r--   0 runner    (1001) docker     (127)     3370 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.7-Advanced UI Methods.md
--rw-r--r--   0 runner    (1001) docker     (127)     4209 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.8-Drag And Drop.md
--rw-r--r--   0 runner    (1001) docker     (127)      717 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.9-Reload System.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.297910 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/
--rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.0-Common Props.md
--rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.1-Button.md
--rw-r--r--   0 runner    (1001) docker     (127)     1544 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.10-Slider.md
--rw-r--r--   0 runner    (1001) docker     (127)    11336 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.11-Tab.md
--rw-r--r--   0 runner    (1001) docker     (127)     2069 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.12-Drawer.md
--rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.13-Dialog.md
--rw-r--r--   0 runner    (1001) docker     (127)     3956 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.14-Collapse.md
--rw-r--r--   0 runner    (1001) docker     (127)     2741 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.15-Chip.md
--rw-r--r--   0 runner    (1001) docker     (127)     4468 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.16-Progress.md
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.17-MenuList.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.18-JsonLikeTree.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.19-DynamicControl.md
--rw-r--r--   0 runner    (1001) docker     (127)     1669 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.2-Typography.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.20-VirtualizedBox.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.21-DataFlexBox.md
--rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.22-DataGrid.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.23-Special.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.24-Editor.md
--rw-r--r--   0 runner    (1001) docker     (127)     3350 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.25-Plotly.md
--rw-r--r--   0 runner    (1001) docker     (127)      783 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.26-Map.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.27-Allotment.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.28-JsonViewer.md
--rw-r--r--   0 runner    (1001) docker     (127)     2666 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.3-Markdown.md
--rw-r--r--   0 runner    (1001) docker     (127)     6342 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.4-TextField.md
--rw-r--r--   0 runner    (1001) docker     (127)     2665 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.5-List.md
--rw-r--r--   0 runner    (1001) docker     (127)     3132 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.6-Select.md
--rw-r--r--   0 runner    (1001) docker     (127)     3738 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.7-AutoComplete.md
--rw-r--r--   0 runner    (1001) docker     (127)     3633 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.8-ToggleButton.md
--rw-r--r--   0 runner    (1001) docker     (127)     1521 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.9-Image.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.297910 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/
--rw-r--r--   0 runner    (1001) docker     (127)     2405 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.1-Hello 3D World.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.2-3D Events.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.3-Controls.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.4-Selection.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.5-Views.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.6-Resources.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.7-Custom Shaders.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.297910 tensorpc-0.10.7/tensorpc/examples/tutorials/04-3d components/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/04-3d components/4.1-Canvas.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/04-3d components/4.2-Mesh.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/04-3d components/4.3-Points.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/04-3d components/4.4-Lines.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/examples/tutorials/05-advanced/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/05-advanced/5.1-Inspector.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/05-advanced/5.2-SimpleCanvas.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/examples/tutorials/06-sample apps/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/06-sample apps/6.1-Chat App.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/06-sample apps/6.2-Deep Learning.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/
--rw-r--r--   0 runner    (1001) docker     (127)     1843 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.1-Basic.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.2-Events.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.3-Lines.md
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.4-Image.md
--rw-r--r--   0 runner    (1001) docker     (127)      244 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.5-Points.md
--rw-r--r--   0 runner    (1001) docker     (127)     1929 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/examples/tutorials.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/flow/
--rw-r--r--   0 runner    (1001) docker     (127)     1131 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19011 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/client.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/flow/close_langserv/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/close_langserv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      773 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/close_langserv/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2337 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7240 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/coretypes.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.301910 tensorpc-0.10.7/tensorpc/flow/flowapp/
--rw-r--r--   0 runner    (1001) docker     (127)      849 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    76779 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/app.py
--rw-r--r--   0 runner    (1001) docker     (127)     8905 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/appcore.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.305910 tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/
--rw-r--r--   0 runner    (1001) docker     (127)      154 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2897 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/canvas.py
--rw-r--r--   0 runner    (1001) docker     (127)     5762 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     7068 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/inspector.py
--rw-r--r--   0 runner    (1001) docker     (127)     6172 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/colors.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.305910 tensorpc-0.10.7/tensorpc/flow/flowapp/components/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      234 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/annocore.py
--rw-r--r--   0 runner    (1001) docker     (127)     7405 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     2584 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/core.py
--rw-r--r--   0 runner    (1001) docker     (127)    13130 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/leaflet.py
--rw-r--r--   0 runner    (1001) docker     (127)   178760 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/mui.py
--rw-r--r--   0 runner    (1001) docker     (127)    15273 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plotly.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.309910 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/
--rw-r--r--   0 runner    (1001) docker     (127)     1490 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      793 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/arraycommon.py
--rw-r--r--   0 runner    (1001) docker     (127)    22472 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/arraygrid.py
--rw-r--r--   0 runner    (1001) docker     (127)    32169 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/canvas.py
--rw-r--r--   0 runner    (1001) docker     (127)     3628 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/collection.py
--rw-r--r--   0 runner    (1001) docker     (127)       51 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/common.py
--rw-r--r--   0 runner    (1001) docker     (127)    26537 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/config.py
--rw-r--r--   0 runner    (1001) docker     (127)     3908 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     5028 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/figure.py
--rw-r--r--   0 runner    (1001) docker     (127)    16189 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/grid_preview_layout.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.309910 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/
--rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10741 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     5494 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/gv_common.py
--rw-r--r--   0 runner    (1001) docker     (127)     6902 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/monitor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.309910 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/
--rw-r--r--   0 runner    (1001) docker     (127)      497 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18922 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)     5348 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/controllers.py
--rw-r--r--   0 runner    (1001) docker     (127)    24337 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/inspector.py
--rw-r--r--   0 runner    (1001) docker     (127)      970 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/inspectpanel.py
--rw-r--r--   0 runner    (1001) docker     (127)     8363 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/reload.py
--rw-r--r--   0 runner    (1001) docker     (127)    44599 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/tree.py
--rw-r--r--   0 runner    (1001) docker     (127)     8058 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/treeitems.py
--rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/options.py
--rw-r--r--   0 runner    (1001) docker     (127)     1719 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/reload_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    16020 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/scheduler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13938 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/scriptmgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     3422 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/sliders.py
--rw-r--r--   0 runner    (1001) docker     (127)     8583 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/tutorials.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    56801 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/canvas.py
--rw-r--r--   0 runner    (1001) docker     (127)     6316 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     4713 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/treeview.py
--rw-r--r--   0 runner    (1001) docker     (127)    27521 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/vapi_core.py
--rw-r--r--   0 runner    (1001) docker     (127)   144462 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/three.py
--rw-r--r--   0 runner    (1001) docker     (127)     1239 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/threecore.py
--rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/components/typemetas.py
--rw-r--r--   0 runner    (1001) docker     (127)    79929 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/core.py
--rw-r--r--   0 runner    (1001) docker     (127)      305 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/coretypes.py
--rw-r--r--   0 runner    (1001) docker     (127)     9629 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/objtree.py
--rw-r--r--   0 runner    (1001) docker     (127)     8238 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/flowapp/reload.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/init_langserv/
--rw-r--r--   0 runner    (1001) docker     (127)      575 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/init_langserv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/init_langserv/__main__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24688 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/jsonlike.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/langserv/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/langserv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3843 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/langserv/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     6845 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/langserv/pyls.py
--rw-r--r--   0 runner    (1001) docker     (127)     1781 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/langserv/pyrightcfg.py
--rw-r--r--   0 runner    (1001) docker     (127)     2956 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/marker.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/runapp/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/runapp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/runapp/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/sampleapp/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    59344 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/app.py
--rw-r--r--   0 runner    (1001) docker     (127)     5225 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/arraygrid.py
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/collection.py
--rw-r--r--   0 runner    (1001) docker     (127)    41653 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/d3.py
--rw-r--r--   0 runner    (1001) docker     (127)     2402 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/file.py
--rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/sample_preview.py
--rw-r--r--   0 runner    (1001) docker     (127)      994 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/sample_reload_fn.py
--rw-r--r--   0 runner    (1001) docker     (127)     5279 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/sampleapp/v_nextgen.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.313910 tensorpc-0.10.7/tensorpc/flow/serv/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/serv/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)   105903 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/serv/core.py
--rw-r--r--   0 runner    (1001) docker     (127)    19192 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/serv/flowapp.py
--rw-r--r--   0 runner    (1001) docker     (127)    29044 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/serv/worker.py
--rw-r--r--   0 runner    (1001) docker     (127)     8826 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/flow/serv_names.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.317910 tensorpc-0.10.7/tensorpc/protos/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2196 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/arraybuf_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)     2242 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/arraybuf_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)      158 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/arraybuf_pb2_grpc.py
--rw-r--r--   0 runner    (1001) docker     (127)     2595 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/remote_object_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)      215 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/remote_object_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)    21852 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/remote_object_pb2_grpc.py
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/rpc_message_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)     5647 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/rpc_message_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)     1157 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/wsdef_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos/wsdef_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)      334 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_export.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.317910 tensorpc-0.10.7/tensorpc/protos_legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9604 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/arraybuf_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)     2242 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/arraybuf_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)     6759 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/remote_object_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)    11472 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/remote_object_pb2_grpc.py
--rw-r--r--   0 runner    (1001) docker     (127)    25985 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/rpc_message_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)     5647 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/rpc_message_pb2.pyi
--rw-r--r--   0 runner    (1001) docker     (127)     3888 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/wsdef_pb2.py
--rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/protos_legacy/wsdef_pb2.pyi
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.317910 tensorpc-0.10.7/tensorpc/scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)        9 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/scheduler/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.317910 tensorpc-0.10.7/tensorpc/serve/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.317910 tensorpc-0.10.7/tensorpc/serve/flowapp_script/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve/flowapp_script/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1009 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve/flowapp_script/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/tensorpc/serve_sync/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve_sync/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1819 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/serve_sync/__main__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/tensorpc/services/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6567 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/collection.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/tensorpc/services/flow/
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/flow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/flow/core.py
--rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/for_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     1671 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/services/vis.py
--rw-r--r--   0 runner    (1001) docker     (127)     4581 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/tools.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/tensorpc/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1131 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      477 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/address.py
--rw-r--r--   0 runner    (1001) docker     (127)     9344 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/df_logging.py
--rw-r--r--   0 runner    (1001) docker     (127)     1704 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/gpuusage.py
--rw-r--r--   0 runner    (1001) docker     (127)     3215 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     2447 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/reload.py
--rw-r--r--   0 runner    (1001) docker     (127)      704 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/subproc.py
--rw-r--r--   0 runner    (1001) docker     (127)      491 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/typeutils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1403 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/uniquename.py
--rw-r--r--   0 runner    (1001) docker     (127)     2599 2024-05-11 05:39:55.000000 tensorpc-0.10.7/tensorpc/utils/wait_tools.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/tensorpc.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)     2105 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    12020 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)      381 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)        9 2024-05-11 05:40:24.000000 tensorpc-0.10.7/tensorpc.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-11 05:40:24.321910 tensorpc-0.10.7/test/
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-11 05:39:55.000000 tensorpc-0.10.7/test/test_tmux_scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.089941 tensorpc-0.11.0/
+-rw-r--r--   0 runner    (1001) docker     (127)    11336 2024-05-24 02:12:43.000000 tensorpc-0.11.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-24 02:12:43.000000 tensorpc-0.11.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)     2105 2024-05-24 02:13:15.089941 tensorpc-0.11.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)      642 2024-05-24 02:12:43.000000 tensorpc-0.11.0/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-24 02:12:43.000000 tensorpc-0.11.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-24 02:13:15.089941 tensorpc-0.11.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     4406 2024-05-24 02:12:43.000000 tensorpc-0.11.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.041941 tensorpc-0.11.0/tensorpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      755 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)       23 2024-05-24 02:13:14.000000 tensorpc-0.11.0/tensorpc/__version__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.041941 tensorpc-0.11.0/tensorpc/apps/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/apps/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.041941 tensorpc-0.11.0/tensorpc/apps/cbvc/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/apps/cbvc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.041941 tensorpc-0.11.0/tensorpc/apps/file/
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/apps/file/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/
+-rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      652 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    52981 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)      824 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/coretypes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/media/
+-rw-r--r--   0 runner    (1001) docker     (127)     5929 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/media/hooks-bash-legacy.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     8475 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/media/hooks-bash.sh
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8086 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)      755 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3234 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/scheduler/init_scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/init_scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      213 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/init_scheduler/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/scheduler/runtask/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/runtask/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3013 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/runtask/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2694 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/task_client.py
+-rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/test_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5637 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/scheduler/tmux.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3983 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/serv_names.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/autossh/services/
+-rw-r--r--   0 runner    (1001) docker     (127)      572 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13380 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/autossh/services/scheduler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/cli/cppls/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/cppls/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      206 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/cppls/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.045941 tensorpc-0.11.0/tensorpc/cli/cpuusage/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/cpuusage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1085 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/cpuusage/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/createmsg/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/createmsg/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1318 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/createmsg/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/download_file/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/download_file/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/download_file/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/free_port/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/free_port/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      216 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/free_port/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/gpuusage/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/gpuusage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1426 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/gpuusage/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/iperf/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/iperf/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1062 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/iperf/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/ping/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/ping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      373 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/ping/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/proto_files/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/proto_files/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      964 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/proto_files/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/proto_root/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/proto_root/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      788 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/proto_root/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/pyls/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/pyls/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      269 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/pyls/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/pyright_launch/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/pyright_launch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      141 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/pyright_launch/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.049941 tensorpc-0.11.0/tensorpc/cli/start_worker/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/start_worker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1462 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/cli/start_worker/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1726 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/compat.py
+-rw-r--r--   0 runner    (1001) docker     (127)      696 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.053941 tensorpc-0.11.0/tensorpc/core/
+-rw-r--r--   0 runner    (1001) docker     (127)     1417 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17227 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/asyncclient.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13373 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/asyncserver.py
+-rw-r--r--   0 runner    (1001) docker     (127)      397 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/asynctools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2988 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/bgserver.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16974 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)      147 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36210 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/core_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/dataclass_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1791 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/defs.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.053941 tensorpc-0.11.0/tensorpc/core/event_emitter/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/event_emitter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3725 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/event_emitter/aio.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12961 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/event_emitter/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7116 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/funcid.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14601 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpclient.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.057941 tensorpc-0.11.0/tensorpc/core/httpservers/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpservers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14786 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpservers/aiohttp_impl.py
+-rw-r--r--   0 runner    (1001) docker     (127)      572 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpservers/all.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11774 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpservers/blacksheep_impl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35936 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/httpservers/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4500 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/inspecttools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2081 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/marker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9219 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/moduleid.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1884 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/prim.py
+-rw-r--r--   0 runner    (1001) docker     (127)      357 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/rprint_dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9617 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/server.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29024 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/server_core.py
+-rw-r--r--   0 runner    (1001) docker     (127)    52742 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/serviceunit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10962 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/tracer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5091 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/core/tree_id.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.057941 tensorpc-0.11.0/tensorpc/examples/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.057941 tensorpc-0.11.0/tensorpc/examples/ai/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/ai/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/ai/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.037941 tensorpc-0.11.0/tensorpc/examples/tutorials/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.057941 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/
+-rw-r--r--   0 runner    (1001) docker     (127)      407 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.1-Hello World.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7599 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.10-Template Component.md
+-rw-r--r--   0 runner    (1001) docker     (127)      958 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.11-Inheritance.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3465 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.12-App Context.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1350 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.13-Component Context.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2230 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.14-Host Resource.md
+-rw-r--r--   0 runner    (1001) docker     (127)     5957 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.2-App Basic Architecture.md
+-rw-r--r--   0 runner    (1001) docker     (127)     9827 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.3-Flex Box Basic.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1869 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.4-Containers.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3592 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.5-Advanced Events.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1655 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.6-Composite Component.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3370 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.7-Advanced UI Methods.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4209 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.8-Drag And Drop.md
+-rw-r--r--   0 runner    (1001) docker     (127)      717 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.9-Reload System.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.061941 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/
+-rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.0-Common Props.md
+-rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.1-Button.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1544 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.10-Slider.md
+-rw-r--r--   0 runner    (1001) docker     (127)    11336 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.11-Tab.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2069 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.12-Drawer.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1301 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.13-Dialog.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3956 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.14-Collapse.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2741 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.15-Chip.md
+-rw-r--r--   0 runner    (1001) docker     (127)     4468 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.16-Progress.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.17-MenuList.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.18-JsonLikeTree.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.19-DynamicControl.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1669 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.2-Typography.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.20-VirtualizedBox.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.21-DataFlexBox.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.22-DataGrid.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.23-Special.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.24-Editor.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3350 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.25-Plotly.md
+-rw-r--r--   0 runner    (1001) docker     (127)      783 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.26-Map.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.27-Allotment.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.28-JsonViewer.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2666 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.3-Markdown.md
+-rw-r--r--   0 runner    (1001) docker     (127)     6342 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.4-TextField.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2665 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.5-List.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3132 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.6-Select.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3738 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.7-AutoComplete.md
+-rw-r--r--   0 runner    (1001) docker     (127)     3633 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.8-ToggleButton.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1521 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.9-Image.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/
+-rw-r--r--   0 runner    (1001) docker     (127)     2405 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.1-Hello 3D World.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.2-3D Events.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.3-Controls.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.4-Selection.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.5-Views.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.6-Resources.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.7-Custom Shaders.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/04-3d components/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/04-3d components/4.1-Canvas.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/04-3d components/4.2-Mesh.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/04-3d components/4.3-Points.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/04-3d components/4.4-Lines.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/05-advanced/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/05-advanced/5.1-Inspector.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/05-advanced/5.2-SimpleCanvas.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/06-sample apps/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/06-sample apps/6.1-Chat App.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/06-sample apps/6.2-Deep Learning.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/
+-rw-r--r--   0 runner    (1001) docker     (127)     1843 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.1-Basic.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.2-Events.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.3-Lines.md
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.4-Image.md
+-rw-r--r--   0 runner    (1001) docker     (127)      244 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.5-Points.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.065941 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/
+-rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.1-overview.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1347 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.2-custom node.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2103 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.3-layout.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1471 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.4-changenode.md
+-rw-r--r--   0 runner    (1001) docker     (127)     1783 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.5-drag and drop.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2506 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.6-handles.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2747 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials/08-flow/8.7-validation.md
+-rw-r--r--   0 runner    (1001) docker     (127)     2136 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/examples/tutorials.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.069941 tensorpc-0.11.0/tensorpc/flow/
+-rw-r--r--   0 runner    (1001) docker     (127)     1137 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19096 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/client.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.069941 tensorpc-0.11.0/tensorpc/flow/close_langserv/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/close_langserv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      773 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/close_langserv/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2336 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7764 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/coretypes.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.069941 tensorpc-0.11.0/tensorpc/flow/flowapp/
+-rw-r--r--   0 runner    (1001) docker     (127)      849 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    78029 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/app.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10123 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/appcore.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.069941 tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/
+-rw-r--r--   0 runner    (1001) docker     (127)      154 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3213 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/canvas.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5789 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7035 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/inspector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6180 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/colors.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.073941 tensorpc-0.11.0/tensorpc/flow/flowapp/components/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      232 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/annocore.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8040 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2579 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24543 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/flow.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13092 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/leaflet.py
+-rw-r--r--   0 runner    (1001) docker     (127)   178921 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/mui.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16199 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plotly.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.077941 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/
+-rw-r--r--   0 runner    (1001) docker     (127)     1490 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      790 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/arraycommon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22683 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/arraygrid.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32859 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/canvas.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3662 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/collection.py
+-rw-r--r--   0 runner    (1001) docker     (127)       51 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26667 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4069 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5028 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/figure.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17071 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/grid_preview_layout.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.077941 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/
+-rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10831 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5536 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/gv_common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7201 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/monitor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.077941 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/
+-rw-r--r--   0 runner    (1001) docker     (127)      499 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19417 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5628 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/controllers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24837 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/inspector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1052 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/inspectpanel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8457 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/reload.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46332 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/tree.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8437 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/treeitems.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1106 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/options.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1868 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/reload_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16401 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14791 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/scriptmgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3422 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/sliders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9651 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/tutorials.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.077941 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    56904 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/canvas.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6629 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4999 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/treeview.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28613 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/vapi_core.py
+-rw-r--r--   0 runner    (1001) docker     (127)   144563 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/three.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1239 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/threecore.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1908 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/components/typemetas.py
+-rw-r--r--   0 runner    (1001) docker     (127)    87768 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)      304 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/coretypes.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10325 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/objtree.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8339 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/flowapp/reload.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.077941 tensorpc-0.11.0/tensorpc/flow/init_langserv/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/init_langserv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      269 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/init_langserv/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24705 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/jsonlike.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.081941 tensorpc-0.11.0/tensorpc/flow/langserv/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/langserv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4021 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/langserv/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6963 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/langserv/pyls.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1869 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/langserv/pyrightcfg.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2957 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/marker.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.081941 tensorpc-0.11.0/tensorpc/flow/runapp/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/runapp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1105 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/runapp/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.081941 tensorpc-0.11.0/tensorpc/flow/sampleapp/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60604 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/app.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5562 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/arraygrid.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/collection.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41665 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/d3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2324 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/file.py
+-rw-r--r--   0 runner    (1001) docker     (127)      435 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/sample_preview.py
+-rw-r--r--   0 runner    (1001) docker     (127)      977 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/sample_reload_fn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5700 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/sampleapp/v_nextgen.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.081941 tensorpc-0.11.0/tensorpc/flow/serv/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/serv/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)   106984 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/serv/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20216 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/serv/flowapp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29044 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/serv/worker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8702 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/flow/serv_names.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/protos/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2241 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/arraybuf_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2242 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/arraybuf_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)      158 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/arraybuf_pb2_grpc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2649 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/remote_object_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      215 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/remote_object_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)    23442 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/remote_object_pb2_grpc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4040 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/rpc_message_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5647 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/rpc_message_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)     1170 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/wsdef_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos/wsdef_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)      335 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_export.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/protos_legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15299 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/arraybuf_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2242 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/arraybuf_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)     7694 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/remote_object_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12754 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/remote_object_pb2_grpc.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33499 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/rpc_message_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5647 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/rpc_message_pb2.pyi
+-rw-r--r--   0 runner    (1001) docker     (127)     5607 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/wsdef_pb2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      936 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/protos_legacy/wsdef_pb2.pyi
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)        9 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)       60 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/scheduler/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/serve/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1923 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/serve/flowapp_script/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve/flowapp_script/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1036 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve/flowapp_script/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/serve_sync/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve_sync/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1819 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/serve_sync/__main__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.085941 tensorpc-0.11.0/tensorpc/services/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6711 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/collection.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.089941 tensorpc-0.11.0/tensorpc/services/flow/
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/flow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      571 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/flow/core.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1880 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/for_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1671 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/services/vis.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4615 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.089941 tensorpc-0.11.0/tensorpc/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1131 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      477 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/address.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9344 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/df_logging.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1704 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/gpuusage.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3216 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2447 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/reload.py
+-rw-r--r--   0 runner    (1001) docker     (127)      704 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/subproc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      491 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/typeutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1528 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/uniquename.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2599 2024-05-24 02:12:43.000000 tensorpc-0.11.0/tensorpc/utils/wait_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.089941 tensorpc-0.11.0/tensorpc.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)     2105 2024-05-24 02:13:14.000000 tensorpc-0.11.0/tensorpc.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    12434 2024-05-24 02:13:15.000000 tensorpc-0.11.0/tensorpc.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-24 02:13:14.000000 tensorpc-0.11.0/tensorpc.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      381 2024-05-24 02:13:14.000000 tensorpc-0.11.0/tensorpc.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        9 2024-05-24 02:13:14.000000 tensorpc-0.11.0/tensorpc.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-24 02:13:15.089941 tensorpc-0.11.0/test/
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-24 02:12:43.000000 tensorpc-0.11.0/test/test_tmux_scheduler.py
```

### Comparing `tensorpc-0.10.7/LICENSE` & `tensorpc-0.11.0/LICENSE`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/PKG-INFO` & `tensorpc-0.11.0/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 Metadata-Version: 2.1
 Name: tensorpc
-Version: 0.10.7
+Version: 0.11.0
 Summary: Backend for devflow.
 Home-page: https://github.com/FindDefinition/tensorpc
 Author: Yan Yan
 Author-email: yanyan.sub@outlook.com
 License: MIT
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: Implementation :: CPython
 Classifier: Programming Language :: Python :: Implementation :: PyPy
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: protobuf>=4.21.6
-Requires-Dist: grpcio>=1.49.0
-Requires-Dist: grpcio-tools>=1.49.0
+Requires-Dist: protobuf>=3.18.0
+Requires-Dist: grpcio>=1.48.2
+Requires-Dist: grpcio-tools>=1.48.2
 Requires-Dist: fire
 Requires-Dist: pytest
 Requires-Dist: pytest-asyncio
 Requires-Dist: pyyaml
 Requires-Dist: numpy
 Requires-Dist: msgpack
 Requires-Dist: requests
```

### Comparing `tensorpc-0.10.7/README.md` & `tensorpc-0.11.0/README.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/setup.py` & `tensorpc-0.11.0/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 EMAIL = 'yanyan.sub@outlook.com'
 AUTHOR = 'Yan Yan'
 REQUIRES_PYTHON = '>=3.8'
 VERSION = None
 
 # What packages are required for this module to be executed?
 REQUIRED = [
-    "protobuf>=4.21.6",
-    "grpcio>=1.49.0",
-    "grpcio-tools>=1.49.0",
+    "protobuf>=3.18.0",
+    "grpcio>=1.48.2",
+    "grpcio-tools>=1.48.2",
     "fire",
     "pytest",
     "pytest-asyncio",
     "pyyaml",
     "numpy",
     "msgpack",
     "requests",
```

### Comparing `tensorpc-0.10.7/tensorpc/__init__.py` & `tensorpc-0.11.0/tensorpc/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/__main__.py` & `tensorpc-0.11.0/tensorpc/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/apps/__init__.py` & `tensorpc-0.11.0/tensorpc/apps/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/apps/cbvc/__init__.py` & `tensorpc-0.11.0/tensorpc/apps/cbvc/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/apps/file/__init__.py` & `tensorpc-0.11.0/tensorpc/apps/file/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/autossh/__init__.py` & `tensorpc-0.11.0/tensorpc/autossh/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/autossh/constants.py` & `tensorpc-0.11.0/tensorpc/autossh/services/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,13 @@
-# Copyright 2023 Yan Yan
-# 
+# Copyright 2023 tusimple
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-import os 
-
-TENSORPC_ASYNCSSH_PROXY = os.getenv("TENSORPC_ASYNCSSH_PROXY", None)
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/core.py` & `tensorpc-0.11.0/tensorpc/autossh/core.py`

 * *Files 0% similar despite different names*

```diff
@@ -78,22 +78,23 @@
     COMMAND_COMPLETE = "D"
     CURRENT_COMMAND = "E"
 
     UPDATE_CWD = "P"
     CONTINUATION_START = "F"
     CONTINUATION_END = "G"
 
+
 class CommandEventParseState(enum.IntEnum):
-    VscPromptStart = 0 # reached when we encounter \033
+    VscPromptStart = 0  # reached when we encounter \033
     # VscCmdIdReached = 1 # reached when we encounter \]784;
-    VscCmdCodeABCFG = 2 # reached when we encounter A/B/C/F/G
-    VscCmdCodeD = 3 # reached when we encounter D
-    VscCmdCodeE = 4 # reached when we encounter E
-    VscCmdCodeP = 5 # reached when we encounter P
-    VscPromptEnd = 100 # reached when we encounter \007, idle state
+    VscCmdCodeABCFG = 2  # reached when we encounter A/B/C/F/G
+    VscCmdCodeD = 3  # reached when we encounter D
+    VscCmdCodeE = 4  # reached when we encounter E
+    VscCmdCodeP = 5  # reached when we encounter P
+    VscPromptEnd = 100  # reached when we encounter \007, idle state
 
 
 class CommandParseSpecialCharactors:
     Start = b"\033"
     StartAll = b"\033]784;"
 
     End = b"\007"
@@ -109,14 +110,15 @@
         return ANSI_ESCAPE_REGEX.sub(b'',
                                      string.encode("utf-8")).decode("utf-8")
     else:
         return ANSI_ESCAPE_REGEX.sub(b'', string).decode("utf-8")
 
 
 class OutData:
+
     def __init__(self) -> None:
         pass
 
 
 class Event:
     name = "Event"
 
@@ -332,14 +334,15 @@
     # more info: https://stackoverflow.com/a/43810272/1113207
     task.cancel()
     with suppress(asyncio.CancelledError):
         await task
 
 
 class ReadResult:
+
     def __init__(self,
                  data: Any,
                  is_eof: bool,
                  is_exc: bool,
                  traceback_str: str = "",
                  should_exit: bool = True) -> None:
         self.data = data
@@ -358,14 +361,15 @@
 
 _ENCODE = "utf-8"
 # _ENCODE = "latin-1"
 
 
 class SocketProxyTunnel:
     """A wrapper which opens a socket you can run an SSH connection over"""
+
     def __init__(self, proxy_url):
         self.proxy_url = proxy_url
 
     async def create_connection(self, protocol_factory, host, port):
         from python_socks.sync import Proxy
         """Return a channel and transport to run SSH over"""
         proxy = Proxy.from_url(self.proxy_url)
@@ -376,14 +380,15 @@
 
 class PeerSSHClient:
     """
     during handle stdout/err message, client will 
     1. identifier extraction
     2. code path detection
     """
+
     def __init__(self,
                  stdin: asyncssh.stream.SSHWriter,
                  stdout: asyncssh.stream.SSHReader,
                  stderr: asyncssh.stream.SSHReader,
                  separators: bytes = _DEFAULT_SEPARATORS,
                  uid: str = "",
                  encoding: Optional[str] = None):
@@ -544,20 +549,22 @@
 
 
 class SSHRequestType(enum.Enum):
     ChangeSize = 0
 
 
 class SSHRequest:
+
     def __init__(self, type: SSHRequestType, data: Any) -> None:
         self.type = type
         self.data = data
 
 
 class MySSHClientStreamSession(asyncssh.stream.SSHClientStreamSession):
+
     def __init__(self) -> None:
         super().__init__()
         self.callback: Optional[Callable[[Event], Awaitable[None]]] = None
         self.uid = ""
 
     def data_received(self, data: bytes, datatype) -> None:
         res = super().data_received(data, datatype)
@@ -646,33 +653,37 @@
                     recv_buf[:curbuf] = []
                     self._recv_buf_len -= buflen
                     self._maybe_resume_reading()
                     raise asyncio.IncompleteReadError(cast(bytes, buf), None)
                 print("WTF")
                 await self._block_read(datatype)
 
-class VscodeStyleSSHClientStreamSession(asyncssh.stream.SSHClientStreamSession):
+
+class VscodeStyleSSHClientStreamSession(asyncssh.stream.SSHClientStreamSession
+                                        ):
+
     def __init__(self) -> None:
         super().__init__()
         self.callback: Optional[Callable[[Event], Awaitable[None]]] = None
         self.uid = ""
 
-        self.state = CommandEventParseState.VscPromptEnd # idle
+        self.state = CommandEventParseState.VscPromptEnd  # idle
 
     def data_received(self, data: bytes, datatype) -> None:
         res = super().data_received(data, datatype)
         if self.callback is not None:
             ts = time.time_ns()
             res_str = data
             loop = asyncio.get_running_loop()
             asyncio.run_coroutine_threadsafe(
                 self.callback(RawEvent(ts, res_str, False, self.uid)), loop)
         return res
 
-    async def readuntil(self, separator: object,
+    async def readuntil(self,
+                        separator: object,
                         datatype: asyncssh.DataType,
                         max_separator_len: int = 0) -> AnyStr:
         """Read data from the channel until a separator is seen"""
 
         if not separator:
             raise ValueError('Separator cannot be empty')
 
@@ -716,19 +727,21 @@
                             else:
                                 raise cast(Exception, exc)
 
                     newbuf = cast(AnyStr, recv_buf[curbuf])
                     buf += newbuf
                     start = 0
                     # rprint(self.state, buf)
-                    idx_start_all = buf.find(CommandParseSpecialCharactors.StartAll)
+                    idx_start_all = buf.find(
+                        CommandParseSpecialCharactors.StartAll)
                     idx_start = buf.find(CommandParseSpecialCharactors.Start)
                     # ensure if buf start is partial, we should wait for all possible string available.
                     if idx_start != -1:
-                        if len(buf) - start >= len(CommandParseSpecialCharactors.StartAll):
+                        if len(buf) - start >= len(
+                                CommandParseSpecialCharactors.StartAll):
                             if idx_start_all == -1:
                                 idx_start = -1
                     idx_end = buf.find(CommandParseSpecialCharactors.End)
                     if idx_start_all != -1 and idx_end != -1:
                         if idx_start_all < idx_end:
                             match = pat.search(buf, start)
                             if match:
@@ -793,15 +806,14 @@
                             buf = buf[:idx]
                             self._recv_buf_len -= idx
                             if not recv_buf[0]:
                                 recv_buf.pop(0)
                             self._maybe_resume_reading()
                             return buf
 
-
                     # if self.state == CommandEventParseState.VscPromptEnd:
                     #     idx = buf.find(CommandParseSpecialCharactors.Start)
                     #     if idx != -1:
                     #         # clear buf before start
                     #         self.state = CommandEventParseState.VscPromptStart
                     #         recv_buf[:curbuf] = []
                     #         recv_buf[0] = buf[idx:]
@@ -935,15 +947,17 @@
                     recv_buf[:curbuf] = []
                     self._recv_buf_len -= buflen
                     self._maybe_resume_reading()
                     raise asyncio.IncompleteReadError(cast(bytes, buf), None)
 
                 await self._block_read(datatype)
 
+
 class SSHClient:
+
     def __init__(self,
                  url: str,
                  username: str,
                  password: str,
                  known_hosts,
                  uid: str = "",
                  encoding: Optional[str] = None) -> None:
@@ -962,16 +976,17 @@
         self.bash_file_inited: bool = False
         self.encoding = encoding
         if TENSORPC_ASYNCSSH_PROXY is not None:
             try:
                 import python_socks
                 self.tunnel = SocketProxyTunnel(TENSORPC_ASYNCSSH_PROXY)
             except ImportError:
-                warnings.warn("you provide TENSORPC_ASYNCSSH_PROXY but python_socks not installed."
-                              " use 'pip install python-socks' and restart server.")
+                warnings.warn(
+                    "you provide TENSORPC_ASYNCSSH_PROXY but python_socks not installed."
+                    " use 'pip install python-socks' and restart server.")
                 self.tunnel = None
         else:
             self.tunnel = None
 
     @classmethod
     def from_ssh_target(cls, target: SSHTarget):
         url = f"{target.hostname}:{target.port}"
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/coretypes.py` & `tensorpc-0.11.0/tensorpc/autossh/coretypes.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,31 +1,32 @@
-
-import dataclasses 
+import dataclasses
 
 import enum
 from typing import Dict, Any, List, Optional, Tuple
 
 
 @dataclasses.dataclass
 class SSHTarget:
-    hostname: str 
-    port: int 
-    username: str 
-    password: str 
-    known_hosts: Optional[str] = None 
-    client_keys: Optional[List[str]] = None 
+    hostname: str
+    port: int
+    username: str
+    password: str
+    known_hosts: Optional[str] = None
+    client_keys: Optional[List[str]] = None
     env: Optional[Dict[str, str]] = None
     uid: str = ""
-    forward_port_pairs: List[Tuple[int, int]] = dataclasses.field(default_factory=list)
-    remote_forward_port_pairs: List[Tuple[int, int]] = dataclasses.field(default_factory=list)
+    forward_port_pairs: List[Tuple[int, int]] = dataclasses.field(
+        default_factory=list)
+    remote_forward_port_pairs: List[Tuple[int, int]] = dataclasses.field(
+        default_factory=list)
     init_commands: str = ""
-    
-    @property 
+
+    @property
     def url(self):
         return f"{self.hostname}:{self.port}"
 
     @staticmethod
     def create_fake_target():
         return SSHTarget("localhost", 22, "root", "root")
 
     def is_localhost(self):
-        return self.hostname == "localhost"
+        return self.hostname == "localhost"
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/media/hooks-bash-legacy.sh` & `tensorpc-0.11.0/tensorpc/autossh/media/hooks-bash-legacy.sh`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/autossh/media/hooks-bash.sh` & `tensorpc-0.11.0/tensorpc/autossh/media/hooks-bash.sh`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/client.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,38 +1,41 @@
 import dataclasses
 import time
-from typing import Dict, List, Set, Tuple 
+from typing import Dict, List, Set, Tuple
 import libtmux
 import asyncio
-from tensorpc.autossh.serv_names import serv_names 
+from tensorpc.autossh.serv_names import serv_names
 from tensorpc.autossh.scheduler.constants import TMUX_SESSION_PREFIX, TMUX_SESSION_NAME_SPLIT
 from tensorpc.autossh.scheduler.core import ResourceType, SSHTarget, Task, TaskStatus, TaskType
 from tensorpc.core.asyncclient import shutdown_server_async, simple_remote_call_async
 from tensorpc.utils.wait_tools import get_free_ports
 from tensorpc.autossh.core import SSHClient
 from tensorpc.constants import TENSORPC_SPLIT
 import base64, json
 import uuid
 from tensorpc import prim
 from tensorpc.autossh.scheduler.tmux import get_tmux_scheduler_info_may_create
 from tensorpc import AsyncRemoteManager, simple_chunk_call_async
 
+
 class SchedulerClient:
+
     def __init__(self, ssh_target: SSHTarget) -> None:
         assert isinstance(ssh_target, SSHTarget)
         self.ssh_target = dataclasses.replace(ssh_target)
         self.tunnel_tasks: List[asyncio.Task] = []
         self.port = -1
 
     async def async_init(self):
         self.shutdown_ev = asyncio.Event()
 
         if prim.is_in_server_context():
             self.shutdown_ev = prim.get_async_shutdown_event()
-            self.shutdown_task = asyncio.create_task(prim.get_async_shutdown_event().wait())
+            self.shutdown_task = asyncio.create_task(
+                prim.get_async_shutdown_event().wait())
         else:
             self.shutdown_task = asyncio.create_task(self.shutdown_ev.wait())
 
         # fetch scheduler session in all ssh targets
         target = self.ssh_target
         hostname = target.hostname.strip()
         is_local = hostname == "localhost" or hostname == "127.0.0.1"
@@ -43,95 +46,121 @@
             target.forward_port_pairs.append((port, port))
             self.port = port
         else:
             # print(target)
             async with client.simple_connect(False) as conn:
                 # try:
                 if target.init_commands:
-                    result = await conn.run(f"bash -i -c \"{target.init_commands} && python -m tensorpc.autossh.scheduler.init_scheduler\"", check=True)
+                    result = await conn.run(
+                        f"bash -i -c \"{target.init_commands} && python -m tensorpc.autossh.scheduler.init_scheduler\"",
+                        check=True)
                 else:
-                    result = await conn.run("bash -i -c \"python -m tensorpc.autossh.scheduler.init_scheduler\"", check=True)
+                    result = await conn.run(
+                        "bash -i -c \"python -m tensorpc.autossh.scheduler.init_scheduler\"",
+                        check=True)
                 # except Exception as e:
                 #     print(e)
-                #     raise e 
+                #     raise e
                 # print(result.stdout, result.stderr)
                 stdout = result.stdout
-                assert stdout is not None 
+                assert stdout is not None
                 if isinstance(stdout, bytes):
                     stdout = stdout.decode("utf-8")
                 port_str, schr_uid = stdout.strip().split(",")
                 port = int(port_str)
             local_free_port = get_free_ports(1)[0]
-            self.tunnel_tasks.append(asyncio.create_task(client.create_local_tunnel([(local_free_port, port)], self.shutdown_task)))
+            self.tunnel_tasks.append(
+                asyncio.create_task(
+                    client.create_local_tunnel([(local_free_port, port)],
+                                               self.shutdown_task)))
             target.forward_port_pairs.append((local_free_port, port))
             self.port = local_free_port
         self.local_url = f"localhost:{self.port}"
         self.schr_uid = schr_uid
         self.schr_session_name = f"{TMUX_SESSION_PREFIX}{TMUX_SESSION_NAME_SPLIT}{port}{TMUX_SESSION_NAME_SPLIT}{schr_uid}"
 
-        # fetch init task state 
+        # fetch init task state
         async with AsyncRemoteManager(self.local_url) as robj:
             await robj.wait_for_channel_ready()
-            all_tasks: List[Task] = await robj.chunked_remote_call(serv_names.SCHED_TASK_GET_ALL_TASK)
+            all_tasks: List[Task] = await robj.chunked_remote_call(
+                serv_names.SCHED_TASK_GET_ALL_TASK)
         self.tasks = {t.id: t for t in all_tasks}
 
     async def update_tasks(self, tmux_pane_lines: int = 0):
         ts_uids = [(t.state.timestamp, t.id) for t in self.tasks.values()]
-        updated, deleted_uids = await simple_chunk_call_async(self.local_url, serv_names.SCHED_TASK_QUERY_UPDATES, ts_uids, tmux_pane_lines)
+        updated, deleted_uids = await simple_chunk_call_async(
+            self.local_url, serv_names.SCHED_TASK_QUERY_UPDATES, ts_uids,
+            tmux_pane_lines)
         for t in updated:
             self.tasks[t.id] = t
         for uid in deleted_uids:
             self.tasks.pop(uid, None)
         return updated, deleted_uids
-    
-    async def query_tmux_panes(self, task_ids: List[str], tmux_pane_lines: int):
-        return await simple_chunk_call_async(self.local_url, serv_names.SCHED_TASK_QUERY_TMUX_PANES, task_ids, tmux_pane_lines)
+
+    async def query_tmux_panes(self, task_ids: List[str],
+                               tmux_pane_lines: int):
+        return await simple_chunk_call_async(
+            self.local_url, serv_names.SCHED_TASK_QUERY_TMUX_PANES, task_ids,
+            tmux_pane_lines)
 
     async def get_resource_usage(self):
-        idle_resources, occupied_resources = await simple_chunk_call_async(self.local_url, serv_names.SCHED_TASK_RESOURCE_USAGE)
+        idle_resources, occupied_resources = await simple_chunk_call_async(
+            self.local_url, serv_names.SCHED_TASK_RESOURCE_USAGE)
         idle_resources: Dict[ResourceType, Set[Tuple[ResourceType, int]]]
         occupied_resources: Dict[ResourceType, Set[Tuple[ResourceType, int]]]
         return idle_resources, occupied_resources
 
     async def submit_task(self, task: Task):
-        return await simple_remote_call_async(self.local_url, serv_names.SCHED_TASK_SUBMIT_TASK, task)
-    
+        return await simple_remote_call_async(
+            self.local_url, serv_names.SCHED_TASK_SUBMIT_TASK, task)
+
     async def cancel_task(self, task_id: str):
         """use Ctrl-C to cancel task
         """
-        return await simple_remote_call_async(self.local_url, serv_names.SCHED_TASK_CANCEL_TASK, task_id)
-    
+        return await simple_remote_call_async(
+            self.local_url, serv_names.SCHED_TASK_CANCEL_TASK, task_id)
+
     async def delete_task(self, task_id: str):
         """delete task from task list
         """
-        return await simple_remote_call_async(self.local_url, serv_names.SCHED_TASK_DELETE, task_id)
+        return await simple_remote_call_async(self.local_url,
+                                              serv_names.SCHED_TASK_DELETE,
+                                              task_id)
 
     async def kill_task(self, task_id: str):
-        return await simple_remote_call_async(self.local_url, serv_names.SCHED_TASK_KILL_TASK, task_id)
+        return await simple_remote_call_async(self.local_url,
+                                              serv_names.SCHED_TASK_KILL_TASK,
+                                              task_id)
 
     async def shutdown_scheduler(self):
         return await shutdown_server_async(self.local_url)
 
     async def set_task_status(self, task_id: str, status: TaskStatus):
         """set task to some specific status (i.e. tell task to do something)
         """
-        return await simple_remote_call_async(self.local_url, serv_names.SCHED_TASK_SET_STATUS, task_id, status)
+        return await simple_remote_call_async(self.local_url,
+                                              serv_names.SCHED_TASK_SET_STATUS,
+                                              task_id, status)
 
     async def soft_cancel_task(self, task_id: str):
         """use status to cancel task, task need to check status by
         TaskClient and cancel itself
         """
         return await self.set_task_status(task_id, TaskStatus.NeedToCancel)
 
+
 def main():
     s = libtmux.Server()
     sessions = s.sessions
     sess_names = [sess.name for sess in sessions]
     print(sess_names, s.socket_path)
-    scheduler_sess_names = [sess_name for sess_name in sess_names if sess_name.startswith(TMUX_SESSION_PREFIX)]
+    scheduler_sess_names = [
+        sess_name for sess_name in sess_names
+        if sess_name.startswith(TMUX_SESSION_PREFIX)
+    ]
     if len(scheduler_sess_names) == 0:
         uuid_str = uuid.uuid4().hex
         uuid_str = "0"
         serv_name = f"tensorpc.autossh.services.scheduler{TENSORPC_SPLIT}Scheduler"
         cfg = {
             serv_name: {
                 "uid": uuid_str,
@@ -139,24 +168,24 @@
         }
         cfg_encoded = base64.b64encode(
             json.dumps(cfg).encode("utf-8")).decode("utf-8")
         port = get_free_ports(1)[0]
         window_command = f"python -m tensorpc.serve --port {port} --serv_config_b64 {cfg_encoded}"
         scheduler_sess_name = f"{TMUX_SESSION_PREFIX}{TMUX_SESSION_NAME_SPLIT}{port}{TMUX_SESSION_NAME_SPLIT}{uuid_str}"
         print(window_command)
-        sess = s.new_session(scheduler_sess_name, window_command=window_command)
+        sess = s.new_session(scheduler_sess_name,
+                             window_command=window_command)
         window = sess.windows[0]
         print("?", s, window)
 
     else:
         assert len(scheduler_sess_names) == 1
         scheduler_sess_name = scheduler_sess_names[0]
         sess_parts = scheduler_sess_name.split(TMUX_SESSION_NAME_SPLIT)
         port = int(sess_parts[1])
         uuid_str = sess_parts[2]
         sess = s.sessions.get(session_name=scheduler_sess_name)
         assert isinstance(sess, libtmux.Session)
-        
-        print(scheduler_sess_name, sess, sess.windows[0].panes[0])
 
-    pass 
+        print(scheduler_sess_name, sess, sess.windows[0].panes[0])
 
+    pass
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/constants.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/constants.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,31 @@
 """ tensorpc tmux program name format:
 
 prefix::port::uuid
 
 """
 
 import os
-from typing import Optional 
+from typing import Optional
+
 TMUX_SESSION_NAME_SPLIT = "-"
 TMUX_SESSION_PREFIX = "__tensorpc_ssh_scheduler"
 
 TMUX_SESSION_TASK_PREFIX = "__tensorpc_ssh_scheduled_task"
 
 TENSORPC_TMUX_TASK_SCHEDULER_PORT = "TENSORPC_TMUX_TASK_SCHEDULER_PORT"
 
 TENSORPC_TMUX_TASK_UID = "TENSORPC_TMUX_TASK_UID"
 
 TENSORPC_TMUX_SCHEDULER_UUID = "TENSORPC_TMUX_SCHEDULER_UUID"
 
+
 class TmuxSchedulerEnvVariables:
     port: Optional[int]
+
     def __init__(self) -> None:
         port = os.environ.get(TENSORPC_TMUX_TASK_SCHEDULER_PORT)
         if port is not None:
             self.port = int(port)
         else:
-            self.port = None 
+            self.port = None
         self.uid = os.environ.get(TENSORPC_TMUX_TASK_UID, "")
-
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/core.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/core.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,105 +1,116 @@
-import dataclasses 
+import dataclasses
 
 import enum
 from typing import Dict, Any, List, Optional, Tuple
 from tensorpc.autossh.coretypes import SSHTarget
 from .constants import TMUX_SESSION_TASK_PREFIX, TMUX_SESSION_NAME_SPLIT
+
+
 class TaskStatus(enum.Enum):
     Unknown = 0
-    Pending = 1 
-    Running = 2 
-    Finished = 3 
-    Failed = 4 
+    Pending = 1
+    Running = 2
+    Finished = 3
+    Failed = 4
     # this means task send finish message to scheduler, but the process is still running
     AlmostFinished = 5
     # user send cancel to task, task need to check this manually and do graceful exit
     NeedToCancel = 6
     AlmostCanceled = 7
     Canceled = 8
 
     Booting = 9
 
 
-ALL_RUNNING_STATUS = set([TaskStatus.Running, TaskStatus.AlmostFinished, TaskStatus.AlmostCanceled, TaskStatus.Booting, TaskStatus.NeedToCancel])
+ALL_RUNNING_STATUS = set([
+    TaskStatus.Running, TaskStatus.AlmostFinished, TaskStatus.AlmostCanceled,
+    TaskStatus.Booting, TaskStatus.NeedToCancel
+])
 
-ALL_CTRL_C_CANCELABLE_STATUS = set([TaskStatus.Running, TaskStatus.Booting, TaskStatus.NeedToCancel])
+ALL_CTRL_C_CANCELABLE_STATUS = set(
+    [TaskStatus.Running, TaskStatus.Booting, TaskStatus.NeedToCancel])
 
 ALL_KILLABLE_STATUS = ALL_RUNNING_STATUS
 
+
 class TaskType(enum.Enum):
     # shell command
     Command = 0
     # function id and args
     FunctionId = 1
 
+
 class ResourceType(enum.Enum):
     GPU = 0
     CPU = 1
 
-@dataclasses.dataclass 
+
+@dataclasses.dataclass
 class TaskOutput:
     userdata: Dict[str, Any]
     paths: List[str]
-    timestamp: int 
+    timestamp: int
     progress: float
 
 
-@dataclasses.dataclass 
+@dataclasses.dataclass
 class TaskState:
     status: TaskStatus
-    progress: float 
+    progress: float
     outputs: List[TaskOutput]
-    pid: int 
+    pid: int
     exception_str: str
     # timestamp when updated
     timestamp: int = -1
     # resources used by this task such as GPUs.
-    resources: List[Tuple[ResourceType, int]] = dataclasses.field(default_factory=list)
+    resources: List[Tuple[ResourceType,
+                          int]] = dataclasses.field(default_factory=list)
     tmux_pane_last_lines: str = ""
 
-@dataclasses.dataclass 
+
+@dataclasses.dataclass
 class Task:
     type: TaskType
     # if type is Command, command is shell command
     # otherwise, command is function id
-    command: str  
+    command: str
     # when provide params as list, this task
     # will be executed multiple times with different params
     params: Optional[List[Dict[str, Any]]] = None
     # set by scheduler
     # if user provide a id, scheduler use this id, otherwise scheduler generate a uuid
-    # if user submit a id that already exists, scheduler will run iff task is not running 
+    # if user submit a id that already exists, scheduler will run iff task is not running
     # (status is pending, failed, cancelled or finished)
     id: str = ""
     # timestamp when submitted
     create_timestamp: int = -1
-    
+
     num_gpu_used: int = 0
     # if user provide allowed targets, scheduler will only run this task on these targets
-    allowed_target_ips: Optional[List[str]] = None 
-    state: TaskState = dataclasses.field(default_factory=lambda: TaskState(TaskStatus.Pending, 0.0, [], -1, ""))
+    allowed_target_ips: Optional[List[str]] = None
+    state: TaskState = dataclasses.field(
+        default_factory=lambda: TaskState(TaskStatus.Pending, 0.0, [], -1, ""))
     # if false, the tmux session will be closed after task finished or failed,
     # user can't check the stdout or error message.
     keep_tmux_session: bool = True
 
-    name: Optional[str] = None 
+    name: Optional[str] = None
 
     desp: str = ""
 
     cwd: str = ""
 
     tags: List[str] = dataclasses.field(default_factory=list)
 
     def empty_state(self):
         self.state = TaskState(TaskStatus.Pending, 0.0, [], -1, "")
-        return self 
+        return self
 
     def push_params(self, **kwargs):
         if self.params is None:
             self.params = []
         self.params.append(kwargs)
-        return self 
-    
+        return self
+
     def get_tmux_session_name(self):
         return f"{TMUX_SESSION_TASK_PREFIX}{TMUX_SESSION_NAME_SPLIT}{self.id}"
-
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/runtask/__main__.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/runtask/__main__.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,74 +1,84 @@
 import base64
-import io 
+import io
 
 import os
 import traceback
-from typing import Optional 
+from typing import Optional
 import fire
 import json
 from tensorpc import simple_remote_call, RemoteManager
-from tensorpc.autossh.scheduler.core import TaskType 
+from tensorpc.autossh.scheduler.core import TaskType
 from tensorpc.autossh.scheduler.constants import TmuxSchedulerEnvVariables
 from tensorpc.autossh.scheduler.task_client import enter_task_conetxt
 from tensorpc.autossh.serv_names import serv_names
 import subprocess
 import importlib
-import inspect 
+import inspect
 import psutil
 
+
 def run_func_in_module(module_func_id: str, *args, **kwargs):
     # module_func_id: tensorpc.xxx.yyy::zzz
     parts = module_func_id.split("::")
     module_import_path = parts[0]
     local_parts = parts[1:]
     mod = importlib.import_module(module_import_path)
     module_dict = mod.__dict__
     func_obj = module_dict[local_parts[0]]
     for part in local_parts[1:]:
         func_obj = getattr(func_obj, part)
     assert inspect.isfunction(func_obj) or inspect.isbuiltin(func_obj)
     print(func_obj)
     return func_obj(*args, **kwargs)
 
+
 def main(type_int: int):
     type = TaskType(type_int)
     # send pid to scheduler
     pid = os.getpid()
     env_vars = TmuxSchedulerEnvVariables()
-    assert env_vars.port is not None 
+    assert env_vars.port is not None
     scheduler_url = f"localhost:{env_vars.port}"
     # tell scheduler we are ready, fetch params if task is func id task.
     # here we must tell scheduler our pid to ensure
     # scheduler can access our status.
-    res = simple_remote_call(scheduler_url, serv_names.SCHED_TASK_INIT, env_vars.uid, pid)
-    assert res is not None 
+    res = simple_remote_call(scheduler_url, serv_names.SCHED_TASK_INIT,
+                             env_vars.uid, pid)
+    assert res is not None
     command, func_id_params = res
     if type == TaskType.Command:
         try:
             # print(command)
             subprocess.run(command, shell=True, check=True)
-            simple_remote_call(scheduler_url, serv_names.SCHED_TASK_SET_FINISHED, env_vars.uid)
+            simple_remote_call(scheduler_url,
+                               serv_names.SCHED_TASK_SET_FINISHED,
+                               env_vars.uid)
 
         except:
             ss = io.StringIO()
             traceback.print_exc(file=ss)
-            simple_remote_call(scheduler_url, serv_names.SCHED_TASK_SET_EXCEPTION, env_vars.uid, ss.getvalue())
+            simple_remote_call(scheduler_url,
+                               serv_names.SCHED_TASK_SET_EXCEPTION,
+                               env_vars.uid, ss.getvalue())
     else:
         assert func_id_params is not None
         assert len(func_id_params) == 1, "currently only support one param"
         kwargs = func_id_params[0]
         with RemoteManager(scheduler_url) as robj:
             with enter_task_conetxt(robj):
                 try:
                     # print(command)
                     run_func_in_module(command, **kwargs)
-                    robj.remote_call(serv_names.SCHED_TASK_SET_FINISHED, env_vars.uid)
+                    robj.remote_call(serv_names.SCHED_TASK_SET_FINISHED,
+                                     env_vars.uid)
                 except:
                     traceback.print_exc()
                     ss = io.StringIO()
                     traceback.print_exc(file=ss)
-                    robj.remote_call(serv_names.SCHED_TASK_SET_EXCEPTION, env_vars.uid, ss.getvalue())
+                    robj.remote_call(serv_names.SCHED_TASK_SET_EXCEPTION,
+                                     env_vars.uid, ss.getvalue())
     # print("EXIT!!!")
 
+
 if __name__ == "__main__":
-    fire.Fire(main)
+    fire.Fire(main)
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/task_client.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/task_client.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,19 @@
-            
 import contextvars
 from typing import Optional
 from tensorpc import simple_remote_call, RemoteManager
 import contextlib
-from tensorpc.autossh.serv_names import serv_names 
+from tensorpc.autossh.serv_names import serv_names
 
 from tensorpc.autossh.scheduler.core import TaskOutput, TaskStatus
 from .constants import TmuxSchedulerEnvVariables
 
 TASK_CONTEXT_VAR: contextvars.ContextVar[
-    Optional[RemoteManager]] = contextvars.ContextVar("tmux_sched_task_context",
-                                                   default=None)
+    Optional[RemoteManager]] = contextvars.ContextVar(
+        "tmux_sched_task_context", default=None)
 
 
 def get_task_context() -> Optional[RemoteManager]:
     return TASK_CONTEXT_VAR.get()
 
 
 @contextlib.contextmanager
@@ -29,49 +28,55 @@
 class TaskClient:
     """used inside task to communicate with scheduler.
     if not in task, all operations are no-op.
 
     if user launch a task without spawn new process (use func id or not distributed)
     this client will reuse the scheduler client to improve speed.
     """
+
     def __init__(self) -> None:
         env = TmuxSchedulerEnvVariables()
-        self.port = env.port 
-        self.uid = env.uid 
+        self.port = env.port
+        self.uid = env.uid
 
     @contextlib.contextmanager
     def _scheduler_robj(self):
         ctx = get_task_context()
         if ctx is not None:
-            yield ctx 
+            yield ctx
         else:
             with RemoteManager(f"localhost:{self.port}") as robj:
                 yield robj
 
-    def update_task(self, progress: float, output: Optional[TaskOutput] = None):
+    def update_task(self,
+                    progress: float,
+                    output: Optional[TaskOutput] = None):
         if self.port is not None:
             with self._scheduler_robj() as robj:
-                robj.remote_call(serv_names.SCHED_TASK_UPDATE_TASK, self.uid, progress, output)
+                robj.remote_call(serv_names.SCHED_TASK_UPDATE_TASK, self.uid,
+                                 progress, output)
 
     def check_need_cancel(self):
         if self.port is None:
-            return False 
+            return False
         with self._scheduler_robj() as robj:
-            status = robj.remote_call(serv_names.SCHED_TASK_CHECK_STATUS, self.uid)
+            status = robj.remote_call(serv_names.SCHED_TASK_CHECK_STATUS,
+                                      self.uid)
         if status == TaskStatus.NeedToCancel:
-            return True 
-        return False 
+            return True
+        return False
 
     def check_need_cancel_torch_dist(self):
         if self.port is None:
-            return False 
+            return False
         import torch.distributed as dist
         if not dist.is_initialized():
             return self.check_need_cancel()
         with self._scheduler_robj() as robj:
-            status = robj.remote_call(serv_names.SCHED_TASK_CHECK_STATUS, self.uid)
+            status = robj.remote_call(serv_names.SCHED_TASK_CHECK_STATUS,
+                                      self.uid)
         world_size = dist.get_world_size()
         res_list = [status] * world_size
         dist.all_gather_object(res_list, status)
         if any([x == TaskStatus.NeedToCancel for x in res_list]):
-            return True 
-        return False  
+            return True
+        return False
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/scheduler/tmux.py` & `tensorpc-0.11.0/tensorpc/autossh/scheduler/tmux.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,20 +8,23 @@
 import base64
 import json
 import uuid
 import psutil
 
 _SPLIT = constants.TMUX_SESSION_NAME_SPLIT
 
+
 def get_tmux_scheduler_info_may_create():
     s = libtmux.Server()
     sessions = s.sessions
     sess_names = [sess.name for sess in sessions]
     scheduler_sess_names = [
-        sess_name for sess_name in sess_names if sess_name.startswith(constants.TMUX_SESSION_PREFIX)]
+        sess_name for sess_name in sess_names
+        if sess_name.startswith(constants.TMUX_SESSION_PREFIX)
+    ]
     if len(scheduler_sess_names) == 0:
         uuid_str = uuid.uuid4().hex
         serv_name = f"tensorpc.autossh.services.scheduler{TENSORPC_SPLIT}Scheduler"
         cfg = {
             serv_name: {
                 "uid": uuid_str,
             }
@@ -40,15 +43,20 @@
         port = int(sess_parts[1])
         uuid_str = sess_parts[2]
         sess = s.sessions.get(session_name=scheduler_sess_name)
         assert isinstance(sess, libtmux.Session)
     return port, uuid_str
 
 
-def launch_tmux_task(uuid_str: str, window_command: str, one_shot: bool, sched_port: int, resources: List[Tuple[ResourceType, int]], cwd: str = ""):
+def launch_tmux_task(uuid_str: str,
+                     window_command: str,
+                     one_shot: bool,
+                     sched_port: int,
+                     resources: List[Tuple[ResourceType, int]],
+                     cwd: str = ""):
     s = libtmux.Server()
     sess_name = f"{constants.TMUX_SESSION_TASK_PREFIX}{_SPLIT}{uuid_str}"
     envs = {
         constants.TENSORPC_TMUX_TASK_SCHEDULER_PORT: sched_port,
         constants.TENSORPC_TMUX_TASK_UID: uuid_str,
     }
     all_gpu_ids: List[int] = []
@@ -90,50 +98,52 @@
             pane = sess.windows[0].panes[0]
             assert isinstance(pane, libtmux.Pane)
             pane.send_keys(env_export_str)
             if cwd:
                 pane.send_keys(f"cd {cwd}")
             pane.send_keys(window_command)
 
+
 def kill_task(uuid_str: str, pid: int):
     s = libtmux.Server()
     sess_name = f"{constants.TMUX_SESSION_TASK_PREFIX}{_SPLIT}{uuid_str}"
     if pid > 0:
         parent = psutil.Process(pid)
-        for child in parent.children(recursive=True): 
+        for child in parent.children(recursive=True):
             child.kill()
         parent.kill()
     else:
         assert s.has_session(sess_name)
         sess = s.sessions.get(session_name=sess_name)
         assert isinstance(sess, libtmux.Session)
         sess.kill_session()
 
+
 def cancel_task(uuid_str: str):
     s = libtmux.Server()
     sess_name = f"{constants.TMUX_SESSION_TASK_PREFIX}{_SPLIT}{uuid_str}"
     assert s.has_session(sess_name)
     # if s.has_session(sess_name):
     sess = s.sessions.get(session_name=sess_name)
     pane: libtmux.Pane = sess.windows[0].panes[0]
     pane.send_keys("\x03")
 
+
 def delete_task(uuid_str: str):
     s = libtmux.Server()
     sess_name = f"{constants.TMUX_SESSION_TASK_PREFIX}{_SPLIT}{uuid_str}"
     assert s.has_session(sess_name)
     sess = s.sessions.get(session_name=sess_name)
     assert isinstance(sess, libtmux.Session)
     pane: libtmux.Pane = sess.windows[0].panes[0]
     pane.send_keys("exit")
 
+
 def capture_pane_last_lines(uuid_str: str, num_lines: int):
     s = libtmux.Server()
     sess_name = f"{constants.TMUX_SESSION_TASK_PREFIX}{_SPLIT}{uuid_str}"
     if not s.has_session(sess_name):
-        return "" 
+        return ""
     sess = s.sessions.get(session_name=sess_name)
     assert isinstance(sess, libtmux.Session)
     pane: libtmux.Pane = sess.windows[0].panes[0]
     return pane.capture_pane(0)[-num_lines:]
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/serv_names.py` & `tensorpc-0.11.0/tensorpc/autossh/serv_names.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,69 +21,83 @@
     def SCHED_TASK_INIT(self):
         from tensorpc.autossh.services.scheduler import Scheduler
         return get_service_key_by_type(Scheduler, Scheduler.init_task.__name__)
 
     @property
     def SCHED_TASK_SET_EXCEPTION(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.set_task_exception.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.set_task_exception.__name__)
+
     @property
     def SCHED_TASK_SET_FINISHED(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.set_task_finished.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.set_task_finished.__name__)
+
     @property
     def SCHED_TASK_QUERY_UPDATES(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.query_task_updates.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.query_task_updates.__name__)
+
     @property
     def SCHED_TASK_GET_ALL_TASK(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.get_all_task_state.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.get_all_task_state.__name__)
+
     @property
     def SCHED_TASK_SUBMIT_TASK(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.submit_task.__name__)
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.submit_task.__name__)
 
     @property
     def SCHED_TASK_UPDATE_TASK(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.update_task.__name__)
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.update_task.__name__)
+
     @property
     def SCHED_TASK_CANCEL_TASK(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.cancel_task.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.cancel_task.__name__)
+
     @property
     def SCHED_TASK_KILL_TASK(self):
         from tensorpc.autossh.services.scheduler import Scheduler
         return get_service_key_by_type(Scheduler, Scheduler.kill_task.__name__)
-    
+
     @property
     def SCHED_TASK_CHECK_STATUS(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.check_task_status.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.check_task_status.__name__)
+
     @property
     def SCHED_TASK_SET_STATUS(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.set_task_status.__name__)
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.set_task_status.__name__)
 
     @property
     def SCHED_TASK_DELETE(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.delete_task.__name__)
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.delete_task.__name__)
 
     @property
     def SCHED_TASK_RESOURCE_USAGE(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.get_resource_usage.__name__)
-    
+        return get_service_key_by_type(Scheduler,
+                                       Scheduler.get_resource_usage.__name__)
+
     @property
     def SCHED_TASK_QUERY_TMUX_PANES(self):
         from tensorpc.autossh.services.scheduler import Scheduler
-        return get_service_key_by_type(Scheduler, Scheduler.query_task_tmux_lines.__name__)
+        return get_service_key_by_type(
+            Scheduler, Scheduler.query_task_tmux_lines.__name__)
+
 
 serv_names = _ServiceNames()
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/services/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/pyls/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
-# Copyright 2023 tusimple
-# 
+# Copyright 2023 Yan Yan
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/autossh/services/scheduler.py` & `tensorpc-0.11.0/tensorpc/autossh/services/scheduler.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,49 +1,56 @@
 import asyncio
 import enum
 from typing import Dict, List, Optional, Set, Tuple
 from tensorpc.autossh.scheduler.core import ALL_CTRL_C_CANCELABLE_STATUS, ALL_KILLABLE_STATUS, ALL_RUNNING_STATUS, Task, TaskOutput, TaskStatus, TaskType, ResourceType
 from tensorpc.autossh.scheduler import tmux
 from tensorpc.core import marker, prim
 import uuid
-import time 
+import time
 import psutil
-import subprocess 
+import subprocess
 import dataclasses
-import io 
-import csv 
+import io
+import csv
 from tensorpc.utils.gpuusage import get_nvidia_gpu_measures
 
 _SUPPORTED_SET_STATUS = set([TaskStatus.NeedToCancel])
 
+
 class ResourceManager:
+
     def __init__(self, num_cpu: int, num_gpu: int) -> None:
-        self.idle_resources: Dict[ResourceType, Set[Tuple[ResourceType, int]]] = {}
-        self.occupied_resources: Dict[ResourceType, Set[Tuple[ResourceType, int]]] = {}
+        self.idle_resources: Dict[ResourceType, Set[Tuple[ResourceType,
+                                                          int]]] = {}
+        self.occupied_resources: Dict[ResourceType, Set[Tuple[ResourceType,
+                                                              int]]] = {}
         self.num_gpu = num_gpu
         for item in ResourceType:
             self.idle_resources[item] = set()
             self.occupied_resources[item] = set()
 
         for i in range(num_cpu):
             self.idle_resources[ResourceType.CPU].add((ResourceType.CPU, i))
         for i in range(num_gpu):
             self.idle_resources[ResourceType.GPU].add((ResourceType.GPU, i))
 
     def __repr__(self):
         num_gpu_idle = len(self.idle_resources[ResourceType.GPU])
         return f"ResourceManager(GPU={num_gpu_idle}/{self.num_gpu})"
 
-    def request_idle_cpus(self, num_cpu: int) -> List[Tuple[ResourceType, int]]:
+    def request_idle_cpus(self,
+                          num_cpu: int) -> List[Tuple[ResourceType, int]]:
         return self._request_idle_resources(ResourceType.CPU, num_cpu)
-    
-    def request_idle_gpus(self, num_gpu: int) -> List[Tuple[ResourceType, int]]:
+
+    def request_idle_gpus(self,
+                          num_gpu: int) -> List[Tuple[ResourceType, int]]:
         return self._request_idle_resources(ResourceType.GPU, num_gpu)
 
-    def _request_idle_resources(self, resource_type: ResourceType, num: int) -> List[Tuple[ResourceType, int]]:
+    def _request_idle_resources(self, resource_type: ResourceType,
+                                num: int) -> List[Tuple[ResourceType, int]]:
         idle_resources = self.idle_resources[resource_type]
         if len(idle_resources) < num:
             return []
         else:
             resources = list(idle_resources)[:num]
             for r in resources:
                 idle_resources.remove(r)
@@ -53,45 +60,47 @@
     def release_resources(self, resources: List[Tuple[ResourceType, int]]):
         for r in resources:
             for item in ResourceType:
                 if r in self.occupied_resources[item]:
                     self.occupied_resources[item].remove(r)
                     self.idle_resources[item].add(r)
 
+
 class Scheduler:
-    def __init__(self, uid: str = "scheduler", max_number_of_task = 32) -> None:
+
+    def __init__(self, uid: str = "scheduler", max_number_of_task=32) -> None:
         self.tasks: Dict[str, Task] = {}
         self.uid = uid
         self.grpc_port = -1
         self.period_check_duration = 1.0
         max_number_of_task = min(psutil.cpu_count(False), max_number_of_task)
-        self.resource_manager = ResourceManager(max_number_of_task, len(get_nvidia_gpu_measures()))
-
+        self.resource_manager = ResourceManager(max_number_of_task,
+                                                len(get_nvidia_gpu_measures()))
 
     @marker.mark_server_event(event_type=marker.ServiceEventType.Init)
     async def init_scheduler(self):
         self.lock = asyncio.Lock()
         self.grpc_port = prim.get_server_grpc_port()
-        self._period_task = asyncio.create_task(self._period_check_task_status())
+        self._period_task = asyncio.create_task(
+            self._period_check_task_status())
 
     def init_task(self, task_id: str, pid: int):
         if task_id in self.tasks:
             task = self.tasks[task_id]
             task.state.status = TaskStatus.Running
-            task.state.pid = pid 
+            task.state.pid = pid
             self._update_task_timestamp(task)
             return task.command, task.params
         # task may be deleted before init and after tmux process launch.
-        return None 
+        return None
 
     def _release_task_resources(self, task: Task):
         self.resource_manager.release_resources(task.state.resources)
         task.state.resources = []
 
-
     async def _period_check_task_status(self):
         await asyncio.sleep(self.period_check_duration)
         # task_changed = False
         for task in self.tasks.values():
             if task.state.status == TaskStatus.Running:
                 pid_exists = psutil.pid_exists(task.state.pid)
                 # print(task.id, task.state.pid, pid_exists, "Running")
@@ -110,30 +119,33 @@
                 if not pid_exists:
                     # ensure the process is end instead of hang.
                     task.state.status = TaskStatus.Finished if is_almost_finish else TaskStatus.Canceled
                     self._update_task_timestamp(task)
                     # print("RELEASE TASK", task.id)
 
                     self._release_task_resources(task)
-                    # task_changed = True 
+                    # task_changed = True
         # if task_changed:
         # print("PERIOD SCHEDULE")
 
         self._do_schedule()
         # print("num idle", len(self.resource_manager.idle_resources[ResourceType.GPU]), "num occ", len(self.resource_manager.occupied_resources[ResourceType.GPU]))
 
-        self._period_task = asyncio.create_task(self._period_check_task_status())
+        self._period_task = asyncio.create_task(
+            self._period_check_task_status())
 
     def get_all_task_state(self):
-        return list(self.tasks.values()) 
-    
+        return list(self.tasks.values())
+
     def get_resource_usage(self):
         return self.resource_manager.idle_resources, self.resource_manager.occupied_resources
 
-    def query_task_updates(self, ts_uids: List[Tuple[int, str]], tmux_pane_lines: int = 0):
+    def query_task_updates(self,
+                           ts_uids: List[Tuple[int, str]],
+                           tmux_pane_lines: int = 0):
         """compare query timestamp, return updated + new and deleted tasks
         """
         deleted_uids: List[str] = []
         update_tasks: List[Task] = []
         all_query_uids = set(x[1] for x in ts_uids)
         for ts, uid in ts_uids:
             if uid in self.tasks:
@@ -150,21 +162,23 @@
             for task in update_tasks:
                 res = tmux.capture_pane_last_lines(task.id, tmux_pane_lines)
                 if isinstance(res, list):
                     res = "\n".join(res)
                 task.state.tmux_pane_last_lines = res
         return update_tasks, deleted_uids
 
-    def query_task_tmux_lines(self, task_uids: List[str], tmux_pane_lines: int = 0):
+    def query_task_tmux_lines(self,
+                              task_uids: List[str],
+                              tmux_pane_lines: int = 0):
         returns: Dict[str, str] = {}
         for task_id in task_uids:
             res = tmux.capture_pane_last_lines(task_id, tmux_pane_lines)
             if isinstance(res, list):
                 res = "\n".join(res)
-            returns[task_id] = res 
+            returns[task_id] = res
         return returns
 
     def submit_task(self, task: Task):
         # print("submit_task START", task.id)
 
         if task.id == "":
             task.id = str(uuid.uuid4())
@@ -172,15 +186,16 @@
             # set init params with empty dict
             task.params = [{}]
         if task.id in self.tasks:
             prev_task = self.tasks[task.id]
             # print("submit_task PREV", prev_task.id, prev_task.state.status)
 
             if prev_task.state.status in ALL_RUNNING_STATUS:
-                raise RuntimeError(f"task {task.id} is already running or pending")
+                raise RuntimeError(
+                    f"task {task.id} is already running or pending")
             else:
                 # replace old task with new one
                 self.tasks[task.id] = task
         else:
             self.tasks[task.id] = task
         task.empty_state()
         task.state.timestamp = time.time_ns()
@@ -191,108 +206,114 @@
 
     def check_task_status(self, task_id: str):
         if task_id in self.tasks:
             task = self.tasks[task_id]
             return task.state.status
         else:
             return TaskStatus.Unknown
-        
+
     def set_task_status(self, task_id: str, status: TaskStatus):
         assert status in _SUPPORTED_SET_STATUS, f"only support set to {list(_SUPPORTED_SET_STATUS)}"
         if task_id in self.tasks:
             task = self.tasks[task_id]
             if status == TaskStatus.NeedToCancel:
                 if task.state.status not in ALL_RUNNING_STATUS:
-                    return False 
-            task.state.status = status 
+                    return False
+            task.state.status = status
             self._update_task_timestamp(task)
             return True
-        return False 
+        return False
 
     def run_task(self, task_id: str):
         task = self.tasks[task_id]
         if task.state.status not in ALL_RUNNING_STATUS:
             # print("RUNTASK", task_id)
             cmd = f"python -m tensorpc.autossh.scheduler.runtask {task.type.value}"
             task.state.status = TaskStatus.Booting
-            tmux.launch_tmux_task(task_id, cmd, not task.keep_tmux_session, self.grpc_port, task.state.resources, task.cwd)
+            tmux.launch_tmux_task(task_id, cmd, not task.keep_tmux_session,
+                                  self.grpc_port, task.state.resources,
+                                  task.cwd)
             self._update_task_timestamp(task)
-            return True 
-        return False 
-    
+            return True
+        return False
+
     def cancel_task(self, task_id: str):
         task = self.tasks[task_id]
         if task.state.status in ALL_CTRL_C_CANCELABLE_STATUS:
             tmux.cancel_task(task_id)
-            return True 
+            return True
         elif task.state.status == TaskStatus.Pending:
             task.state.status = TaskStatus.Canceled
             self._update_task_timestamp(task)
             return True
-        return False 
-    
+        return False
+
     def kill_task(self, task_id: str):
         task = self.tasks[task_id]
         if task.state.status in ALL_KILLABLE_STATUS:
             tmux.kill_task(task_id, task.state.pid)
-            return True 
+            return True
         elif task.state.status == TaskStatus.Pending:
             task.state.status = TaskStatus.Canceled
             self._update_task_timestamp(task)
             return True
-        return False 
-    
+        return False
+
     def delete_task(self, task_id: str):
         if task_id in self.tasks:
             task = self.tasks[task_id]
             assert task.state.status not in ALL_RUNNING_STATUS, "you can't delete a running task"
             self.tasks.pop(task_id)
             tmux.delete_task(task_id)
-            return True 
-        return False 
-    
+            return True
+        return False
+
     def _update_task_timestamp(self, task: Task):
         task.state.timestamp = time.time_ns()
 
     def _do_schedule(self):
         pending_tasks: List[Task] = []
         for task in self.tasks.values():
             if task.state.status == TaskStatus.Pending:
                 pending_tasks.append(task)
         if not pending_tasks:
-            return 
+            return
         pending_tasks.sort(key=lambda x: x.create_timestamp)
         for task in pending_tasks:
-            num_cpu_used = 1 
+            num_cpu_used = 1
             num_gpu_used = task.num_gpu_used
             resources = self.resource_manager.request_idle_cpus(num_cpu_used)
             if len(resources) == 0:
-                break 
+                break
             if num_gpu_used > 0:
                 # print("BEFORE REQUEST GPU", self.resource_manager)
 
-                gpu_resources = self.resource_manager.request_idle_gpus(num_gpu_used)
+                gpu_resources = self.resource_manager.request_idle_gpus(
+                    num_gpu_used)
                 # print(task.id, num_gpu_used, gpu_resources, self.resource_manager)
                 if len(gpu_resources) == 0:
                     self.resource_manager.release_resources(resources)
-                    continue 
+                    continue
                 resources.extend(gpu_resources)
             task.state.resources = resources
             self.run_task(task.id)
 
     def set_task_exception(self, task_id: str, exception_str: str):
         if task_id in self.tasks:
             task = self.tasks[task_id]
             task.state.status = TaskStatus.Failed
             task.state.exception_str = exception_str
             self._update_task_timestamp(task)
             self._release_task_resources(task)
             self._do_schedule()
 
-    def update_task(self, task_id: str, progress: float, output: Optional[TaskOutput] = None):
+    def update_task(self,
+                    task_id: str,
+                    progress: float,
+                    output: Optional[TaskOutput] = None):
         if task_id in self.tasks:
             task = self.tasks[task_id]
             task.state.progress = max(min(progress, 1.0), 0.0)
             if output is not None:
                 task.state.outputs.append(output)
             self._update_task_timestamp(task)
 
@@ -301,8 +322,7 @@
             task = self.tasks[task_id]
             if task.state.status == TaskStatus.NeedToCancel:
                 task.state.status = TaskStatus.AlmostCanceled
             else:
                 task.state.status = TaskStatus.AlmostFinished
                 task.state.progress = 1.0
             self._update_task_timestamp(task)
-
```

### Comparing `tensorpc-0.10.7/tensorpc/cli/cpuusage/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/cpuusage/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/cpuusage/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/cpuusage/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/createmsg/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/createmsg/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/createmsg/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/createmsg/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/free_port/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/free_port/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/gpuusage/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/gpuusage/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/gpuusage/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/gpuusage/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/iperf/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/iperf/__main__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,38 @@
 import asyncio
 import traceback
 
 import fire
 from tensorpc import simple_chunk_call_async
-import numpy as np 
-import time 
+import numpy as np
+import time
+
 
 async def main_async(addr: str, size: int, is_send: bool):
     try:
         start = time.time()
         if is_send:
             data = np.empty([size * 1024 * 1024], dtype=np.uint8)
-            await simple_chunk_call_async(addr, "tensorpc.services.collection::SpeedTestServer.recv_data", data)
+            await simple_chunk_call_async(
+                addr,
+                "tensorpc.services.collection::SpeedTestServer.recv_data",
+                data)
         else:
-            await simple_chunk_call_async(addr, "tensorpc.services.collection::SpeedTestServer.send_data", size)
-        end_time = time.time() 
+            await simple_chunk_call_async(
+                addr,
+                "tensorpc.services.collection::SpeedTestServer.send_data",
+                size)
+        end_time = time.time()
         speed = size / (end_time - start)
-        print("usetime: {}, speed: {:.2f} MB/s".format(end_time - start, speed))
+        print("usetime: {}, speed: {:.2f} MB/s".format(end_time - start,
+                                                       speed))
     except:
         traceback.print_exc()
         raise
 
+
 def main(addr: str, size: int, is_send: bool):
     return asyncio.run(main_async(addr, size, is_send))
 
+
 if __name__ == "__main__":
-    fire.Fire(main)
+    fire.Fire(main)
```

### Comparing `tensorpc-0.10.7/tensorpc/cli/ping/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/ping/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/proto_files/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/proto_files/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/proto_files/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/proto_files/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/proto_root/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/proto_root/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/proto_root/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/proto_root/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/pyls/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/pyright_launch/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/cli/pyright_launch/__init__.py` & `tensorpc-0.11.0/tensorpc/examples/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/cli/start_worker/__init__.py` & `tensorpc-0.11.0/tensorpc/cli/start_worker/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/cli/start_worker/__main__.py` & `tensorpc-0.11.0/tensorpc/cli/start_worker/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/compat.py` & `tensorpc-0.11.0/tensorpc/compat.py`

 * *Files 4% similar despite different names*

```diff
@@ -49,14 +49,14 @@
 if platform.system() == "Darwin":
     InMacOS = True
     OS = OSType.MacOS
 
 
 def is_relative_to(path: Path, other: Path):
     if Python3_9AndLater:
-        return path.is_relative_to(other) # type: ignore
+        return path.is_relative_to(other)  # type: ignore
     else:
         try:
             path.relative_to(other)
         except:
-            return False 
-        return True
+            return False
+        return True
```

### Comparing `tensorpc-0.10.7/tensorpc/constants.py` & `tensorpc-0.11.0/tensorpc/constants.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,14 @@
 TENSORPC_CLASS_META_KEY = "__tensorpc_class_meta"
 
 TENSORPC_WEBSOCKET_MSG_SIZE = (4 << 20)
 TENSORPC_SPLIT = "::"
 
 TENSORPC_SUBPROCESS_SMEM = "TENSORPC_SUBPROCESS_SMEM"
 
-
 TENSORPC_READUNTIL = "__tensorpc_readuntil_string"
 
 TENSORPC_FILE_NAME_PREFIX = "__tensorpc_inmemory_fname"
 
 TENSORPC_OBSERVED_FUNCTION_ATTR = "__tensorpc_observed_function__"
 
-TENSORPC_PORT_MAX_TRY = 15
+TENSORPC_PORT_MAX_TRY = 15
```

### Comparing `tensorpc-0.10.7/tensorpc/core/__init__.py` & `tensorpc-0.11.0/tensorpc/core/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,19 +13,21 @@
 # limitations under the License.
 
 from tensorpc.core.defs import Service, ServiceDef, from_yaml_path
 from tensorpc.constants import TENSORPC_SPLIT
 
 BUILTIN_SERVICES = [
     Service(f"tensorpc.services.collection{TENSORPC_SPLIT}FileOps", {}),
-    Service(f"tensorpc.services.collection{TENSORPC_SPLIT}SpeedTestServer", {}),
+    Service(f"tensorpc.services.collection{TENSORPC_SPLIT}SpeedTestServer",
+            {}),
     Service(f"tensorpc.flow.serv.core{TENSORPC_SPLIT}Flow", {}),
     Service(f"tensorpc.flow.serv.worker{TENSORPC_SPLIT}FlowWorker", {}),
     Service(f"tensorpc.services.collection{TENSORPC_SPLIT}Simple", {}),
-    Service(f"tensorpc.autossh.services.scheduler{TENSORPC_SPLIT}Scheduler", {}),
+    Service(f"tensorpc.autossh.services.scheduler{TENSORPC_SPLIT}Scheduler",
+            {}),
 ]
 
 
 def get_http_url(url: str, port: int):
     return f"http://{url}:{port}/api/rpc"
```

### Comparing `tensorpc-0.10.7/tensorpc/core/asyncclient.py` & `tensorpc-0.11.0/tensorpc/core/asyncclient.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,14 +53,15 @@
     func_dict: Dict[str, Any]
     name: str
     shared_mem: np.ndarray
     output_shared_mem: np.ndarray
     num_blocks: int
     """
     _stub: Optional[remote_object_pb2_grpc.RemoteObjectStub]
+
     def __init__(self,
                  channel: Optional[grpc.aio.Channel],
                  name="",
                  print_stdout=True):
 
         self._channel = channel
         if channel is not None:
@@ -433,10 +434,11 @@
         return await robj.remote_call(key, *args, timeout=timeout, **kwargs)
 
 
 async def simple_chunk_call_async(addr, key, *args, **kwargs):
     async with AsyncRemoteManager(addr) as robj:
         return await robj.chunked_remote_call(key, *args, **kwargs)
 
+
 async def shutdown_server_async(addr):
     async with AsyncRemoteManager(addr) as robj:
         return await robj.shutdown()
```

### Comparing `tensorpc-0.10.7/tensorpc/core/asyncserver.py` & `tensorpc-0.11.0/tensorpc/core/asyncserver.py`

 * *Files 1% similar despite different names*

```diff
@@ -118,29 +118,31 @@
         return rpc_message_pb2.HelloReply(data=request.data)
 
 
 async def _await_thread_ev(ev, loop, timeout=None):
     waiter = partial(ev.wait, timeout=timeout)
     return await loop.run_in_executor(None, waiter)
 
+
 # https://github.com/grpc/grpc/blob/master/examples/python/helloworld/async_greeter_server_with_graceful_shutdown.py
 # Coroutines to be invoked when the event loop is shutting down.
 _cleanup_coroutines = []
 
 
-async def serve_service(service: AsyncRemoteObjectService,
-                        wait_time=-1,
-                        port=50051,
-                        length=-1,
-                        is_local=False,
-                        max_threads=10,
-                        process_id=-1,
-                        ssl_key_path: str = "",
-                        ssl_crt_path: str = "",
-                        grpc_options: Optional[List[Tuple[str, Union[str, int]]]] = None):
+async def serve_service(
+        service: AsyncRemoteObjectService,
+        wait_time=-1,
+        port=50051,
+        length=-1,
+        is_local=False,
+        max_threads=10,
+        process_id=-1,
+        ssl_key_path: str = "",
+        ssl_crt_path: str = "",
+        grpc_options: Optional[List[Tuple[str, Union[str, int]]]] = None):
     assert isinstance(service, AsyncRemoteObjectService)
     if is_local and process_id >= 0:
         if hasattr(os, "sched_setaffinity"):
             # lock process to cpu to increase performance.
             LOGGER.info("lock worker {} to core {}".format(
                 process_id, process_id))
             os.sched_setaffinity(0, [process_id])
@@ -149,26 +151,26 @@
         wait_interval = wait_time
     options = []
     if length > 0:
         options = [('grpc.max_send_message_length', length * 1024 * 1024),
                    ('grpc.max_receive_message_length', length * 1024 * 1024)]
     options.append(('grpc.so_reuseport', 0))
     if grpc_options is not None:
-        options = grpc_options # override
+        options = grpc_options  # override
     server = grpc.aio.server(options=options)
     remote_object_pb2_grpc.add_RemoteObjectServicer_to_server(service, server)
     credentials = None
     if ssl_key_path != "" and ssl_key_path != "":
         with open(ssl_key_path, "rb") as f:
             private_key = f.read()
         with open(ssl_crt_path, "rb") as f:
             certificate_chain = f.read()
         credentials = grpc.ssl_server_credentials([(private_key,
                                                     certificate_chain)])
-    
+
     for i in range(TENSORPC_PORT_MAX_TRY):
         if port == -1:
             port = get_free_ports(1)[0]
         url = '[::]:{}'.format(port)
         try:
             if credentials is not None:
                 server.add_secure_port(url, credentials)
@@ -182,28 +184,29 @@
     if port == -1:
         raise RuntimeError("Cannot find free port")
     server_core = service.server_core
     server_core._set_port(port)
     await server_core.run_event_async(ServiceEventType.BeforeServerStart)
     await server.start()
     loop = asyncio.get_running_loop()
+
     async def server_graceful_shutdown():
         # Shuts down the server with 5 seconds of grace period. During the
         # grace period, the server won't accept new connections and allow
         # existing RPCs to continue within the grace period.
         await server.stop(5)
+
     _cleanup_coroutines.append(server_graceful_shutdown())
     await server_core.async_shutdown_event.wait()
     await server.stop(0)
     await server.wait_for_termination()
     # exec cleanup functions
     await server_core.run_event_async(ServiceEventType.Exit)
 
 
-
 async def serve_with_http_async(server_core: ProtobufServiceCore,
                                 url: str,
                                 wait_time=-1,
                                 port=50051,
                                 http_port=50052,
                                 length=-1,
                                 is_local=False,
@@ -260,15 +263,15 @@
     with server_core.enter_global_context():
         await server_core._init_async_members()
         await server_core.run_event_async(ServiceEventType.Init)
         service = AsyncRemoteObjectService(server_core, is_local, length)
         grpc_task = serve_service(service, wait_time, port, length, is_local,
                                   max_threads, process_id, ssl_key_path,
                                   ssl_crt_path)
-                                  
+
         return await grpc_task
 
 
 def serve(service_def: ServiceDef,
           wait_time=-1,
           port=50051,
           length=-1,
```

### Comparing `tensorpc-0.10.7/tensorpc/core/bgserver.py` & `tensorpc-0.11.0/tensorpc/core/bgserver.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,77 +2,85 @@
 import queue
 from typing import Optional
 from tensorpc.core.asyncserver import serve as serve_async
 
 from tensorpc.core.client import RemoteManager
 from tensorpc.core.defs import ServiceDef, Service
 from tensorpc.core.server import serve
-import threading 
-import atexit 
+import threading
+import atexit
 from tensorpc.core import BUILTIN_SERVICES
 
 
 class BackgroundServer:
+
     def __init__(self):
         self._thread: Optional[threading.Thread] = None
         self.port = -1
         # atexit.register(self.stop)
 
     @property
     def is_started(self):
         return self._thread is not None and self._thread.is_alive()
 
-    def start(self, service_def: Optional[ServiceDef] = None, port: int = -1, max_workers: int = 2):
+    def start(self,
+              service_def: Optional[ServiceDef] = None,
+              port: int = -1,
+              max_workers: int = 2):
         assert not self.is_started
         if service_def is None:
             service_def = ServiceDef([])
             service_def.services.extend(BUILTIN_SERVICES)
         port_res_queue = queue.Queue()
         if port < 0:
-            service_def.services.append(Service("tensorpc.services.collection::ProcessObserver", {
-                "q": port_res_queue
-            }))
-        self._thread = threading.Thread(target=serve, kwargs={
-            "service_def": service_def,
-            "port": port,
-            "max_threads": max_workers
-        })
+            service_def.services.append(
+                Service("tensorpc.services.collection::ProcessObserver",
+                        {"q": port_res_queue}))
+        self._thread = threading.Thread(target=serve,
+                                        kwargs={
+                                            "service_def": service_def,
+                                            "port": port,
+                                            "max_threads": max_workers
+                                        })
         self._thread.daemon = True
         self._thread.start()
         if port < 0:
             port = port_res_queue.get(timeout=20)
         self.port = port
         return port
 
-
-    def start_async(self, service_def: Optional[ServiceDef] = None, port: int = -1):
+    def start_async(self,
+                    service_def: Optional[ServiceDef] = None,
+                    port: int = -1):
         assert not self.is_started
         if service_def is None:
             service_def = ServiceDef([])
             service_def.services.extend(BUILTIN_SERVICES)
         port_res_queue = queue.Queue()
         if port < 0:
-            service_def.services.append(Service("tensorpc.services.collection::ProcessObserver", {
-                "q": port_res_queue
-            }))
-
-        self._thread = threading.Thread(target=serve_async, kwargs={
-            "service_def": service_def,
-            "port": port,
-            "create_loop": True
-        })
+            service_def.services.append(
+                Service("tensorpc.services.collection::ProcessObserver",
+                        {"q": port_res_queue}))
+
+        self._thread = threading.Thread(target=serve_async,
+                                        kwargs={
+                                            "service_def": service_def,
+                                            "port": port,
+                                            "create_loop": True
+                                        })
         self._thread.daemon = True
         self._thread.start()
         if port < 0:
             port = port_res_queue.get(timeout=20)
         self.port = port
         return port
 
     def stop(self):
         if self.is_started:
-            assert self._thread is not None 
+            assert self._thread is not None
             robj = RemoteManager(f"localhost:{self.port}")
             robj.shutdown()
             self._thread.join()
             self._thread = None
 
-BACKGROUND_SERVER = BackgroundServer()
+
+BACKGROUND_SERVER = BackgroundServer()
```

### Comparing `tensorpc-0.10.7/tensorpc/core/client.py` & `tensorpc-0.11.0/tensorpc/core/client.py`

 * *Files 0% similar despite different names*

```diff
@@ -44,14 +44,15 @@
     func_dict: Dict[str, Any]
     name: str
     shared_mem: np.ndarray
     output_shared_mem: np.ndarray
     num_blocks: int
     """
     _stub: Optional[remote_object_pb2_grpc.RemoteObjectStub]
+
     def __init__(self,
                  channel: Optional[grpc.Channel],
                  name="",
                  print_stdout=True):
 
         self._channel = channel
         if channel is not None:
```

### Comparing `tensorpc-0.10.7/tensorpc/core/core_io.py` & `tensorpc-0.11.0/tensorpc/core/core_io.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 import json
 import pickle
 from collections import abc
 from enum import Enum
 from functools import reduce
 from typing import Any, Callable, Dict, Hashable, List, Optional, Tuple, TypeVar, Union
 from typing_extensions import Literal
@@ -90,22 +89,24 @@
 
 BYTES_JSONARRAY_CODE = 100
 BYTES_SKELETON_CODE = 101
 
 INV_NPDTYPE_TO_PB_MAP = _inv_map(NPDTYPE_TO_PB_MAP)
 INV_NPDTYPE_TO_JSONARRAY_MAP = _inv_map(NPDTYPE_TO_JSONARRAY_MAP)
 
-NPBYTEORDER_TO_PB_MAP: Dict[Literal["=", "<", ">", "|"], "arraybuf_pb2.dtype.ByteOrder"] = {
-    "=": arraybuf_pb2.dtype.native,
-    "<": arraybuf_pb2.dtype.littleEndian,
-    ">": arraybuf_pb2.dtype.bigEndian,
-    "|": arraybuf_pb2.dtype.na,
-}
-INV_NPBYTEORDER_TO_PB_MAP: Dict["arraybuf_pb2.dtype.ByteOrder", Literal["=", "<", ">", "|"]] = _inv_map(NPBYTEORDER_TO_PB_MAP)
-
+NPBYTEORDER_TO_PB_MAP: Dict[Literal["=", "<", ">", "|"],
+                            "arraybuf_pb2.dtype.ByteOrder"] = {
+                                "=": arraybuf_pb2.dtype.native,
+                                "<": arraybuf_pb2.dtype.littleEndian,
+                                ">": arraybuf_pb2.dtype.bigEndian,
+                                "|": arraybuf_pb2.dtype.na,
+                            }
+INV_NPBYTEORDER_TO_PB_MAP: Dict["arraybuf_pb2.dtype.ByteOrder",
+                                Literal["=", "<", ">",
+                                        "|"]] = _inv_map(NPBYTEORDER_TO_PB_MAP)
 
 
 def bytes2pb(data: bytes, send_data=True) -> arraybuf_pb2.ndarray:
     dtype = arraybuf_pb2.dtype.CustomBytes
     pb = arraybuf_pb2.ndarray(
         dtype=arraybuf_pb2.dtype(type=dtype),
         shape=[len(data)],
@@ -118,15 +119,15 @@
 def array2pb(array: npt.NDArray, send_data=True) -> arraybuf_pb2.ndarray:
     if array.ndim > 0 and send_data:
         if not array.flags['C_CONTIGUOUS']:
             array = np.ascontiguousarray(array)
     assert isinstance(array, np.ndarray)
     dtype = NPDTYPE_TO_PB_MAP[array.dtype]
     assert array.dtype.byteorder in ("=", "<", ">", "|")
-    order = NPBYTEORDER_TO_PB_MAP[array.dtype.byteorder] # type: ignore
+    order = NPBYTEORDER_TO_PB_MAP[array.dtype.byteorder]  # type: ignore
     pb_dtype = arraybuf_pb2.dtype(
         type=dtype,
         byte_order=order,
     )
     pb = arraybuf_pb2.ndarray(
         shape=list(array.shape),
         dtype=pb_dtype,
@@ -159,18 +160,21 @@
     if isinstance(array_or_bytes, np.ndarray):
         return array2pb(array_or_bytes, send_data)
     elif isinstance(array_or_bytes, bytes):
         return bytes2pb(array_or_bytes, send_data)
     else:
         raise NotImplementedError("only support ndarray/bytes.")
 
+
 class JsonOnlyData:
+
     def __init__(self, data) -> None:
         self.data = data
 
+
 class FromBufferStream(object):
 
     def __init__(self):
         self.current_buf_idx = -1
         self.num_args = -1
         self.current_buf_length = -1
         self.current_buf_shape = None
@@ -234,25 +238,26 @@
     arg_ids[-1] = -1
     num_args = len(data_list)
     for arg_idx, arg in enumerate(data_list):
         if isinstance(arg, np.ndarray):
             if not arg.flags['C_CONTIGUOUS']:
                 data_bytes = arg.tobytes()
             else:
-                data_bytes = None 
+                data_bytes = None
             order = NPBYTEORDER_TO_PB_MAP[arg.dtype.byteorder]
             data_dtype = arraybuf_pb2.dtype(
                 type=NPDTYPE_TO_PB_MAP[arg.dtype],
                 byte_order=order,
             )
             # ref_buf = array2pb(arg)
             shape = arg.shape
             length = arg.nbytes
         elif isinstance(arg, bytes):
-            data_dtype = arraybuf_pb2.dtype(type=arraybuf_pb2.dtype.CustomBytes)
+            data_dtype = arraybuf_pb2.dtype(
+                type=arraybuf_pb2.dtype.CustomBytes)
             # ref_buf = bytes2pb(arg)
             data_bytes = arg
             shape = ()
             length = len(data_bytes)
 
         else:
             raise NotImplementedError
@@ -268,28 +273,30 @@
                 buf = rpc_message_pb2.RemoteCallStream(
                     num_chunk=num_chunk,
                     chunk_id=i,
                     num_args=num_args,
                     arg_id=arg_idx,
                     dtype=data_dtype,
                     func_key=func_key,
-                    chunked_data=arg_view[i * chunk_size:(i + 1) * chunk_size].tobytes(),
+                    chunked_data=arg_view[i * chunk_size:(i + 1) *
+                                          chunk_size].tobytes(),
                     shape=[],
                     flags=flags,
                 )
             else:
-                assert data_bytes is not None 
+                assert data_bytes is not None
                 buf = rpc_message_pb2.RemoteCallStream(
                     num_chunk=num_chunk,
                     chunk_id=i,
                     num_args=num_args,
                     arg_id=arg_idx,
                     dtype=data_dtype,
                     func_key=func_key,
-                    chunked_data=data_bytes[i * chunk_size:(i + 1) * chunk_size],
+                    chunked_data=data_bytes[i * chunk_size:(i + 1) *
+                                            chunk_size],
                     shape=[],
                     flags=flags,
                 )
             bufs.append(buf)
         assert len(bufs) > 0
         bufs[0].shape[:] = shape
         streams += bufs
@@ -333,15 +340,15 @@
                     data_skeleton[k] = Placeholder(len(arrays), byte_size(v))
                 arrays.append(v)
             else:
                 data_skeleton[k] = _extract_arrays_from_data(
                     arrays, v, object_classes, json_index)
         return data_skeleton
     elif isinstance(data, JsonOnlyData):
-        return data.data 
+        return data.data
     else:
         data_skeleton = None
         if isinstance(data, object_classes):
             if json_index:
                 data_skeleton = {JSON_INDEX_KEY: len(arrays)}
             else:
                 data_skeleton = Placeholder(len(arrays), byte_size(data))
@@ -704,16 +711,14 @@
     RPCError = 0x20
     UserError = 0x30
     SubscribeError = 0x40
     OnConnectError = 0x50
 
     ErrorMask = 0xF0
 
-    
-
 
 def encode_protobuf_uint(val: int):
     """this function encode protobuf fised uint to make sure
     message size is stable.
     """
     assert val >= 0
     return val + 1
@@ -749,15 +754,16 @@
 
     1~5: header length
     5~X: header 
     X~Y: array data
 
     """
 
-    def __init__(self, data, skeleton_size_limit: int = int(1024 * 1024 * 3.6)) -> None:
+    def __init__(
+        self, data, skeleton_size_limit: int = int(1024 * 1024 * 3.6)) -> None:
         arrays, data_skeleton = extract_arrays_from_data(data, json_index=True)
         self.arrays: List[Union[np.ndarray, bytes]] = arrays
         self.data_skeleton = data_skeleton
         self._total_size = 0
         self._arr_metadata: List[Tuple[int, List[int]]] = []
         for arr in self.arrays:
             if isinstance(arr, np.ndarray):
@@ -770,15 +776,16 @@
         self._ser_skeleton = json.dumps(self.get_skeleton())
         # print(self._ser_skeleton)
         if len(self._ser_skeleton) > skeleton_size_limit:
             data_skeleton_pack = msgpack.packb(self.data_skeleton)
             assert data_skeleton_pack is not None
             self.arrays.append(data_skeleton_pack)
             self._total_size += len(data_skeleton_pack)
-            self._arr_metadata.append((BYTES_SKELETON_CODE, [len(data_skeleton_pack)]))
+            self._arr_metadata.append(
+                (BYTES_SKELETON_CODE, [len(data_skeleton_pack)]))
             self.data_skeleton = {}
             self._ser_skeleton = json.dumps(self.get_skeleton())
 
     def get_total_array_binary_size(self):
         return self._total_size
 
     def get_skeleton(self):
@@ -988,8 +995,7 @@
     return {"error": type, "detail": detail}
 
 
 def get_exception_json(exc: BaseException):
     detail = traceback.format_exc()
     exception_json = {"error": str(exc), "detail": detail}
     return exception_json
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/defs.py` & `tensorpc-0.11.0/tensorpc/core/defs.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,26 +37,29 @@
 
 @dataclass
 class File:
     name: str
     content: bytes
     data: Any
 
+
 @dataclass
 class FileResource:
     name: str
-    path: Optional[str] = None 
+    path: Optional[str] = None
     content: Optional[Union[str, bytes]] = None
     chunk_size: Optional[int] = None
-    content_type: Optional[str] = None 
+    content_type: Optional[str] = None
+
 
-@dataclass 
+@dataclass
 class FileDesp:
     name: str
-    content_type: Optional[str] = None 
+    content_type: Optional[str] = None
+
 
 def from_yaml_path(path: Union[Path, str]) -> ServiceDef:
     """read yaml config with strong-type check
     """
     p = Path(path)
     with p.open("r") as f:
         data = f.read()
```

### Comparing `tensorpc-0.10.7/tensorpc/core/event_emitter/aio.py` & `tensorpc-0.11.0/tensorpc/core/event_emitter/aio.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from .base import EventEmitter, KT, VTs, ExceptionParam
 from typing import (Any, Callable, Dict, Generic, List, Mapping, Optional, Set,
                     Tuple, TypeVar, Union, cast)
 from asyncio import AbstractEventLoop, ensure_future, Future, iscoroutine
 
 from typing_extensions import TypeVarTuple, Unpack
 
+
 class AsyncIOEventEmitter(EventEmitter[KT, Unpack[VTs]]):
     """An event emitter class which can run asyncio coroutines in addition to
     synchronous blocking functions. For example:
 
     ```py
     @ee.on('event')
     async def async_handler(*args, **kwargs):
```

### Comparing `tensorpc-0.10.7/tensorpc/core/event_emitter/base.py` & `tensorpc-0.11.0/tensorpc/core/event_emitter/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -213,15 +213,15 @@
     async def _emit_run_async(
         self,
         f: Callable[[Unpack[VTs]], Any],
         args: Tuple[Unpack[VTs]],
     ) -> None:
         coro = f(*args)
         if inspect.iscoroutine(coro):
-            return await coro 
+            return await coro
         else:
             return coro
 
     def event_names(self) -> Set[KT]:
         """Get a set of events that this emitter is listening to."""
         return set(self._events.keys())
 
@@ -411,9 +411,7 @@
                 self._events[event] = OrderedDict()
             else:
                 self._events = dict()
 
     def listeners(self, event: KT) -> List[Callable[[Unpack[VTs]], Any]]:
         """Returns a list of all listeners registered to the `event`."""
         return list(self._events.get(event, OrderedDict()).keys())
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/funcid.py` & `tensorpc-0.11.0/tensorpc/core/funcid.py`

 * *Files 0% similar despite different names*

```diff
@@ -155,33 +155,36 @@
     _get_attribute_name(node, parts)
     return parts[::-1]
 
 
 def get_attribute_name(node):
     return ".".join(get_attribute_name_parts(node))
 
+
 def determine_code_common_indent(code: str):
     lines = code.split("\n")
     indent = None
     for line in lines:
         if line.strip() == "":
             continue
         line_indent = len(line) - len(line.lstrip())
         if indent is None:
             indent = line_indent
         else:
             indent = min(indent, line_indent)
     return indent
 
+
 def remove_common_indent_from_code(code: str):
     common_indent = determine_code_common_indent(code)
     code_without_indent = "\n".join(
         [l[common_indent:] for l in code.split("\n")])
     return code_without_indent
 
+
 def get_body_blocks_from_code(code: str, autorun_block_symbol: str = ""):
     code = remove_common_indent_from_code(code)
     tree = ast.parse(code)
     func_node = get_toplevel_func_node(tree)[0][0]
     body_start = func_node.body[0].lineno
     body_code_lines = code.split("\n")[body_start - 1:]
     # if a line start with '#%%', it's a block splitter.
```

### Comparing `tensorpc-0.10.7/tensorpc/core/httpclient.py` & `tensorpc-0.11.0/tensorpc/core/httpclient.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/core/httpservers/aiohttp_impl.py` & `tensorpc-0.11.0/tensorpc/core/httpservers/aiohttp_impl.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,74 +17,81 @@
 from tensorpc.protos_export import remote_object_pb2
 from tensorpc.protos_export import remote_object_pb2 as remote_object_pb2
 from tensorpc.protos_export import rpc_message_pb2
 from contextlib import suppress
 from aiohttp import streamer
 from tensorpc.core.serviceunit import ServiceEventType
 
+
 @streamer
 async def file_sender(writer, file_bytes: bytes, chunk_size=2**16):
     """
     This function will read large file chunk by chunk and send it through HTTP
     without reading them into memory
     """
     bio = io.BytesIO(file_bytes)
-    chunk = bio.read(2 ** 16)
+    chunk = bio.read(2**16)
     while chunk:
         await writer.write(chunk)
-        chunk = bio.read(2 ** 16)
+        chunk = bio.read(2**16)
+
 
 @streamer
 async def grpc_iter_file_sender(writer, reader):
     """
     This function will read large file chunk by chunk and send it through HTTP
     without reading them into memory
     """
     async for chunk, is_exc in reader:
         if is_exc:
             raise ValueError(chunk)
         await writer.write(chunk)
 
 
 from .core import WebsocketClientBase, WebsocketMsg, WebsocketMsgType, WebsocketHandler
+
+
 async def _cancel(task):
     # more info: https://stackoverflow.com/a/43810272/1113207
     task.cancel()
     with suppress(asyncio.CancelledError):
         await task
 
+
 class AiohttpWebsocketClient(WebsocketClientBase):
+
     def __init__(self,
                  id: str,
                  ws: web.WebSocketResponse,
                  serv_id_to_name: Dict[int, str],
                  uid: Optional[int] = None):
         super().__init__(id, serv_id_to_name, uid)
         self.ws = ws
 
     async def close(self):
         return await self.ws.close()
 
-    def get_msg_max_size(self) -> int: 
+    def get_msg_max_size(self) -> int:
         return self.ws._max_msg_size
 
-    async def send_bytes(self, data: bytes): 
+    async def send_bytes(self, data: bytes):
         return await self.ws.send_bytes(data)
 
-
-    def get_client_id(self) -> int: 
+    def get_client_id(self) -> int:
         return id(self.ws)
 
-    async def binary_msg_generator(self, shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]: 
+    async def binary_msg_generator(
+            self,
+            shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]:
         # while True:
         #     recv_task = asyncio.create_task(self.ws.receive(), name="recv_task")
         #     st_task = asyncio.create_task(shutdown_ev.wait(), name="shutdown_task")
         #     done, pending = await asyncio.wait([recv_task, st_task], return_when=asyncio.FIRST_COMPLETED)
         #     if st_task in done:
-        #         # cancel recv task 
+        #         # cancel recv task
         #         await cancel_task(recv_task)
         #         break
         #     else:
         #         msg = recv_task.result()
         #         if msg.type == aiohttp.WSMsgType.BINARY:
         #             yield WebsocketMsg(msg.data, WebsocketMsgType.Binary)
         #         elif msg.type == aiohttp.WSMsgType.TEXT:
@@ -95,40 +102,44 @@
         #         recv_task = asyncio.create_task(self.ws.receive(), name="recv_task")
         async for msg in self.ws:
             if msg.type == aiohttp.WSMsgType.BINARY:
                 yield WebsocketMsg(msg.data, WebsocketMsgType.Binary)
             elif msg.type == aiohttp.WSMsgType.TEXT:
                 yield WebsocketMsg(msg.data, WebsocketMsgType.Text)
             elif msg.type == aiohttp.WSMsgType.ERROR:
-                raise Exception("websocket connection closed with exception %s" %
-                                self.ws.exception())
+                raise Exception(
+                    "websocket connection closed with exception %s" %
+                    self.ws.exception())
+
 
 class AiohttpWebsocketHandler(WebsocketHandler):
+
     async def handle_new_connection_aiohttp(self, request):
         client_id = request.match_info.get('client_id')
         print("NEW CONN", client_id, request)
         service_core = self.service_core
         ws = web.WebSocketResponse()
         await ws.prepare(request)
-        client = AiohttpWebsocketClient(client_id, 
-            ws, service_core.service_units.get_service_id_to_name())
+        client = AiohttpWebsocketClient(
+            client_id, ws, service_core.service_units.get_service_id_to_name())
         await self.handle_new_connection(client, client_id)
         return ws
 
     async def handle_new_backup_connection_aiohttp(self, request):
         client_id = request.match_info.get('client_id')
         print("NEW BACKUP CONN", client_id, request)
         service_core = self.service_core
         ws = web.WebSocketResponse()
         await ws.prepare(request)
-        client = AiohttpWebsocketClient(client_id, 
-            ws, service_core.service_units.get_service_id_to_name())
+        client = AiohttpWebsocketClient(
+            client_id, ws, service_core.service_units.get_service_id_to_name())
         await self.handle_new_connection(client, client_id, True)
         return ws
 
+
 class HttpService:
 
     def __init__(self, service_core: ProtobufServiceCore):
         self.service_core = service_core
 
     async def remote_json_call_http(self, request: web.Request):
         try:
@@ -146,56 +157,79 @@
             'Access-Control-Allow-Origin': '*',
             # 'Access-Control-Allow-Headers': '*',
             # 'Access-Control-Allow-Method': 'POST',
         }
         res = web.Response(body=byte, headers=headers)
         return res
 
+    async def simple_remote_json_call_http(self, request: web.Request):
+        try:
+            # json body must be {"service_key": "serv_key", "data": "data"}
+            data_bin = await request.read()
+            data = json.loads(data_bin)
+            pb_data = rpc_message_pb2.RemoteJsonCallRequest()
+            pb_data.data = json.dumps(data["data"])
+            pb_data.service_key = data["service_key"]
+            pb_data.flags = rpc_message_pb2.JsonArray
+            res = await self.service_core.remote_json_call_async(pb_data)
+            res_json_str = res.data
+
+        except Exception as e:
+            data = self.service_core._remote_exception_json(e)
+            res = rpc_message_pb2.RemoteCallReply(exception=data)
+            res_json_str = data
+        # TODO better headers
+        headers = {
+            'Access-Control-Allow-Origin': '*',
+            # 'Access-Control-Allow-Headers': '*',
+            # 'Access-Control-Allow-Method': 'POST',
+        }
+        res = web.Response(body=res_json_str, headers=headers)
+        return res
+
     async def fetch_status(self, request: web.Request):
         status = {
             "status": "ok",
         }
         # TODO better headers
         headers = {
             'Access-Control-Allow-Origin': '*',
             # 'Access-Control-Allow-Headers': '*',
             # 'Access-Control-Allow-Method': 'POST',
         }
         res = web.json_response(status, headers=headers)
         return res
-    
 
     async def resource_download_call(self, request: web.Request):
         params = request.rel_url.query
         node_uid = params.get('nodeUid')
         resource_key = params.get('key')
         headers = {
             'Access-Control-Allow-Origin': '*',
             "Content-Disposition": f"Attachment;filename={resource_key}",
             # 'Access-Control-Allow-Headers': '*',
             # 'Access-Control-Allow-Method': 'POST',
         }
         if node_uid is not None and resource_key is not None:
             ait = self.service_core.execute_async_generator_service(
-                "tensorpc.flow.serv.core::Flow.app_get_file", [node_uid, resource_key], {}, json_call=False)
+                "tensorpc.flow.serv.core::Flow.app_get_file",
+                [node_uid, resource_key], {},
+                json_call=False)
             desp, is_exc = await ait.__anext__()
             if is_exc:
                 return web.Response(status=500, text=desp, headers=headers)
             assert isinstance(desp, defs.FileDesp)
             headers["Content-Disposition"] = f"Attachment;filename={desp.name}"
             if desp.content_type is not None:
                 headers["Content-Type"] = desp.content_type
-            return web.Response(
-                body=grpc_iter_file_sender(reader=ait),
-                headers=headers
-            )
+            return web.Response(body=grpc_iter_file_sender(reader=ait),
+                                headers=headers)
         else:
             raise web.HTTPBadRequest(text="nodeUid or key is None")
 
-
     async def file_upload_call(self, request: web.Request):
         reader = await request.multipart()
         # /!\ Don't forget to validate your inputs /!\
         # reader.next() will `yield` the fields of your form
         headers = {
             'Access-Control-Allow-Origin': '*',
             # 'Access-Control-Allow-Headers': '*',
@@ -281,14 +315,15 @@
                                   ws_name="/api/ws/{client_id}",
                                   is_sync: bool = False,
                                   rpc_pickle_name: str = "/api/rpc_pickle",
                                   client_max_size: int = 16 * 1024**2,
                                   standalone: bool = True,
                                   ssl_key_path: str = "",
                                   ssl_crt_path: str = "",
+                                  simple_json_rpc_name="/api/simple_json_rpc",
                                   ws_backup_name="/api/ws_backup/{client_id}"):
     # client_max_size 4MB is enough for most image upload.
     http_service = HttpService(server_core)
     ctx = contextlib.nullcontext()
     if standalone:
         ctx = server_core.enter_global_context()
     with ctx:
@@ -298,22 +333,28 @@
 
         ws_service = AiohttpWebsocketHandler(server_core)
         # print("???????", client_max_size)
         app = web.Application(client_max_size=client_max_size)
         # TODO should we create a global client session for all http call in server?
         loop_task = asyncio.create_task(ws_service.event_provide_executor())
         app.router.add_post(rpc_name, http_service.remote_json_call_http)
+        app.router.add_post(simple_json_rpc_name,
+                            http_service.simple_remote_json_call_http)
+
         app.router.add_post(rpc_pickle_name,
                             http_service.remote_pickle_call_http)
-        app.router.add_post(TENSORPC_API_FILE_UPLOAD, http_service.file_upload_call)
-        app.router.add_get(TENSORPC_API_FILE_DOWNLOAD, http_service.resource_download_call)
+        app.router.add_post(TENSORPC_API_FILE_UPLOAD,
+                            http_service.file_upload_call)
+        app.router.add_get(TENSORPC_API_FILE_DOWNLOAD,
+                           http_service.resource_download_call)
         app.router.add_get(TENSORPC_FETCH_STATUS, http_service.fetch_status)
 
         app.router.add_get(ws_name, ws_service.handle_new_connection_aiohttp)
-        app.router.add_get(ws_backup_name, ws_service.handle_new_backup_connection_aiohttp)
+        app.router.add_get(ws_backup_name,
+                           ws_service.handle_new_backup_connection_aiohttp)
 
         ssl_context = None
         if ssl_key_path != "" and ssl_key_path != "":
             ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
             ssl_context.load_cert_chain(ssl_crt_path, ssl_key_path)
         return await asyncio.gather(
             serve_app(app,
```

### Comparing `tensorpc-0.10.7/tensorpc/core/httpservers/all.py` & `tensorpc-0.11.0/tensorpc/core/httpservers/all.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 tusimple
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/httpservers/blacksheep_impl.py` & `tensorpc-0.11.0/tensorpc/core/httpservers/blacksheep_impl.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,15 +26,15 @@
 
 from .core import WebsocketClientBase, WebsocketMsg, WebsocketMsgType, WebsocketHandler
 
 
 class BlacksheepWebsocketClient(WebsocketClientBase):
 
     def __init__(self,
-                id: str,
+                 id: str,
                  ws: WebSocket,
                  serv_id_to_name: Dict[int, str],
                  uid: Optional[int] = None,
                  client_max_size: int = -1):
         super().__init__(id, serv_id_to_name, uid)
         self.ws = ws
         self.client_max_size = client_max_size
@@ -43,59 +43,69 @@
         return await self.ws.close()
 
     def get_msg_max_size(self) -> int:
         if self.client_max_size == -1:
             return TENSORPC_WEBSOCKET_MSG_SIZE
         else:
             return self.client_max_size
-        
+
     async def send_bytes(self, data: bytes):
         return await self.ws.send_bytes(data)
 
     def get_client_id(self) -> int:
         return id(self.ws)
 
-    async def binary_msg_generator(self, shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]:
+    async def binary_msg_generator(
+            self,
+            shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]:
         while True:
             msg = await self.ws.receive_bytes()
             yield WebsocketMsg(msg, WebsocketMsgType.Binary)
 
             # recv_task = asyncio.create_task(self.ws.receive_bytes(), name="recv_task")
             # st_task = asyncio.create_task(shutdown_ev.wait(), name="shutdown_task")
             # done, pending = await asyncio.wait([recv_task, st_task], return_when=asyncio.FIRST_COMPLETED)
             # if st_task in done:
-            #     # cancel recv task 
+            #     # cancel recv task
             #     await cancel_task(recv_task)
             #     break
             # else:
             #     msg = recv_task.result()
             #     yield WebsocketMsg(msg, WebsocketMsgType.Binary)
             #     recv_task = asyncio.create_task(self.ws.receive_bytes(), name="recv_task")
 
 
 class BlacksheepWebsocketHandler(WebsocketHandler):
-    def __init__(self, service_core: ProtobufServiceCore, client_max_size: int):
+
+    def __init__(self, service_core: ProtobufServiceCore,
+                 client_max_size: int):
         super().__init__(service_core)
         self.client_max_size = client_max_size
 
-    async def handle_new_connection_blacksheep(self, request: WebSocket, client_id: str):
+    async def handle_new_connection_blacksheep(self, request: WebSocket,
+                                               client_id: str):
         print("NEW CONN", client_id, request)
         service_core = self.service_core
         await request.accept()
-        client = BlacksheepWebsocketClient(client_id, 
-            request, service_core.service_units.get_service_id_to_name(), 
+        client = BlacksheepWebsocketClient(
+            client_id,
+            request,
+            service_core.service_units.get_service_id_to_name(),
             client_max_size=self.client_max_size)
         return await self.handle_new_connection(client, client_id)
 
-    async def handle_new_backup_connection_blacksheep(self, request: WebSocket, client_id: str):
+    async def handle_new_backup_connection_blacksheep(self, request: WebSocket,
+                                                      client_id: str):
         print("NEW CONN", client_id, request)
         service_core = self.service_core
         await request.accept()
-        client = BlacksheepWebsocketClient(client_id, 
-            request, service_core.service_units.get_service_id_to_name(), 
+        client = BlacksheepWebsocketClient(
+            client_id,
+            request,
+            service_core.service_units.get_service_id_to_name(),
             client_max_size=self.client_max_size)
         return await self.handle_new_connection(client, client_id, True)
 
 
 class HttpService:
 
     def __init__(self, service_core: ProtobufServiceCore):
@@ -107,15 +117,14 @@
         }
         res = web.Response(
             status=200,
             content=web.JSONContent(status),
         )
         res.add_header(b'Access-Control-Allow-Origin', b'*')
         return res
-    
 
     async def remote_json_call_http(self,
                                     request: web.Request) -> web.Response:
         try:
             data_bin = await request.read()
             assert data_bin is not None
             pb_data = rpc_message_pb2.RemoteJsonCallRequest()
@@ -130,14 +139,38 @@
         res = web.Response(
             status=200,
             content=web.Content(b"", byte),
         )
         res.add_header(b'Access-Control-Allow-Origin', b'*')
         return res
 
+    async def simple_remote_json_call_http(self, request: web.Request):
+        try:
+            # json body must be {"service_key": "serv_key", "data": "data"}
+            data_bin = await request.read()
+            data = json.loads(data_bin)
+            pb_data = rpc_message_pb2.RemoteJsonCallRequest()
+            pb_data.data = json.dumps(data["data"])
+            pb_data.service_key = data["service_key"]
+            pb_data.flags = rpc_message_pb2.JsonArray
+            res = await self.service_core.remote_json_call_async(pb_data)
+            res_json_str = res.data
+
+        except Exception as e:
+            data = self.service_core._remote_exception_json(e)
+            res = rpc_message_pb2.RemoteCallReply(exception=data)
+            res_json_str = data
+        # TODO better headers
+        res = web.Response(
+            status=200,
+            content=web.Content(b"", res_json_str),
+        )
+        res.add_header(b'Access-Control-Allow-Origin', b'*')
+        return res
+
     async def remote_pickle_call_http(self, request: web.Request):
         try:
             data_bin = await request.read()
             assert data_bin is not None
 
             pb_data = rpc_message_pb2.RemoteCallRequest()
             pb_data.ParseFromString(data_bin)
@@ -161,29 +194,31 @@
         handled_fnames: List[str] = []
         for part in files:
             if part.name == b"data":
                 metadata = json.loads(part.data)
             elif part.name == b"file":
                 assert metadata is not None
                 filename = part.file_name
-                assert filename is not None 
+                assert filename is not None
                 serv_key = metadata["serv_key"]
                 serv_data = metadata["serv_data"]
                 file_size = metadata["file_size"]
                 handled_fnames.append(filename.decode())
                 f = defs.File(filename.decode(), part.data, serv_data)
                 res, is_exc = await self.service_core.execute_async_service(
                     serv_key, [f], {}, json_call=False)
                 if is_exc:
                     return web.text(status=500, value=res)
         return web.text('{} successfully stored'.format(handled_fnames))
 
+
 async def _await_shutdown(shutdown_ev, loop):
     return await loop.run_in_executor(None, shutdown_ev.wait)
 
+
 async def serve_app(serv: uvicorn.Server,
                     shutdown_ev: threading.Event,
                     async_shutdown_ev: asyncio.Event,
                     is_sync: bool = False):
     loop = asyncio.get_running_loop()
     await serv.serve()
     async_shutdown_ev.set()
@@ -200,51 +235,59 @@
                                   ws_name="/api/ws/{client_id}",
                                   is_sync: bool = False,
                                   rpc_pickle_name: str = "/api/rpc_pickle",
                                   client_max_size: int = 4 * 1024**2,
                                   standalone: bool = True,
                                   ssl_key_path: str = "",
                                   ssl_crt_path: str = "",
+                                  simple_json_rpc_name="/api/simple_json_rpc",
                                   ws_backup_name="/api/ws_backup/{client_id}"):
     http_service = HttpService(server_core)
     ctx = contextlib.nullcontext()
     if standalone:
         ctx = server_core.enter_global_context()
     with ctx:
         if standalone:
             await server_core._init_async_members()
             await server_core.run_event_async(ServiceEventType.Init)
 
         ws_service = BlacksheepWebsocketHandler(server_core, client_max_size)
         app = web.Application()
         # TODO should we create a global client session for all http call in server?
         loop_task = asyncio.create_task(ws_service.event_provide_executor())
+
         @app.router.post(rpc_name)
         async def _handle_rpc(request):
             return await http_service.remote_json_call_http(request)
 
+        @app.router.post(simple_json_rpc_name)
+        async def _handle_simple_rpc(request):
+            return await http_service.simple_remote_json_call_http(request)
+
         @app.router.post(rpc_pickle_name)
         async def _handle_rpc_pkl(request):
             return await http_service.remote_pickle_call_http(request)
 
         @app.router.post(TENSORPC_API_FILE_UPLOAD)
         async def _handle_rpc_file(request):
             return await http_service.file_upload_call(request)
-        
+
         @app.router.get(TENSORPC_FETCH_STATUS)
         async def _fetch_status(request):
             return await http_service.fetch_status(request)
 
         @app.router.ws(ws_name)
         async def _handle_new_connection(request, client_id):
-            return await ws_service.handle_new_connection_blacksheep(request, client_id)
+            return await ws_service.handle_new_connection_blacksheep(
+                request, client_id)
 
         @app.router.ws(ws_backup_name)
         async def _handle_new_backup_connection(request, client_id):
-            return await ws_service.handle_new_backup_connection_blacksheep(request, client_id)
+            return await ws_service.handle_new_backup_connection_blacksheep(
+                request, client_id)
 
         config = uvicorn.Config(app,
                                 port=port,
                                 log_level="warning",
                                 ws_max_size=client_max_size,
                                 ssl_keyfile=ssl_key_path,
                                 ssl_certfile=ssl_crt_path)
```

### Comparing `tensorpc-0.10.7/tensorpc/core/httpservers/core.py` & `tensorpc-0.11.0/tensorpc/core/httpservers/core.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,33 +30,38 @@
 from tensorpc.constants import TENSORPC_WEBSOCKET_MSG_SIZE
 from contextlib import suppress
 import numpy as np
 import time
 from concurrent.futures import ThreadPoolExecutor
 
 LOGGER = df_logging.get_logger()
-JS_MAX_SAFE_INT = 2 ** 53 - 1
+JS_MAX_SAFE_INT = 2**53 - 1
+
 
 class HttpServerType(enum.IntEnum):
     AioHttp = 0
     Blacksheep = 1
 
+
 class WebsocketMsgType(enum.IntEnum):
     Binary = 0
     Text = 1
     Error = 2
 
+
 @dataclasses.dataclass
 class WebsocketMsg:
     data: bytes
     type: WebsocketMsgType
 
+
 class JsonEncodeException(Exception):
     pass
 
+
 class WebsocketClientBase(abc.ABC):
     # TODO peer client use a async queue instead of recv because
     # aiohttp don't allow parallel recv
     def __init__(self,
                  id: str,
                  serv_id_to_name: Dict[int, str],
                  uid: Optional[int] = None):
@@ -72,43 +77,53 @@
         self._allow_recv_event.set()
         self.id = id
         self.is_backup: bool = False
         self._large_data_ws: Optional["WebsocketClientBase"] = None
         self.hang_shutdown_event = asyncio.Event()
 
     @abc.abstractmethod
-    async def close(self): ...
+    async def close(self):
+        ...
 
     @abc.abstractmethod
-    def get_msg_max_size(self) -> int: ...
+    def get_msg_max_size(self) -> int:
+        ...
 
     @abc.abstractmethod
-    async def send_bytes(self, data: bytes): ...
+    async def send_bytes(self, data: bytes):
+        ...
 
     @abc.abstractmethod
-    def get_client_id(self) -> int: ...
+    def get_client_id(self) -> int:
+        ...
 
     def get_event_id(self):
         self._ev_cnt = (self._ev_cnt + 1) % JS_MAX_SAFE_INT
         return self._ev_cnt
-    
+
     def get_pingpong_id(self):
         self._pingpong_cnt = (self._pingpong_cnt + 1) % JS_MAX_SAFE_INT
         return self._pingpong_cnt
 
     def __hash__(self):
         return self.get_client_id()
-    
+
     async def send_ping(self):
         rid = self.get_pingpong_id()
-        await self.send("", core_io.SocketMsgType.Ping, request_id=rid, is_json=True)
+        await self.send("",
+                        core_io.SocketMsgType.Ping,
+                        request_id=rid,
+                        is_json=True)
         return rid
-    
+
     async def send_pong(self, rpc_id: int):
-        await self.send("", core_io.SocketMsgType.Pong, request_id=rpc_id, is_json=True)
+        await self.send("",
+                        core_io.SocketMsgType.Pong,
+                        request_id=rpc_id,
+                        is_json=True)
 
     async def send(self,
                    data,
                    msg_type: core_io.SocketMsgType,
                    service_key: str = "",
                    request_id: int = 0,
                    is_json: bool = False,
@@ -137,68 +152,74 @@
                 traceback.print_exc()
                 raise JsonEncodeException(str(e))
         max_size = self.get_msg_max_size() - 128
         send_large_chunk_size_thresh = 65536
         # max_size = 1024 * 1024
         # TODO reslove "8192"
         try:
-            encoder = core_io.SocketMessageEncoder(data, skeleton_size_limit=max_size - 8192)
+            encoder = core_io.SocketMessageEncoder(
+                data, skeleton_size_limit=max_size - 8192)
         except Exception as e:
             traceback.print_exc()
             raise JsonEncodeException(str(e))
         assert self._large_data_ws is not self
         use_large_data_ws = False
         try:
             cnt = 0
             for chunk in encoder.get_message_chunks(msg_type, req, max_size):
-                use_large_data_ws = prefer_large_data and self._large_data_ws is not None and len(chunk) > send_large_chunk_size_thresh
+                use_large_data_ws = prefer_large_data and self._large_data_ws is not None and len(
+                    chunk) > send_large_chunk_size_thresh
                 assert len(chunk) <= max_size
                 async with async_timeout.timeout(5):
                     if use_large_data_ws:
-                        assert self._large_data_ws is not None 
+                        assert self._large_data_ws is not None
                         # print("SEND WITH LARGE DATA WS", cnt, max_size)
                         await self._large_data_ws.send_bytes(chunk)
                     else:
                         # print("SEND WITH MAIN WS")
                         await self.send_bytes(chunk)
                     cnt += 1
         except ConnectionResetError:
             print("CLIENT SEND ERROR, RETURN")
         except:
             # use main ws to send client reset msg to frontend
             if use_large_data_ws:
-                print("LARGE CLIENT SEND TIMEOUT ERROR. data will be dropped, client should reset large-data-ws")
-                assert self._large_data_ws is not None 
+                print(
+                    "LARGE CLIENT SEND TIMEOUT ERROR. data will be dropped, client should reset large-data-ws"
+                )
+                assert self._large_data_ws is not None
                 print("LARGE CLIENT USE BACKUP TO SEND.")
                 await self.send_bytes(
-                    core_io.json_only_encode({}, core_io.SocketMsgType.ResetLargeDataClient, req))
+                    core_io.json_only_encode(
+                        {}, core_io.SocketMsgType.ResetLargeDataClient, req))
                 print("LARGE CLIENT Closing....")
                 async with async_timeout.timeout(5):
                     await self._large_data_ws.close()
                 print("LARGE CLIENT Closed.")
 
             else:
                 print("CLIENT SEND TIMEOUT ERROR", )
-                assert self._large_data_ws is not None 
+                assert self._large_data_ws is not None
                 await self._large_data_ws.send_bytes(
-                    core_io.json_only_encode({}, core_io.SocketMsgType.ResetLargeDataClient, req))
+                    core_io.json_only_encode(
+                        {}, core_io.SocketMsgType.ResetLargeDataClient, req))
                 print("CLIENT SEND TIMEOUT ERROR 2", )
                 traceback.print_exc()
                 return
-                # raise 
+                # raise
         finally:
-            return 
+            return
 
     async def send_with_lock(self,
-                   data,
-                   msg_type: core_io.SocketMsgType,
-                   service_key: str = "",
-                   request_id: int = 0,
-                   is_json: bool = False,
-                   dynamic_key: str = ""):
+                             data,
+                             msg_type: core_io.SocketMsgType,
+                             service_key: str = "",
+                             request_id: int = 0,
+                             is_json: bool = False,
+                             dynamic_key: str = ""):
         """data must not be encoded.
         """
         if self._uid is not None:
             request_id = self._uid
         sid = 0
         if service_key != "":
             sid = self._name_to_serv_id[service_key]
@@ -212,15 +233,17 @@
                                    dynamic_key=dynamic_key)
         if is_json:
             return await self.send_bytes(
                 core_io.json_only_encode(data, msg_type, req))
         max_size = self.get_msg_max_size() - 128
         # max_size = 1024 * 1024
         # TODO reslove "8192"
-        encoder = core_io.SocketMessageEncoder(data, skeleton_size_limit=max_size - 8192)
+        encoder = core_io.SocketMessageEncoder(data,
+                                               skeleton_size_limit=max_size -
+                                               8192)
         # tasks = []
         # max_size = TENSORPC_WEBSOCKET_MSG_SIZE
         # t = time.time()
         # chunks = list(encoder.get_message_chunks(msg_type, req, max_size))
         # print("ENCODE TEIM", len(chunks), time.time() - t)
         # if len(chunks) > 1:
         #     header_rec = core_io.TensoRPCHeader(chunks[0])
@@ -231,33 +254,34 @@
         self._recv_cancel_event.set()
         self._allow_recv_event.clear()
         try:
             # if encoder.get_total_array_binary_size() > max_size:
             #     print("WS PREPARE SEND", encoder.get_total_array_binary_size())
             # t = time.time()
             async with self._lock:
-                for chunk in encoder.get_message_chunks(msg_type, req, max_size):
+                for chunk in encoder.get_message_chunks(
+                        msg_type, req, max_size):
                     assert len(chunk) <= max_size
                     # tasks.append(self.ws.send_bytes(chunk))
                     async with async_timeout.timeout(10):
                         await self.send_bytes(chunk)
             # if encoder.get_total_array_binary_size() > max_size:
 
-                # print("WS SEND TIME", time.time() - t)
+            # print("WS SEND TIME", time.time() - t)
         except ConnectionResetError:
             print("CLIENT SEND ERROR, RETURN")
         except:
             print("CLIENT SEND TIMEOUT ERROR", )
             raise
         finally:
 
             self._recv_cancel_event.clear()
             self._allow_recv_event.set()
 
-            return 
+            return
         # await tasks[0]
         # if len(tasks) > 1:
         #     tasks = [asyncio.create_task(t) for t in tasks[1:]]
         #     await asyncio.wait(tasks)
 
     async def send_exception(self, exc: BaseException,
                              type: core_io.SocketMsgType, request_id: int):
@@ -289,15 +313,18 @@
 
     async def send_subscribe_error(self, exc, request_id: int):
         return await self.send_exception(exc,
                                          core_io.SocketMsgType.SubscribeError,
                                          request_id)
 
     @abc.abstractmethod
-    async def binary_msg_generator(self, shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]: ...
+    async def binary_msg_generator(
+            self,
+            shutdown_ev: asyncio.Event) -> AsyncGenerator[WebsocketMsg, None]:
+        ...
 
 
 def create_task(coro):
     if compat.Python3_7AndLater:
         return asyncio.create_task(coro)
     else:
         return asyncio.ensure_future(coro)
@@ -372,43 +399,47 @@
 
     async def send_ping_loop(self, client: WebsocketClientBase):
         while True:
             await asyncio.sleep(5)
             rid = await client.send_ping()
             print("sent ping", rid)
 
-    async def handle_new_connection(self, client: WebsocketClientBase, client_id: str, is_backup: bool = False):
+    async def handle_new_connection(self,
+                                    client: WebsocketClientBase,
+                                    client_id: str,
+                                    is_backup: bool = False):
         print(f"NEW CONN {client.id}, is backup: ", is_backup)
         service_core = self.service_core
         conn_st_ev = asyncio.Event()
         # wait at most 100 rpcs
         conn_rpc_queue: "asyncio.Queue[asyncio.Task]" = asyncio.Queue(1000)
         with service_core._enter_exec_context():
             try:
-                await service_core.service_units.run_event_async(ServiceEventType.WebSocketOnConnect, client)
+                await service_core.service_units.run_event_async(
+                    ServiceEventType.WebSocketOnConnect, client)
             except Exception as e:
                 await client.send_user_error(e, 0)
         # assert not self.event_to_clients and not self.client_to_events
         # pingpong_task = asyncio.create_task(self.send_ping_loop(client))
         # To avoid possible hang when send large data to client, we use
-        # two websockets, one for small payload, one for large payload. 
+        # two websockets, one for small payload, one for large payload.
         # this only enabled we receive websocket config with pair enabled from client.
 
         # when hang happens, we will receive exception in ws.send or get
         # message from client. Then we can close the websocket (send), client
         # should restart a new websocket connection when it detect hang or receive
-        # restart request in backup connection. 
+        # restart request in backup connection.
         if not is_backup:
             if client_id not in self.client_id_to_client:
                 self.client_id_to_client[client_id] = client
             else:
                 client_prev = self.client_id_to_client[client_id]
-                assert client_prev.is_backup 
-                self.client_id_to_client[client_id] = client 
-                client.is_backup = False 
+                assert client_prev.is_backup
+                self.client_id_to_client[client_id] = client
+                client.is_backup = False
                 client._large_data_ws = client_prev
         else:
             if client_id in self.client_id_to_client:
                 client_main = self.client_id_to_client[client_id]
                 client_main._large_data_ws = client
             else:
                 client.is_backup = True
@@ -420,15 +451,16 @@
             # send serv ids first
             await client.send(self._name_to_serv_id,
                               core_io.SocketMsgType.QueryServiceIds,
                               request_id=0,
                               is_json=True)
             # TODO we should wait shutdown here
             # TODO handle send error
-            async for ws_msg in client.binary_msg_generator(client.hang_shutdown_event):
+            async for ws_msg in client.binary_msg_generator(
+                client.hang_shutdown_event):
                 if ws_msg.type == WebsocketMsgType.Binary:
                     data = ws_msg.data
                     try:
                         header = core_io.TensoRPCHeader(data)
                         msg_type = header.type
                         req = header.req
                     except Exception as e:
@@ -513,15 +545,15 @@
                             if not client_evs:
                                 self.client_to_events.pop(client)
                         # TODO send error if this event is already subscribed
                         # send OK
                         await client.send([],
                                           msg_type=msg_type,
                                           request_id=req.rpc_id)
-                        
+
                     elif msg_type == core_io.SocketMsgType.RPC:
                         arg_data = core_io.parse_message_chunks(header, [data])
                         # TODO if full for some time, drop rpc (raise busy error)
                         await conn_rpc_queue.put(
                             asyncio.create_task(
                                 self._handle_rpc(client, serv_key, arg_data,
                                                  req.rpc_id, False)))
@@ -555,15 +587,16 @@
                         self._delete_events.add(ev)
                         self.event_to_clients.pop(ev)
                 self.client_to_events.pop(client)
             # tell event executor remove task for this client
             if self._delete_events:
                 self.delete_event_ev.set()
             try:
-                await service_core.service_units.run_event_async(ServiceEventType.WebSocketOnDisConnect, client)
+                await service_core.service_units.run_event_async(
+                    ServiceEventType.WebSocketOnDisConnect, client)
             except:
                 traceback.print_exc()
             conn_st_ev.set()
             await wait_task
             # await _cancel(pingpong_task)
             # cancel all rpc
             while True:
@@ -574,15 +607,15 @@
                     break
         if client_id in self.client_id_to_client:
             client_may_main = self.client_id_to_client[client_id]
             if is_backup:
                 if client_may_main.is_backup:
                     self.client_id_to_client.pop(client_id)
                 else:
-                    client_may_main._large_data_ws = None 
+                    client_may_main._large_data_ws = None
             else:
                 self.client_id_to_client.pop(client_id)
         print(f"CONN {client_id} (is_backup={is_backup}) disconnected.")
 
     async def rpc_awaiter(self, rpc_queue: "asyncio.Queue[asyncio.Task]",
                           shutdown_ev: asyncio.Event):
         _shutdown_task = asyncio.create_task(shutdown_ev.wait())
@@ -609,17 +642,18 @@
     async def event_provide_executor(self):
         subed_evs = [(k, self.all_ev_providers[k])
                      for k in self.event_to_clients.keys()]
         ev_tasks = {
             k: asyncio.create_task(ev.fn(), name=k)
             for k, ev in subed_evs
         }
-        task_to_ev: Dict[asyncio.Task,
-                         str] = {v: k
-                                 for k, v in ev_tasks.items()}
+        task_to_ev: Dict[asyncio.Task, str] = {
+            v: k
+            for k, v in ev_tasks.items()
+        }
         wait_new_ev_task = asyncio.create_task(self.new_event_ev.wait(),
                                                name="new_event")
         wait_del_ev_task = asyncio.create_task(self.delete_event_ev.wait(),
                                                name="delete_event")
         if self._shutdown_task is None:
             self._shutdown_task = asyncio.create_task(self.shutdown_ev.wait())
         wait_tasks: List[asyncio.Task] = [
@@ -785,8 +819,7 @@
                                         msg_type=msg_type,
                                         request_id=rpc_id,
                                         is_json=True,
                                         dynamic_key=""))
 
             # print("SEND TIME", cur_ev, time.time() - t)
             task_to_ev = new_task_to_ev
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/inspecttools.py` & `tensorpc-0.11.0/tensorpc/core/inspecttools.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import inspect
-from typing import Any, Callable, Dict, List, Mapping, Optional, Set, Type, Union 
+from typing import Any, Callable, Dict, List, Mapping, Optional, Set, Type, Union
 
 import types
 
 
 def isclassmethod(method):
     # https://stackoverflow.com/questions/19227724/check-if-a-function-uses-classmethod
     bound_to = getattr(method, '__self__', None)
@@ -94,40 +94,42 @@
         for key in dir(obj_type):
             if not key.startswith("__") and not key.endswith("__"):
                 class_attr = getattr(obj_type, key)
                 if isinstance(class_attr, property):
                     res.add(key)
     return res
 
+
 def is_obj_builtin_or_module(v):
     if isinstance(v, types.ModuleType):
-        return True 
-    if inspect.isfunction(v) or inspect.ismethod(v) or inspect.isbuiltin(
-            v):
-        return True 
-    return False 
+        return True
+    if inspect.isfunction(v) or inspect.ismethod(v) or inspect.isbuiltin(v):
+        return True
+    return False
+
 
 def filter_local_vars(local_var: Mapping[str, Any]) -> Mapping[str, Any]:
     new_local_vars: Dict[str, Any] = {}
     for k, v in local_var.items():
         if not is_obj_builtin_or_module(v) and k != "__class__":
-            new_local_vars[k] = v 
+            new_local_vars[k] = v
 
     return new_local_vars
 
+
 def get_function_defined_type(func: Callable):
     func = inspect.unwrap(func)
     mod = inspect.getmodule(func)
     if mod is None:
-        return None 
-    if mod.__name__.startswith("tensorpc") and not mod.__name__.startswith("tensorpc.flow.sampleapp"):
+        return None
+    if mod.__name__.startswith("tensorpc") and not mod.__name__.startswith(
+            "tensorpc.flow.sampleapp"):
         # ignore all tensorpc type
         return None
     func_qname = func.__qualname__
     func_qname_parts = func_qname.split(".")
     res: Union[Type, types.ModuleType] = mod
     cur_obj = mod.__dict__
     for part in func_qname_parts[:-1]:
         cur_obj = cur_obj[part]
         res = cur_obj
     return res
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/marker.py` & `tensorpc-0.11.0/tensorpc/core/marker.py`

 * *Files 6% similar despite different names*

```diff
@@ -25,15 +25,18 @@
         return func
 
     if func is not None:
         return wrapper(func)
     else:
         return wrapper
 
-def mark_server_event(*, func=None, event_type: ServiceEventType = ServiceEventType.Normal):
+
+def mark_server_event(*,
+                      func=None,
+                      event_type: ServiceEventType = ServiceEventType.Normal):
     meta = FunctionUserMeta(ServiceType.Event, event_type=event_type)
     return meta_decorator(func, meta)
 
 
 def mark_client_stream(func=None):
     meta = FunctionUserMeta(ServiceType.ClientStream)
     return meta_decorator(func, meta)
```

### Comparing `tensorpc-0.10.7/tensorpc/core/moduleid.py` & `tensorpc-0.11.0/tensorpc/core/moduleid.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,100 +30,112 @@
 
 def get_qualname_of_type(klass: Type) -> str:
     module = klass.__module__
     if module == 'builtins':
         return klass.__qualname__  # avoid outputs like 'builtins.str'
     return module + '.' + klass.__qualname__
 
+
 def get_mro_qualnames_of_type(klass: Type) -> Set[str]:
     mros = inspect.getmro(klass)
     return set(get_qualname_of_type(mro) for mro in mros)
 
+
 def is_lambda(obj: Callable):
     if not inspect.isfunction(obj) and not inspect.ismethod(obj):
         return False
     return obj.__qualname__ == "<lambda>"
 
 
 def is_valid_function(obj: Callable):
     return inspect.isfunction(obj) or inspect.ismethod(obj)
 
 
 def get_function_qualname(obj: Callable):
     return obj.__qualname__
 
+
 if sys.version_info >= (3, 10):
     _ClassInfo: TypeAlias = type | types.UnionType | tuple["_ClassInfo", ...]
 else:
-    _ClassInfo: TypeAlias = Union[type, Tuple["_ClassInfo", ...]] 
+    _ClassInfo: TypeAlias = Union[type, Tuple["_ClassInfo", ...]]
+
 
 def loose_isinstance(obj, _class_or_tuple: _ClassInfo):
     """for reloaded code, the type of obj may be different from the type of the class in the current module.
     """
     obj_qnames = get_mro_qualnames_of_type(type(obj))
     if not isinstance(_class_or_tuple, (list, tuple)):
-        _class_or_tuple = (_class_or_tuple,)
+        _class_or_tuple = (_class_or_tuple, )
 
     for c in _class_or_tuple:
         if get_qualname_of_type(c) in obj_qnames:
-            return True 
-    return False 
+            return True
+    return False
+
 
 @dataclasses.dataclass
 class InMemoryFSItem:
-    path: str 
-    st_size: int 
+    path: str
+    st_size: int
     st_mtime: float
     st_ctime: float
     content: str
 
+
 class InMemoryFS:
+
     def __init__(self):
         self.fs_dict: Dict[str, InMemoryFSItem] = {}
 
     def add_file(self, path: str, content: str):
-        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(), time.time(), content)
+        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(),
+                                            time.time(), content)
 
     def modify_file(self, path: str, content: str):
         if path not in self.fs_dict:
             raise ValueError("file not exist")
-        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(), self.fs_dict[path].st_ctime, content)
-    
+        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(),
+                                            self.fs_dict[path].st_ctime,
+                                            content)
+
     def add_or_modify_file(self, path: str, content: str):
         if path not in self.fs_dict:
             return self.add_file(path, content)
-        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(), self.fs_dict[path].st_ctime, content)
+        self.fs_dict[path] = InMemoryFSItem(path, len(content), time.time(),
+                                            self.fs_dict[path].st_ctime,
+                                            content)
 
     def stat(self, path: str):
         if path not in self.fs_dict:
             raise OSError(f"file not exist {path}")
         return self.fs_dict[path]
 
     def __contains__(self, path: str):
         return path in self.fs_dict
-    
+
     def __getitem__(self, path: str):
         return self.fs_dict[path]
 
     def load_in_memory_module(self, path: str):
         module = types.ModuleType(path)
         spec = importlib.machinery.ModuleSpec(path, None)
         module.__file__ = path
         module.__spec__ = spec
         code_comp = compile(self[path].content, path, "exec")
         exec(code_comp, module.__dict__)
         # we need to add module to sys.modules to get inspect.getfile work.
         sys.modules[path] = module
         return module
-    
 
 
 def is_tensorpc_dynamic_path(path: str):
     return path.startswith(f"<{TENSORPC_FILE_NAME_PREFIX}")
 
+
 @dataclasses.dataclass
 class TypeMeta:
     module_key: str
     local_key: str
     is_path: bool
     is_in_memory: bool = False
 
@@ -145,15 +157,16 @@
                 return None
             module_dict = module.__dict__
             return module_dict, module
         else:
             if self.is_in_memory:
                 assert in_memory_fs is not None
             if in_memory_fs is not None and self.is_in_memory:
-                standard_module = in_memory_fs.load_in_memory_module(self.module_key)
+                standard_module = in_memory_fs.load_in_memory_module(
+                    self.module_key)
             else:
                 mod_name = Path(self.module_key).stem + "_" + uuid.uuid4().hex
                 mod_name = f"<{mod_name}>"
                 spec = importlib.util.spec_from_file_location(
                     mod_name, self.module_key)
                 assert spec is not None, f"your {self.module_key} not exists"
                 standard_module = importlib.util.module_from_spec(spec)
@@ -167,53 +180,55 @@
     def get_reloaded_module_dict(self):
         res = self.get_reloaded_module()
         if res is not None:
             return res[0]
         return None
 
     @staticmethod
-    def get_local_type_from_module_dict_qualname(qualname: str, module_dict: Dict[str, Any]):
+    def get_local_type_from_module_dict_qualname(qualname: str,
+                                                 module_dict: Dict[str, Any]):
         parts = qualname.split(".")
         obj = module_dict[parts[0]]
         for part in parts[1:]:
             obj = getattr(obj, part)
         return obj
 
     def get_local_type_from_module_dict(self, module_dict: Dict[str, Any]):
         parts = self.local_key.split("::")
         obj = module_dict[parts[0]]
         for part in parts[1:]:
             obj = getattr(obj, part)
         return obj
 
+
 def get_obj_type_meta(obj_type) -> Optional[TypeMeta]:
     qualname = get_qualname_of_type(obj_type)
     spec = importlib.util.find_spec(qualname.split(".")[0])
     is_standard_module = True
     is_in_memory = False
     module_path = ""
     if spec is None or spec.origin is None:
         is_standard_module = False
         try:
             path = inspect.getfile(obj_type)
             if path.startswith(f"<{TENSORPC_FILE_NAME_PREFIX}"):
                 module_path = path
-                is_in_memory = True 
+                is_in_memory = True
             else:
                 module_path_p = Path(path).resolve()
                 module_path = str(module_path_p)
         except:
             # all tensorpc dynamic class store path in __module__
             type_path = obj_type.__module__
             if type_path.startswith(f"<{TENSORPC_FILE_NAME_PREFIX}"):
                 module_path = type_path
-                is_in_memory = True 
+                is_in_memory = True
             else:
                 return None
-    # assert spec is not None 
+    # assert spec is not None
     if spec is not None and spec.origin is not None:
         if "<" in spec.name:
             is_standard_module = False
             module_path = spec.origin
     # else:
     #     try:
     #         module_path_p =  Path(inspect.getfile(obj_type)).resolve()
```

### Comparing `tensorpc-0.10.7/tensorpc/core/prim.py` & `tensorpc-0.11.0/tensorpc/core/prim.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/core/server.py` & `tensorpc-0.11.0/tensorpc/core/server.py`

 * *Files 0% similar despite different names*

```diff
@@ -130,15 +130,16 @@
                   wait_time=-1,
                   port=50051,
                   length=-1,
                   is_local=False,
                   max_threads=10,
                   process_id=-1,
                   credentials=None,
-                  grpc_options: Optional[List[Tuple[str, Union[str, int]]]] = None):
+                  grpc_options: Optional[List[Tuple[str, Union[str,
+                                                               int]]]] = None):
     assert isinstance(service, RemoteObjectService)
     if is_local and process_id >= 0:
         if hasattr(os, "sched_setaffinity"):
             # lock process to cpu to increase performance.
             LOGGER.info("lock worker {} to core {}".format(
                 process_id, process_id))
             os.sched_setaffinity(0, [process_id])
@@ -147,15 +148,15 @@
         wait_interval = wait_time
     options = []
     if length > 0:
         options = [('grpc.max_send_message_length', length * 1024 * 1024),
                    ('grpc.max_receive_message_length', length * 1024 * 1024)]
     options.append(('grpc.so_reuseport', 0))
     if grpc_options is not None:
-        options = grpc_options # override
+        options = grpc_options  # override
     server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_threads),
                          options=options)
     remote_object_pb2_grpc.add_RemoteObjectServicer_to_server(service, server)
     for i in range(TENSORPC_PORT_MAX_TRY):
         if port == -1:
             port = get_free_ports(1)[0]
         url = '[::]:{}'.format(port)
@@ -206,15 +207,15 @@
     url = '[::]:{}'.format(port)
     smeta = ServerMeta(port=port, http_port=-1)
     server_core = ProtobufServiceCore(url, service_def, True, smeta)
     with server_core.enter_global_context():
         server_core.run_event(ServiceEventType.Init)
         service = RemoteObjectService(server_core, is_local, length)
         return serve_service(service, wait_time, port, length, is_local,
-                            max_threads, process_id, credentials)
+                             max_threads, process_id, credentials)
 
 
 def serve_with_http(service_def: ServiceDef,
                     wait_time=-1,
                     port=50051,
                     http_port=50052,
                     length=-1,
```

### Comparing `tensorpc-0.10.7/tensorpc/core/server_core.py` & `tensorpc-0.11.0/tensorpc/core/server_core.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,27 +7,28 @@
 import os
 import pickle
 import sys
 import tempfile
 import threading
 import time
 import traceback
-from typing import (TYPE_CHECKING, Any, AsyncIterator, Callable, Dict, Iterator, List,
-                    Mapping, Optional, Sequence, Union)
+from typing import (TYPE_CHECKING, Any, AsyncIterator, Callable, Dict,
+                    Iterator, List, Mapping, Optional, Sequence, Union)
 import dataclasses
 
 import aiohttp
 from tensorpc.core.defs import Service, ServiceDef
 from tensorpc import compat
 from tensorpc.core import core_io, serviceunit
 from tensorpc.protos_export import remote_object_pb2 as remote_object_pb2
 from tensorpc.protos_export import rpc_message_pb2 as rpc_msg_pb2
 from tensorpc.core.serviceunit import ServiceEventType
 from tensorpc.utils import df_logging
 import contextvars
+
 LOGGER = df_logging.get_logger()
 
 
 @dataclasses.dataclass
 class ServerMeta:
     port: int
     http_port: int
@@ -162,25 +163,25 @@
                                                    server_meta)
 
     async def _init_async_members(self):
         # in future python versions, asyncio event can't be created if no event loop running.
         self.async_shutdown_event = asyncio.Event()
         self._exposed_props._async_shutdown_event = self.async_shutdown_event
 
-
     def _set_port(self, port: int):
         self._exposed_props.server_meta.port = port
 
     def init_http_client_session(self, sess: aiohttp.ClientSession):
         self._global_context.http_client_session = sess
 
     def run_event(self, event: serviceunit.ServiceEventType, *args: Any):
         return self.service_units.run_event(event, *args)
-    
-    async def run_event_async(self, event: serviceunit.ServiceEventType, *args: Any):
+
+    async def run_event_async(self, event: serviceunit.ServiceEventType, *args:
+                              Any):
         return await self.service_units.run_event_async(event, *args)
 
     def _reset_timeout(self):
         with self.reset_timeout_lock:
             self.latest_active_time = time.time()
 
     def _remote_exception_json(self, e: BaseException):
```

### Comparing `tensorpc-0.10.7/tensorpc/core/serviceunit.py` & `tensorpc-0.11.0/tensorpc/core/serviceunit.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,15 +21,17 @@
                     Optional, Set, Tuple, Type, Union)
 from typing_extensions import TypeAlias
 from tensorpc import compat
 from tensorpc.constants import (TENSORPC_FLOW_FUNC_META_KEY,
                                 TENSORPC_FUNC_META_KEY, TENSORPC_SPLIT)
 from tensorpc.core import inspecttools
 from typing import Protocol
-from tensorpc.core.funcid import (determine_code_common_indent, get_body_blocks_from_code, get_toplevel_class_node,
+from tensorpc.core.funcid import (determine_code_common_indent,
+                                  get_body_blocks_from_code,
+                                  get_toplevel_class_node,
                                   get_toplevel_func_node)
 from tensorpc.core.moduleid import TypeMeta, get_obj_type_meta, get_qualname_of_type, InMemoryFS, is_tensorpc_dynamic_path
 from functools import wraps
 
 from tensorpc.constants import TENSORPC_OBSERVED_FUNCTION_ATTR
 from tensorpc.core.rprint_dispatch import rprint
 
@@ -76,19 +78,20 @@
     ClientStream = "ClientStream"  # only support grpc for now
     AsyncWebSocket = "AsyncWebSocket"  # only support ws
     WebSocketEventProvider = "EventProvider"  # only support ws
     WebSocketOnConnect = "WebSocketOnConnect"  # only support ws
     WebSocketOnDisConnect = "WebSocketOnDisConnect"  # only support ws
     Event = "Event"
 
+
 class ServiceEventType(Enum):
     Normal = "Normal"
     Exit = "Exit"
     Init = "AsyncInit"
-    BeforeServerStart = "BeforeServerStart" # all server meta are ready
+    BeforeServerStart = "BeforeServerStart"  # all server meta are ready
     WebSocketOnConnect = "WebSocketOnConnect"  # only support ws
     WebSocketOnDisConnect = "WebSocketOnDisConnect"  # only support ws
 
 
 class AppFuncType(Enum):
     CreateLayout = "CreateLayout"
     # TODO support preview layout in tensorpc.flow
@@ -100,15 +103,18 @@
     ComponentDidMount = "ComponentDidMount"
     ComponentWillUnmount = "ComponentWillUnmount"
     Effect = "Effect"
 
 
 class AppFunctionMeta:
 
-    def __init__(self, type: AppFuncType, name: str = "", data: Optional[Any] = None) -> None:
+    def __init__(self,
+                 type: AppFuncType,
+                 name: str = "",
+                 data: Optional[Any] = None) -> None:
         self.type = type
         self.name = name
         self.data = data
 
     def to_dict(self):
         return {"type": self.type.value, "name": self.name}
 
@@ -133,15 +139,15 @@
     user_app_meta: Optional[AppFunctionMeta] = None
     binded_fn: Optional[Callable] = None
 
     def __init__(self,
                  fn: Callable,
                  name: str,
                  type: ServiceType,
-                event_type: ServiceEventType,
+                 event_type: ServiceEventType,
                  sig: inspect.Signature,
                  is_gen: bool,
                  is_async: bool,
                  is_static: bool,
                  is_binded: bool,
                  qualname: str = "",
                  user_app_meta: Optional[AppFunctionMeta] = None) -> None:
@@ -158,17 +164,17 @@
         self.fn = fn
         self.code = ""
         self.qualname = qualname
         self.user_app_meta = user_app_meta
         self.event_type = event_type
 
     def copy(self):
-        return ServFunctionMeta(self.fn, self.name, self.type, self.event_type, self.sig,
-                                self.is_gen, self.is_async, self.is_static,
-                                self.is_binded, self.qualname,
+        return ServFunctionMeta(self.fn, self.name, self.type, self.event_type,
+                                self.sig, self.is_gen, self.is_async,
+                                self.is_static, self.is_binded, self.qualname,
                                 self.user_app_meta)
 
     def to_json(self):
         if self.user_app_meta is not None:
             user_app_meta = self.user_app_meta.to_dict()
         else:
             user_app_meta = None
@@ -218,14 +224,15 @@
     recorded_data: Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]] = None
     autorun_when_changed: bool = False
     autorun_block_symbol: str = ""
     autorun_locals: dict = {}
     body_code_blocks: List[str] = []
     first_changed_block_idx: int = 0
     userdata: Optional[Any] = None
+
     def run_function_with_record(self) -> Any:
         ...
 
 
 @dataclass
 class ObservedFunction:
     name: str
@@ -238,14 +245,15 @@
     recorded_data: Optional[Tuple[Tuple[Any, ...], Dict[str, Any]]] = None
     autorun_when_changed: bool = False
     autorun_block_symbol: str = ""
     autorun_locals: dict = dataclasses.field(default_factory=dict)
     body_code_blocks: List[str] = dataclasses.field(default_factory=list)
     first_changed_block_idx: int = 0
     userdata: Optional[Any] = None
+
     def run_function_with_record(self):
         assert self.recorded_data is not None
         return self.current_func(*self.recorded_data[0],
                                  **self.recorded_data[1])
 
 
 class ObservedFunctionRegistryProtocol(Protocol):
@@ -297,30 +305,37 @@
 
     def is_enabled(self) -> bool:
         return not self.is_frozen
 
     def get_path_to_qname(self):
         return self.path_to_qname
 
-    def register(self, func=None, autorun_when_changed: bool = False, userdata: Optional[Any] = None, autorun_block_symbol: str = ""):
+    def register(self,
+                 func=None,
+                 autorun_when_changed: bool = False,
+                 userdata: Optional[Any] = None,
+                 autorun_block_symbol: str = ""):
+
         def wrapper(func):
             if not self.is_enabled():
                 return func
             func_unwrap = inspect.unwrap(func)
 
             is_static = isinstance(func_unwrap, staticmethod)
             if is_static:
-                raise NotImplementedError("you must register before staticmethod apply")
+                raise NotImplementedError(
+                    "you must register before staticmethod apply")
             try:
                 path = inspect.getfile(func_unwrap)
                 path = str(Path(path).resolve())
             except:
                 raise ValueError(f"can't get file path of function, {func}")
             code = inspect.getsource(func_unwrap)
-            body_code_blocks = get_body_blocks_from_code(code, autorun_block_symbol)
+            body_code_blocks = get_body_blocks_from_code(
+                code, autorun_block_symbol)
             # TODO check func is a function
             qname = get_qualname_of_type(func_unwrap)
             func_sig = inspect.signature(func_unwrap)
             if autorun_when_changed:
                 # check all parameters has default
                 for p in func_sig.parameters.values():
                     if p.default is inspect.Parameter.empty:
@@ -329,40 +344,51 @@
                             f"{p.name} of function {func} has no default value. "
                             f"autorun observed function must not be a member function."
                         )
             if qname in self.global_dict:
                 # compare blocks between old and new
                 first_changed_block_idx = len(body_code_blocks)
                 for i in range(len(body_code_blocks)):
-                    if i >= len(self.global_dict[qname].body_code_blocks) or body_code_blocks[i] != self.global_dict[qname].body_code_blocks[i]:
+                    if i >= len(self.global_dict[qname].body_code_blocks
+                                ) or body_code_blocks[i] != self.global_dict[
+                                    qname].body_code_blocks[i]:
                         first_changed_block_idx = i
                         break
                 self.global_dict[qname].current_func = func
                 self.global_dict[qname].current_sig = func_sig
-                self.global_dict[qname].first_changed_block_idx = first_changed_block_idx
+                self.global_dict[
+                    qname].first_changed_block_idx = first_changed_block_idx
                 self.global_dict[qname].body_code_blocks = body_code_blocks
             else:
                 if path not in self.path_to_qname:
                     self.path_to_qname[path] = []
-                self.path_to_qname[path].append((qname, func_unwrap.__qualname__))
+                self.path_to_qname[path].append(
+                    (qname, func_unwrap.__qualname__))
                 self.global_dict[qname] = ObservedFunction(
-                    func_unwrap.__name__, qname, func, func, func_sig, path, 
-                    autorun_when_changed=autorun_when_changed, userdata=userdata,
+                    func_unwrap.__name__,
+                    qname,
+                    func,
+                    func,
+                    func_sig,
+                    path,
+                    autorun_when_changed=autorun_when_changed,
+                    userdata=userdata,
                     body_code_blocks=body_code_blocks)
 
             @wraps(func)
             def wrapped_func(*args, **kwargs):
                 if qname in self.global_dict:
                     entry = self.global_dict[qname]
                     if entry.enable_args_record:
                         entry.recorded_data = (args, kwargs)
                         # self.handle_record(entry, args, kwargs)
                     return entry.current_func(*args, **kwargs)
                 else:
                     return func(*args, **kwargs)
+
             setattr(wrapped_func, TENSORPC_OBSERVED_FUNCTION_ATTR, True)
             return wrapped_func
 
         if func is None:
             return wrapper
         else:
             return wrapper(func)
@@ -552,15 +578,15 @@
 
 
 @dataclass
 class ObjectReloadResultWithType(ObjectReloadResult):
     type_meta: TypeMeta
 
 
-@dataclasses.dataclass 
+@dataclasses.dataclass
 class MethodMetaItem:
     type: Optional[Type]
     metas: List[ServFunctionMeta]
     is_leaf: bool
 
 
 class ObjectReloadManager:
@@ -572,26 +598,28 @@
     def __init__(
         self,
         observed_registry: Optional[ObservedFunctionRegistryProtocol] = None
     ) -> None:
         self.file_cache: Dict[str, FileCacheEntry] = {}
         # self.type_cache: Dict[str, TypeCacheEntry] = {}
         self.type_meta_cache: Dict[ObjectReloadManager.TypeUID, TypeMeta] = {}
-        self.type_method_meta_cache: Dict[Tuple[ObjectReloadManager.TypeUID, bool],
+        self.type_method_meta_cache: Dict[Tuple[ObjectReloadManager.TypeUID,
+                                                bool],
                                           List[ServFunctionMeta]] = {}
         self.module_cache: Dict[str, ModuleCacheEntry] = {}
 
         self.observed_registry = observed_registry
 
         self.in_memory_fs = InMemoryFS()
 
     def _is_memory_fs_path(self, path: str):
         return path in self.in_memory_fs
 
-    def update_observed_registry(self, observed_registry: ObservedFunctionRegistryProtocol):
+    def update_observed_registry(
+            self, observed_registry: ObservedFunctionRegistryProtocol):
         self.observed_registry = observed_registry
 
     def check_file_cache(self, path: str):
         if path not in self.file_cache:
             return
         entry = self.file_cache[path]
         if entry.invalid:
@@ -719,26 +747,26 @@
         #         qnames = self.observed_registry.get_path_to_qname()[resolved_path]
         #         for _, local_qname in qnames:
         #             new_func = TypeMeta.get_local_type_from_module_dict_qualname(
         #                 local_qname, res[0])
         #             if isinstance(new_func, staticmethod):
         #                 new_func = new_func.__func__
         #             new_func_unwrap = inspect.unwrap(new_func, stop=lambda fn: not hasattr(fn, TENSORPC_OBSERVED_FUNCTION_ATTR))
-                    # self.observed_registry.reload_func(local_qname, new_func_unwrap)
+        # self.observed_registry.reload_func(local_qname, new_func_unwrap)
         return ObjectReloadResultWithType(self.module_cache[path], True,
                                           self.file_cache[path], meta)
 
     def get_type_unique_id(self, type: Type):
         return (self._inspect_get_file_resolved(type), type.__qualname__)
 
-    def query_type_method_meta(self,
-                               type: Type,
-                               no_code: bool = False,
-                               include_base: bool = False
-                               ) -> List[ServFunctionMeta]:
+    def query_type_method_meta(
+            self,
+            type: Type,
+            no_code: bool = False,
+            include_base: bool = False) -> List[ServFunctionMeta]:
         """we should always use new type (after reload) with this function.
         """
         try:
             uid = self.get_type_unique_id(type)
         except:
             return []
         method_meta_cache_key = (uid, include_base)
@@ -776,35 +804,41 @@
             self.type_method_meta_cache[method_meta_cache_key] = new_metas
         else:
             new_metas = ReloadableDynamicClass.get_metas_of_regular_methods(
                 inspect_type, include_base=include_base, no_code=True)
         new_metas = [m.copy() for m in new_metas]
         return new_metas
 
-    def query_type_method_meta_dict(self, this_type: Type,
-                               no_code: bool = False) -> Dict["ObjectReloadManager.TypeUID", MethodMetaItem]:
-        res_list = self.query_type_method_meta(this_type, no_code, include_base=True)
+    def query_type_method_meta_dict(
+        self,
+        this_type: Type,
+        no_code: bool = False
+    ) -> Dict["ObjectReloadManager.TypeUID", MethodMetaItem]:
+        res_list = self.query_type_method_meta(this_type,
+                                               no_code,
+                                               include_base=True)
         res: Dict[Tuple[str, str], MethodMetaItem] = {}
         for meta in res_list:
             fn = meta.fn
             type = inspecttools.get_function_defined_type(fn)
             if type is None:
-                continue 
+                continue
             key = type
             if inspect.ismodule(key):
                 key = None
-                is_leaf = True 
+                is_leaf = True
             else:
-                is_leaf = self.get_type_unique_id(this_type) == self.get_type_unique_id(type)
+                is_leaf = self.get_type_unique_id(
+                    this_type) == self.get_type_unique_id(type)
             uid = self.get_type_unique_id(type)
             if uid not in res:
                 res[uid] = MethodMetaItem(key, [], is_leaf)
             res[uid].metas.append(meta)
         return res
-    
+
     def _tokenize_read_path_lines(self, path: str):
         if path in self.in_memory_fs:
             ss = io.StringIO(self.in_memory_fs[path].content)
             return ss.readlines()
         else:
             with tokenize.open(path) as f:
                 return f.readlines()
@@ -1109,19 +1143,20 @@
     """
     rc = DynamicClass(module_name)
     return rc.obj_type, rc.alias, rc.module_key
 
 
 class FunctionUserMeta:
 
-    def __init__(self,
-                 type: ServiceType,
-                 event_name: str = "",
-                 is_dynamic: bool = False,
-                 event_type: ServiceEventType = ServiceEventType.Normal) -> None:
+    def __init__(
+            self,
+            type: ServiceType,
+            event_name: str = "",
+            is_dynamic: bool = False,
+            event_type: ServiceEventType = ServiceEventType.Normal) -> None:
         self.type = type
         self._event_name = event_name
         self.is_dynamic = is_dynamic
         self.event_type = event_type
 
     @property
     def event_name(self):
@@ -1161,15 +1196,16 @@
 
     def __init__(self, module_name: str, config: Dict[str, Any]) -> None:
         super().__init__(module_name)
         assert config is not None, "please use {} in yaml if config is empty"
         # self.obj_type, self.alias, self.module_key = get_cls_obj_from_module_name(
         #     module_name)
         self.services: Dict[str, ServFunctionMeta] = {}
-        self._event_to_handlers: Dict[ServiceEventType, List[ServFunctionMeta]] = {}
+        self._event_to_handlers: Dict[ServiceEventType,
+                                      List[ServFunctionMeta]] = {}
         self.name_to_events: Dict[str, EventProvider] = {}
         self.serv_metas = self._init_all_metas(self.obj_type)
         assert len(
             self.services
         ) > 0, f"your service {module_name} must have at least one valid method"
         # self.obj = self.obj_type(**config)
         self.obj: Optional[Any] = None
@@ -1322,15 +1358,15 @@
         fn = self.services[serv_key].get_binded_fn()
         return fn(*args, **kwargs)
 
     def run_service_from_fn(self, fn: Callable, *args, **kwargs):
         self.init_service()
         assert self.obj is not None
         return fn(*args, **kwargs)
-    
+
     def run_event(self, event: ServiceEventType, *args: Any):
         if event in self._event_to_handlers:
             if not self.is_inited():
                 self.init_service()
             for h in self._event_to_handlers[event]:
                 if not h.is_async:
                     h.get_binded_fn()(*args)
@@ -1392,9 +1428,9 @@
     def get_all_event_providers(self):
         res: Dict[str, EventProvider] = {}
         for su in self.sus:
             res.update(su.get_all_event_providers())
         return res
 
     def run_init(self):
-        pass 
+        pass
         # self.init_service()
```

### Comparing `tensorpc-0.10.7/tensorpc/core/tracer.py` & `tensorpc-0.11.0/tensorpc/core/tracer.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,18 +5,19 @@
 import sys
 import inspect
 import threading
 import time
 from types import CodeType, FrameType
 from typing import Any, Callable, Dict, Mapping, Optional, Set, Tuple, Type
 from tensorpc import compat
-from tensorpc.constants import TENSORPC_FILE_NAME_PREFIX 
+from tensorpc.constants import TENSORPC_FILE_NAME_PREFIX
 
 THREAD_GLOBALS = threading.local()
 
+
 class TraceType(enum.Enum):
     Call = 0
     Return = 1
 
 
 @dataclass
 class FrameResult:
@@ -39,14 +40,15 @@
     """A simple tracer for python functions.
     Reference: https://github.com/cool-RR/PySnooper/blob/1.1.1/pysnooper/tracer.py
     filters: 
     1. class base types 
     2. method/function names
     3. include folders
     """
+
     def __init__(self,
                  callback: Callable[[FrameResult], Any],
                  traced_types: Optional[Tuple[Type]] = None,
                  traced_names: Optional[Set[str]] = None,
                  traced_folders: Optional[Set[str]] = None,
                  trace_return: bool = True,
                  depth: int = 5,
@@ -69,60 +71,63 @@
         self.callback = callback
         if traced_folders is not None and len(traced_folders) > 0:
             self.traced_folders = set(
                 Path(folder) for folder in traced_folders)
         self._frame_cnt = _frame_cnt
         self.use_profile = use_profile
 
-        self._inner_frame_fnames: Set[str] = set([Tracer.__enter__.__code__.co_filename])
+        self._inner_frame_fnames: Set[str] = set(
+            [Tracer.__enter__.__code__.co_filename])
 
     def _filter_frame(self, frame: FrameType):
         if frame.f_code in self.code_res:
             return self.code_res[frame.f_code][0]
         is_traced_types = True
         is_traced_names = True
         is_traced_folders = True
         co_name = frame.f_code.co_name
         # TODO better handle tensorpc scripts
         if frame.f_code.co_filename.startswith(self._tensorpc_prefix):
             self.code_res[frame.f_code] = (True, True)
-            return True 
+            return True
         if co_name.startswith("<") and co_name.endswith(">"):
             # ignore all comp frame such as <listcomp>
             # listcomp frame will be removed in python 3.12
             self.code_res[frame.f_code] = (False, False)
-            return False 
+            return False
         # TODO better check
         keep_one_frame = False
-        if co_name == "__getattr__" or co_name == "__setattr__" :
+        if co_name == "__getattr__" or co_name == "__setattr__":
             self.code_res[frame.f_code] = (False, False)
-            return False 
+            return False
         if self.traced_types is not None and "self" in frame.f_locals:
-            is_traced_types = isinstance(frame.f_locals["self"],self.traced_types)
+            is_traced_types = isinstance(frame.f_locals["self"],
+                                         self.traced_types)
         if self.traced_names is not None:
             is_traced_names = frame.f_code.co_name in self.traced_names
         if self.ignored_names is not None:
             if frame.f_code.co_name in self.ignored_names:
                 self.code_res[frame.f_code] = (False, False)
-                return False 
+                return False
         if self.traced_folders is not None:
             code_path = Path(frame.f_code.co_filename)
             found = False
             for candidate in self.traced_folders:
                 if compat.is_relative_to(code_path, candidate):
                     found = True
                     break
             is_traced_folders = found
             if not found:
                 # keep external trace for one depth
                 back = frame.f_back
-                if self.keep_one_frame_for_ignored and back is not None and back.f_code in self.code_res and self.code_res[back.f_code][1]:
+                if self.keep_one_frame_for_ignored and back is not None and back.f_code in self.code_res and self.code_res[
+                        back.f_code][1]:
                     keep_one_frame = True
         res = is_traced_types and is_traced_names and is_traced_folders
-        filter_res = res 
+        filter_res = res
         if keep_one_frame:
             filter_res = keep_one_frame
         self.code_res[frame.f_code] = (filter_res, res)
         return res
 
     def __enter__(self):
         THREAD_GLOBALS.__dict__.setdefault('depth', 0)
@@ -176,24 +181,29 @@
     def _is_internal_frame(self, frame: FrameType):
         return frame.f_code.co_filename in self._inner_frame_fnames
 
     def trace_lite(self, frame: FrameType, event, arg):
         if event == 'return':
             THREAD_GLOBALS.depth -= 1
             # print(event, frame.f_code.co_name, "THREAD_GLOBALS.depth", THREAD_GLOBALS.depth)
-            self.callback(self.get_frame_result(TraceType.Return, frame, THREAD_GLOBALS.depth))
+            self.callback(
+                self.get_frame_result(TraceType.Return, frame,
+                                      THREAD_GLOBALS.depth))
 
     @staticmethod
-    def get_frame_result(trace_type: TraceType, frame: FrameType, depth: int=-1, c_call_obj: Optional[object]=None):
+    def get_frame_result(trace_type: TraceType,
+                         frame: FrameType,
+                         depth: int = -1,
+                         c_call_obj: Optional[object] = None):
         qname = frame.f_code.co_name
         if sys.version_info[:2] >= (3, 11):
-            qname = frame.f_code.co_qualname # type: ignore
+            qname = frame.f_code.co_qualname  # type: ignore
         else:
             if "self" in frame.f_locals:
-                qname = type(frame.f_locals["self"]).__qualname__ + "." + qname                
+                qname = type(frame.f_locals["self"]).__qualname__ + "." + qname
         module = inspect.getmodule(frame)
         module_qname = ""
         if module is not None:
             module_qname = module.__name__
         return FrameResult(
             type=trace_type,
             qualname=qname,
@@ -259,14 +269,15 @@
                         return None
 
         # we only handle methods and global functions.
         # print(event, frame.f_code.co_name)
         if not self._filter_frame(frame):
             return None
         if event == "call":
-            self.callback(self.get_frame_result(TraceType.Call, frame, THREAD_GLOBALS.depth))
+            self.callback(
+                self.get_frame_result(TraceType.Call, frame,
+                                      THREAD_GLOBALS.depth))
             # print("THREAD_GLOBALS.depth CALL", THREAD_GLOBALS.depth)
 
             THREAD_GLOBALS.depth += 1
 
         return self.trace_lite
-
```

### Comparing `tensorpc-0.10.7/tensorpc/core/tree_id.py` & `tensorpc-0.11.0/tensorpc/core/tree_id.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,49 +1,56 @@
-
 from typing import (TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable,
-                    Coroutine, Dict, Generic, Iterable, List, Optional, Set, Tuple,
-                    Type, TypeVar, Union)
+                    Coroutine, Dict, Generic, Iterable, List, Optional, Set,
+                    Tuple, Type, TypeVar, Union)
 from pydantic_core import core_schema
 from pydantic import (
     GetCoreSchemaHandler, )
 
 
 class UniqueTreeId:
     # format: length1,length2,length3|part1::part2::part3
     # part names may contains splitter '::', so we need lengths to split
     def __init__(self, uid: str, splitter_length: int = 1) -> None:
         self.uid_encoded = uid
         # init_parts = uid.split("|")
         splitter_first_index = uid.find("|")
         if splitter_first_index == -1:
             # empty uid, means uid must be ""
-            assert len(uid) == 0, f"uid should be empty if no splitter exists, but got {uid}"
+            assert len(
+                uid
+            ) == 0, f"uid should be empty if no splitter exists, but got {uid}"
             self.parts: List[str] = []
             uid_part = ""
             lengths: List[int] = []
         else:
             length_part = uid[:splitter_first_index]
             uid_part = uid[splitter_first_index + 1:]
             lengths = [int(n) for n in length_part.split(",")]
-            assert sum(lengths) == len(uid_part) - splitter_length * (len(lengths) - 1), f"{uid} not valid, {lengths}, {uid_part}"
+            assert sum(lengths) == len(uid_part) - splitter_length * (
+                len(lengths) - 1), f"{uid} not valid, {lengths}, {uid_part}"
         start = 0
         self.parts: List[str] = []
         for l in lengths:
             self.parts.append(uid_part[start:start + l])
             start += l + splitter_length
         self.splitter_length = splitter_length
 
     def empty(self):
         return len(self.uid_encoded) == 0
 
     @classmethod
-    def from_parts(cls, parts: List[str], splitter: str = ".") -> "UniqueTreeId":
+    def from_parts(cls,
+                   parts: List[str],
+                   splitter: str = ".") -> "UniqueTreeId":
         if len(parts) == 0:
             return cls("", len(splitter))
-        return cls(",".join([str(len(p)) for p in parts]) + "|" + splitter.join(parts), len(splitter))
+        return cls(
+            ",".join([str(len(p))
+                      for p in parts]) + "|" + splitter.join(parts),
+            len(splitter))
 
     def __repr__(self) -> str:
         return f"UniqueTreeId({self.uid_encoded})"
 
     def __hash__(self) -> int:
         return hash(self.uid_encoded)
 
@@ -88,34 +95,47 @@
         for i in range(len(other.parts)):
             if self.parts[i] != other.parts[i]:
                 return False
         return True
 
     def common_prefix(self, other: "UniqueTreeId") -> "UniqueTreeId":
         i = 0
-        while i < len(self.parts) and i < len(other.parts) and self.parts[i] == other.parts[i]:
+        while i < len(self.parts) and i < len(
+                other.parts) and self.parts[i] == other.parts[i]:
             i += 1
         return UniqueTreeId.from_parts(self.parts[:i])
 
     def common_prefix_index(self, other: "UniqueTreeId") -> int:
         i = 0
-        while i < len(self.parts) and i < len(other.parts) and self.parts[i] == other.parts[i]:
+        while i < len(self.parts) and i < len(
+                other.parts) and self.parts[i] == other.parts[i]:
             i += 1
         return i
 
+
 class UniqueTreeIdForTree(UniqueTreeId):
+
     @classmethod
-    def from_parts(cls, parts: List[str], splitter: str = ":") -> "UniqueTreeIdForTree":
+    def from_parts(cls,
+                   parts: List[str],
+                   splitter: str = ":") -> "UniqueTreeIdForTree":
         if len(parts) == 0:
             return cls("", len(splitter))
-        return cls(",".join([str(len(p)) for p in parts]) + "|" + splitter.join(parts), len(splitter))
-
-    def append_part(self, part: str, splitter: str = ":") -> "UniqueTreeIdForTree":
+        return cls(
+            ",".join([str(len(p))
+                      for p in parts]) + "|" + splitter.join(parts),
+            len(splitter))
+
+    def append_part(self,
+                    part: str,
+                    splitter: str = ":") -> "UniqueTreeIdForTree":
         return UniqueTreeIdForTree.from_parts(self.parts + [part], splitter)
 
     def pop(self):
         return UniqueTreeIdForTree.from_parts(self.parts[:-1])
 
-    def __add__(self, other: Union["UniqueTreeIdForTree", str]) -> "UniqueTreeIdForTree":
+    def __add__(
+            self, other: Union["UniqueTreeIdForTree",
+                               str]) -> "UniqueTreeIdForTree":
         if isinstance(other, str):
             return UniqueTreeIdForTree.from_parts(self.parts + [other], ":")
         return UniqueTreeIdForTree.from_parts(self.parts + other.parts, ":")
```

### Comparing `tensorpc-0.10.7/tensorpc/examples/__init__.py` & `tensorpc-0.11.0/tensorpc/examples/ai/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/examples/ai/__init__.py` & `tensorpc-0.11.0/tensorpc/examples/ai/engine.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/examples/ai/engine.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/reload.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.10-Template Component.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.10-Template Component.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.11-Inheritance.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.11-Inheritance.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.12-App Context.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.12-App Context.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.13-Component Context.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.13-Component Context.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.14-Host Resource.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.14-Host Resource.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.2-App Basic Architecture.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.2-App Basic Architecture.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.3-Flex Box Basic.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.3-Flex Box Basic.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.4-Containers.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.4-Containers.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.5-Advanced Events.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.5-Advanced Events.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.6-Composite Component.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.6-Composite Component.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.7-Advanced UI Methods.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.7-Advanced UI Methods.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.8-Drag And Drop.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.8-Drag And Drop.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/01-basic/1.9-Reload System.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/01-basic/1.9-Reload System.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.0-Common Props.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.0-Common Props.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.1-Button.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.1-Button.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.10-Slider.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.10-Slider.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.11-Tab.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.11-Tab.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.12-Drawer.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.12-Drawer.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.13-Dialog.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.13-Dialog.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.14-Collapse.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.14-Collapse.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.15-Chip.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.15-Chip.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.16-Progress.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.16-Progress.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.17-MenuList.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.17-MenuList.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.2-Typography.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.2-Typography.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.22-DataGrid.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.22-DataGrid.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.25-Plotly.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.25-Plotly.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.26-Map.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.26-Map.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.3-Markdown.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.3-Markdown.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.4-TextField.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.4-TextField.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.5-List.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.5-List.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.6-Select.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.6-Select.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.7-AutoComplete.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.7-AutoComplete.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.8-ToggleButton.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.8-ToggleButton.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/02-components/2.9-Image.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/02-components/2.9-Image.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/03-3d basic/3.1-Hello 3D World.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/03-3d basic/3.1-Hello 3D World.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials/07-V Api/7.1-Basic.md` & `tensorpc-0.11.0/tensorpc/examples/tutorials/07-V Api/7.1-Basic.md`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/examples/tutorials.py` & `tensorpc-0.11.0/tensorpc/examples/tutorials.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,38 +8,41 @@
 from tensorpc.flow.marker import mark_did_mount
 
 
 class MarkdownTutorialsTree:
 
     @mark_create_layout
     def my_layout(self):
-        appctx.set_app_z_index(200) # required for drawer/dialog.
+        appctx.set_app_z_index(200)  # required for drawer/dialog.
         appctx.get_app().set_enable_language_server(True)
         pyright_setting = appctx.get_app().get_language_server_settings()
         pyright_setting.python.analysis.pythonPath = sys.executable
         pyright_setting.python.analysis.extraPaths = [
             str(PACKAGE_ROOT.parent),
         ]
         tutorials_path = PACKAGE_ROOT / "examples" / "tutorials"
         tutorials: Dict[str, Any] = {}
         paths = list(tutorials_path.rglob("*.md"))
-        paths.sort(key=lambda p: list(map(int, p.stem.split("-")[0].split("."))))
+        paths.sort(key=lambda p: list(map(int,
+                                          p.stem.split("-")[0].split("."))))
         for p in paths:
             md_relative_path = p.relative_to(tutorials_path)
             parts = md_relative_path.parts
             tutorials_cur = tutorials
             for part in parts[:-1]:
                 if part not in tutorials:
                     tutorials_cur[part] = {}
                 tutorials_cur = tutorials_cur[part]
             md_content = p.read_text()
             tutorials_cur[md_relative_path.stem] = plus.MarkdownTutorial(
                 md_content, str(md_relative_path)).prop(width="100%",
-                    height="100%",
-                    overflow="auto")
+                                                        height="100%",
+                                                        overflow="auto")
         self.tutorials = tutorials
         self.panel = plus.InspectPanel({}, use_fast_tree=True)
         return self.panel.prop(width="100%", height="100%", overflow="hidden")
 
     @mark_did_mount
     async def _on_init(self):
-        await self.panel.inspector.set_object(self.tutorials, key="tutorials", expand_level=2)
+        await self.panel.inspector.set_object(self.tutorials,
+                                              key="tutorials",
+                                              expand_level=2)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,13 +13,13 @@
 # limitations under the License.
 
 from tensorpc.core.moduleid import loose_isinstance
 
 from . import constants, marker
 from .flowapp import App, EditableApp, EditableLayoutApp, appctx
 from .flowapp.appcore import observe_function, AppSpecialEventType
-from .flowapp.components import leaflet, mui, plotly, plus, three
+from .flowapp.components import leaflet, mui, plotly, plus, three, flow
 from .flowapp.objtree import UserObjTree
 from .marker import (mark_autorun, mark_create_layout, mark_create_object,
                      mark_create_preview_layout, mark_did_mount,
                      mark_will_unmount)
 from .flowapp.components.plus.vis import vapi_core as V
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/client.py` & `tensorpc-0.11.0/tensorpc/flow/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,16 @@
 import tensorpc
 
 import os
 import time
 from tensorpc.flow.coretypes import Message, MessageItem, MessageLevel, RelayUpdateNodeEvent
 import uuid
 
-import psutil 
+import psutil
+
 
 class MasterMeta:
 
     def __init__(self) -> None:
         gid = os.getenv(constants.TENSORPC_FLOW_GRAPH_ID)
         nid = os.getenv(constants.TENSORPC_FLOW_NODE_ID)
         nrid = os.getenv(constants.TENSORPC_FLOW_NODE_READABLE_ID)
@@ -64,18 +65,21 @@
         self._graph_id = gid
         self._node_id = nid
         self.grpc_port = gport
         self.http_port = port
         self.grpc_url = grpc_url
         self.http_url = url
         self.is_worker = is_worker
-        lsp_port = os.getenv(constants.TENSORPC_FLOW_APP_LANG_SERVER_PORT, None)
-        self.lsp_port = int(lsp_port) if lsp_port is not None else None 
-        lsp_fwd_port = os.getenv(constants.TENSORPC_FLOW_APP_LANG_SERVER_FWD_PORT, None)
-        self.lsp_fwd_port = int(lsp_fwd_port) if lsp_fwd_port is not None else None 
+        lsp_port = os.getenv(constants.TENSORPC_FLOW_APP_LANG_SERVER_PORT,
+                             None)
+        self.lsp_port = int(lsp_port) if lsp_port is not None else None
+        lsp_fwd_port = os.getenv(
+            constants.TENSORPC_FLOW_APP_LANG_SERVER_FWD_PORT, None)
+        self.lsp_fwd_port = int(
+            lsp_fwd_port) if lsp_fwd_port is not None else None
 
         self.is_grpc_valid = grpc_url != ""
         self.is_http_valid = self.http_url != ""
         self.is_inside_devflow = gid is not None and nid is not None
 
     @property
     def graph_id(self):
@@ -117,32 +121,36 @@
             grpc_url = f"localhost:{gport}"
         self.grpc_port = gport
         self.http_port = port
         self.grpc_url = grpc_url
         self.http_url = url
         self.is_inside_devflow = gport is not None and port is not None and self.module_name != ""
 
+
 @dataclasses.dataclass
 class AppProcessMeta:
-    name: str 
+    name: str
     pid: int
-    port: int 
-    grpc_port: int 
-    app_port: int 
+    port: int
+    grpc_port: int
+    app_port: int
     app_grpc_port: int
 
+
 def list_all_app_in_machine():
     res: List[AppProcessMeta] = []
     for proc in psutil.process_iter(['pid', 'name']):
         proc_name = proc.info["name"]
         if proc_name.startswith(constants.TENSORPC_FLOW_PROCESS_NAME_PREFIX):
             ports = list(map(int, proc_name.split("-")[1:]))
-            meta = AppProcessMeta(proc_name, proc.info["pid"], ports[0], ports[1], ports[2], ports[3])
+            meta = AppProcessMeta(proc_name, proc.info["pid"], ports[0],
+                                  ports[1], ports[2], ports[3])
             res.append(meta)
-    return res 
+    return res
+
 
 def is_inside_devflow():
     meta = MasterMeta()
     return meta.is_inside_devflow
 
 
 def is_inside_app_session():
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/close_langserv/__main__.py` & `tensorpc-0.11.0/tensorpc/flow/close_langserv/__main__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import fire
 from tensorpc.flow.langserv import close_tmux_lang_server
 
+
 def main(uid: str):
     port = close_tmux_lang_server(uid)
     print(f"{port}")
 
+
 if __name__ == "__main__":
-    fire.Fire(main)
+    fire.Fire(main)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/constants.py` & `tensorpc-0.11.0/tensorpc/flow/constants.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,12 +53,10 @@
 TENSORPC_FLOW_LANG_SERVER_PREFIX = "__tensorpc_lang_server"
 
 # basic uid use ".", flexlayout use "#" and "-"
 # FIXME use safe key
 TENSORPC_FLOW_COMP_UID_TEMPLATE_SPLIT = "$&&"
 TENSORPC_FLOW_COMP_UID_STRUCTURE_SPLIT = ":"
 
-
 TENSORPC_FLOW_EFFECTS_OBSERVE = "TENSORPC_FLOW_EFFECTS_OBSERVE"
 
-
-TENSORPC_FLOW_PROCESS_NAME_PREFIX = "__tensorpc_flow"
+TENSORPC_FLOW_PROCESS_NAME_PREFIX = "__tensorpc_flow"
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/coretypes.py` & `tensorpc-0.11.0/tensorpc/flow/coretypes.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 import enum
 
 from typing import (TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable,
-                    Coroutine, Dict, Generic, Iterable, List, Optional, Set, Tuple,
-                    Type, TypeVar, Union)
+                    Coroutine, Dict, Generic, Iterable, List, Optional, Set,
+                    Tuple, Type, TypeVar, Union)
 from tensorpc.autossh.core import Event, event_from_dict
+from tensorpc.core import dataclass_dispatch
 from tensorpc.core.moduleid import get_qualname_of_type
 from .jsonlike import JsonLikeNode, as_dict_no_undefined, Undefined
+
+
 def get_uid(graph_id: str, node_id: str):
     return f"{graph_id}@{node_id}"
 
 
 class StorageDataItem:
 
-    def __init__(self, data: bytes, 
-                 meta: JsonLikeNode) -> None:
+    def __init__(self, data: bytes, meta: JsonLikeNode) -> None:
         self.data = data
         self.meta = meta
-        assert not isinstance(self.meta.userdata, Undefined) 
-
+        assert not isinstance(self.meta.userdata, Undefined)
 
     def empty(self):
         return len(self.data) > 0
 
-    @property 
+    @property
     def timestamp(self):
-        assert not isinstance(self.meta.userdata, Undefined) 
+        assert not isinstance(self.meta.userdata, Undefined)
         return self.meta.userdata["timestamp"]
 
     def __len__(self):
         return len(self.data)
 
     def get_meta_dict(self):
         return as_dict_no_undefined(self.meta)
@@ -279,7 +280,33 @@
             "envs": self.envs,
         }
 
     @classmethod
     def from_dict(cls, data):
         return cls(data["ts"], data["data"], data["envs"])
 
+
+class VscodeTensorpcMessageType(enum.IntEnum):
+    UpdateActiveTab = 0
+    UpdateCursorPosition = 1
+
+
+@dataclass_dispatch.dataclass
+class Position:
+    line: int
+    character: int
+
+
+@dataclass_dispatch.dataclass
+class Selection:
+    start: Position
+    end: Position
+    anchor: Position
+    active: Position
+
+
+@dataclass_dispatch.dataclass
+class VscodeTensorpcMessage:
+    type: VscodeTensorpcMessageType
+    currentUri: str
+    workspaceUri: str
+    selections: Optional[List[Selection]] = None
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/app.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/app.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,15 +67,15 @@
                                        ObservedFunctionRegistryProtocol,
                                        ReloadableDynamicClass,
                                        ServFunctionMeta, ServiceUnit,
                                        SimpleCodeManager, get_qualname_to_code)
 from tensorpc.flow.client import MasterMeta
 from tensorpc.flow.constants import TENSORPC_FLOW_COMP_UID_TEMPLATE_SPLIT, TENSORPC_FLOW_EFFECTS_OBSERVE
 from tensorpc.core.tree_id import UniqueTreeId, UniqueTreeIdForTree
-from tensorpc.flow.coretypes import ScheduleEvent, StorageDataItem
+from tensorpc.flow.coretypes import ScheduleEvent, StorageDataItem, VscodeTensorpcMessage
 
 from tensorpc.flow.flowapp.components.plus.objinspect.inspector import get_exception_frame_stack
 from tensorpc.flow.flowapp.components.plus.objinspect.treeitems import TraceTreeItem
 from tensorpc.flow.flowapp.reload import (AppReloadManager,
                                           bind_and_reset_object_methods,
                                           reload_object_methods)
 from tensorpc.flow.jsonlike import JsonLikeNode, parse_obj_to_jsonlike
@@ -83,15 +83,15 @@
 from tensorpc.flow.marker import AppFunctionMeta, AppFuncType
 from tensorpc.flow.serv_names import serv_names
 from tensorpc.utils.registry import HashableRegistry
 from tensorpc.utils.reload import reload_method
 from tensorpc.utils.uniquename import UniqueNamePool
 
 from .appcore import (ALL_OBSERVED_FUNCTIONS, AppContext, AppSpecialEventType,
-                      _CompReloadMeta, Event, create_reload_metas)
+                      _CompReloadMeta, Event, EventHandlingContext, create_reload_metas, enter_event_handling_conetxt)
 from .appcore import enter_app_conetxt
 from .appcore import enter_app_conetxt as _enter_app_conetxt
 from .appcore import get_app, get_app_context
 from .components import mui, plus, three
 from tensorpc.core.tracer import FrameResult, Tracer, TraceType
 from .core import (AppComponentCore, AppEditorEvent, AppEditorEventType,
                    AppEditorFrontendEvent, AppEditorFrontendEventType,
@@ -99,16 +99,14 @@
                    ContainerBase, CopyToClipboardEvent, EventHandler,
                    FlowSpecialMethods, ForEachResult, FrontendEventType,
                    LayoutEvent, TaskLoopEvent, UIEvent, UIExceptionEvent,
                    UIRunStatus, UIType, UIUpdateEvent, Undefined, UserMessage,
                    ValueType, undefined)
 from tensorpc.core.event_emitter.aio import AsyncIOEventEmitter
 
-
-
 ALL_APP_EVENTS = HashableRegistry()
 P = ParamSpec('P')
 
 T = TypeVar('T')
 
 T_comp = TypeVar("T_comp")
 
@@ -188,43 +186,51 @@
 
 T = TypeVar("T")
 
 
 @dataclasses.dataclass
 class _LayoutObserveMeta:
     # one type (base class) may related to multiple layouts
-    layouts: Dict[Union[mui.FlexBox, "App"], Optional[Callable[[mui.FlexBox, ServFunctionMeta],
-                                Coroutine[None, None, Optional[mui.FlexBox]]]]]
+    layouts: Dict[Union[mui.FlexBox, "App"],
+                  Optional[Callable[[mui.FlexBox, ServFunctionMeta],
+                                    Coroutine[None, None,
+                                              Optional[mui.FlexBox]]]]]
     qualname_prefix: str
     # if type is None, it means they are defined in global scope.
     type: Type
     is_leaf: bool
     metas: List[ServFunctionMeta]
     # callback: Optional[Callable[[mui.FlexBox, ServFunctionMeta],
     #                             Coroutine[None, None, Optional[mui.FlexBox]]]]
 
 
 @dataclasses.dataclass
 class _WatchDogWatchEntry:
     obmetas: Dict[ObjectReloadManager.TypeUID, _LayoutObserveMeta]
     watch: Optional[ObservedWatch]
 
+
 class _FlowAppObserveContext:
+
     def __init__(self) -> None:
         self._removed_layouts: List[mui.FlexBox] = []
-        self._added_layouts_and_cbs: Dict[mui.FlexBox, Optional[Callable[[mui.FlexBox, ServFunctionMeta],
-                                    Coroutine]]] = {}
+        self._added_layouts_and_cbs: Dict[mui.FlexBox, Optional[Callable[
+            [mui.FlexBox, ServFunctionMeta], Coroutine]]] = {}
         self._reloaded_layout_pairs: List[Tuple[mui.FlexBox, mui.FlexBox]] = []
 
+
 _FLOWAPP_OBSERVE_CONTEXT: contextvars.ContextVar[
-    Optional[_FlowAppObserveContext]] = contextvars.ContextVar("_FLOWAPP_OBSERVE_CONTEXT", default=None)
+    Optional[_FlowAppObserveContext]] = contextvars.ContextVar(
+        "_FLOWAPP_OBSERVE_CONTEXT", default=None)
+
 
 def _get_flowapp_observe_context():
     return _FLOWAPP_OBSERVE_CONTEXT.get()
 
+
 @contextlib.contextmanager
 def _enter_flowapp_observe_context(ctx: _FlowAppObserveContext):
     token = _FLOWAPP_OBSERVE_CONTEXT.set(ctx)
     try:
         yield ctx
     finally:
         _FLOWAPP_OBSERVE_CONTEXT.reset(token)
@@ -328,16 +334,16 @@
     @property
     def _flow_reload_manager(self):
         return self._flow_app_comp_core.reload_mgr
 
     def add_file_resource(
         self, key: str,
         handler: Callable[..., Union[bytes, FileResource,
-                                    Coroutine[None, None,
-                                              Union[bytes, FileResource]]]]):
+                                     Coroutine[None, None,
+                                               Union[bytes, FileResource]]]]):
         self._flowapp_file_resource_handlers[key] = handler
 
     def remove_file_resource(
         self,
         key: str,
     ):
         if key in self._flowapp_file_resource_handlers:
@@ -395,15 +401,16 @@
                                 in_memory_limit: int = 1000):
         data_enc = pickle.dumps(data)
         assert self.__flowapp_master_meta.is_inside_devflow, "you must call this in devflow apps."
         if graph_id is None:
             graph_id = self.__flowapp_master_meta.graph_id
         if node_id is None:
             node_id = self.__flowapp_master_meta.node_id
-        meta = parse_obj_to_jsonlike(data, key, UniqueTreeIdForTree.from_parts([key]))
+        meta = parse_obj_to_jsonlike(data, key,
+                                     UniqueTreeIdForTree.from_parts([key]))
         in_memory_limit_bytes = in_memory_limit * 1024 * 1024
         meta.userdata = {
             "timestamp": time.time_ns(),
         }
         item = StorageDataItem(data_enc, meta)
         if len(data_enc) <= in_memory_limit_bytes:
             self.__flowapp_storage_cache[key] = item
@@ -618,16 +625,15 @@
                         "type": c._flow_comp_type.value,
                         "state": user_state,
                     }
         if reload:
             detached = self.root._detach()
             # make sure did_mount is called from root to leaf (breadth first order)
             detached_items = list(detached.items())
-            detached_items.sort(key=lambda x: len(x[0].split(".")),
-                                reverse=False)
+            detached_items.sort(key=lambda x: len(x[0].parts), reverse=False)
 
             await self.root._run_special_methods(
                 [], [x[1] for x in detached_items], self._flow_reload_manager)
             del detached
         root_uid = UniqueTreeId.from_parts([_ROOT])
 
         await self.root._clear()
@@ -692,20 +698,22 @@
         if reload:
             # comps = self.root._get_all_nested_childs()
             with _enter_app_conetxt(self):
                 for comp in uid_to_comp.values():
                     if comp._flow_uid_encoded in prev_comps:
                         if comp._flow_comp_type.value == prev_comps[
                                 comp._flow_uid_encoded]["type"]:
-                            comp.set_props(prev_comps[comp._flow_uid_encoded]["props"])
+                            comp.set_props(
+                                prev_comps[comp._flow_uid_encoded]["props"])
                     if comp._flow_uid_encoded in prev_user_states:
                         if comp._flow_comp_type.value == prev_user_states[
                                 comp._flow_uid_encoded]["type"]:
                             await comp.set_persist_props_async(
-                                prev_user_states[comp._flow_uid_encoded]["state"])
+                                prev_user_states[
+                                    comp._flow_uid_encoded]["state"])
             del prev_comps
             del prev_user_states
 
         if send_layout_ev:
             ev = AppEvent(
                 "", {
                     AppEventType.UpdateLayout:
@@ -730,31 +738,29 @@
     async def app_initialize_async(self):
         """override this to init app before server start
         """
         self._loop = asyncio.get_running_loop()
         uid_to_comp = self.root._get_uid_to_comp_dict()
         # make sure did_mount is called from leaf to root (reversed breadth first order)
         uid_to_comp_items = list(uid_to_comp.items())
-        uid_to_comp_items.sort(key=lambda x: len(x[0].parts),
-                               reverse=True)
+        uid_to_comp_items.sort(key=lambda x: len(x[0].parts), reverse=True)
         with enter_app_conetxt(self):
             for _, v in uid_to_comp_items:
                 special_methods = v.get_special_methods(
                     self._flow_reload_manager)
                 if special_methods.did_mount is not None:
                     await v.run_callback(
                         special_methods.did_mount.get_binded_fn(),
                         sync_status_first=False,
                         change_status=False)
                 for k, effects in v.effects._flow_effects.items():
                     for effect in effects:
-                        res = await v.run_callback(
-                            effect,
-                            sync_status_first=False,
-                            change_status=False)
+                        res = await v.run_callback(effect,
+                                                   sync_status_first=False,
+                                                   change_status=False)
                         if res is not None:
                             # res is effect
                             v.effects._flow_unmounted_effects[k].append(res)
 
     def app_terminate(self):
         """override this to init app after server stop
         """
@@ -950,24 +956,28 @@
                 collect_handlers = src_comp.get_event_handlers(
                     FrontendEventType.DragCollect.value)
                 comp = self.root._get_comp_by_uid(uid)
                 handlers = comp.get_event_handlers(data[0])
                 # print(src_uid, comp, src_comp, handler, collect_handler)
                 if handlers is not None and collect_handlers is not None:
                     src_event = Event(FrontendEventType.DragCollect.value,
-                                      src_data["data"], keys, indexes)
+                                    src_data["data"], keys, indexes)
                     cbs = []
                     for handler in handlers.handlers:
                         cb = partial(self.__handle_dnd_event,
-                                     handler=handler,
-                                     src_handler=collect_handlers.handlers[0],
-                                     src_event=src_event)
+                                    handler=handler,
+                                    src_handler=collect_handlers.handlers[0],
+                                    src_event=src_event)
                         cbs.append(cb)
                     comp._task = asyncio.create_task(
                         comp.run_callbacks(cbs, sync_status_first=False))
+                else:
+                    # drag object already contains drag data.
+                    res[uid_original] = await comp.handle_event(
+                        event, is_sync=is_sync)
             elif event.type == FrontendEventType.FileDrop.value:
                 # for file drop, we can't use regular drop above, so
                 # just convert it to drop event, no drag collect needed.
                 comps = self.root._get_comps_by_uid(uid)
                 ctxes = [
                     c._flow_event_context_creator() for c in comps
                     if c._flow_event_context_creator is not None
@@ -993,23 +1003,28 @@
                         event, is_sync=is_sync)
         if is_sync:
             return res
 
     async def _handle_event_with_ctx(self, ev: UIEvent, is_sync: bool = False):
         # TODO run control from other component
         with _enter_app_conetxt(self):
-            return await self.handle_event(ev, is_sync)
+            res = await self.handle_event(ev, is_sync)
+        return res 
+
+    async def run_vscode_event(self, data: VscodeTensorpcMessage):
+        return await self._flowapp_special_eemitter.emit_async(
+            AppSpecialEventType.VscodeTensorpcMessage, data)
 
     async def _run_autorun(self, cb: Callable):
         try:
             coro = cb()
             if inspect.iscoroutine(coro):
                 await coro
-            self._flowapp_special_eemitter.emit(AppSpecialEventType.AutoRunEnd,
-                                                None)
+            await self._flowapp_special_eemitter.emit_async(
+                AppSpecialEventType.AutoRunEnd, None)
         except:
             traceback.print_exc()
             if self._flowapp_enable_exception_inspect:
                 await self._inspect_exception()
 
     async def _inspect_exception(self):
         try:
@@ -1194,31 +1209,38 @@
             self._watchdog_observer = observer
         else:
             self._flowapp_code_mgr = None
         self._watchdog_ignore_next = False
         self._loop = asyncio.get_running_loop()
         self._watch_lock = threading.Lock()
 
-    def __observe_layout_effect(self, obj: mui.FlexBox, callback: Optional[Callable[[mui.FlexBox, ServFunctionMeta],
+    def __observe_layout_effect(
+        self,
+        obj: mui.FlexBox,
+        callback: Optional[Callable[[mui.FlexBox, ServFunctionMeta],
                                     Coroutine]] = None):
         self._flowapp_observe(obj, callback)
         return partial(self._flowapp_remove_observer, obj)
 
     def observe_layout(
         self,
         obj: mui.FlexBox,
         callback: Optional[Callable[[mui.FlexBox, ServFunctionMeta],
                                     Coroutine]] = None):
         if not obj.effects.has_effect_key(TENSORPC_FLOW_EFFECTS_OBSERVE):
             # already observed
-            obj.effects.use_effect(partial(self.__observe_layout_effect, obj, callback), key=TENSORPC_FLOW_EFFECTS_OBSERVE)
+            obj.effects.use_effect(partial(self.__observe_layout_effect, obj,
+                                           callback),
+                                   key=TENSORPC_FLOW_EFFECTS_OBSERVE)
             if obj.is_mounted():
                 self._flowapp_observe(obj, callback)
                 # TODO better code
-                obj.effects._flow_unmounted_effects[TENSORPC_FLOW_EFFECTS_OBSERVE].append(partial(self._flowapp_remove_observer, obj))
+                obj.effects._flow_unmounted_effects[
+                    TENSORPC_FLOW_EFFECTS_OBSERVE].append(
+                        partial(self._flowapp_remove_observer, obj))
 
     def _flowapp_observe(
         self,
         obj: mui.FlexBox,
         callback: Optional[Callable[[mui.FlexBox, ServFunctionMeta],
                                     Coroutine]] = None):
         ctx = _get_flowapp_observe_context()
@@ -1250,15 +1272,16 @@
 
         for meta_type_uid, meta_item in metas_dict.items():
             if meta_item.type is not None:
                 if meta_type_uid in obentry.obmetas:
                     obentry.obmetas[meta_type_uid].layouts[obj] = callback
                 else:
                     qualname_prefix = meta_type_uid[1]
-                    obmeta = _LayoutObserveMeta({obj: callback}, qualname_prefix,
+                    obmeta = _LayoutObserveMeta({obj: callback},
+                                                qualname_prefix,
                                                 meta_item.type,
                                                 meta_item.is_leaf,
                                                 meta_item.metas)
                     obentry.obmetas[meta_type_uid] = obmeta
 
     def _flowapp_remove_observer(self, obj: mui.FlexBox):
         ctx = _get_flowapp_observe_context()
@@ -1352,30 +1375,31 @@
             with self._flowapp_protect_app_observe_call(observe_ctx):
                 if resolved_path in self._flowapp_change_observers:
                     obmetas = self._flowapp_change_observers[
                         resolved_path].obmetas.copy()
                     obmetas_items = list(obmetas.items())
                     # sort obmetas_items by mro
                     obmetas_items.sort(key=lambda x: len(x[1].type.mro()),
-                                    reverse=True)
+                                       reverse=True)
                     # store accessed metas in inheritance tree
                     resolved_metas: Dict[ObjectReloadManager.TypeUID,
-                                        Set[str]] = {}
+                                         Set[str]] = {}
                     # print("len(obmetas)", resolved_path, len(obmetas))
                     for type_uid, obmeta in obmetas_items:
                         # get changed metas for special methods
                         # print(new, change)
                         if type_uid not in resolved_metas:
                             resolved_metas[type_uid] = set()
                         changed_metas: List[ServFunctionMeta] = []
                         for m in obmeta.metas:
                             if m.qualname in change:
                                 changed_metas.append(m)
                         new_method_names: List[str] = [
-                            x for x in new if x.startswith(obmeta.qualname_prefix)
+                            x for x in new
+                            if x.startswith(obmeta.qualname_prefix)
                             and x != obmeta.qualname_prefix
                         ]
                         # layout = obmeta.layout
                         for layout in obmeta.layouts:
                             if not is_callback_change:
                                 if isinstance(layout, App):
                                     callbacks_of_this_file = self.__get_callback_metas_in_file(
@@ -1390,44 +1414,48 @@
                                     if cb_meta.cb_qualname in change:
                                         is_callback_change = True
                                         break
                             # print("is_callback_change", is_callback_change)
                         # for m in changed_metas:
                         #     print(m.qualname, "CHANGED")
                         # do reload, run special methods
-                        flow_special_for_check = FlowSpecialMethods(changed_metas)
+                        flow_special_for_check = FlowSpecialMethods(
+                            changed_metas)
                         do_reload = flow_special_for_check.contains_special_method(
                         ) or bool(new_method_names)
                         # print("do_reload", do_reload)
                         if not do_reload:
                             continue
                         # rprint(obmeta.layouts)
-                        for i, (layout, layout_reload_cb) in enumerate(obmeta.layouts.items()):
+                        for i, (layout, layout_reload_cb) in enumerate(
+                                obmeta.layouts.items()):
                             changed_user_obj = None
 
                             if layout is self:
                                 # reload app
                                 if changed_metas or bool(new_method_names):
                                     # reload app servunit and method
-                                    changed_user_obj = self._get_user_app_object()
+                                    changed_user_obj = self._get_user_app_object(
+                                    )
                                     # self._get_app_dynamic_cls(
                                     # ).reload_obj_methods(user_obj, {}, self._flow_reload_manager)
                                     self._get_app_service_unit().reload_metas(
                                         self._flow_reload_manager)
                             else:
-                                assert isinstance(layout,
-                                                mui.FlexBox), f"{type(layout)}"
+                                assert isinstance(
+                                    layout, mui.FlexBox), f"{type(layout)}"
                                 # if self.code_editor.external_path is not None and new_code is None:
                                 #     if str(
                                 #             Path(self.code_editor.external_path).
                                 #             resolve()) == resolved_path:
                                 #         await self.set_editor_value(new_data, lineno=0)
                                 # reload dynamic layout
                                 if changed_metas or bool(new_method_names):
-                                    changed_user_obj = layout._get_user_object()
+                                    changed_user_obj = layout._get_user_object(
+                                    )
                             # print("RTX", changed_user_obj, new_method_names)
                             if changed_user_obj is not None:
                                 # reload_res = self._flow_reload_manager.reload_type(
                                 #     type(changed_user_obj))
                                 reload_res = self._flow_reload_manager.reload_type(
                                     obmeta.type)
 
@@ -1482,15 +1510,16 @@
                                 if layout is self:
                                     self._get_app_dynamic_cls(
                                     ).module_dict = reload_res.module_entry.module_dict
                                     self._get_app_service_unit().reload_metas(
                                         self._flow_reload_manager)
                             # use updated metas to run special methods such as create_layout and auto_run
                             if changed_metas:
-                                flow_special = FlowSpecialMethods(changed_metas)
+                                flow_special = FlowSpecialMethods(
+                                    changed_metas)
                                 with _enter_app_conetxt(self):
                                     if flow_special.create_layout:
                                         fn = flow_special.create_layout.get_binded_fn(
                                         )
                                         if isinstance(layout, App):
                                             await self._app_run_layout_function(
                                                 True,
@@ -1500,27 +1529,27 @@
                                         else:
                                             if layout_reload_cb is not None:
                                                 # handle layout in callback
                                                 new_layout = await layout_reload_cb(
                                                     layout,
                                                     flow_special.create_layout)
                                                 # if new_layout is not None:
-                                                    # obmeta.layouts[i] = new_layout
-                                                    # observe_ctx._reloaded_layout_pairs.append((layout, new_layout))
+                                                # obmeta.layouts[i] = new_layout
+                                                # observe_ctx._reloaded_layout_pairs.append((layout, new_layout))
                                             # dynamic layout
                                     if flow_special.create_preview_layout:
                                         if not isinstance(layout, App):
                                             if layout_reload_cb is not None:
                                                 # handle layout in callback
                                                 new_layout = await layout_reload_cb(
                                                     layout, flow_special.
                                                     create_preview_layout)
                                                 # if new_layout is not None:
-                                                    # obmeta.layouts[i] = new_layout
-                                                    # observe_ctx._reloaded_layout_pairs.append((layout, new_layout))
+                                                # obmeta.layouts[i] = new_layout
+                                                # observe_ctx._reloaded_layout_pairs.append((layout, new_layout))
                                     for auto_run in flow_special.auto_runs:
                                         if auto_run is not None:
                                             await self._run_autorun(
                                                 auto_run.get_binded_fn())
                             # handle special methods
                 ob_registry = self.get_observed_func_registry()
                 observed_func_changed = ob_registry.observed_func_changed(
@@ -1539,25 +1568,32 @@
                     with _enter_app_conetxt(self):
                         for qname in observed_func_changed:
                             entry = ob_registry[qname]
                             if len(entry.autorun_block_symbol) == 0:
                                 if entry.autorun_when_changed:
                                     await self._run_autorun(entry.current_func)
                             else:
-                                if entry.first_changed_block_idx < len(entry.body_code_blocks):
+                                if entry.first_changed_block_idx < len(
+                                        entry.body_code_blocks):
                                     first_changed_block_idx = entry.first_changed_block_idx
                                     if not entry.autorun_locals:
                                         first_changed_block_idx = 0
-                                    code_to_run = "\n".join(entry.body_code_blocks[first_changed_block_idx:])
-                                    code_to_run = remove_common_indent_from_code(code_to_run)
+                                    code_to_run = "\n".join(
+                                        entry.body_code_blocks[
+                                            first_changed_block_idx:])
+                                    code_to_run = remove_common_indent_from_code(
+                                        code_to_run)
                                     code_locals = entry.autorun_locals
                                     func_globals = entry.current_func.__globals__
-                                    code_comp = compile(code_to_run, f"<{TENSORPC_FILE_NAME_PREFIX}-scripts-{entry.qualname}>", "exec")
+                                    code_comp = compile(
+                                        code_to_run,
+                                        f"<{TENSORPC_FILE_NAME_PREFIX}-scripts-{entry.qualname}>",
+                                        "exec")
                                     exec(code_comp, func_globals, code_locals)
-                        self._flowapp_special_eemitter.emit(
+                        await self._flowapp_special_eemitter.emit_async(
                             AppSpecialEventType.ObservedFunctionChange,
                             observed_func_changed)
                 # print(is_callback_change, is_reload)
                 if is_callback_change or is_reload:
                     # reset all callbacks in this file
                     if callbacks_of_this_file is None:
                         callbacks_of_this_file = self.__get_callback_metas_in_file(
@@ -1586,15 +1622,15 @@
             #     observe_ctx._removed_layouts.remove(pair[0])
             #     observe_ctx._added_layouts_and_cbs.pop(pair[1])
             # print(len(observe_ctx._removed_layouts), len(observe_ctx._added_layouts_and_cbs))
             for layout in observe_ctx._removed_layouts:
                 self._flowapp_remove_observer(layout)
             for layout, cb in observe_ctx._added_layouts_and_cbs.items():
                 self._flowapp_observe(layout, cb)
-            return 
+            return
 
     def _watchdog_on_modified(self, ev: _WATCHDOG_MODIFY_EVENT_TYPES):
         # which event trigger reload?
         # 1. special method code change
         # 2. callback code change (handled outsite)
         # 3. new methods detected in layout
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/appcore.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/appcore.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 import dataclasses
 import enum
 import inspect
 import traceback
 from functools import partial
 from pathlib import Path
 from typing import (TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable,
-                    Coroutine, Dict, Generic, Iterable, List, Optional, Set, Tuple,
-                    Type, TypeVar, Union)
+                    Coroutine, Dict, Generic, Iterable, List, Optional, Set,
+                    Tuple, Type, TypeVar, Union)
 
 from typing_extensions import (Concatenate, Literal, ParamSpec, Protocol, Self,
                                TypeAlias)
 
 from tensorpc.core.moduleid import (get_qualname_of_type, is_lambda,
                                     is_valid_function)
 from tensorpc.core.tree_id import UniqueTreeId
@@ -33,38 +33,38 @@
 NumberType: TypeAlias = Union[int, float]
 
 SimpleEventType: TypeAlias = Tuple[EventDataType, Any]
 T = TypeVar("T")
 
 T_comp = TypeVar("T_comp")
 
+
 @dataclasses.dataclass
 class Event:
     type: EventDataType
     data: Any
     # only used for template component such as table.
     # key indicates the id of template item.
     keys: Union[Undefined, List[str]] = undefined
     # for template control components.
     indexes: Union[Undefined, List[int]] = undefined
 
 
 class EventHandler:
-    def __init__(self,
-                 cb: Callable,
-                 simple_event: bool = True) -> None:
+
+    def __init__(self, cb: Callable, simple_event: bool = True) -> None:
         self.cb = cb
         self.simple_event = simple_event
 
     def run_event(self, event: Event) -> CORO_ANY:
         if self.simple_event:
             return self.cb(event.data)
         else:
             return self.cb(event)
-        
+
     async def run_event_async(self, event: Event):
         if self.simple_event:
             coro = self.cb(event.data)
         else:
             coro = self.cb(event)
         if inspect.iscoroutine(coro):
             res = await coro
@@ -74,15 +74,17 @@
 
     def run_noarg_event(self, event: Event) -> CORO_ANY:
         if self.simple_event:
             return self.cb()
         else:
             return self.cb(event)
 
+
 class EventHandlers:
+
     def __init__(self,
                  handlers: List[EventHandler],
                  stop_propagation: bool = False,
                  throttle: Optional[NumberType] = None,
                  debounce: Optional[NumberType] = None,
                  backend_only: bool = False,
                  simple_event: bool = True) -> None:
@@ -101,97 +103,143 @@
         if self.debounce is not None:
             res["debounce"] = self.debounce
         if self.throttle is not None:
             res["throttle"] = self.throttle
         return res
 
     def get_bind_event_handlers(self, event: Event):
-        return [partial(handler.run_event, event=event) for handler in self.handlers]
-    
+        return [
+            partial(handler.run_event, event=event)
+            for handler in self.handlers
+        ]
+
     def get_bind_event_handlers_noarg(self, event: Event):
-        return [partial(handler.run_noarg_event, event=event) for handler in self.handlers]
+        return [
+            partial(handler.run_noarg_event, event=event)
+            for handler in self.handlers
+        ]
 
     def remove_handler(self, handler: Callable):
         self.handlers = [h for h in self.handlers if h.cb != handler]
         return
 
+
 class AppContext:
 
     def __init__(self, app: "App") -> None:
         self.app = app
 
     def is_editable_app(self):
         return self.app._is_editable_app()
 
+class EventHandlingContext:
+
+    def __init__(self, uid: UniqueTreeId) -> None:
+        self.comp_uid = uid
+        self.delayed_callbacks: List[Callable[[], CORO_ANY]] = []
+
 
 APP_CONTEXT_VAR: contextvars.ContextVar[
     Optional[AppContext]] = contextvars.ContextVar("flowapp_context",
                                                    default=None)
 
+EVENT_HANDLING_CONTEXT_VAR: contextvars.ContextVar[
+    Optional[EventHandlingContext]] = contextvars.ContextVar("flowapp_event_context",
+                                                   default=None)
+
 
 def get_app_context() -> Optional[AppContext]:
     return APP_CONTEXT_VAR.get()
 
+def get_event_handling_context() -> Optional[EventHandlingContext]:
+    return EVENT_HANDLING_CONTEXT_VAR.get()
+
 def is_inside_app():
     return is_inside_app_session() and get_app_context() is not None
 
+
 def get_editable_app() -> "EditableApp":
     ctx = get_app_context()
     assert ctx is not None and ctx.is_editable_app()
-    return ctx.app # type: ignore
+    return ctx.app  # type: ignore
+
 
 def get_app() -> "App":
     ctx = get_app_context()
     assert ctx is not None
     return ctx.app
 
+
 @contextlib.contextmanager
 def enter_app_conetxt(app: "App"):
     ctx = AppContext(app)
     token = APP_CONTEXT_VAR.set(ctx)
     try:
         yield ctx
     finally:
         APP_CONTEXT_VAR.reset(token)
 
+@contextlib.contextmanager
+def enter_event_handling_conetxt(uid: UniqueTreeId):
+    ctx = EventHandlingContext(uid)
+    token = EVENT_HANDLING_CONTEXT_VAR.set(ctx)
+    try:
+        yield ctx
+    finally:
+        EVENT_HANDLING_CONTEXT_VAR.reset(token)
 
 def get_app_storage():
     ctx = get_app_context()
     assert ctx is not None
     return ctx.app.get_persist_storage()
 
+def enqueue_delayed_callback(cb: Callable[[], CORO_ANY]):
+    ctx = get_event_handling_context()
+    assert ctx is not None
+    print("EVCTX", id(ctx))
+    ctx.delayed_callbacks.append(cb)
 
-def find_component(type: Type[T_comp], validator: Optional[Callable[[T_comp], bool]] = None) -> Optional[T_comp]:
+def find_component(
+        type: Type[T_comp],
+        validator: Optional[Callable[[T_comp],
+                                     bool]] = None) -> Optional[T_comp]:
     appctx = get_app_context()
     assert appctx is not None, "you must use this function in app"
     return appctx.app.find_component(type, validator)
 
-def find_all_components(type: Type[T_comp], check_nested: bool = False, 
-                        validator: Optional[Callable[[T_comp], bool]] = None) -> List[T_comp]:
+
+def find_all_components(
+        type: Type[T_comp],
+        check_nested: bool = False,
+        validator: Optional[Callable[[T_comp], bool]] = None) -> List[T_comp]:
     appctx = get_app_context()
     assert appctx is not None, "you must use this function in app"
     return appctx.app.find_all_components(type, check_nested, validator)
 
+
 def find_component_by_uid(uid: str) -> Optional["Component"]:
     appctx = get_app_context()
     assert appctx is not None, "you must use this function in app"
     try:
         return appctx.app.root._get_comp_by_uid(uid)
     except KeyError:
-        return None 
-    
-def find_component_by_uid_with_type_check(uid: str, type: Type[T_comp]) -> Optional[T_comp]:
+        return None
+
+
+def find_component_by_uid_with_type_check(
+        uid: str, type: Type[T_comp]) -> Optional[T_comp]:
     appctx = get_app_context()
     assert appctx is not None, "you must use this function in app"
     try:
         res = appctx.app.root._get_comp_by_uid(uid)
         assert isinstance(res, type)
         return res
     except KeyError:
-        return None 
+        return None
+
 
 def get_reload_manager():
     appctx = get_app_context()
     assert appctx is not None, "you must use this function in app"
     return appctx.app._flow_reload_manager
 
 
@@ -207,14 +255,16 @@
 
     CodeEditorSave = "CodeEditorSave"
     WatchDogChange = "WatchDogChange"
 
     ObservedFunctionChange = "ObservedFunctionChange"
     # emitted when layout update is sent to frontend.
     LayoutChange = "LayoutChange"
+    VscodeTensorpcMessage = "VscodeTensorpcMessage"
+
 
 @dataclasses.dataclass
 class _CompReloadMeta:
     uid: str
     handler: EventHandler
     cb_file: str
     cb_real: Callable
@@ -242,35 +292,44 @@
 
                 if not is_valid_func or is_lambda(cb):
                     continue
                 cb_file = str(Path(inspect.getfile(cb_real)).resolve())
                 if cb_file != path_resolve:
                     continue
                 # code, _ = inspect.getsourcelines(cb_real)
-                metas.append(_CompReloadMeta(k, handler, cb_file, cb_real, cb_real.__qualname__))
+                metas.append(
+                    _CompReloadMeta(k, handler, cb_file, cb_real,
+                                    cb_real.__qualname__))
     return metas
 
 
 async def _run_zeroarg_func(cb: Callable):
     try:
         coro = cb()
         if inspect.iscoroutine(coro):
             await coro
     except:
         traceback.print_exc()
 
 
-class AppObservedFunctionRegistry(ObservedFunctionRegistry):    
+class AppObservedFunctionRegistry(ObservedFunctionRegistry):
+
     def is_enabled(self):
         return not self.is_frozen and is_inside_app_session()
-    
-ALL_OBSERVED_FUNCTIONS: ObservedFunctionRegistryProtocol = AppObservedFunctionRegistry()
+
+
+ALL_OBSERVED_FUNCTIONS: ObservedFunctionRegistryProtocol = AppObservedFunctionRegistry(
+)
+
 
 def observe_function(func: Callable):
     return ALL_OBSERVED_FUNCTIONS.register(func)
 
 
 def observe_autorun_function(func: Callable):
     return ALL_OBSERVED_FUNCTIONS.register(func, autorun_when_changed=True)
 
+
 def observe_autorun_script(func: Callable):
-    return ALL_OBSERVED_FUNCTIONS.register(func, autorun_when_changed=True, autorun_block_symbol=r"#%%")
+    return ALL_OBSERVED_FUNCTIONS.register(func,
+                                           autorun_when_changed=True,
+                                           autorun_block_symbol=r"#%%")
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/canvas.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/canvas.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,14 @@
-
-
 import asyncio
 import contextlib
 from functools import partial
 import inspect
-from typing import (TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable, Coroutine, Dict,
-                    Iterable, List, Optional, Set, Tuple, Type, TypeVar, Union)
+from typing import (TYPE_CHECKING, Any, AsyncGenerator, Awaitable, Callable,
+                    Coroutine, Dict, Iterable, List, Optional, Set, Tuple,
+                    Type, TypeVar, Union)
 
 from typing_extensions import ParamSpec
 
 from tensorpc.flow.flowapp.appcore import (enter_app_conetxt, find_component,
                                            get_app)
 from tensorpc.utils.uniquename import UniqueNamePool
 if TYPE_CHECKING:
@@ -32,34 +31,47 @@
     """
     if key is not None:
         comp = find_component(plus.SimpleCanvas, lambda x: x.key == key)
     else:
         comp = find_component(plus.SimpleCanvas)
     return comp
 
-async def unknown_visualization(obj: Any, tree_id: str, key: Optional[str] = None):
-    return await get_simple_canvas(key)._unknown_visualization(tree_id, obj, ignore_registry=False)
-
-async def unknown_visualization_no_registry(obj: Any, tree_id: str, key: Optional[str] = None):
-    return await get_simple_canvas(key)._unknown_visualization(tree_id, obj, ignore_registry=True)
 
-async def unknown_visualization_temp_objs(*objs, vis_root_id: str = "", canvas_key: Optional[str] = None, **kwobjs):
+async def unknown_visualization(obj: Any,
+                                tree_id: str,
+                                key: Optional[str] = None):
+    return await get_simple_canvas(key)._unknown_visualization(
+        tree_id, obj, ignore_registry=False)
+
+
+async def unknown_visualization_no_registry(obj: Any,
+                                            tree_id: str,
+                                            key: Optional[str] = None):
+    return await get_simple_canvas(key)._unknown_visualization(
+        tree_id, obj, ignore_registry=True)
+
+
+async def unknown_visualization_temp_objs(*objs,
+                                          vis_root_id: str = "",
+                                          canvas_key: Optional[str] = None,
+                                          **kwobjs):
     pool = UniqueNamePool()
     canvas = get_simple_canvas(canvas_key)
     for i, o in enumerate(objs):
         uid = pool(str(i))
         if vis_root_id != "":
             uid = f"{vis_root_id}.{uid}"
         await canvas._unknown_visualization(uid, o)
     for k, o in kwobjs.items():
         uid = pool(k)
         if vis_root_id != "":
             uid = f"{vis_root_id}.{uid}"
         await canvas._unknown_visualization(uid, o)
 
+
 def get_canvas(key: Optional[str] = None) -> "ComplexCanvas":
     from tensorpc.flow.flowapp.components import plus
     if key is not None:
         comp = find_component(plus.ComplexCanvas, lambda x: x.key == key)
     else:
         comp = find_component(plus.ComplexCanvas)
     assert comp is not None, "you must add simple canvas to your UI"
@@ -71,9 +83,7 @@
     """for conditional visualization
     """
     if key is not None:
         comp = find_component(plus.ComplexCanvas, lambda x: x.key == key)
     else:
         comp = find_component(plus.ComplexCanvas)
     return comp
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/core.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,19 +22,18 @@
 from typing_extensions import ParamSpec
 
 from tensorpc.core.serviceunit import ObservedFunctionRegistryProtocol
 from tensorpc.flow.flowapp.appcore import (enter_app_conetxt, find_component,
                                            find_component_by_uid, get_app,
                                            get_app_context, get_editable_app,
                                            get_reload_manager, is_inside_app,
-                                           observe_function)
+                                           observe_function, enqueue_delayed_callback)
 from tensorpc.flow.flowapp.components import plus
 from tensorpc.flow.flowapp.components.plus.objinspect.controllers import ThreadLocker
 
-
 P = ParamSpec('P')
 
 T = TypeVar('T')
 
 
 def thread_locker_wait_sync(*, _frame_cnt: int = 2):
     comp = find_component(ThreadLocker)
@@ -82,22 +81,25 @@
 
 
 async def list_all_data_storage_nodes(
         graph_id: Optional[str] = None) -> List[str]:
     app = get_app()
     return await app.list_all_data_storage_nodes(graph_id)
 
+
 def set_app_z_index(z_index: int):
     app = get_app()
-    app._dialog_z_index = z_index 
+    app._dialog_z_index = z_index
+
 
 def set_observed_func_registry(registry: ObservedFunctionRegistryProtocol):
     app = get_app()
     return app.set_observed_func_registry(registry)
 
+
 def run_with_exception_inspect(func: Callable[P, T], *args: P.args,
                                **kwargs: P.kwargs) -> T:
     """WARNING: we shouldn't run this function in run_in_executor.
     """
     comp = find_component(plus.ObjectInspector)
     assert comp is not None, "you must add inspector to your UI to use exception inspect"
     return comp.run_with_exception_inspect(func, *args, **kwargs)
@@ -112,34 +114,35 @@
 
 def _run_func_with_app(app, func: Callable[P, T], *args: P.args,
                        **kwargs: P.kwargs) -> T:
     with enter_app_conetxt(app):
         return func(*args, **kwargs)
 
 
-async def run_in_executor_with_exception_inspect(func: Callable[P, T],
-                                                 *args: P.args,
+async def run_in_executor_with_exception_inspect(func: Callable[P, T], *args:
+                                                 P.args,
                                                  **kwargs: P.kwargs) -> T:
     """run a sync function in executor with exception inspect.
     """
     comp = find_component(plus.ObjectInspector)
     if comp is None:
         return await asyncio.get_running_loop().run_in_executor(
-            None, _run_func_with_app, get_app(), func, *args, **kwargs)  # type: ignore
+            None, _run_func_with_app, get_app(), func, *args,
+            **kwargs)  # type: ignore
     assert comp is not None, "you must add inspector to your UI to use exception inspect"
     return await comp.run_in_executor_with_exception_inspect(
         _run_func_with_app, get_app(), func, *args, **kwargs)
 
+
 def run_coro_sync(coro: Coroutine) -> Any:
     loop = get_app()._loop
     assert loop is not None
     if get_app()._flowapp_thread_id == threading.get_ident():
         # we can't wait fut here
         task = asyncio.create_task(coro)
         # we can't wait fut here
         return task
         # return fut
     else:
         # we can wait fut here.
-        fut = asyncio.run_coroutine_threadsafe(
-            coro, loop)
+        fut = asyncio.run_coroutine_threadsafe(coro, loop)
         return fut.result()
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/appctx/inspector.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/appctx/inspector.py`

 * *Files 2% similar despite different names*

```diff
@@ -167,28 +167,30 @@
 
 async def read_item(uid: str):
     app = get_app()
     comp = app.find_component(plus.ObjectInspector)
     assert comp is not None, "you must add inspector to your UI to use exception inspect"
     return await comp.get_object_by_uid(uid)
 
+
 def has_object(key: str):
     comp = find_component(plus.ObjectInspector)
     if comp is None:
         return False
     assert comp is not None, "you must add inspector to your UI"
     return comp.tree.has_object(key)
 
+
 def set_custom_layout_sync(layout: mui.FlexBox):
     comp = find_component(plus.ObjectInspector)
     if comp is None:
         return
     assert comp is not None, "you must add inspector to your UI"
-    return comp.set_custom_layout_sync(loop=get_app()._loop,
-                                    layout=layout)
+    return comp.set_custom_layout_sync(loop=get_app()._loop, layout=layout)
+
 
 async def set_custom_layout(layout: mui.FlexBox):
     comp = find_component(plus.ObjectInspector)
     if comp is None:
         return
     assert comp is not None, "you must add inspector to your UI"
     return await comp.set_custom_layout(layout=layout)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/colors.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/colors.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import re
 from typing import Dict, Tuple
-import numpy as np 
+import numpy as np
 # https://developer.mozilla.org/en-US/docs/Web/CSS/color_value
 
 CSS_COLORS_TO_HEX = {
     "aliceblue": "#f0f8ff",
     "antiquewhite": "#faebd7",
     "aqua": "#00ffff",
     "aquamarine": "#7fffd4",
@@ -177,15 +177,14 @@
     "purple",
     "slateblue",
     "green",
     "saddlebrown",
     "yellow",
     "black",
     "grey",
-
 ]
 
 CSS_COLORS = list(CSS_COLORS_TO_HEX.keys())
 
 HEX_COLOR_PATTERN = re.compile(r"#(?:[0-9a-fA-F]{3}){1,2}")
 
 
@@ -213,12 +212,13 @@
 def str_to_rgb_float(color: str):
     r, g, b = CSS_COLORS_TO_RGB[color]
     return (r / 255, g / 255, b / 255)
 
 
 def colors_for_label_array(label_array: np.ndarray):
     unique_labels, inverse = np.unique(label_array, return_inverse=True)
-    color_rgbs = np.array([str_to_rgb_float(color) for color in RANDOM_COLORS_FOR_UI])
+    color_rgbs = np.array(
+        [str_to_rgb_float(color) for color in RANDOM_COLORS_FOR_UI])
 
     color_for_each_unique_label = color_rgbs[unique_labels % len(color_rgbs)]
     label_color = color_for_each_unique_label[inverse]
-    return label_color
+    return label_color
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/common.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/common.py`

 * *Files 4% similar despite different names*

```diff
@@ -30,15 +30,14 @@
     FrontendEventType.TreeItemExpandChange.value,
     FrontendEventType.TreeItemToggle.value,
     FrontendEventType.TreeLazyExpand.value,
     FrontendEventType.TreeItemFocus.value,
     FrontendEventType.TreeItemButton.value,
     FrontendEventType.ContextMenuSelect.value,
     FrontendEventType.TreeItemRename.value,
-
 ])
 
 _ONEARG_COMPLEXL_EVENTS = set([
     FrontendEventType.ComplexLayoutCloseTab.value,
     FrontendEventType.ComplexLayoutSelectTab.value,
     FrontendEventType.ComplexLayoutTabReload.value,
     FrontendEventType.ComplexLayoutSelectTabSet.value,
@@ -49,39 +48,45 @@
     FrontendEventType.EditorSave.value,
     FrontendEventType.EditorSaveState.value,
 ])
 
 _ONEARG_SPECIAL_EVENTS = set([
     FrontendEventType.Drop.value,
     FrontendEventType.SelectNewItem.value,
-
+    FrontendEventType.FlowSelectionChange.value,
+    FrontendEventType.FlowNodesInitialized.value,
+    FrontendEventType.FlowNodeDelete.value,
+    FrontendEventType.FlowEdgeConnection.value,
+    FrontendEventType.FlowEdgeDelete.value,
 ])
+
 _ONEARG_DATAGRID_EVENTS = set([
     FrontendEventType.DataGridRowSelection.value,
     FrontendEventType.DataGridFetchDetail.value,
     FrontendEventType.DataGridRowRangeChanged.value,
     FrontendEventType.DataGridProxyLazyLoadRange.value,
-
 ])
 
 _ONEARG_EVENTS = set(
-    ALL_POINTER_EVENTS) | _ONEARG_TREE_EVENTS | _ONEARG_COMPLEXL_EVENTS | _ONEARG_SPECIAL_EVENTS | _ONEARG_EDITOR_EVENTS
+    ALL_POINTER_EVENTS
+) | _ONEARG_TREE_EVENTS | _ONEARG_COMPLEXL_EVENTS | _ONEARG_SPECIAL_EVENTS | _ONEARG_EDITOR_EVENTS
 _ONEARG_EVENTS = _ONEARG_EVENTS | _ONEARG_DATAGRID_EVENTS
 
 _NOARG_EVENTS = set([
     FrontendEventType.Click.value,
     FrontendEventType.EditorReady.value,
     FrontendEventType.DoubleClick.value,
     FrontendEventType.EditorQueryState.value,
     FrontendEventType.Delete.value,
-
 ])
 
 
-async def handle_raw_event(event: Event, comp: Component, just_run: bool = False):
+async def handle_raw_event(event: Event,
+                           comp: Component,
+                           just_run: bool = False):
     # ev: [type, data]
     type = event.type
     data = event.data
     handlers = comp.get_event_handlers(event.type)
     if handlers is None:
         return
     if comp.props.status == UIRunStatus.Running.value:
@@ -109,16 +114,16 @@
     # print("WTF", event.type, event.data, comp.props.status)
     if comp.props.status == UIRunStatus.Running.value:
         # msg = create_ignore_usr_msg(comp)
         # await comp.send_and_wait(msg)
         return
     elif comp.props.status == UIRunStatus.Stop.value:
         if not isinstance(event.keys, Undefined):
-            # for all template components, we must disable 
-            # status change and sync. status indicator 
+            # for all template components, we must disable
+            # status change and sync. status indicator
             # in Button and IconButton will be disabled.
             sync_status_first = False
             change_status = False
         # print("WTF2x", event.type, event.data)
 
         if event.type in _STATE_CHANGE_EVENTS:
             # print("WTF2", event.type, event.data)
@@ -128,46 +133,58 @@
             # for template components, we don't need to sync state.
             if isinstance(event.keys, Undefined):
                 comp.state_change_callback(event.data, event.type)
                 sync_state = True
             if handlers is not None:
                 # state change events must sync state after callback
                 if is_sync:
-                    return await comp.run_callbacks(handlers.get_bind_event_handlers(event),
-                                      sync_state, 
-                                      sync_status_first=False, change_status=change_status)
+                    return await comp.run_callbacks(
+                        handlers.get_bind_event_handlers(event),
+                        sync_state,
+                        sync_status_first=False,
+                        change_status=change_status)
                 else:
                     comp._task = asyncio.create_task(
-                        comp.run_callbacks(handlers.get_bind_event_handlers(event),
-                                        sync_state,
-                                        sync_status_first=sync_status_first, change_status=change_status))
+                        comp.run_callbacks(
+                            handlers.get_bind_event_handlers(event),
+                            sync_state,
+                            sync_status_first=sync_status_first,
+                            change_status=change_status))
             else:
                 # all controlled component must sync state after state change
                 if sync_state_after_change:
                     await comp.sync_status(sync_state)
         elif event.type in _NOARG_EVENTS:
             handlers = comp.get_event_handlers(event.type)
             # other events don't need to sync state
             if handlers is not None:
                 run_funcs = handlers.get_bind_event_handlers_noarg(event)
                 if is_sync:
-                    return await comp.run_callbacks(run_funcs, sync_status_first=False)
+                    return await comp.run_callbacks(run_funcs,
+                                                    sync_status_first=False)
                 else:
                     comp._task = asyncio.create_task(
-                        comp.run_callbacks(run_funcs, sync_status_first=sync_status_first, change_status=change_status))
+                        comp.run_callbacks(run_funcs,
+                                           sync_status_first=sync_status_first,
+                                           change_status=change_status))
         elif event.type in _ONEARG_EVENTS:
             handlers = comp.get_event_handlers(event.type)
             # other events don't need to sync state
             if handlers is not None:
                 run_funcs = handlers.get_bind_event_handlers(event)
                 if is_sync:
-                    return await comp.run_callbacks(run_funcs, sync_status_first=False, change_status=change_status)
+                    return await comp.run_callbacks(
+                        run_funcs,
+                        sync_status_first=False,
+                        change_status=change_status)
                 else:
                     comp._task = asyncio.create_task(
-                        comp.run_callbacks(run_funcs, sync_status_first=sync_status_first, change_status=change_status))
+                        comp.run_callbacks(run_funcs,
+                                           sync_status_first=sync_status_first,
+                                           change_status=change_status))
 
         else:
             raise NotImplementedError
 
 
 # async def handle_change_event_no_arg(comp: Component, sync_status_first: bool = False):
 #     if comp.props.status == UIRunStatus.Running.value:
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/core.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-
-
 from typing import Any, Optional
 from tensorpc.flow.jsonlike import TensorType
 from tensorpc.core.moduleid import get_qualname_of_type
-import numpy as np 
+import numpy as np
+
 
 def _try_cast_tensor_dtype(obj: Any) -> Optional[np.dtype]:
     try:
         if isinstance(obj, np.ndarray):
             return obj.dtype
         elif get_qualname_of_type(type(obj)) == TensorType.TVTensor.value:
             from cumm.dtypes import get_npdtype_from_tvdtype
@@ -24,53 +23,58 @@
                 torch.int16: np.dtype(np.int16),
                 torch.uint8: np.dtype(np.uint8),
             }
             return _TORCH_DTYPE_TO_NP[obj.dtype]
     except:
         return None
 
+
 def _get_tensor_type(obj):
     if isinstance(obj, np.ndarray):
         return TensorType.NpArray
     elif get_qualname_of_type(type(obj)) == TensorType.TVTensor.value:
         return TensorType.TVTensor
     elif get_qualname_of_type(type(obj)) == TensorType.TorchTensor.value:
         return TensorType.TorchTensor
     else:
         return TensorType.Unknown
 
+
 def _cast_tensor_to_np(obj: Any) -> Optional[np.ndarray]:
     if isinstance(obj, np.ndarray):
         return obj
     elif get_qualname_of_type(type(obj)) == TensorType.TVTensor.value:
         if obj.device == 0:
             return obj.cpu().numpy()
         return obj.numpy()
 
     elif get_qualname_of_type(type(obj)) == TensorType.TorchTensor.value:
         if obj.is_cuda:
             return obj.detach().cpu().numpy()
         return obj.numpy()
     return None
 
+
 class TensorContainer:
+
     def __init__(self, obj: Any, type: TensorType, dtype: np.dtype) -> None:
         self.type = type
         self.dtype = dtype
         self._obj = obj
 
     def numpy(self):
         res = _cast_tensor_to_np(self._obj)
-        assert res is not None 
-        return res 
-    
-    @property 
+        assert res is not None
+        return res
+
+    @property
     def shape(self):
         return list(self._obj.shape)
 
+
 def get_tensor_container(obj) -> Optional[TensorContainer]:
     type = _get_tensor_type(obj)
     if type == TensorType.Unknown:
-        return None 
+        return None
     dtype = _try_cast_tensor_dtype(obj)
-    assert dtype is not None 
-    return TensorContainer(obj, type, dtype)
+    assert dtype is not None
+    return TensorContainer(obj, type, dtype)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/leaflet.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/leaflet.py`

 * *Files 1% similar despite different names*

```diff
@@ -101,16 +101,18 @@
         return res
 
 
 @dataclasses.dataclass
 class MapContainerProps(ContainerBaseProps, FlexBoxProps):
     pass
 
-MapLayoutType = Union[List[MapComponentType], Dict[str,
-                                    MapComponentType]]
+
+MapLayoutType = Union[List[MapComponentType], Dict[str, MapComponentType]]
+
+
 class MapContainer(MUIContainerBase[MapContainerProps, MapComponentType]):
 
     def __init__(self,
                  center: Tuple[NumberType, NumberType],
                  zoom: NumberType,
                  children: MapLayoutType,
                  inited: bool = False) -> None:
@@ -162,14 +164,15 @@
     url: str = ""
 
 
 class TileLayer(MapComponentBase[TileLayerProps]):
     """see https://leaflet-extras.github.io/leaflet-providers/preview/
     for all leaflet providers.
     """
+
     def __init__(
         self,
         url: str = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png",
         attribution:
         str = r"&copy; <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors"
     ) -> None:
         super().__init__(UIType.LeafletTileLayer, TileLayerProps)
@@ -190,24 +193,24 @@
 @dataclasses.dataclass
 class PathOptions:
     stroke: Union[Undefined, bool] = undefined
     color: Union[Undefined, str] = undefined
     weight: Union[Undefined, NumberType] = undefined
     opacity: Union[Undefined, NumberType] = undefined
     lineCap: Union[Undefined, Literal['butt', 'round', 'square',
-                                       'inherit']] = undefined
+                                      'inherit']] = undefined
     lineJoin: Union[Undefined, Literal['miter', 'round', 'bevel',
-                                        'inherit']] = undefined
+                                       'inherit']] = undefined
     dashArray: Union[Undefined, str, List[NumberType]] = undefined
     dashOffset: Union[Undefined, str] = undefined
     fill: Union[Undefined, bool] = undefined
     fillColor: Union[Undefined, str] = undefined
     fillOpacity: Union[Undefined, NumberType] = undefined
     fillRule: Union[Undefined, Literal['nonzero', 'evenodd',
-                                        'inherit']] = undefined
+                                       'inherit']] = undefined
 
 
 @dataclasses.dataclass
 class TooltipProps(ContainerBaseProps):
     sticky: Union[bool, Undefined] = undefined
     opacity: Union[NumberType, Undefined] = undefined
     direction: Union[Literal['right', 'left', 'top', 'bottom', 'center',
@@ -277,15 +280,15 @@
                          Undefined] = undefined,
         children: Optional[Dict[str, MapElementChildType]] = None,
     ) -> None:
         super().__init__(UIType.LeafletPolyline,
                          PolylineProps,
                          _children=children,
                          allowed_events=[
-                                FrontendEventType.Click.value,
+                             FrontendEventType.Click.value,
                          ])
         self.props.color = color
         self.props.positions = positions
         self.event_click = self._create_event_slot(FrontendEventType.Click)
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
         await handle_standard_event(self, ev, is_sync=is_sync)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/mui.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/mui.py`

 * *Files 0% similar despite different names*

```diff
@@ -101,15 +101,16 @@
     position: Union[Literal["absolute", "relative", "fixed"],
                     Undefined] = undefined
     top: Union[ValueType, Undefined] = undefined
     bottom: Union[ValueType, Undefined] = undefined
     left: Union[ValueType, Undefined] = undefined
     right: Union[ValueType, Undefined] = undefined
     zIndex: Union[ValueType, Undefined] = undefined
-    textAlign: Union[Literal["start", "end", "inherit", "left", "right", "center", "justify"], Undefined] = undefined
+    textAlign: Union[Literal["start", "end", "inherit", "left", "right",
+                             "center", "justify"], Undefined] = undefined
     flex: Union[ValueType, Undefined] = undefined
     alignSelf: Union[Literal["auto", "flex-start", "flex-end", "center",
                              "baseline", "stretch"], Undefined] = undefined
     flexGrow: Union[ValueType, Undefined] = undefined
     flexShrink: Union[ValueType, Undefined] = undefined
     flexBasis: Union[ValueType, Undefined] = undefined
 
@@ -562,15 +563,14 @@
     AccountCircle = 42
     BugReport = 43
     Dashboard = 44
     DashboardCustomize = 45
     Check = 46
 
 
-
 @dataclasses.dataclass
 class IconBaseProps:
     icon: Union[int, str] = 0
     iconSize: Union[Literal["small", "medium", "large", "inherit"],
                     Undefined] = undefined
     iconFontSize: Union[ValueType, Undefined] = undefined
 
@@ -1463,16 +1463,17 @@
     def state_change_callback(self,
                               data: str,
                               type: ValueType = FrontendEventType.Change.value
                               ):
         self.props.value = data
 
     async def headless_write(self, content: str):
-        uiev = UIEvent(
-            {self._flow_uid_encoded: (FrontendEventType.Change.value, content)})
+        uiev = UIEvent({
+            self._flow_uid_encoded: (FrontendEventType.Change.value, content)
+        })
         return await self.put_app_event(
             AppEvent("", {AppEventType.UIEvent: uiev}))
 
     def json(self):
         assert not isinstance(self.props.value, Undefined)
         return json.loads(self.props.value)
 
@@ -1699,16 +1700,17 @@
     def state_change_callback(
             self,
             data: bool,
             type: ValueType = FrontendEventType.Change.value):
         self.props.checked = data
 
     async def headless_write(self, checked: bool):
-        uiev = UIEvent(
-            {self._flow_uid_encoded: (FrontendEventType.Change.value, checked)})
+        uiev = UIEvent({
+            self._flow_uid_encoded: (FrontendEventType.Change.value, checked)
+        })
         return await self.put_app_event(
             AppEvent("", {AppEventType.UIEvent: uiev}))
 
     def __bool__(self):
         return self.props.checked
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
@@ -2207,14 +2209,15 @@
 
 @dataclasses.dataclass
 class SliderProps(SliderBaseProps):
     value: Union[Undefined, NumberType] = undefined
     defaultValue: Union[Undefined, NumberType] = undefined
     marks: Union[Undefined, bool] = undefined
 
+
 class Slider(MUIComponentBase[SliderProps]):
 
     def __init__(self,
                  begin: NumberType,
                  end: NumberType,
                  step: Optional[NumberType] = None,
                  callback: Optional[Callable[[NumberType], _CORO_NONE]] = None,
@@ -2719,16 +2722,17 @@
             self.update_event(progresses=self.props.progresses))
 
     async def update_label(self, label: str):
         await self.send_and_wait(self.update_event(label=label))
         self.props.label = label
 
     async def headless_event(self, ev: TaskLoopEvent):
-        uiev = UIEvent(
-            {self._flow_uid_encoded: (FrontendEventType.Change.value, ev.value)})
+        uiev = UIEvent({
+            self._flow_uid_encoded: (FrontendEventType.Change.value, ev.value)
+        })
         return await self.put_app_event(
             AppEvent("", {AppEventType.UIEvent: uiev}))
 
     async def handle_event(self, data: Event, is_sync: bool = False):
         return await handle_standard_event(self, data, is_sync=is_sync)
 
     @property
@@ -3160,14 +3164,15 @@
     tooltipPlacement: Union[_TooltipPlacement, Undefined] = undefined
     tooltipMultiline: Union[bool, Undefined] = undefined
     # 300 by default
     tooltipEnterDelay: Union[NumberType, Undefined] = undefined
     tooltipLeaveDelay: Union[NumberType, Undefined] = undefined
     tooltipEnterNextDelay: Union[NumberType, Undefined] = undefined
 
+
 @dataclasses.dataclass
 class TabDef:
     label: str
     value: str
     component: Component
     wrapped: Union[Undefined, bool] = undefined
     disabled: Union[Undefined, bool] = undefined
@@ -3393,15 +3398,15 @@
                      name: Optional[str] = None) -> None:
             self.comp = comp
             if name is None:
                 name = type(comp).__name__
             self.name = name
 
         def get_model_dict(self):
-            assert self.comp._flow_uid is not None 
+            assert self.comp._flow_uid is not None
             comp_last_uid = self.comp._flow_uid.parts[-1]
             return {
                 "type": "tab",
                 "id": comp_last_uid,
                 "name": self.name,
                 "component": "app",
                 "config": {
@@ -3703,16 +3708,16 @@
     @property
     def update_event(self):
         propcls = self.propcls
         return self._update_props_base(propcls)
 
 
 def _default_json_node():
-    return JsonLikeNode(UniqueTreeIdForTree.from_parts(["root"]), "root", JsonLikeType.Object.value, "Object",
-                        undefined, 0, [])
+    return JsonLikeNode(UniqueTreeIdForTree.from_parts(["root"]), "root",
+                        JsonLikeType.Object.value, "Object", undefined, 0, [])
 
 
 class _TreeControlType(enum.IntEnum):
     UpdateSubTree = 0
 
 
 @dataclasses.dataclass
@@ -4491,20 +4496,19 @@
     @property
     def update_event(self):
         propcls = self.propcls
         return self._update_props_base(propcls)
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
         return await handle_standard_event(self, ev, is_sync=is_sync)
-    
+
     async def scroll_to_index(self, index: int):
         return await self.send_and_wait(
             self.create_comp_event({
-                "type":
-                DataListControlType.ScrollToIndex.value,
+                "type": DataListControlType.ScrollToIndex.value,
                 "index": index,
             }))
 
     async def update_data_in_index(self, index: int, updates: Dict[str, Any]):
         return await self.update_datas_in_index([DataUpdate(index, updates)])
 
     async def update_datas_in_index(self, updates: List[DataUpdate]):
@@ -4558,24 +4562,26 @@
             comp.register_event_handler(FrontendEventType.Change.value,
                                         partial(self._comp_bind_update_data,
                                                 prop_name=prop_name),
                                         simple_event=False)
         else:
             raise ValueError("only support components with change event")
 
+
 @dataclasses.dataclass(config=dataclasses.PyDanticConfigForAnyObject)
 class MatrixDataGridItem:
-    array: np.ndarray 
+    array: np.ndarray
     columnOffset: int = 0
 
 
 @dataclasses.dataclass(config=dataclasses.PyDanticConfigForAnyObject)
 class MatrixDataGridProps(MUIFlexBoxProps, DataGridPropsBase):
     # dict of matrix with same number of rows.
-    dataList: Dict[str, MatrixDataGridItem] = dataclasses.field(default_factory=dict)
+    dataList: Dict[str, MatrixDataGridItem] = dataclasses.field(
+        default_factory=dict)
     rowOffset: Union[Undefined, int] = undefined
 
 
 class MatrixDataGrid(MUIContainerBase[MatrixDataGridProps, MUIComponentType]):
     """matrix data grid, it takes dict of np.ndarray.
     don't support data edit. it should only be used for matrix visualization.
     """
@@ -4620,25 +4626,31 @@
                   customFooterDatas=customFooterDatas)
 
     @staticmethod
     def _check_data(data_list: Dict[str, MatrixDataGridItem]):
         assert len(data_list) > 0, "empty data list not allowed"
         init_shape: List[int] = []
         for k, v in data_list.items():
-            assert isinstance(v, MatrixDataGridItem), "data must be MatrixDataGridItem"
-            assert isinstance(v.array, np.ndarray), "data must be MatrixDataGridItem"
+            assert isinstance(
+                v, MatrixDataGridItem), "data must be MatrixDataGridItem"
+            assert isinstance(v.array,
+                              np.ndarray), "data must be MatrixDataGridItem"
             assert v.array.dtype != np.bool_ and v.array.dtype != np.float16, "bool, float16 and object dtype not supported"
             assert v.array.size > 0, "empty array not allowed"
             if not init_shape:
                 init_shape = list(v.array.shape)
             else:
                 if len(data_list) > 1:
-                    assert len(v.array.shape) == len(init_shape), "all matrix must have same number of dimensions"
-                    assert list(v.array.shape[:-1]) == list(init_shape[:-1]), f"all matrix must have same number of rows, {k} has {v.array.shape[:-1]} while others has {init_shape[:-1]}"
-    
+                    assert len(v.array.shape) == len(
+                        init_shape
+                    ), "all matrix must have same number of dimensions"
+                    assert list(v.array.shape[:-1]) == list(
+                        init_shape[:-1]
+                    ), f"all matrix must have same number of rows, {k} has {v.array.shape[:-1]} while others has {init_shape[:-1]}"
+
     @staticmethod
     def _check_data_np_dict(data_list: Dict[str, np.ndarray]):
         data_list_items: Dict[str, MatrixDataGridItem] = {}
         for k, v in data_list.items():
             if isinstance(v, np.ndarray):
                 data_list_items[k] = MatrixDataGridItem(v)
             else:
@@ -4658,20 +4670,19 @@
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
         return await handle_standard_event(self, ev, is_sync=is_sync)
 
     @staticmethod
     def get_column_id(arr_key: str, column: int):
         return f"{arr_key}-{column}"
-    
+
     async def scroll_to_index(self, index: int):
         return await self.send_and_wait(
             self.create_comp_event({
-                "type":
-                DataListControlType.ScrollToIndex.value,
+                "type": DataListControlType.ScrollToIndex.value,
                 "index": index,
             }))
 
 
 def flex_wrapper(obj: Any,
                  metas: Optional[List[ServFunctionMeta]] = None,
                  reload_mgr: Optional[ObjectReloadManager] = None):
@@ -4794,14 +4805,15 @@
 
 
 @dataclasses.dataclass
 class Anchor:
     vertical: Literal["top", "center", "bottom"]
     horizontal: Literal["left", "center", "right"]
 
+
 @dataclasses.dataclass
 class MenuItem:
     id: str
     label: Union[Undefined, str] = undefined
     icon: Union[IconType, Undefined, str] = undefined
     inset: Union[Undefined, bool] = undefined
     iconSize: Union[Undefined, Literal["inherit", "large", "medium",
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plotly.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plotly.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,17 +21,17 @@
 
 from tensorpc.core.asynctools import cancel_task
 from tensorpc.flow.flowapp.appcore import Event
 from tensorpc.flow.flowapp.components.common import (handle_standard_event)
 from typing_extensions import Literal, TypeAlias
 
 from ..core import (AppEvent, AppEventType, BasicProps, Component,
-                    ContainerBase, FrontendEventType, NumberType, T_child, TaskLoopEvent, UIEvent,
-                    UIRunStatus, UIType, Undefined, undefined,
-                    as_dict_no_undefined)
+                    ContainerBase, FrontendEventType, NumberType, T_child,
+                    TaskLoopEvent, UIEvent, UIRunStatus, UIType, Undefined,
+                    undefined, as_dict_no_undefined)
 from .mui import MUIComponentBase
 
 
 @dataclasses.dataclass
 class Font:
     size: Union[Undefined, NumberType] = undefined
     family: Union[Undefined, str] = undefined
@@ -55,25 +55,28 @@
 
 
 @dataclasses.dataclass
 class Trace:
     x: Union[Undefined, List[Union[NumberType, str]]] = undefined
     y: Union[Undefined, List[Union[NumberType, str]]] = undefined
     z: Union[Undefined, List[Union[NumberType, str]]] = undefined
-    type: Union[Undefined, Literal["scatter", "scattergl", "bar", "image"]] = undefined
-    mode: Union[Undefined, Literal["markers", "lines", "lines+markers"]] = undefined
+    type: Union[Undefined, Literal["scatter", "scattergl", "bar",
+                                   "image"]] = undefined
+    mode: Union[Undefined, Literal["markers", "lines",
+                                   "lines+markers"]] = undefined
     visible: Union[Undefined, bool] = undefined
     name: Union[Undefined, str] = undefined
     line: Union[Undefined, Line] = undefined
     marker: Union[Undefined, Marker] = undefined
     values: Union[Undefined, List[Union[NumberType, str]]] = undefined
     labels: Union[Undefined, List[Union[NumberType, str]]] = undefined
     width: Union[Undefined, NumberType, List[NumberType]] = undefined
     text: Union[Undefined, str, List[str]] = undefined
 
+
 @dataclasses.dataclass
 class Margin:
     l: Union[Undefined, NumberType] = undefined
     r: Union[Undefined, NumberType] = undefined
     t: Union[Undefined, NumberType] = undefined
     b: Union[Undefined, NumberType] = undefined
     pad: Union[Undefined, NumberType] = undefined
@@ -111,40 +114,49 @@
     tickcolor: Union[Undefined, str] = undefined
     tickwidth: Union[Undefined, NumberType] = undefined
     ticklen: Union[Undefined, NumberType] = undefined
     tickfont: Union[Undefined, Font] = undefined
     automargin: Union[Undefined, bool] = undefined
     domain: Union[Undefined, List[NumberType]] = undefined
 
+
 @dataclasses.dataclass
 class LegendTitle:
     text: Union[Undefined, str] = undefined
     font: Union[Undefined, Font] = undefined
     side: Union[Undefined, Literal["top", "left", "top left"]] = undefined
 
+
 @dataclasses.dataclass
 class Legend:
     borderwidth: Union[Undefined, NumberType] = undefined
-    groupclick: Union[Undefined, Literal["toggleitem", "togglegroup"]] = undefined
+    groupclick: Union[Undefined, Literal["toggleitem",
+                                         "togglegroup"]] = undefined
     grouptitlefont: Union[Undefined, Font] = undefined
-    itemclick: Union[Undefined, Literal["toggle", "toggleothers", False]] = undefined
-    itemdoubleclick: Union[Undefined, Literal["toggle", "toggleothers", False]] = undefined
+    itemclick: Union[Undefined, Literal["toggle", "toggleothers",
+                                        False]] = undefined
+    itemdoubleclick: Union[Undefined, Literal["toggle", "toggleothers",
+                                              False]] = undefined
     itemsizing: Union[Undefined, Literal["trace", "constant"]] = undefined
     itemwidth: Union[Undefined, NumberType] = undefined
     orientation: Union[Undefined, Literal["v", "h"]] = undefined
     title: Union[Undefined, LegendTitle] = undefined
     tracegroupgap: Union[Undefined, NumberType] = undefined
-    traceorder: Union[Undefined, Literal["grouped", "normal", "reversed", "reversed+grouped"]] = undefined
+    traceorder: Union[Undefined, Literal["grouped", "normal", "reversed",
+                                         "reversed+grouped"]] = undefined
     uirevision: Union[Undefined, NumberType, str] = undefined
     uid: Union[Undefined, str] = undefined
     valign: Union[Undefined, Literal["top", "middle", "bottom"]] = undefined
     x: Union[Undefined, NumberType] = undefined
-    xanchor: Union[Undefined, Literal["auto", "left", "center", "right"]] = undefined
+    xanchor: Union[Undefined, Literal["auto", "left", "center",
+                                      "right"]] = undefined
     y: Union[Undefined, NumberType] = undefined
-    yanchor: Union[Undefined, Literal["auto", "top", "middle", "bottom"]] = undefined
+    yanchor: Union[Undefined, Literal["auto", "top", "middle",
+                                      "bottom"]] = undefined
+
 
 @dataclasses.dataclass
 class Layout:
     title: Union[Undefined, str] = undefined
     width: Union[Undefined, NumberType] = undefined
     height: Union[Undefined, NumberType] = undefined
     showlegend: Union[Undefined, bool] = undefined
@@ -189,35 +201,41 @@
 
 
 class ChartControlType(enum.IntEnum):
     ExtendData = 0
     ClearData = 1
     UpdateTrace = 2
 
+
 @dataclasses.dataclass
 class PlotlyTraceDataUpdate:
-    traceUpdateIndex: int 
+    traceUpdateIndex: int
     dataMaxCount: Union[Undefined, int] = undefined
     x: Union[Undefined, List[Union[NumberType, str]]] = undefined
     y: Union[Undefined, List[Union[NumberType, str]]] = undefined
     z: Union[Undefined, List[Union[NumberType, str]]] = undefined
     width: Union[Undefined, List[NumberType]] = undefined
     text: Union[Undefined, List[str]] = undefined
     markerSize: Union[Undefined, List[NumberType]] = undefined
     markerColor: Union[Undefined, List[str]] = undefined
     markerOpacity: Union[Undefined, List[NumberType]] = undefined
     values: Union[Undefined, List[Union[NumberType, str]]] = undefined
     labels: Union[Undefined, List[Union[NumberType, str]]] = undefined
 
+
 class Plotly(MUIComponentBase[PlotlyProps]):
     TraceDataUpdate = PlotlyTraceDataUpdate
     """see https://plotly.com/javascript/ for documentation"""
 
-    def __init__(self, data: Optional[List[Trace]] = None, layout: Optional[Layout] = None) -> None:
-        super().__init__(UIType.Plotly, PlotlyProps, allowed_events=[FrontendEventType.Click])
+    def __init__(self,
+                 data: Optional[List[Trace]] = None,
+                 layout: Optional[Layout] = None) -> None:
+        super().__init__(UIType.Plotly,
+                         PlotlyProps,
+                         allowed_events=[FrontendEventType.Click])
         self.event_click = self._create_event_slot(FrontendEventType.Click)
         if data is not None:
             self.prop(data=data)
         if layout is not None:
             self.prop(layout=layout)
 
     async def show_raw(self, data: List[Trace], layout: Layout):
@@ -257,72 +275,93 @@
                       yaxis=Axis(automargin=True))
 
     async def clear_data(self, clear_trace_idxes: List[int] = []):
         ev = self.create_comp_event({
             "type": ChartControlType.ClearData.value,
             "deleteTraceIndexes": clear_trace_idxes,
         })
-        data = self.props.data 
+        data = self.props.data
         for idx in clear_trace_idxes:
             if idx >= len(data) or idx < 0:
-                continue 
+                continue
             trace = data[idx]
             if not isinstance(trace.x, Undefined):
                 trace.x.clear()
             if not isinstance(trace.y, Undefined):
                 trace.y.clear()
             if not isinstance(trace.z, Undefined):
                 trace.z.clear()
-            if not isinstance(trace.text, Undefined) and isinstance(trace.text, list):
+            if not isinstance(trace.text, Undefined) and isinstance(
+                    trace.text, list):
                 trace.text.clear()
-            if not isinstance(trace.width, Undefined) and isinstance(trace.width, list):
+            if not isinstance(trace.width, Undefined) and isinstance(
+                    trace.width, list):
                 trace.width.clear()
-            if not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(trace.marker, Undefined) and isinstance(
+                    trace.marker, Marker):
                 if isinstance(trace.marker.size, list):
                     trace.marker.size.clear()
-            if not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(trace.marker, Undefined) and isinstance(
+                    trace.marker, Marker):
                 if isinstance(trace.marker.color, list):
                     trace.marker.color.clear()
-            if not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(trace.marker, Undefined) and isinstance(
+                    trace.marker, Marker):
                 if isinstance(trace.marker.opacity, list):
                     trace.marker.opacity.clear()
             if not isinstance(trace.values, Undefined):
                 trace.values.clear()
             if not isinstance(trace.labels, Undefined):
                 trace.labels.clear()
 
         await self.send_and_wait(ev)
 
     async def extend_data(self, updates: List[PlotlyTraceDataUpdate]):
         ev = self.create_comp_event({
             "type": ChartControlType.ExtendData.value,
             "updates": updates,
         })
-        data = self.props.data 
+        data = self.props.data
         for update in updates:
-            if update.traceUpdateIndex >= len(data) or update.traceUpdateIndex < 0:
-                continue 
+            if update.traceUpdateIndex >= len(
+                    data) or update.traceUpdateIndex < 0:
+                continue
             trace = data[update.traceUpdateIndex]
-            if not isinstance(update.x, Undefined) and not isinstance(trace.x, Undefined):
+            if not isinstance(update.x, Undefined) and not isinstance(
+                    trace.x, Undefined):
                 trace.x.extend(update.x)
-            if not isinstance(update.y, Undefined) and not isinstance(trace.y, Undefined):
+            if not isinstance(update.y, Undefined) and not isinstance(
+                    trace.y, Undefined):
                 trace.y.extend(update.y)
-            if not isinstance(update.z, Undefined) and not isinstance(trace.z, Undefined):
+            if not isinstance(update.z, Undefined) and not isinstance(
+                    trace.z, Undefined):
                 trace.z.extend(update.z)
-            if not isinstance(update.text, Undefined) and not isinstance(trace.text, Undefined) and isinstance(trace.text, list):
+            if not isinstance(update.text, Undefined) and not isinstance(
+                    trace.text, Undefined) and isinstance(trace.text, list):
                 trace.text.extend(update.text)
-            if not isinstance(update.width, Undefined) and not isinstance(trace.width, Undefined) and isinstance(trace.width, list):
+            if not isinstance(update.width, Undefined) and not isinstance(
+                    trace.width, Undefined) and isinstance(trace.width, list):
                 trace.width.extend(update.width)
-            if not isinstance(update.markerSize, Undefined) and not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(update.markerSize, Undefined) and not isinstance(
+                    trace.marker, Undefined) and isinstance(
+                        trace.marker, Marker):
                 if isinstance(trace.marker.size, list):
                     trace.marker.size.extend(update.markerSize)
-            if not isinstance(update.markerColor, Undefined) and not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(update.markerColor,
+                              Undefined) and not isinstance(
+                                  trace.marker, Undefined) and isinstance(
+                                      trace.marker, Marker):
                 if isinstance(trace.marker.color, list):
                     trace.marker.color.extend(update.markerColor)
-            if not isinstance(update.markerOpacity, Undefined) and not isinstance(trace.marker, Undefined) and isinstance(trace.marker, Marker):
+            if not isinstance(update.markerOpacity,
+                              Undefined) and not isinstance(
+                                  trace.marker, Undefined) and isinstance(
+                                      trace.marker, Marker):
                 if isinstance(trace.marker.opacity, list):
                     trace.marker.opacity.extend(update.markerOpacity)
-            if not isinstance(update.values, Undefined) and not isinstance(trace.values, Undefined):
+            if not isinstance(update.values, Undefined) and not isinstance(
+                    trace.values, Undefined):
                 trace.values.extend(update.values)
-            if not isinstance(update.labels, Undefined) and not isinstance(trace.labels, Undefined):
+            if not isinstance(update.labels, Undefined) and not isinstance(
+                    trace.labels, Undefined):
                 trace.labels.extend(update.labels)
         await self.send_and_wait(ev)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/arraycommon.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/arraycommon.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,25 @@
 from typing import Any, Optional
 
 from tensorpc.core.moduleid import get_qualname_of_type
 from .common import CommonQualNames
-import numpy as np 
+import numpy as np
+
 
 def can_cast_to_np_array(obj: Any):
     if isinstance(obj, np.ndarray):
-        return True 
+        return True
     elif get_qualname_of_type(type(obj)) == CommonQualNames.TorchTensor:
         return True
     elif get_qualname_of_type(type(obj)) == CommonQualNames.TVTensor:
-        return True 
-    return False 
+        return True
+    return False
+
 
 def try_cast_to_np_array(obj: Any) -> Optional[np.ndarray]:
     if isinstance(obj, np.ndarray):
         return obj
     elif get_qualname_of_type(type(obj)) == CommonQualNames.TorchTensor:
         return obj.detach().cpu().numpy()
     elif get_qualname_of_type(type(obj)) == CommonQualNames.TVTensor:
-        return obj.cpu().numpy() 
-    return None 
+        return obj.cpu().numpy()
+    return None
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/arraygrid.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/arraygrid.py`

 * *Files 1% similar despite different names*

```diff
@@ -389,15 +389,16 @@
 
 
 class NumpyArrayGridTable(mui.FlexBox):
 
     def __init__(self,
                  init_array_items: Optional[Dict[str,
                                                  Union[Dict[str, np.ndarray],
-                                                       np.ndarray, int, float, bool]]] = None,
+                                                       np.ndarray, int, float,
+                                                       bool]]] = None,
                  max_columns: int = 25,
                  max_size_row_split: int = 1000000):
         super().__init__()
         self.max_columns = max_columns
         self.max_size_row_split = max_size_row_split
         btn = mui.Button("Viewer").prop(size="small", loading=False)
         self.grid_container = mui.HBox([])
@@ -409,29 +410,30 @@
                 includeFormControl=False)
 
         self.dialog = dialog
         btn.event_click.on_standard(
             self._on_btn_select).configure(stop_propagation=True)
         value_cell = mui.MatchCase([
             mui.MatchCase.ExprCase("x != \"scalar\"", btn),
-            mui.MatchCase.Case(mui.undefined, mui.Typography("").set_override_props(value="value")),
+            mui.MatchCase.Case(
+                mui.undefined,
+                mui.Typography("").set_override_props(value="value")),
         ]).set_override_props(condition="shape")
         cbox = mui.Checkbox().prop(size="small", disabled=True)
         column_defs = [
             mui.DataGrid.ColumnDef("name", accessorKey="name"),
             mui.DataGrid.ColumnDef("dtype", accessorKey="dtype"),
             mui.DataGrid.ColumnDef("shape", accessorKey="shape"),
-
             mui.DataGrid.ColumnDef("contiguous",
                                    accessorKey="contiguous",
                                    cell=cbox),
             mui.DataGrid.ColumnDef("value", cell=value_cell),
         ]
-        self.array_items: Dict[str, Union[Dict[str, np.ndarray],
-                                          np.ndarray, int, float, bool]] = {}
+        self.array_items: Dict[str, Union[Dict[str, np.ndarray], np.ndarray,
+                                          int, float, bool]] = {}
         if init_array_items is not None:
             self.array_items = init_array_items
 
         dgrid = mui.DataGrid(column_defs,
                              self._extract_table_data_from_array_items()).prop(
                                  idKey="id",
                                  rowHover=True,
@@ -444,24 +446,26 @@
         self.init_add_layout([dgrid.prop(flex=1), dialog])
         self.dgrid = dgrid
         self.prop(width="100%", height="100%", overflow="hidden")
 
     async def update_array_items(self,
                                  array_items: Dict[str, Union[Dict[str,
                                                                    np.ndarray],
-                                                              np.ndarray, int, float, bool]]):
+                                                              np.ndarray, int,
+                                                              float, bool]]):
         self.array_items.update(array_items)
         item_datas = self._extract_table_data_from_array_items()
         await self.send_and_wait(self.dgrid.update_event(dataList=item_datas))
 
     async def set_new_array_items(self,
                                   new_array_items: Dict[str,
                                                         Union[Dict[str,
                                                                    np.ndarray],
-                                                              np.ndarray, int, float, bool]]):
+                                                              np.ndarray, int,
+                                                              float, bool]]):
         self.array_items = new_array_items
         item_datas = self._extract_table_data_from_array_items()
         await self.send_and_wait(self.dgrid.update_event(dataList=item_datas))
 
     async def clear_array_items(self):
         self.array_items = {}
         await self.send_and_wait(self.dgrid.update_event(dataList=[]))
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/canvas.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/canvas.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,107 +27,111 @@
 from tensorpc.flow.flowapp.core import FrontendEventType
 from tensorpc.flow.flowapp.coretypes import TreeDragTarget
 from tensorpc.flow.flowapp import colors
 from tensorpc.flow.jsonlike import TreeItem
 from tensorpc.utils.registry import HashableSeqRegistryKeyOnly
 from tensorpc.flow.flowapp.components.core import get_tensor_container
 
-UNKNOWN_VIS_REGISTRY: HashableSeqRegistryKeyOnly[Callable[[Any, str, "SimpleCanvas"], Coroutine[None, None, bool]]] = HashableSeqRegistryKeyOnly()
+UNKNOWN_VIS_REGISTRY: HashableSeqRegistryKeyOnly[
+    Callable[[Any, str, "SimpleCanvas"],
+             Coroutine[None, None, bool]]] = HashableSeqRegistryKeyOnly()
 
 
 def _try_cast_to_point_cloud(obj: Any):
     tc = get_tensor_container(obj)
     if tc is None:
-        return None 
+        return None
 
     ndim = obj.ndim
     if ndim == 2:
         dtype = tc.dtype
         if dtype == np.float32 or dtype == np.float16 or dtype == np.float64:
             num_ft = obj.shape[1]
             if num_ft >= 3 and num_ft <= 4:
                 return tc.numpy()
     return None
 
 
 def _try_cast_to_box3d(obj: Any):
     tc = get_tensor_container(obj)
     if tc is None:
-        return None 
+        return None
     ndim = obj.ndim
     if ndim == 2:
         dtype = tc.dtype
         if dtype == np.float32 or dtype == np.float16 or dtype == np.float64:
             num_ft = obj.shape[1]
             if num_ft == 7:
                 return tc.numpy()
     return None
 
 
 def _try_cast_to_lines(obj: Any):
     tc = get_tensor_container(obj)
     if tc is None:
-        return None 
+        return None
     ndim = obj.ndim
     if ndim == 3:
         dtype = tc.dtype
         if dtype == np.float32 or dtype == np.float16 or dtype == np.float64:
             if obj.shape[1] == 2 and obj.shape[2] == 3:
                 return tc.numpy()
     return None
 
 
 def _try_cast_to_image(obj: Any):
     tc = get_tensor_container(obj)
     if tc is None:
-        return None 
+        return None
     ndim = obj.ndim
     valid = False
     is_rgba = False
     if ndim == 2:
         valid = tc.dtype == np.uint8
     elif ndim == 3:
         valid = tc.dtype == np.uint8 and (obj.shape[2] == 3
-                                           or obj.shape[2] == 4)
+                                          or obj.shape[2] == 4)
         is_rgba = obj.shape[2] == 4
     if valid:
         res = tc.numpy()
         if is_rgba and res is not None:
             res = res[..., :3]
         return res
     return None
 
+
 class CanvasTreeItem(TreeItem):
-    pass 
+    pass
+
 
 @dataclasses.dataclass
 class PointCfg:
     size: float = dataclasses.field(default=3,
                                     metadata=ConfigPanel.slider_meta(1, 10))
     encode_method: Literal["none", "int16"] = "none"
-    encode_scale: mui.NumberType = dataclasses.field(default=50,
-                                    metadata=ConfigPanel.slider_meta(25, 100))
+    encode_scale: mui.NumberType = dataclasses.field(
+        default=50, metadata=ConfigPanel.slider_meta(25, 100))
+
 
 @dataclasses.dataclass
 class BoxCfg:
     edge_width: float = dataclasses.field(default=1,
                                           metadata=ConfigPanel.slider_meta(
                                               1, 5))
     add_cross: bool = True
     opacity: float = dataclasses.field(default=0.2,
-                                          metadata=ConfigPanel.slider_meta(
-                                              0.0, 1.0))
+                                       metadata=ConfigPanel.slider_meta(
+                                           0.0, 1.0))
 
 
 @dataclasses.dataclass
 class GlobalCfg:
     background: mui.ControlColorRGBA
     enable_perf: bool = dataclasses.field(
-        default=False,
-        metadata=ConfigPanel.base_meta(alias="Enable Perf"))
+        default=False, metadata=ConfigPanel.base_meta(alias="Enable Perf"))
 
 
 class CamCtrlKeyboardMode(enum.Enum):
     Fly = "Fly"
     Helicopter = "Helicopter"
 
 
@@ -158,28 +162,29 @@
 
     def __init__(
             self,
             camera: Optional[three.PerspectiveCamera] = None,
             screenshot_callback: Optional[Callable[[bytes, Any],
                                                    mui._CORO_NONE]] = None,
             transparent_canvas: bool = False,
-            init_canvas_childs: Optional[List[three.ThreeComponentType]] = None,
+            init_canvas_childs: Optional[List[
+                three.ThreeComponentType]] = None,
             key: str = "canvas",
             sync_canvases: Optional[Set["SimpleCanvas"]] = None):
         if camera is None:
             camera = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
         self.camera = camera
         # self.ctrl = three.FirstPersonControl().prop(makeDefault=True,
-        #                                             enabled=True, activeLook=True, constrainVertical=False, 
+        #                                             enabled=True, activeLook=True, constrainVertical=False,
         #                                             autoForward=False, heightCoef=1, heightMin=0, heightMax=1,
         #                                             lookVertical=True, lookSpeed=0.005, movementSpeed=1, verticalMax=np.pi, verticalMin=0,)
         # self.ctrl = three.PointerLockControl().prop(
         #                                             enabled=True, makeDefault=True)
         self.ctrl = three.CameraControl().prop(makeDefault=True)
-        
+
         infgrid = three.InfiniteGridHelper(5, 50, "gray")
         self.axis_helper = three.AxesHelper(20)
         self.infgrid = infgrid
         self._is_transparent = transparent_canvas
         self._dynamic_grid = three.Group([infgrid, self.axis_helper])
         gcfg = GlobalCfg(mui.ControlColorRGBA(255, 255, 255, 1))
         self.gcfg = gcfg
@@ -195,15 +200,14 @@
         self._screen_shot_v2 = three.ScreenShotSyncReturn()
         self.background_img = mui.Image()
 
         self._screenshot_callback = screenshot_callback
         if init_canvas_childs is None:
             init_canvas_childs = []
         canvas_layout = [
-
             self.ctrl,
             self.camera,
             self._dynamic_pcs,
             self._dynamic_lines,
             self._dynamic_images,
             self._dynamic_boxes,
             # three.AxesHelper(20),
@@ -211,21 +215,20 @@
             # self._screen_shot,
             self._screen_shot_v2,
             self._dynamic_voxels,
             self._dynamic_custom_objs,
 
             # three.GizmoHelper().prop(alignment="bottom-right", renderPriority=1),
             *init_canvas_childs,
-
         ]
         # if with_grid:
         #     canvas_layout.append(infgrid)
         self._ctrl_container = mui.Fragment([])
-        self.canvas = three.Canvas(canvas_layout).prop(
-            flex=1, allowKeyboardEvent=True)
+        self.canvas = three.Canvas(canvas_layout).prop(flex=1,
+                                                       allowKeyboardEvent=True)
         if not self._is_transparent:
             self.canvas.prop(threeBackgroundColor="#ffffff")
         self._point_dict: Dict[str, three.Points] = {}
         self._image_dict: Dict[str, three.Image] = {}
         self._segment_dict: Dict[str, three.Segments] = {}
         self._box_dict: Dict[str, three.BoundingBox] = {}
         self._voxels_dict: Dict[str, three.VoxelMesh] = {}
@@ -235,29 +238,31 @@
         self._dnd_trees: Set[str] = set()
         # for find_component selection
         self.key = key
         if sync_canvases is None:
             sync_canvases = set()
         self._sync_canvases: Set[SimpleCanvas] = sync_canvases
         if len(sync_canvases) > 0:
-            self.ctrl.event_change.on(self._sync_camera_ctrl).configure(throttle=100)
+            self.ctrl.event_change.on(
+                self._sync_camera_ctrl).configure(throttle=100)
 
         super().__init__()
         self.init_add_layout([*self._layout_func()])
 
     def __get_cfg_panel(self):
         _cfg_panel = ConfigPanelV2(self.cfg, self._on_cfg_change)
-        _cfg_panel.prop(border="1px solid",
-                             borderColor="gray",
-                             backgroundColor="white",
-                            #  collapsed=True,
-                            #  title="configs",
-                             marginLeft="5px",
-                             width="400px",
-                             height="300px")
+        _cfg_panel.prop(
+            border="1px solid",
+            borderColor="gray",
+            backgroundColor="white",
+            #  collapsed=True,
+            #  title="configs",
+            marginLeft="5px",
+            width="400px",
+            height="300px")
         return _cfg_panel
 
     async def _on_screen_shot_finish(self, img_and_data: Tuple[str, Any]):
         if self._screenshot_callback:
             img = img_and_data[0]
             data = img_and_data[1]
             resp = urllib.request.urlopen(img)
@@ -351,27 +356,30 @@
                     mui.ToggleButton("enableCfgPanel",
                                      icon=mui.IconType.Settings,
                                      callback=self._on_enable_cfg_panel).prop(
                                          selected=False,
                                          size="small",
                                          tooltip="Enable Config Panel",
                                          tooltipPlacement="right"),
-
                     mui.IconButton(mui.IconType.Clear,
                                    callback=self._on_clear).prop(
                                        tooltip="Clear",
                                        tooltipPlacement="right"),
                     mui.IconButton(mui.IconType.Refresh,
                                    callback=self._on_reset_cam).prop(
                                        tooltip="Reset Camera",
                                        tooltipPlacement="right"),
                 ]),
                 # self._cfg_panel,
                 self._ctrl_container,
-            ]).prop(position="absolute", top=3, left=3, zIndex=5, maxHeight="10%"),
+            ]).prop(position="absolute",
+                    top=3,
+                    left=3,
+                    zIndex=5,
+                    maxHeight="10%"),
             mui.IconButton(mui.IconType.Help,
                            lambda: None).prop(tooltip=help_string,
                                               position="absolute",
                                               tooltipMultiline=True,
                                               top=3,
                                               right=3,
                                               zIndex=5),
@@ -395,41 +403,41 @@
             width="100%",
             height="100%",
             overflow="hidden",
             border="4px solid transparent",
             sxOverDrop={"border": "4px solid green"},
         )
         return layout
-    
+
     async def set_transparent(self, is_transparent: bool):
         if is_transparent:
             await self.canvas.send_and_wait(
                 self.canvas.update_event(threeBackgroundColor=mui.undefined))
         else:
             await self.canvas.send_and_wait(
                 self.canvas.update_event(threeBackgroundColor="#ffffff"))
 
-    @staticmethod 
+    @staticmethod
     def register_unknown_vis_handler(key: Type):
         """register a handler for unknown vis. the handle must be a 
         function with (obj, uid) argument.
         """
         return UNKNOWN_VIS_REGISTRY.register(key)
-    
-    @staticmethod 
+
+    @staticmethod
     def get_tensor_container(obj: Any):
         return get_tensor_container(obj)
-        
-    async def register_cam_control_event_handler(self,
-                                           handler: Callable[[Any],
-                                                             mui.CORO_NONE],
-                                           throttle: int = 100,
-                                           debounce: Optional[int] = None):
+
+    async def register_cam_control_event_handler(
+            self,
+            handler: Callable[[Any], mui.CORO_NONE],
+            throttle: int = 100,
+            debounce: Optional[int] = None):
         self.ctrl.event_change.on(handler).configure(throttle=throttle,
-                                         debounce=debounce)
+                                                     debounce=debounce)
         await self.ctrl.sync_used_events()
 
     async def clear_cam_control_event_handler(self):
         self.ctrl.remove_event_handlers(self.ctrl.event_change.event_type)
         await self.ctrl.sync_used_events()
 
     async def _on_enable_grid(self, selected):
@@ -437,58 +445,63 @@
             await self._dynamic_grid.set_new_layout(
                 [self.infgrid, self.axis_helper])
         else:
             await self._dynamic_grid.set_new_layout([])
 
     async def _on_enable_cfg_panel(self, selected):
         if selected:
-            await self._ctrl_container.set_new_layout(
-                [self.__get_cfg_panel()])
+            await self._ctrl_container.set_new_layout([self.__get_cfg_panel()])
         else:
             await self._ctrl_container.set_new_layout([])
 
     async def register_sync_canvases(self, *canvas: "SimpleCanvas"):
         """add camera handler for canvas, if changed, will
         set camera pose for all canvas.
         """
         for c in canvas:
             self._sync_canvases.add(c)
         if self._sync_canvases:
-            await self.register_cam_control_event_handler(self._sync_camera_ctrl)
+            await self.register_cam_control_event_handler(
+                self._sync_camera_ctrl)
 
     async def _sync_camera_ctrl(self, camdata):
         # print(camdata)
         # TODO this looks so ugly
         mat = np.array(camdata["matrixWorld"]).reshape(4, 4).T
         for canvas in self._sync_canvases:
             await canvas.set_cam2world(mat, 50)
 
-    async def _unknown_visualization(self, tree_id: str, obj: Any, ignore_registry: bool = False):
+    async def _unknown_visualization(self,
+                                     tree_id: str,
+                                     obj: Any,
+                                     ignore_registry: bool = False):
         obj_type = type(obj)
         if obj_type in UNKNOWN_VIS_REGISTRY and not ignore_registry:
             handlers = UNKNOWN_VIS_REGISTRY[obj_type]
             for handler in handlers:
                 res = await handler(obj, tree_id, self)
                 if res == True:
-                    return True 
+                    return True
         # found nothing in registry. use default one.
         pc_obj = _try_cast_to_point_cloud(obj)
         if pc_obj is not None:
             if tree_id in self._random_colors:
                 pick = self._random_colors[tree_id]
             else:
                 random_colors = colors.RANDOM_COLORS_FOR_UI
                 pick = random_colors[len(self._dynamic_pcs) %
                                      len(random_colors)]
                 self._random_colors[tree_id] = pick
             colors_pc: Optional[str] = None
             if pc_obj.shape[1] == 3:
                 colors_pc = pick
-            await self.show_points(tree_id, pc_obj.astype(np.float32),
-                                   pc_obj.shape[0], colors=colors_pc)
+            await self.show_points(tree_id,
+                                   pc_obj.astype(np.float32),
+                                   pc_obj.shape[0],
+                                   colors=colors_pc)
             return True
         img_obj = _try_cast_to_image(obj)
         if img_obj is not None:
             await self.show_image(tree_id, img_obj, (0, 0, 0), (0, 0, 0), 3)
             return True
         b3d_obj = _try_cast_to_box3d(obj)
         if b3d_obj is not None:
@@ -513,29 +526,33 @@
                 pick = self._random_colors[tree_id]
             else:
                 random_colors = colors.RANDOM_COLORS_FOR_UI
                 pick = random_colors[len(self._dynamic_lines) %
                                      len(random_colors)]
                 self._random_colors[tree_id] = pick
 
-            await self.show_lines(tree_id, line_obj, line_obj.shape[0], color=pick)
+            await self.show_lines(tree_id,
+                                  line_obj,
+                                  line_obj.shape[0],
+                                  color=pick)
             return True
         return False
 
     async def _dnd_cb(self, uid: UniqueTreeIdForTree, data: Any):
         await self._unknown_visualization(uid.uid_encoded, data)
 
     async def _on_drop(self, data):
         from tensorpc.flow.flowapp.components.plus import BasicObjectTree
         if isinstance(data, TreeDragTarget):
             obj = data.obj
             success = await self._unknown_visualization(data.tree_id, obj)
             if success:
                 # register to tree
-                tree = find_component_by_uid_with_type_check(data.source_comp_uid, BasicObjectTree)
+                tree = find_component_by_uid_with_type_check(
+                    data.source_comp_uid, BasicObjectTree)
                 if tree is not None:
                     tree._register_dnd_uid(UniqueTreeIdForTree(data.tree_id),
                                            self._dnd_cb)
                     self._dnd_trees.add(data.source_comp_uid)
 
     async def _on_pan_to_fwd(self, selected):
         await self.ctrl.send_and_wait(
@@ -574,56 +591,67 @@
         return await self.ctrl.set_cam2world(cam2world,
                                              distance,
                                              update_now=update_now)
 
     async def reset_camera(self):
         return await self.ctrl.reset_camera()
 
-    async def show_points(self,
-                          key: str,
-                          points: np.ndarray,
-                          limit: int,
-                          colors: Optional[Union[np.ndarray, str]] = None,
-                          sizes: Optional[Union[mui.Undefined,
-                                                np.ndarray]] = None,
-                          size_attenuation: bool = False,
-                          size: Optional[float] = None,
-                          encode_method: Optional[Union[Literal["none", "int16"], mui.Undefined]] = None, 
-                          encode_scale: Optional[Union[mui.NumberType, mui.Undefined]] = None,
-                        attrs: Optional[Union[np.ndarray,
-                                                mui.Undefined]] = None,
-                        attr_fields: Optional[List[str]] = None,
-
-                          ):
+    async def show_points(
+        self,
+        key: str,
+        points: np.ndarray,
+        limit: int,
+        colors: Optional[Union[np.ndarray, str]] = None,
+        sizes: Optional[Union[mui.Undefined, np.ndarray]] = None,
+        size_attenuation: bool = False,
+        size: Optional[float] = None,
+        encode_method: Optional[Union[Literal["none", "int16"],
+                                      mui.Undefined]] = None,
+        encode_scale: Optional[Union[mui.NumberType, mui.Undefined]] = None,
+        attrs: Optional[Union[np.ndarray, mui.Undefined]] = None,
+        attr_fields: Optional[List[str]] = None,
+    ):
         if encode_method is None:
             encode_method = self.cfg.point.encode_method
         if encode_scale is None:
             encode_scale = self.cfg.point.encode_scale
         if key not in self._point_dict:
             if encode_method is not None:
                 if attrs is None:
-                    ui = three.Points(limit).prop(encodeMethod=encode_method, encodeScale=encode_scale, colorMap=three.ColorMap(min=points[:, 2].min(), max=points[:, 2].max()))
+                    ui = three.Points(limit).prop(encodeMethod=encode_method,
+                                                  encodeScale=encode_scale,
+                                                  colorMap=three.ColorMap(
+                                                      min=points[:, 2].min(),
+                                                      max=points[:, 2].max()))
                 else:
-                    assert attr_fields is not None 
-                    ui = three.Points(limit).prop(encodeMethod=encode_method, encodeScale=encode_scale, attrs=attrs, attrFields=attr_fields, colorMap=three.ColorMap(min=points[:, 2].min(), max=points[:, 2].max()))
+                    assert attr_fields is not None
+                    ui = three.Points(limit).prop(encodeMethod=encode_method,
+                                                  encodeScale=encode_scale,
+                                                  attrs=attrs,
+                                                  attrFields=attr_fields,
+                                                  colorMap=three.ColorMap(
+                                                      min=points[:, 2].min(),
+                                                      max=points[:, 2].max()))
             else:
-                ui = three.Points(limit).prop(colorMap=three.ColorMap(min=points[:, 2].min(), max=points[:, 2].max()))
+                ui = three.Points(limit).prop(colorMap=three.ColorMap(
+                    min=points[:, 2].min(), max=points[:, 2].max()))
             self._point_dict[key] = ui
             await self._dynamic_pcs.update_childs({key: ui})
         point_ui = self._point_dict[key]
-        await point_ui.update_points(points,
-                                     colors,
-                                     limit=limit,
-                                     size=self.cfg.point.size if size is None else size,
-                                     sizes=sizes,
-                                     size_attenuation=size_attenuation,
-                                     encode_method=encode_method, 
-                                     encode_scale=encode_scale,
-                                     attrs=attrs,
-                                     attr_fields=attr_fields)
+        await point_ui.update_points(
+            points,
+            colors,
+            limit=limit,
+            size=self.cfg.point.size if size is None else size,
+            sizes=sizes,
+            size_attenuation=size_attenuation,
+            encode_method=encode_method,
+            encode_scale=encode_scale,
+            attrs=attrs,
+            attr_fields=attr_fields)
         return point_ui
 
     async def clear_points(self, clear_keys: Optional[List[str]] = None):
         if clear_keys is None:
             clear_keys = list(self._point_dict.keys())
         for k in clear_keys:
             await self._point_dict[k].clear()
@@ -683,44 +711,60 @@
         await ui.update_lines(lines, limit=limit)
 
     async def clear_all_lines(self):
         # TODO currently no way to clear lines without unmount
         self._segment_dict.clear()
         await self._dynamic_lines.set_new_layout({})
 
-    async def show_voxels(self,
-                         key: str,
-                         centers: np.ndarray,
-                         colors: Union[np.ndarray, str],
-                         size: float,
-                         limit: int):
+    async def show_voxels(self, key: str, centers: np.ndarray,
+                          colors: Union[np.ndarray,
+                                        str], size: float, limit: int):
         if key not in self._voxels_dict:
             # ui = three.VoxelMesh(centers, size, limit, [
             #     three.MeshStandardMaterial().prop(vertexColors=isinstance(colors, np.ndarray), color=colors if isinstance(colors, str) else mui.undefined),
             # ], colors=colors if isinstance(colors, np.ndarray) else mui.undefined)
-            ui = three.InstancedMesh(centers, limit, [
-                three.BoxGeometry(size, size, size),
-                three.MeshBasicMaterial().prop(vertexColors=False, color=colors if isinstance(colors, str) else mui.undefined),
-            ], colors=colors if isinstance(colors, np.ndarray) else mui.undefined)
+            ui = three.InstancedMesh(
+                centers,
+                limit, [
+                    three.BoxGeometry(size, size, size),
+                    three.MeshBasicMaterial().prop(
+                        vertexColors=False,
+                        color=colors
+                        if isinstance(colors, str) else mui.undefined),
+                ],
+                colors=colors
+                if isinstance(colors, np.ndarray) else mui.undefined)
             self._voxels_dict[key] = ui
             await self._dynamic_voxels.update_childs({key: ui})
-            return 
+            return
         ui = self._voxels_dict[key]
         limit_prev = ui.props.limit
         assert not isinstance(limit_prev, mui.Undefined)
         if isinstance(ui, three.InstancedMesh):
             if limit <= limit_prev:
-                await ui.send_and_wait(ui.update_event(size=size, colors=colors, transforms=centers))
+                await ui.send_and_wait(
+                    ui.update_event(size=size,
+                                    colors=colors,
+                                    transforms=centers))
             else:
-                await ui.send_and_wait(ui.update_event(size=size, colors=colors, transforms=centers, limit=limit))
+                await ui.send_and_wait(
+                    ui.update_event(size=size,
+                                    colors=colors,
+                                    transforms=centers,
+                                    limit=limit))
         else:
             if limit <= limit_prev:
-                await ui.send_and_wait(ui.update_event(size=size, colors=colors, centers=centers))
+                await ui.send_and_wait(
+                    ui.update_event(size=size, colors=colors, centers=centers))
             else:
-                await ui.send_and_wait(ui.update_event(size=size, colors=colors, centers=centers, limit=limit))
+                await ui.send_and_wait(
+                    ui.update_event(size=size,
+                                    colors=colors,
+                                    centers=centers,
+                                    limit=limit))
 
     async def clear_all_voxels(self):
         # TODO currently no way to clear lines without unmount
         self._voxels_dict.clear()
         await self._dynamic_voxels.set_new_layout({})
 
     async def show_image(self, key: str, image: np.ndarray,
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/collection.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/collection.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 from runpy import run_path
 import shlex
 
 
 class SimpleFileReader(mui.FlexBox):
     """support json/pickle (.json/.pkl/.pickle) and numpy (.npy/.npz) files
     """
+
     def __init__(self):
         self.text = mui.Typography("Drop file here")
         self.text.prop(color="secondary")
         self.text.prop(align="center")
         self.text.prop(variant="body2")
 
         super().__init__([self.text])
@@ -59,38 +60,39 @@
                 data = dict(data)
         else:
             raise NotImplementedError
         await self.text.write(f"Loaded {file.name}")
         await appctx.inspector.set_object(data, "droppedFile")
 
 
-
 class ScriptExecutor(mui.FlexBox):
+
     def __init__(self):
         self.path = mui.TextField(label="Path").prop(muiMargin="dense")
         self.args = mui.TextField(label="Args").prop(muiMargin="dense")
 
-        super().__init__(
-            [self.path, self.args,
-             mui.HBox([
-                 mui.Button("Run", self._run),
-                 mui.Button("Cancel", self._cancel),
-
-             ])])
+        super().__init__([
+            self.path, self.args,
+            mui.HBox([
+                mui.Button("Run", self._run),
+                mui.Button("Cancel", self._cancel),
+            ])
+        ])
         self.prop(flexDirection="column")
         self._external_argv_task: Optional[asyncio.Future] = None
 
     async def _run(self):
         if self._external_argv_task is not None:
             raise RuntimeError("already running")
         self._external_argv_task = asyncio.create_task(
             appctx.run_in_executor_with_exception_inspect(
                 partial(self._run_app_script,
                         path=self.path.str(),
-                        argv=shlex.split(" ".join([self.path.str(), self.args.str()]))), ))
+                        argv=shlex.split(" ".join(
+                            [self.path.str(), self.args.str()]))), ))
 
     async def _cancel(self):
         if self._external_argv_task is None:
             raise RuntimeError("not running")
         self._external_argv_task.cancel()
 
     def _run_app_script(self, path: str, argv: List[str]):
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/config.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/config.py`

 * *Files 1% similar despite different names*

```diff
@@ -426,15 +426,16 @@
                 if res is not None:
                     child_node.type = mui.ControlNodeType.RangeNumber.value
                     if first_anno_meta.default is not None:
                         if isinstance(child_node.initValue, mui.Undefined):
                             child_node.initValue = first_anno_meta.default
                     child_node.min = first_anno_meta.lo
                     child_node.max = first_anno_meta.hi
-                    child_node.isInteger = isinstance(first_anno_meta, typemetas.RangedInt)
+                    child_node.isInteger = isinstance(first_anno_meta,
+                                                      typemetas.RangedInt)
                     child_node.alias = mui.undefined if first_anno_meta.alias is None else first_anno_meta.alias
                     child_node.step = mui.undefined if first_anno_meta.step is None else first_anno_meta.step
                     if first_anno_meta.step is None and ty is int:
                         child_node.step = 1
                     res_node.children.append(child_node)
                     obj_uid_to_meta[child_node.id] = res
                     continue
@@ -456,17 +457,21 @@
             res_node.children.append(child_node)
             obj_uid_to_meta[child_node.id] = res
             continue
 
     return res_node
 
 
-def control_nodes_v1_to_v2(ctrl_node_v1: mui.ControlNode, uid_to_json_like_node: Dict[str, mui.JsonLikeNode]) -> mui.JsonLikeNode:
+def control_nodes_v1_to_v2(
+        ctrl_node_v1: mui.ControlNode,
+        uid_to_json_like_node: Dict[str,
+                                    mui.JsonLikeNode]) -> mui.JsonLikeNode:
     childs: List[mui.JsonLikeNode] = [
-        control_nodes_v1_to_v2(c, uid_to_json_like_node) for c in ctrl_node_v1.children
+        control_nodes_v1_to_v2(c, uid_to_json_like_node)
+        for c in ctrl_node_v1.children
     ]
     ctrl_desp = mui.ControlDesp(type=ctrl_node_v1.type,
                                 initValue=ctrl_node_v1.initValue,
                                 min=ctrl_node_v1.min,
                                 max=ctrl_node_v1.max,
                                 step=ctrl_node_v1.step,
                                 selects=ctrl_node_v1.selects,
@@ -482,14 +487,15 @@
         children=childs,
         userdata=ctrl_desp)
     uid_to_json_like_node[node.id.uid_encoded] = node
     return node
 
 
 class ConfigPanel(mui.DynamicControls):
+
     def __init__(self,
                  config_obj: Any,
                  callback: Optional[Callable[[str, Any],
                                              mui._CORO_NONE]] = None):
         assert dataclasses.is_dataclass(config_obj)
         # parse config dataclass.
         self._obj_to_ctrl_meta: Dict[str, ControlItemMeta] = {}
@@ -554,29 +560,31 @@
                     alias: Optional[str] = None):
         return {
             _CONFIG_META_KEY: SliderMeta(begin=begin, end=end, alias=alias)
         }
 
 
 class ConfigPanelV2(mui.SimpleControls):
+
     def __init__(self,
                  config_obj: Any,
                  callback: Optional[Callable[[str, Any],
                                              mui._CORO_NONE]] = None,
                  ignored_field_names: Optional[Set[str]] = None):
         assert dataclasses.is_dataclass(config_obj)
         # parse config dataclass.
         self._obj_to_ctrl_meta: Dict[str, ControlItemMeta] = {}
         self.uid_to_json_like_node: Dict[str, mui.JsonLikeNode] = {}
         node = parse_to_control_nodes(config_obj,
                                       config_obj,
                                       "",
                                       self._obj_to_ctrl_meta,
                                       ignored_field_names=ignored_field_names)
-        super().__init__(init=control_nodes_v1_to_v2(node, self.uid_to_json_like_node).children,
+        super().__init__(init=control_nodes_v1_to_v2(
+            node, self.uid_to_json_like_node).children,
                          callback=self.callback)
         self.__config_obj = config_obj
         self.__callback_key = "config_panel_v3_handler"
         if callback is not None:
             self.register_event_handler(self.__callback_key,
                                         callback,
                                         backend_only=True)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/core.py`

 * *Files 15% similar despite different names*

```diff
@@ -22,15 +22,14 @@
 CustomTreeItemHandler: full control
 
 TreeItem: control self and direct child
 
 UserObjTree: none
 """
 
-
 import abc
 import dataclasses
 import enum
 import inspect
 import types
 from functools import partial
 from typing import (Any, Callable, Dict, Hashable, Iterable, List, Optional,
@@ -49,104 +48,123 @@
 class PriorityCommon(enum.IntEnum):
     Lowest = 0
     Low = 20
     Normal = 40
     High = 60
     Highest = 80
 
+
 @dataclasses.dataclass
 class ObjectGridItemConfig:
-    width: float  = 1.0
+    width: float = 1.0
     height: float = 1.0
     priority: int = 0
 
     # used for internal layout only
     x: int = 0
     y: int = 0
     w: int = 0
     h: int = 0
 
+
 USER_OBJ_TREE_TYPES: Set[Any] = {UserObjTree}
 
+
 def register_user_obj_tree_type(type):
     USER_OBJ_TREE_TYPES.add(type)
 
+
 class ObjectPreviewHandler(mui.FlexBox):
+
     @abc.abstractmethod
     async def bind(self, obj: Any, uid: str):
         pass
 
 
 class ObjectLayoutHandler(abc.ABC):
+
     @abc.abstractmethod
     def create_layout(self, obj: Any) -> mui.FlexBox:
         raise NotImplementedError
 
     def get_grid_layout_item(self, obj: Any) -> ObjectGridItemConfig:
         return ObjectGridItemConfig(1.0, 1.0)
 
+
 class ObjectLayoutCreator(abc.ABC):
 
     @abc.abstractmethod
     def create(self) -> mui.FlexBox:
         raise NotImplementedError
 
-class ObjectLayoutHandlerRegistry(HashableRegistryKeyOnly[Type[ObjectLayoutHandler]]):
+
+class ObjectLayoutHandlerRegistry(
+        HashableRegistryKeyOnly[Type[ObjectLayoutHandler]]):
+
     def check_type_exists(self, type: Type) -> bool:
         qname = get_qualname_of_type(type)
         if type in self:
-            return True 
+            return True
         return qname in self
- 
 
-ALL_OBJECT_PREVIEW_HANDLERS: HashableRegistryKeyOnly[Type[ObjectPreviewHandler]] = HashableRegistryKeyOnly(allow_duplicate=True)
 
-ALL_OBJECT_LAYOUT_HANDLERS: ObjectLayoutHandlerRegistry = ObjectLayoutHandlerRegistry(allow_duplicate=True)
+ALL_OBJECT_PREVIEW_HANDLERS: HashableRegistryKeyOnly[
+    Type[ObjectPreviewHandler]] = HashableRegistryKeyOnly(allow_duplicate=True)
+
+ALL_OBJECT_LAYOUT_HANDLERS: ObjectLayoutHandlerRegistry = ObjectLayoutHandlerRegistry(
+    allow_duplicate=True)
+
+ALL_OBJECT_LAYOUT_CREATORS: HashableRegistryKeyOnly[
+    Type[ObjectLayoutCreator]] = HashableRegistryKeyOnly()
 
-ALL_OBJECT_LAYOUT_CREATORS: HashableRegistryKeyOnly[Type[ObjectLayoutCreator]] = HashableRegistryKeyOnly()
 
 class ContextMenuType(enum.Enum):
     DataStorageStore = 0
     DataStorageItemDelete = 1
     DataStorageItemCommand = 2
 
     CopyReadItemCode = 3
 
+
 class DataClassesType:
     """a placeholder that used for custom handlers.
     user need to register this type to make sure
     handler is used if object is dataclass.
     """
     pass
 
 
 class CustomTreeItemHandler(abc.ABC):
     """
     TODO should we use lazy load in TreeItem?
     """
+
     @abc.abstractmethod
     async def get_childs(self, obj: Any) -> Optional[Dict[str, Any]]:
         """if return None, we will use default method to extract childs
         of object.
         """
         raise NotImplementedError
-    
+
     @abc.abstractmethod
-    def patch_node(self, obj: Any, node: JsonLikeNode) -> Optional[JsonLikeNode]:
+    def patch_node(self, obj: Any,
+                   node: JsonLikeNode) -> Optional[JsonLikeNode]:
         """modify/patch node created from `parse_obj_to_tree_node`
         """
-    
-    async def handle_button(self, obj_trace: List[Any], node_trace: List[JsonLikeNode], button_id: str) -> Optional[bool]:
+
+    async def handle_button(self, obj_trace: List[Any],
+                            node_trace: List[JsonLikeNode],
+                            button_id: str) -> Optional[bool]:
         return None
-    
-    async def handle_context_menu(self, obj_trace: List[Any], node_trace: List[JsonLikeNode], userdata: Dict[str, Any]) -> Optional[bool]:
+
+    async def handle_context_menu(self, obj_trace: List[Any],
+                                  node_trace: List[JsonLikeNode],
+                                  userdata: Dict[str, Any]) -> Optional[bool]:
         return None
 
 
 def register_obj_preview_handler(cls):
     return ALL_OBJECT_PREVIEW_HANDLERS.register(cls)
 
 
 def register_obj_layout_handler(cls):
     return ALL_OBJECT_LAYOUT_HANDLERS.register(cls)
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/figure.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/figure.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/grid_preview_layout.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/grid_preview_layout.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,26 +17,27 @@
     ALL_OBJECT_LAYOUT_HANDLERS, ObjectGridItemConfig, ObjectLayoutHandler,
     DataClassesType)
 # from tensorpc.flow.flowapp.components.plus.handlers.common import DefaultHandler
 
 from typing import List, Tuple
 
 
-
 def layout_rectangles_with_priority(
         rectangles: List[Tuple[int, int, int]],
         bounding_width: int) -> List[Tuple[int, int, Tuple[int, int]]]:
     # author: copilot
     # Sort rectangles by height
     rectangles_with_index = [(i, x) for i, x in enumerate(rectangles)]
     rectangles_with_index.sort(key=lambda r: (r[1][1], r[1][2]), reverse=True)
 
     # Initialize variables
     x, y, row_height = 0, 0, 0
-    layout: List[Tuple[int, int, Tuple[int, int]]] = [(0, 0, (0, 0)) for _ in range(len(rectangles))]
+    layout: List[Tuple[int, int,
+                       Tuple[int, int]]] = [(0, 0, (0, 0))
+                                            for _ in range(len(rectangles))]
 
     # Layout rectangles
     for i, rectangle in rectangles_with_index:
         width, height, _ = rectangle
 
         # If rectangle doesn't fit in current row, start a new row
         if x + width > bounding_width:
@@ -51,15 +52,20 @@
         row_height = max(row_height, height)
 
     return layout
 
 
 class GridPreviewContainer(mui.FlexBox):
 
-    def __init__(self, preview_layout: mui.FlexBox, name: str, close_callback: Optional[Callable[[], Coroutine[None, None, None]]] = None):
+    def __init__(self,
+                 preview_layout: mui.FlexBox,
+                 name: str,
+                 close_callback: Optional[Callable[[],
+                                                   Coroutine[None, None,
+                                                             None]]] = None):
         super().__init__({
             "header":
             mui.HBox([
                 mui.HBox([
                     mui.Icon(mui.IconType.DragIndicator).prop(iconSize="small")
                 ]).prop(className="grid-layout-drag-handle",
                         alignItems="center",
@@ -81,19 +87,21 @@
 @dataclasses.dataclass
 class _GridLayoutItem:
     obj: Any
     layout: mui.FlexBox
     item_cfg: ObjectGridItemConfig
     is_preview_layout: bool
 
+
 @dataclasses.dataclass
 class _GridLayoutedItem:
     layout_item: _GridLayoutItem
     grid_item: mui.GridItem
 
+
 class GridPreviewLayout(mui.FlexBox):
 
     def __init__(self,
                  init_children: Dict[str, Any],
                  tree_root: Optional[UserObjTreeProtocol] = None,
                  max_cols: int = 4,
                  width_rate: int = 4,
@@ -120,22 +128,21 @@
     def _check_type_support_preview(self, type: Type) -> bool:
         if type in self._type_to_handler_object:
             return True
         reload_mgr = self.flow_app_comp_core.reload_mgr
         metas = reload_mgr.query_type_method_meta(type, True, True)
         special_methods = FlowSpecialMethods(metas)
         if special_methods.create_preview_layout is not None:
-            return True 
+            return True
         return ALL_OBJECT_LAYOUT_HANDLERS.check_type_exists(type)
 
     def _parse_obj_to_grid_item(self, obj: Any):
         from tensorpc.flow.flowapp import appctx
         if isinstance(obj, mui.FlexBox):
-            obj_grid_item = obj.find_user_meta_by_type(
-                ObjectGridItemConfig)
+            obj_grid_item = obj.find_user_meta_by_type(ObjectGridItemConfig)
             if obj_grid_item is None:
                 obj_grid_item = ObjectGridItemConfig(1.0, 1.0)
             return _GridLayoutItem(obj, obj, obj_grid_item, False)
         obj_type = type(obj)
 
         reload_mgr = appctx.get_reload_manager()
         is_dcls = dataclasses.is_dataclass(obj)
@@ -149,16 +156,15 @@
             handler = self._type_to_handler_object[DataClassesType]
         else:
             if special_methods.create_preview_layout is not None:
                 if self._tree_root is None:
                     preview_layout = mui.flex_preview_wrapper(
                         obj, metas, reload_mgr)
                 else:
-                    with self._tree_root.enter_context(
-                            self._tree_root):
+                    with self._tree_root.enter_context(self._tree_root):
                         preview_layout = mui.flex_preview_wrapper(
                             obj, metas, reload_mgr)
             else:
                 obj_qualname = get_qualname_of_type(type(obj))
                 handler_type: Optional[Type[ObjectLayoutHandler]] = None
                 if obj is not None:
                     # check standard type first, if not found, check datasetclass type.
@@ -172,15 +178,16 @@
                 if handler_type is not None:
                     handler = handler_type()
         if preview_layout is not None:
             preview_grid_item = preview_layout.find_user_meta_by_type(
                 ObjectGridItemConfig)
             if preview_grid_item is None:
                 preview_grid_item = ObjectGridItemConfig(1.0, 1.0)
-            return _GridLayoutItem(obj, preview_layout, preview_grid_item, True)
+            return _GridLayoutItem(obj, preview_layout, preview_grid_item,
+                                   True)
         elif handler is not None:
             layout = handler.create_layout(obj)
             item = handler.get_grid_layout_item(obj)
             assert isinstance(
                 layout,
                 mui.FlexBox), "you must return a mui Flexbox in create_layout"
             assert isinstance(
@@ -212,33 +219,37 @@
                  preview_layouts_before_packing, rect_layout):
             item = items[name]
             item.item_cfg.x = new_layout[0]
             item.item_cfg.y = new_layout[1]
             item.item_cfg.w = layout_w
             item.item_cfg.h = layout_h
 
-    def _grid_layout_items_to_ui_items(self, name_to_grid_items: Dict[str, _GridLayoutItem], use_typename_as_title: bool):
+    def _grid_layout_items_to_ui_items(
+            self, name_to_grid_items: Dict[str, _GridLayoutItem],
+            use_typename_as_title: bool):
         grid_items: List[_GridLayoutedItem] = []
         for name, grid_layout_item in name_to_grid_items.items():
             obj = grid_layout_item.obj
             obj_type = type(obj)
             obj_type_name = obj_type.__name__
-            container = GridPreviewContainer(grid_layout_item.layout, obj_type_name if use_typename_as_title else name)
+            container = GridPreviewContainer(
+                grid_layout_item.layout,
+                obj_type_name if use_typename_as_title else name)
             if grid_layout_item.is_preview_layout:
                 get_editable_app().observe_layout(
                     grid_layout_item.layout,
                     partial(self._on_preview_layout_reload,
                             container=container))
             item = mui.GridItem(
-                    container, name,
-                    mui.GridItemProps(i=name,
-                                      x=grid_layout_item.item_cfg.x,
-                                      y=grid_layout_item.item_cfg.y,
-                                      w=grid_layout_item.item_cfg.w,
-                                      h=grid_layout_item.item_cfg.h))
+                container, name,
+                mui.GridItemProps(i=name,
+                                  x=grid_layout_item.item_cfg.x,
+                                  y=grid_layout_item.item_cfg.y,
+                                  w=grid_layout_item.item_cfg.w,
+                                  h=grid_layout_item.item_cfg.h))
             grid_items.append(_GridLayoutedItem(grid_layout_item, item))
         return grid_items
 
     @mark_create_layout
     def _layout_func(self):
         # res = mui.FlexBox()
         if self._tree_root is not None:
@@ -249,38 +260,38 @@
         name_to_grid_item: Dict[str, _GridLayoutItem] = {}
         for name, obj in self._init_children.items():
             grid_item = self._parse_obj_to_grid_item(obj)
             if grid_item is None:
                 continue
             name_to_grid_item[name] = grid_item
         self._layout_items_inplace(name_to_grid_item)
-        grid_items: List[_GridLayoutedItem] = self._grid_layout_items_to_ui_items(name_to_grid_item, self.use_typename_as_title)
+        grid_items: List[
+            _GridLayoutedItem] = self._grid_layout_items_to_ui_items(
+                name_to_grid_item, self.use_typename_as_title)
         # res.init_add_layout([
         #     mui.GridLayout(preview_layouts_v2).prop(flex=1, cols=12, draggableHandle=".grid-layout-drag-handle", rowHeight=300)
         # ])
         # print(preview_layouts_v2, self._init_children)
         self.prop(flexDirection="row", flex=1, width="100%", height="100%")
         self.grid_items = grid_items
         return [
-            mui.GridLayout([x.grid_item for x in grid_items]).prop(
-                flex=1,
-                cols=int(cols),
-                draggableHandle=".grid-layout-drag-handle",
-                rowHeight=50
-                )
+            mui.GridLayout([x.grid_item for x in grid_items
+                            ]).prop(flex=1,
+                                    cols=int(cols),
+                                    draggableHandle=".grid-layout-drag-handle",
+                                    rowHeight=50)
         ]
 
     async def _on_preview_layout_reload(self, layout: mui.FlexBox,
                                         create_layout: ServFunctionMeta,
                                         container: GridPreviewContainer):
         # print("DO PREVIEW LAYOUT RELOAD", create_layout.user_app_meta)
         ctx = nullcontext()
         if self._tree_root is not None:
-            ctx = self._tree_root.enter_context(
-                    self._tree_root)
+            ctx = self._tree_root.enter_context(self._tree_root)
         with ctx:
             layout_flex = await preview_layout_reload(
                 lambda x: container.update_childs({"layout": x}), layout,
                 create_layout)
         if layout_flex is not None:
             get_editable_app().observe_layout(
                 layout_flex,
@@ -296,61 +307,86 @@
         name_to_grid_item: Dict[str, _GridLayoutItem] = {}
         for name, obj in item.items():
             grid_item = self._parse_obj_to_grid_item(obj)
             if grid_item is None:
                 continue
             name_to_grid_item[name] = grid_item
         self._layout_items_inplace(name_to_grid_item)
-        grid_items: List[_GridLayoutedItem] = self._grid_layout_items_to_ui_items(name_to_grid_item, self.use_typename_as_title)
+        grid_items: List[
+            _GridLayoutedItem] = self._grid_layout_items_to_ui_items(
+                name_to_grid_item, self.use_typename_as_title)
         self.grid_items = grid_items
         cols = self.width_rate * self.max_cols
-        await self.set_new_layout([mui.GridLayout([x.grid_item for x in grid_items]).prop(
-                flex=1,
-                cols=cols,
-                draggableHandle=".grid-layout-drag-handle",
-                rowHeight=50)])
+        await self.set_new_layout([
+            mui.GridLayout([x.grid_item for x in grid_items
+                            ]).prop(flex=1,
+                                    cols=cols,
+                                    draggableHandle=".grid-layout-drag-handle",
+                                    rowHeight=50)
+        ])
 
     async def update_items(self, item: Dict[str, Any]):
         name_to_grid_item: Dict[str, _GridLayoutItem] = {}
         prev_layouted_items = self.grid_items.copy()
-        name_to_prev_layouted_items = {x.grid_item.name: x for x in prev_layouted_items}
+        name_to_prev_layouted_items = {
+            x.grid_item.name: x
+            for x in prev_layouted_items
+        }
         for name, obj in item.items():
             grid_item = self._parse_obj_to_grid_item(obj)
             if grid_item is None:
                 continue
             name_to_grid_item[name] = grid_item
             if name in name_to_prev_layouted_items:
                 name_to_prev_layouted_items.pop(name)
-        name_to_grid_item.update({x.grid_item.name: x.layout_item for x in name_to_prev_layouted_items.values()})
+        name_to_grid_item.update({
+            x.grid_item.name: x.layout_item
+            for x in name_to_prev_layouted_items.values()
+        })
         self._layout_items_inplace(name_to_grid_item)
-        grid_items: List[_GridLayoutedItem] = self._grid_layout_items_to_ui_items(name_to_grid_item, self.use_typename_as_title)
+        grid_items: List[
+            _GridLayoutedItem] = self._grid_layout_items_to_ui_items(
+                name_to_grid_item, self.use_typename_as_title)
         cols = self.width_rate * self.max_cols
-        await self.set_new_layout([mui.GridLayout([x.grid_item for x in grid_items]).prop(
-                flex=1,
-                cols=cols,
-                draggableHandle=".grid-layout-drag-handle",
-                rowHeight=50)])
+        await self.set_new_layout([
+            mui.GridLayout([x.grid_item for x in grid_items
+                            ]).prop(flex=1,
+                                    cols=cols,
+                                    draggableHandle=".grid-layout-drag-handle",
+                                    rowHeight=50)
+        ])
 
     async def delete_items(self, item: List[str]):
         name_to_grid_item: Dict[str, _GridLayoutItem] = {}
         prev_layouted_items = self.grid_items.copy()
-        name_to_prev_layouted_items = {x.grid_item.name: x for x in prev_layouted_items}
+        name_to_prev_layouted_items = {
+            x.grid_item.name: x
+            for x in prev_layouted_items
+        }
         for name in name_to_prev_layouted_items.keys():
             if name in item:
                 name_to_prev_layouted_items.pop(name)
-        name_to_grid_item.update({x.grid_item.name: x.layout_item for x in name_to_prev_layouted_items.values()})
+        name_to_grid_item.update({
+            x.grid_item.name: x.layout_item
+            for x in name_to_prev_layouted_items.values()
+        })
         self._layout_items_inplace(name_to_grid_item)
-        grid_items: List[_GridLayoutedItem] = self._grid_layout_items_to_ui_items(name_to_grid_item, self.use_typename_as_title)
+        grid_items: List[
+            _GridLayoutedItem] = self._grid_layout_items_to_ui_items(
+                name_to_grid_item, self.use_typename_as_title)
         cols = self.width_rate * self.max_cols
-        await self.set_new_layout([mui.GridLayout([x.grid_item for x in grid_items]).prop(
-                flex=1,
-                cols=cols,
-                draggableHandle=".grid-layout-drag-handle",
-                rowHeight=50)])
-        
+        await self.set_new_layout([
+            mui.GridLayout([x.grid_item for x in grid_items
+                            ]).prop(flex=1,
+                                    cols=cols,
+                                    draggableHandle=".grid-layout-drag-handle",
+                                    rowHeight=50)
+        ])
+
     async def clear_items(self):
         cols = self.width_rate * self.max_cols
-        await self.set_new_layout([mui.GridLayout([]).prop(
-                flex=1,
-                cols=cols,
-                draggableHandle=".grid-layout-drag-handle",
-                rowHeight=50)])
+        await self.set_new_layout([
+            mui.GridLayout([]).prop(flex=1,
+                                    cols=cols,
+                                    draggableHandle=".grid-layout-drag-handle",
+                                    rowHeight=50)
+        ])
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/common.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/common.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 import inspect
 from pathlib import PosixPath, WindowsPath
 import traceback
 from typing import Any, Dict
 
 import numpy as np
-import io 
+import io
 from tensorpc.core.moduleid import get_qualname_of_type
 from tensorpc.core.serviceunit import ObservedFunction
 from tensorpc.flow.flowapp import appctx
 from tensorpc.flow.flowapp.components import mui
 from tensorpc.flow.flowapp.components.plus.canvas import SimpleCanvas
 from tensorpc.flow.flowapp.components.plus.config import ConfigPanelV2
 
 from ..common import CommonQualNames
 from ..core import ALL_OBJECT_PREVIEW_HANDLERS, ObjectPreviewHandler, DataClassesType
 from ..arraygrid import NumpyArrayGrid
 
-
 monospace_14px = dict(fontFamily="monospace", fontSize="14px")
 _MAX_STRING_IN_DETAIL = 10000
 
 
 @ALL_OBJECT_PREVIEW_HANDLERS.register(np.ndarray)
 @ALL_OBJECT_PREVIEW_HANDLERS.register(CommonQualNames.TorchTensor)
 @ALL_OBJECT_PREVIEW_HANDLERS.register(CommonQualNames.TVTensor)
@@ -28,16 +27,17 @@
 
     def __init__(self) -> None:
         self.tags = mui.FlexBox().prop(flexFlow="row wrap")
         self.title = mui.Typography("np.ndarray shape = []")
         self.data_print = mui.Typography("").prop(fontFamily="monospace",
                                                   fontSize="12px",
                                                   whiteSpace="pre-wrap")
-        self.slice_val = mui.TextField("Slice", callback=self._slice_change).prop(size="small",
-                                                 muiMargin="dense")
+        self.slice_val = mui.TextField(
+            "Slice", callback=self._slice_change).prop(size="small",
+                                                       muiMargin="dense")
         self.grid_container = mui.HBox([])
         dialog = mui.Dialog([
             self.grid_container.prop(flex=1, height="70vh", width="100%")
         ]).prop(title="Array Viewer", maxWidth="xl", fullWidth=True)
         self.dialog = dialog
 
         layout = [
@@ -60,15 +60,17 @@
         self.prop(flexDirection="column", flex=1)
         self.obj: Any = np.zeros([1])
         self.obj_uid: str = ""
         self._tensor_slices: Dict[str, str] = {}
 
     async def _on_show_viewer_dialog(self):
         await self.grid_container.set_new_layout([
-             NumpyArrayGrid(self.obj).prop(width="100%", height="100%", overflow="hidden")
+            NumpyArrayGrid(self.obj).prop(width="100%",
+                                          height="100%",
+                                          overflow="hidden")
         ])
         await self.dialog.set_open(True)
 
     async def _on_show_slice(self):
         slice_eval_expr = f"a{self.slice_val.value}"
         try:
             res = eval(slice_eval_expr, {"a": self.obj})
@@ -84,25 +86,25 @@
             res = res.cpu().numpy()
         else:
             res = res
         await self.data_print.write(str(res))
 
     async def _slice_change(self, value: str):
         if self.obj_uid != "":
-             self._tensor_slices[self.obj_uid] = value
+            self._tensor_slices[self.obj_uid] = value
 
     async def _on_3d_vis(self):
         if self.obj_uid in self._tensor_slices:
             slice_eval_expr = f"a{self._tensor_slices[self.obj_uid]}"
         else:
             slice_eval_expr = "a"
         slice_eval_expr = f"a{self._tensor_slices[self.obj_uid]}"
         res = eval(slice_eval_expr, {"a": self.obj})
         canvas = appctx.find_component(SimpleCanvas)
-        assert canvas is not None 
+        assert canvas is not None
         await canvas._unknown_visualization(self.obj_uid, res)
 
     async def bind(self, obj, uid: str):
         # bind np object, update all metadata
         qualname = "np.ndarray"
         device = None
         dtype = obj.dtype
@@ -158,21 +160,21 @@
             tags.append(
                 mui.Chip("non-contiguous").prop(muiColor="warning",
                                                 size="small",
                                                 clickable=False))
         if hasnan:
             tags.append(
                 mui.Chip("nan").prop(muiColor="error",
-                                    size="small",
-                                    clickable=False))
+                                     size="small",
+                                     clickable=False))
         if hasinf:
             tags.append(
                 mui.Chip("inf").prop(muiColor="error",
-                                    size="small",
-                                    clickable=False))
+                                     size="small",
+                                     clickable=False))
         await self.tags.set_new_layout([*tags])
 
 
 @ALL_OBJECT_PREVIEW_HANDLERS.register(str)
 @ALL_OBJECT_PREVIEW_HANDLERS.register(int)
 @ALL_OBJECT_PREVIEW_HANDLERS.register(float)
 @ALL_OBJECT_PREVIEW_HANDLERS.register(complex)
@@ -210,34 +212,34 @@
         self.prop(flexDirection="column")
 
     async def bind(self, obj: ObservedFunction, uid: str):
         await self.qualname.write(obj.qualname)
         await self.path.write(obj.path)
 
 
-
 @ALL_OBJECT_PREVIEW_HANDLERS.register(DataClassesType)
 class DataclassesHandler(ObjectPreviewHandler):
 
     def __init__(self) -> None:
         self.cfg_ctrl_container = mui.Fragment([])
-        super().__init__(
-            [self.cfg_ctrl_container])
+        super().__init__([self.cfg_ctrl_container])
         self.prop(flexDirection="column", flex=1)
 
     async def bind(self, obj: Any, uid: str):
         # for uncontrolled component, use react_key to force remount.
         # TODO currently no way to update if obj dataclass def is changed with same uid.
         panel = ConfigPanelV2(obj).prop(reactKey=uid)
         await self.cfg_ctrl_container.set_new_layout([panel])
 
+
 class DefaultHandler(ObjectPreviewHandler):
     """
     TODO if the object support any-layout, add a button to enable it.
     """
+
     def __init__(self) -> None:
         self.tags = mui.FlexBox().prop(flexFlow="row wrap")
         self.title = mui.Typography("").prop(wordBreak="break-word")
         self.path = mui.Typography("").prop(wordBreak="break-word")
 
         self.data_print = mui.Typography("").prop(fontFamily="monospace",
                                                   fontSize="12px",
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/handlers/gv_common.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/handlers/gv_common.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 import tensorpc.core.dataclass_dispatch as dataclasses
 import inspect
 from pathlib import PosixPath, WindowsPath
 import traceback
 from typing import Any, Dict, List, Union
 
 import numpy as np
-import io 
+import io
 from tensorpc.core.moduleid import get_qualname_of_type
 from tensorpc.core.serviceunit import ObservedFunction
 from tensorpc.flow.flowapp import appctx
 from tensorpc.flow.flowapp.components import mui
 from tensorpc.flow.flowapp.components.plus.canvas import SimpleCanvas
 from tensorpc.flow.flowapp.components.plus.config import ConfigPanelV2
 
 from ..common import CommonQualNames
 from ..core import ALL_OBJECT_LAYOUT_HANDLERS, ObjectGridItemConfig, ObjectLayoutHandler, DataClassesType, PriorityCommon
 
-
 monospace_14px = dict(fontFamily="monospace", fontSize="14px")
 _MAX_STRING_IN_DETAIL = 10000
 
 
 @dataclasses.dataclass
 class TensorMeta:
     qualname: str
@@ -28,44 +27,46 @@
     dtype: str
     device: str
     is_contiguous: bool
     hasnan: bool
     hasinf: bool
     is_float: bool
     min_value: Union[float, int]
-    max_value: Union[float, int] 
+    max_value: Union[float, int]
 
     def get_tags(self):
         tags = [
             mui.Chip(str(self.dtype)).prop(size="small", clickable=False),
         ]
         if self.device is not None:
-            tags.append(mui.Chip(self.device).prop(size="small", clickable=False))
+            tags.append(
+                mui.Chip(self.device).prop(size="small", clickable=False))
         if self.is_contiguous:
             tags.append(
                 mui.Chip("contiguous").prop(muiColor="success",
                                             size="small",
                                             clickable=False))
         else:
             tags.append(
                 mui.Chip("non-contiguous").prop(muiColor="warning",
                                                 size="small",
                                                 clickable=False))
         if self.hasnan:
             tags.append(
                 mui.Chip("nan").prop(muiColor="error",
-                                    size="small",
-                                    clickable=False))
+                                     size="small",
+                                     clickable=False))
         if self.hasinf:
             tags.append(
                 mui.Chip("inf").prop(muiColor="error",
-                                    size="small",
-                                    clickable=False))
+                                     size="small",
+                                     clickable=False))
         return tags
 
+
 def _get_tensor_meta(obj):
     qualname = "np.ndarray"
     device = None
     dtype = obj.dtype
     is_contig = False
     hasnan = False
     hasinf = False
@@ -102,15 +103,16 @@
         hasnan = bool(np.isnan(obj_cpu).any().item())
         hasinf = bool(np.isinf(obj_cpu).any().item())
         min_value = obj_cpu.min().item()
         max_value = obj_cpu.max().item()
         is_float = np.issubdtype(dtype, np.floating)
     else:
         raise NotImplementedError
-    return TensorMeta(qualname, shape, str(dtype), device, is_contig, is_float, hasnan, hasinf, min_value, max_value)
+    return TensorMeta(qualname, shape, str(dtype), device, is_contig, is_float,
+                      hasnan, hasinf, min_value, max_value)
 
 
 class TensorPreview(mui.FlexBox):
 
     def __init__(self, obj) -> None:
         meta = _get_tensor_meta(obj)
         self.meta = meta
@@ -128,21 +130,23 @@
         layout = [
             self.title.prop(fontSize="14px", fontFamily="monospace"),
             self.tags,
         ]
         super().__init__(layout)
         self.prop(flexDirection="column", flex=1)
 
+
 @ALL_OBJECT_LAYOUT_HANDLERS.register(np.ndarray)
 @ALL_OBJECT_LAYOUT_HANDLERS.register(CommonQualNames.TorchTensor)
 @ALL_OBJECT_LAYOUT_HANDLERS.register(CommonQualNames.TVTensor)
 class TensorHandler(ObjectLayoutHandler):
+
     def create_layout(self, obj: Any) -> mui.FlexBox:
         res = TensorPreview(obj)
-        return res 
+        return res
 
     def get_grid_layout_item(self, obj: Any) -> ObjectGridItemConfig:
         meta = _get_tensor_meta(obj)
         priority = 0
         if meta.hasinf or meta.hasnan:
             priority = PriorityCommon.Highest
         return ObjectGridItemConfig(1.0, 0.5, int(priority))
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/monitor.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/monitor.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,37 +25,40 @@
 import asyncio
 import psutil
 import io
 import csv
 from dataclasses import dataclass
 import humanize
 
+
 @dataclass
 class GPUMeasure:
     name: str
     gpuusage: int
     memusage: int
     temperature: int
     memused: int
     memtotal: int
 
     def to_string(self):
         msg = f"gpu={self.gpuusage}%,mem={self.memused}/{self.memtotal}MB,"
         msg += f"{self.temperature}\u2103,io={self.memusage}%"
         return msg
 
-@dataclass 
+
+@dataclass
 class NetworkMeasure:
-    name: str 
+    name: str
     bytes_recv: int
     bytes_sent: int
-    recv_speed: float 
+    recv_speed: float
     send_speed: float
     timestamp_second: float
 
+
 @dataclass
 class GPUMonitor:
     util: mui.CircularProgress
     mem: mui.CircularProgress
 
 
 class ComputeResourceMonitor(mui.FlexBox):
@@ -72,15 +75,17 @@
         for i in range(num_gpu):
             util = mui.CircularProgress().prop(color="blue",
                                                variant="determinate")
             mem = mui.CircularProgress().prop(color="sliver",
                                               variant="determinate")
             self.gpus.append(GPUMonitor(util, mem))
             gpu_uis.extend([mui.Divider("vertical"), util, mem])
-        self.net_info_str = mui.Typography("").prop(whiteSpace="pre-wrap", fontSize="14px", fontFamily="monospace")
+        self.net_info_str = mui.Typography("").prop(whiteSpace="pre-wrap",
+                                                    fontSize="14px",
+                                                    fontFamily="monospace")
         super().__init__([
             mui.HBox([
                 self.cpu,
                 self.mem,
                 *gpu_uis,
             ]),
             self.net_info_str,
@@ -108,27 +113,31 @@
         for nic, stats in all_stats.items():
             if stats.isup:
                 io_counters = all_io_counters[nic]
                 ts = time.time()
                 if nic in self.prev_net_measures:
                     prev_measure = prev_measures[nic]
                     recv_speed = (io_counters.bytes_recv -
-                                  prev_measure.bytes_recv) / (ts - prev_measure.timestamp_second)
+                                  prev_measure.bytes_recv) / (
+                                      ts - prev_measure.timestamp_second)
                     send_speed = (io_counters.bytes_sent -
-                                  prev_measure.bytes_sent) / (ts - prev_measure.timestamp_second)
+                                  prev_measure.bytes_sent) / (
+                                      ts - prev_measure.timestamp_second)
                 else:
                     recv_speed = 0
                     send_speed = 0
-                measure = NetworkMeasure(nic, io_counters.bytes_recv, io_counters.bytes_sent, recv_speed, send_speed, ts)
+                measure = NetworkMeasure(nic, io_counters.bytes_recv,
+                                         io_counters.bytes_sent, recv_speed,
+                                         send_speed, ts)
                 self.prev_net_measures[nic] = measure
                 network_measures.append(measure)
         return network_measures
-        
+
     def _get_gpu_measures(self) -> List[GPUMeasure]:
-        
+
         querys = [
             "gpu_name",
             "utilization.gpu",
             "utilization.memory",
             "temperature.gpu",
             "memory.used",
             "memory.total",
@@ -158,21 +167,25 @@
             return gpumeasures
         except:
             return []
 
     async def get_resource(self):
         while True:
             cpu_percent = psutil.cpu_percent()
-            
+
             vm_percent = psutil.virtual_memory().percent
             net_measures = self._get_network_measure()
-            net_info_strs = [f"{m.name}: recv={humanize.naturalsize(m.recv_speed)}/s, send={humanize.naturalsize(m.send_speed)}/s" for m in net_measures]
+            net_info_strs = [
+                f"{m.name}: recv={humanize.naturalsize(m.recv_speed)}/s, send={humanize.naturalsize(m.send_speed)}/s"
+                for m in net_measures
+            ]
             ev = self.cpu.update_event(value=cpu_percent)
             ev += self.mem.update_event(value=vm_percent)
-            ev += self.net_info_str.update_event(value="\n".join(net_info_strs))
+            ev += self.net_info_str.update_event(
+                value="\n".join(net_info_strs))
             if len(self.gpus) > 0:
                 gpumeasures: List[GPUMeasure] = self._get_gpu_measures()
                 for g, m in zip(gpumeasures, self.gpus):
                     ev += m.util.update_event(value=g.gpuusage)
                     ev += m.mem.update_event(value=g.memused / g.memtotal *
                                              100)
             await self.send_and_wait(ev)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/analysis.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/analysis.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,54 +2,62 @@
 import dataclasses
 import enum
 import inspect
 from re import T
 import traceback
 import types
 from pathlib import Path, PurePath
-from typing import (Any, Callable, Dict, Hashable, Iterable, List, Mapping, Optional,
-                    Set, Tuple, Type, TypeVar, Union)
+from typing import (Any, Callable, Dict, Hashable, Iterable, List, Mapping,
+                    Optional, Set, Tuple, Type, TypeVar, Union)
 import contextvars
 from tensorpc.core import inspecttools
 from tensorpc.core.serviceunit import ReloadableDynamicClass
 from tensorpc.core.tree_id import UniqueTreeIdForTree
 from tensorpc.flow.flowapp.components import mui
 from tensorpc.flow.flowapp.components.plus.core import (
-    ALL_OBJECT_LAYOUT_HANDLERS, USER_OBJ_TREE_TYPES, CustomTreeItemHandler, ObjectLayoutCreator)
+    ALL_OBJECT_LAYOUT_HANDLERS, USER_OBJ_TREE_TYPES, CustomTreeItemHandler,
+    ObjectLayoutCreator)
 from tensorpc.flow.flowapp.core import FlowSpecialMethods
 from tensorpc.flow.jsonlike import (IconButtonData, TreeItem,
                                     parse_obj_to_jsonlike)
 # GLOBAL_SPLIT = "::"
 STRING_LENGTH_LIMIT = 500
 _IGNORE_ATTR_NAMES = set(["_abc_impl", "__abstractmethods__"])
 
 SET_CONTAINER_LIMIT_SIZE = 50
 
+
 class ButtonType(enum.Enum):
     Reload = "reload"
     Delete = "delete"
     Watch = "watch"
     Record = "record"
 
-_SHOULD_EXPAND_TYPES = {mui.JsonLikeType.List.value, mui.JsonLikeType.Tuple.value,
-                        mui.JsonLikeType.Dict.value, mui.JsonLikeType.Object.value,
-                            mui.JsonLikeType.ListFolder.value, mui.JsonLikeType.DictFolder.value,
-                            mui.JsonLikeType.Layout.value}
+
+_SHOULD_EXPAND_TYPES = {
+    mui.JsonLikeType.List.value, mui.JsonLikeType.Tuple.value,
+    mui.JsonLikeType.Dict.value, mui.JsonLikeType.Object.value,
+    mui.JsonLikeType.ListFolder.value, mui.JsonLikeType.DictFolder.value,
+    mui.JsonLikeType.Layout.value
+}
 
 
 class ObjectTreeParser:
     """expandable: determine a object can be expand
     parseable: determine a object can be parsed to JsonLikeNode
     attr_parseable: determine a attribute of a object can be parsed to JsonLikeNode
     """
-    def __init__(self, 
-                 cared_types: Optional[Set[Type]] = None,
-                 ignored_types: Optional[Set[Type]] = None,
-                 custom_type_expanders: Optional[Dict[Type, Callable[[Any], dict]]] = None,
-                 custom_tree_item_handler: Optional[CustomTreeItemHandler] = None):
+
+    def __init__(
+            self,
+            cared_types: Optional[Set[Type]] = None,
+            ignored_types: Optional[Set[Type]] = None,
+            custom_type_expanders: Optional[Dict[Type, Callable[[Any],
+                                                                dict]]] = None,
+            custom_tree_item_handler: Optional[CustomTreeItemHandler] = None):
         if cared_types is None:
             cared_types = set()
         if ignored_types is None:
             ignored_types = set()
         self._cared_types = cared_types
         self._ignored_types = ignored_types
         self._obj_meta_cache = {}
@@ -60,17 +68,21 @@
         self.custom_tree_item_handler = custom_tree_item_handler
 
     def parseable(self, obj, check_obj: bool = True):
         if not self._check_is_valid(obj) and check_obj:
             return False
         if inspecttools.is_obj_builtin_or_module(obj):
             return False
-        return True 
+        return True
 
-    def attr_parseable(self, obj, attr_name: str, user_defined_prop_keys: Set[str], check_obj: bool = True):
+    def attr_parseable(self,
+                       obj,
+                       attr_name: str,
+                       user_defined_prop_keys: Set[str],
+                       check_obj: bool = True):
         res = self.parseable(obj, check_obj)
         if not res:
             return False, None
         if attr_name.startswith("__"):
             return False, None
         if attr_name in _IGNORE_ATTR_NAMES:
             return False, None
@@ -88,15 +100,15 @@
         if len(self._cared_types) != 0:
             valid &= obj_type in self._cared_types
         if len(self._ignored_types) != 0:
             valid &= obj_type not in self._ignored_types
         return valid
 
     def _valid_check(self, obj_type):
-        return True 
+        return True
 
     async def expand_object(self, obj, check_obj: bool = True):
         if not self.parseable(obj, check_obj):
             return {}
         res: Dict[Any, Any] = {}
         if isinstance(obj, (list, tuple, set)):
             if isinstance(obj, set):
@@ -108,37 +120,42 @@
         elif isinstance(obj, dict):
             # return {k: v for k, v in obj.items() if not _is_obj_builtin_or_module(v)}
             return obj
         elif isinstance(obj, tuple(USER_OBJ_TREE_TYPES)):
             return obj.get_childs()
         elif isinstance(obj, TreeItem):
             # this is very special, we need to lazy access the child of a treeitem.
-            return await obj.get_child_desps(UniqueTreeIdForTree("")) 
+            return await obj.get_child_desps(UniqueTreeIdForTree(""))
         if self.custom_tree_item_handler is not None:
             res_tmp = await self.custom_tree_item_handler.get_childs(obj)
             if res_tmp is not None:
                 return res_tmp
         for k, v in self._custom_type_expanders.items():
             if isinstance(obj, k):
                 return v(obj)
-        user_defined_prop_keys = inspecttools.get_obj_userdefined_properties(obj)
+        user_defined_prop_keys = inspecttools.get_obj_userdefined_properties(
+            obj)
         for k in dir(obj):
-            valid, attr = self.attr_parseable(obj, k, user_defined_prop_keys, check_obj)
+            valid, attr = self.attr_parseable(obj, k, user_defined_prop_keys,
+                                              check_obj)
             if not valid:
                 continue
             res[k] = attr
         return res
 
-    async def parse_obj_to_tree_node(self, obj, name: str, obj_meta_cache=None):
+    async def parse_obj_to_tree_node(self,
+                                     obj,
+                                     name: str,
+                                     obj_meta_cache=None):
         obj_type = type(obj)
         try:
             isinst = isinstance(obj, TreeItem)
         except:
             print("???", type(obj))
-            raise 
+            raise
         uid_name = UniqueTreeIdForTree.from_parts([name])
         if isinst:
             node_candidate = obj.get_json_like_node(uid_name)
             if node_candidate is not None:
                 return node_candidate
         node = parse_obj_to_jsonlike(obj, name, uid_name)
         if isinstance(obj, mui.JsonLikeNode):
@@ -176,92 +193,99 @@
                 is_draggable = obj._flow_reference_count == 0
             if isinstance(obj, ObjectLayoutCreator):
                 is_draggable = True
                 is_layout = True
             is_draggable = True
             if is_layout:
                 t = mui.JsonLikeType.Layout
-            return mui.JsonLikeNode(uid_name,
-                                    name,
-                                    t.value,
-                                    value=value,
-                                    typeStr=obj_type.__qualname__,
-                                    cnt=count,
-                                    drag=is_draggable,
-                                    iconBtns=[
-                                        IconButtonData(ButtonType.Reload.value,
-                                                    mui.IconType.Refresh.value,
-                                                    "Reload Object")
-                                    ])
+            return mui.JsonLikeNode(
+                uid_name,
+                name,
+                t.value,
+                value=value,
+                typeStr=obj_type.__qualname__,
+                cnt=count,
+                drag=is_draggable,
+                iconBtns=[
+                    IconButtonData(ButtonType.Reload.value,
+                                   mui.IconType.Refresh.value, "Reload Object")
+                ])
         return node
 
-    async def parse_obj_dict_to_nodes(self, obj_dict: Mapping[Any, Any],
-                    ns: UniqueTreeIdForTree, obj_meta_cache=None):
+    async def parse_obj_dict_to_nodes(self,
+                                      obj_dict: Mapping[Any, Any],
+                                      ns: UniqueTreeIdForTree,
+                                      obj_meta_cache=None):
         res_node: List[mui.JsonLikeNode] = []
         for k, v in obj_dict.items():
             str_k = str(k)
-            node = await self.parse_obj_to_tree_node(v,
-                                str_k, obj_meta_cache)
+            node = await self.parse_obj_to_tree_node(v, str_k, obj_meta_cache)
             if self.custom_tree_item_handler is not None:
                 node_tmp = self.custom_tree_item_handler.patch_node(v, node)
                 if node_tmp is not None:
                     node = node_tmp
             # node.id = f"{ns}{GLOBAL_SPLIT}{str_k}"
             node.id = ns.append_part(str_k)
             if not isinstance(k, str):
                 node.dictKey = mui.BackendOnlyProp(k)
             res_node.append(node)
         return res_node
 
-    async def parse_obj_to_tree(self, obj, node: mui.JsonLikeNode, total_expand_level: int = 0):
+    async def parse_obj_to_tree(self,
+                                obj,
+                                node: mui.JsonLikeNode,
+                                total_expand_level: int = 0):
         """parse object to json like tree.
         """
         if not self._should_expand_node(obj, node, total_expand_level):
-            return 
+            return
         if isinstance(obj, TreeItem):
             obj_dict = await obj.get_child_desps(node.id)
             # for k, v in obj_dict.items():
             #     # v.id = f"{node.id}{GLOBAL_SPLIT}{v.id}"
             #     v.id = node.id + v.id
             tree_children = list(obj_dict.values())
         else:
             obj_dict = await self.expand_object(obj)
-            tree_children = await self.parse_obj_dict_to_nodes(obj_dict, node.id, self._obj_meta_cache)
+            tree_children = await self.parse_obj_dict_to_nodes(
+                obj_dict, node.id, self._obj_meta_cache)
         node.children = tree_children
         node.cnt = len(obj_dict)
         for (k, v), child_node in zip(obj_dict.items(), node.children):
             # should_expand = child_node.id in self._cached_lazy_expand_uids or total_expand_level > 0
             # if isinstance(v, TreeItem) and v.default_expand():
             #     should_expand = True
             # if should_expand:
-            await self.parse_obj_to_tree(v, child_node,
-                            total_expand_level - 1)
-    
-    def _should_expand_node(self, obj, node: mui.JsonLikeNode, total_expand_level: int):
+            await self.parse_obj_to_tree(v, child_node, total_expand_level - 1)
+
+    def _should_expand_node(self, obj, node: mui.JsonLikeNode,
+                            total_expand_level: int):
         if node.type not in _SHOULD_EXPAND_TYPES:
             return False
         should_expand = node.id in self._cached_lazy_expand_uids or total_expand_level > 0
         if isinstance(obj, TreeItem) and obj.default_expand():
             should_expand = True
-        elif isinstance(obj, tuple(USER_OBJ_TREE_TYPES)) and obj.default_expand():
+        elif isinstance(obj,
+                        tuple(USER_OBJ_TREE_TYPES)) and obj.default_expand():
             should_expand = True
         return should_expand
 
-
     def update_lazy_expand_uids(self, new_uid: UniqueTreeIdForTree):
         # if we lazy-expand a node, we should remove all its children from cached_lazy_expand_uids
         new_lazy_expand_uids: List[UniqueTreeIdForTree] = list(
             filter(lambda n: not n.startswith(new_uid),
-                    self._cached_lazy_expand_uids))
+                   self._cached_lazy_expand_uids))
         new_lazy_expand_uids.append(new_uid)
         self._cached_lazy_expand_uids = set(new_lazy_expand_uids)
 
-    def get_obj_single_attr(self, obj,
-                            key: str,
-                            check_obj: bool = True) -> Union[mui.Undefined, Any]:
+    def get_obj_single_attr(
+            self,
+            obj,
+            key: str,
+            check_obj: bool = True) -> Union[mui.Undefined, Any]:
         # if isinstance(obj, (list, tuple, set)):
         #     try:
         #         key_int = int(key)
         #     except:
         #         return mui.undefined
         #     if key_int < 0 or key_int >= len(obj):
         #         return mui.undefined
@@ -287,38 +311,39 @@
                 v = getattr(obj, key)
             except:
                 return mui.undefined
             if not (self._check_is_valid(v)):
                 return mui.undefined
             if isinstance(v, types.ModuleType):
                 return mui.undefined
-            if inspect.isfunction(v) or inspect.ismethod(v) or inspect.isbuiltin(
-                    v):
+            if inspect.isfunction(v) or inspect.ismethod(
+                    v) or inspect.isbuiltin(v):
                 return mui.undefined
             return v
         return mui.undefined
 
-
     async def get_obj_by_uid(
         self,
         obj,
         uid: UniqueTreeIdForTree,
         real_keys: Optional[List[Union[mui.Undefined, Hashable]]] = None
     ) -> Tuple[Any, bool]:
         parts = uid.parts
         if real_keys is None:
             real_keys = [mui.undefined for _ in range(len(parts))]
         if len(parts) == 1:
             return obj, True
         # uid contains root, remove it at first.
-        return await self.get_obj_by_uid_resursive(obj, parts[1:], real_keys[1:])
-
+        return await self.get_obj_by_uid_resursive(obj, parts[1:],
+                                                   real_keys[1:])
 
     async def get_obj_by_uid_resursive(
-            self, obj, parts: List[str], real_keys: List[Union[mui.Undefined, Hashable]]) -> Tuple[Any, bool]:
+            self, obj, parts: List[str],
+            real_keys: List[Union[mui.Undefined,
+                                  Hashable]]) -> Tuple[Any, bool]:
         key = parts[0]
         real_key = real_keys[0]
         if isinstance(obj, (list, tuple, set)):
             if isinstance(obj, set):
                 obj_list = list(obj)
             else:
                 obj_list = obj
@@ -351,36 +376,36 @@
                 child_obj = self.get_obj_single_attr(obj, key, check_obj=False)
                 if isinstance(obj, mui.Undefined):
                     return obj, False
         if len(parts) == 1:
             return child_obj, True
         else:
             return await self.get_obj_by_uid_resursive(child_obj, parts[1:],
-                                                real_keys[1:])
-
+                                                       real_keys[1:])
 
     async def get_obj_by_uid_trace(
-        self, 
+        self,
         obj,
         uid: UniqueTreeIdForTree,
         real_keys: Optional[List[Union[mui.Undefined, Hashable]]] = None
     ) -> Tuple[Any, bool]:
         parts = uid.parts
         if real_keys is None:
             real_keys = [mui.undefined for _ in range(len(parts))]
         if len(parts) == 1:
             return [obj], True
         # uid contains root, remove it at first.
         trace, found = await self.get_obj_by_uid_trace_resursive(
             obj, parts[1:], real_keys[1:])
         return [obj] + trace, found
 
-
     async def get_obj_by_uid_trace_resursive(
-            self, obj, parts: List[str], real_keys: List[Union[mui.Undefined, Hashable]] ) -> Tuple[List[Any], bool]:
+        self, obj, parts: List[str],
+        real_keys: List[Union[mui.Undefined,
+                              Hashable]]) -> Tuple[List[Any], bool]:
         key = parts[0]
         real_key = real_keys[0]
         if isinstance(obj, (list, tuple, set)):
             if isinstance(obj, set):
                 obj_list = list(obj)
             else:
                 obj_list = obj
@@ -416,47 +441,57 @@
         if len(parts) == 1:
             return [child_obj], True
         else:
             trace, found = await self.get_obj_by_uid_trace_resursive(
                 child_obj, parts[1:], real_keys[1:])
             return [child_obj] + trace, found
 
-
-    async def get_root_tree(self, obj_root, root_name: str, expand_level: int, ns: UniqueTreeIdForTree = UniqueTreeIdForTree("")):
-        root_node = await self.parse_obj_to_tree_node(obj_root, root_name, self._obj_meta_cache)
+    async def get_root_tree(self,
+                            obj_root,
+                            root_name: str,
+                            expand_level: int,
+                            ns: UniqueTreeIdForTree = UniqueTreeIdForTree("")):
+        root_node = await self.parse_obj_to_tree_node(obj_root, root_name,
+                                                      self._obj_meta_cache)
         if not ns.empty():
             # root_node.id = f"{ns}{GLOBAL_SPLIT}{root_node.id}"
             root_node.id = ns + root_node.id
         await self.parse_obj_to_tree(obj_root, root_node, expand_level)
         return root_node
 
+
 T = TypeVar("T")
 
+
 class TreeContext:
-    def __init__(self, parser: ObjectTreeParser, tree: Union[mui.JsonLikeTree, mui.TanstackJsonLikeTree], tree_instance: Any) -> None:
+
+    def __init__(self, parser: ObjectTreeParser,
+                 tree: Union[mui.JsonLikeTree, mui.TanstackJsonLikeTree],
+                 tree_instance: Any) -> None:
         self.parser = parser
         self.tree = tree
         self._tree_instance = tree_instance
 
     def get_tree_instance(self, tree_type: Type[T]) -> T:
         assert isinstance(self._tree_instance, tree_type)
         return self._tree_instance
 
+
 TREE_CONTEXT_VAR: contextvars.ContextVar[
-    Optional[TreeContext]] = contextvars.ContextVar("treectx",
-                                                   default=None)
+    Optional[TreeContext]] = contextvars.ContextVar("treectx", default=None)
 
 
 def get_tree_context() -> Optional[TreeContext]:
     return TREE_CONTEXT_VAR.get()
 
+
 def get_tree_context_noexcept() -> TreeContext:
     res = TREE_CONTEXT_VAR.get()
-    assert res is not None 
-    return res 
+    assert res is not None
+    return res
 
 
 @contextlib.contextmanager
 def enter_tree_conetxt(ctx: TreeContext):
     """expose tree apis for user defined tree items.
     """
     token = TREE_CONTEXT_VAR.set(ctx)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/controllers.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/controllers.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,92 +14,107 @@
 
 import threading
 from types import FrameType
 from typing import Any, Callable, Optional, List, Dict, TypeVar, Generic, Union
 from tensorpc.core import inspecttools
 from tensorpc.flow.flowapp.components import mui
 
-from tensorpc.flow.marker import mark_autorun, mark_create_preview_layout, mark_did_mount, mark_will_unmount 
+from tensorpc.flow.marker import mark_autorun, mark_create_preview_layout, mark_did_mount, mark_will_unmount
 import inspect
 import asyncio
 from tensorpc.flow.flowapp.appcore import get_app
 
-
 T = TypeVar("T")
 
+
 class CallbackSlider(mui.FlexBox):
     """a slider that used for list.
     """
 
     def __init__(self) -> None:
         self.slider = mui.Slider(0, 1, 1).prop(flex=1)
         super().__init__([self.slider])
         self.slider.register_event_handler(mui.FrontendEventType.Change.value,
-                                    self._default_callback)
-        self.prop(width="100%", flexFlow="row nowrap", paddingLeft="5px", paddingRight="5px")
+                                           self._default_callback)
+        self.prop(width="100%",
+                  flexFlow="row nowrap",
+                  paddingLeft="5px",
+                  paddingRight="5px")
 
     async def _default_callback(self, index):
         pass
-    
+
     @mark_create_preview_layout
     def tensorpc_flow_preview_layout(self):
         return self
 
-    async def update_callback(self, length: int, cb: Callable[[Any], mui._CORO_NONE]):
+    async def update_callback(self, length: int, cb: Callable[[Any],
+                                                              mui._CORO_NONE]):
         self.slider.register_event_handler(mui.FrontendEventType.Change.value,
-                                    cb)
+                                           cb)
         await self.slider.update_ranges(0, length - 1, 1)
 
 
 class ThreadLocker(mui.FlexBox):
     """a locker designed for long-time running sync function, you can
     use locker to wait for GUI release. e.g., you run a deeplearning
     train function, you can visualize intermediate results in GUI (via capture), and
     wait for GUI release to continue training.
     """
+
     def __init__(self) -> None:
         self.text = mui.Markdown("### Thread Locker\n:green[Unlocked]")
         self.event = threading.Event()
         super().__init__([
             self.text,
             mui.HBox([
                 mui.Button("Release", self._on_release),
                 mui.Button("Raise", self._on_raise),
                 mui.Button("Capture", self._on_capture),
-            ], wrap=True),
+            ],
+                     wrap=True),
         ])
-        self.prop(width="100%", flexFlow="column", paddingLeft="5px", paddingRight="5px")
+        self.prop(width="100%",
+                  flexFlow="column",
+                  paddingLeft="5px",
+                  paddingRight="5px")
 
         self._prev_frame: Optional[FrameType] = None
 
         self._need_raise: bool = False
 
     async def _on_release(self):
         self.event.set()
 
     async def _on_raise(self):
         self._need_raise = True
         self.event.set()
-    
+
     async def _on_capture(self):
         """capture current locals to inspector.
         """
-        from tensorpc.flow import appctx , plus
+        from tensorpc.flow import appctx, plus
         if self._prev_frame is None:
-            return 
+            return
         frame_name = self._prev_frame.f_code.co_name
         inspector = appctx.find_component(plus.ObjectInspector)
-        assert inspector is not None 
+        assert inspector is not None
         local_vars = self._prev_frame.f_locals.copy()
-        await inspector.tree.set_object(inspecttools.filter_local_vars(local_vars),
-                                   "locals" + f"-{frame_name}")
-
-    def wait_sync(self, loop: Optional[asyncio.AbstractEventLoop] = None, msg: str = "", *, _frame_cnt: int = 1):
-        assert get_app()._flowapp_thread_id != threading.get_ident(), "you must use this function in a thread."
-        self._need_raise = False 
+        await inspector.tree.set_object(
+            inspecttools.filter_local_vars(local_vars),
+            "locals" + f"-{frame_name}")
+
+    def wait_sync(self,
+                  loop: Optional[asyncio.AbstractEventLoop] = None,
+                  msg: str = "",
+                  *,
+                  _frame_cnt: int = 1):
+        assert get_app()._flowapp_thread_id != threading.get_ident(
+        ), "you must use this function in a thread."
+        self._need_raise = False
         cur_frame = inspect.currentframe()
         assert cur_frame is not None
         frame = cur_frame
         while _frame_cnt > 0:
             frame = cur_frame.f_back
             assert frame is not None
             cur_frame = frame
@@ -112,36 +127,38 @@
         self._prev_frame = cur_frame
         print(f"ThreadLocker wait {msg}")
         self.event.clear()
         self.event.wait()
         fut = asyncio.run_coroutine_threadsafe(
             self.text.write("### Thread Locker\n:green[Unlocked]"), loop)
 
-        self._prev_frame = None 
+        self._prev_frame = None
         if self._need_raise == True:
-            self._need_raise = False 
-            raise ValueError("You click raise exception button in ThreadLocker.")
+            self._need_raise = False
+            raise ValueError(
+                "You click raise exception button in ThreadLocker.")
 
     @mark_will_unmount
     async def _unmount(self):
         self.event.set()
 
     @mark_did_mount
     async def _mount(self):
         self.event.clear()
 
+
 class MarkdownViewer(mui.FlexBox):
     """this class is existed for standard InspectPanel-based usage.
     When you add it to your layout, you can access it by 
     appctx.find_component(plus.MarkdownViewer).
     this class has been added to Inspector builtins.
     """
+
     def __init__(self) -> None:
         self.info = mui.Markdown()
         super().__init__([
             self.info,
         ])
         self.prop(width="100%")
 
     async def write(self, content: str):
         return await self.info.write(content)
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/inspector.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/inspector.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
 from tensorpc.flow.flowapp.components import mui, three
 from tensorpc.flow.flowapp.components.plus.objinspect.treeitems import TraceTreeItem, parse_frame_result_to_trace_item
 from tensorpc.flow.flowapp.components.plus.reload_utils import preview_layout_reload
 from tensorpc.flow.flowapp.core import FlowSpecialMethods, FrontendEventType, _get_obj_def_path
 from tensorpc.flow.flowapp.objtree import UserObjTreeProtocol
 from ..handlers.common import DefaultHandler
 from ..core import (ALL_OBJECT_PREVIEW_HANDLERS, USER_OBJ_TREE_TYPES,
-                   ObjectPreviewHandler, DataClassesType)
+                    ObjectPreviewHandler, DataClassesType)
 from .tree import _DEFAULT_OBJ_NAME, FOLDER_TYPES, ObjectTree
 from tensorpc.core import inspecttools
 
 _DEFAULT_LOCALS_NAME = "locals"
 
 _MAX_STRING_IN_DETAIL = 10000
 P = ParamSpec('P')
@@ -63,14 +63,15 @@
     for tb_frame, tb_lineno in traceback.walk_tb(exc_traceback):
         fr = Tracer.get_frame_result(TraceType.Return, tb_frame)
         frame_stacks[fr.get_unique_id()] = TraceTreeItem(fr)
     return frame_stacks
 
 
 class ObjectInspector(mui.FlexBox):
+
     def __init__(self,
                  init: Optional[Any] = None,
                  cared_types: Optional[Set[Type]] = None,
                  ignored_types: Optional[Set[Type]] = None,
                  with_detail: bool = True,
                  use_allotment: bool = True,
                  enable_exception_inspect: bool = True,
@@ -78,56 +79,60 @@
                  fixed_size: bool = False) -> None:
         self.preview_container = mui.HBox([]).prop(overflow="auto",
                                                    padding="3px",
                                                    flex=1,
                                                    width="100%",
                                                    height="100%")
         self.fast_layout_container = mui.HBox([]).prop(overflow="auto",
-                                                   padding="3px",
-                                                   flex=1,
-                                                   width="100%",
-                                                   height="100%")
+                                                       padding="3px",
+                                                       flex=1,
+                                                       width="100%",
+                                                       height="100%")
 
         tab_theme = mui.Theme(
             components={
                 "MuiTab": {
                     "styleOverrides": {
                         "root": {
                             "padding": "0",
                             "minWidth": "28px",
                             "minHeight": "28px",
                         }
                     }
                 }
             })
-            
+
         self.detail_container = mui.HBox([
             mui.ThemeProvider([
                 mui.Tabs([
                     mui.TabDef("",
                                "1",
                                self.preview_container,
                                icon=mui.IconType.Preview,
                                tooltip="preview layout of item"),
                     mui.TabDef("",
                                "2",
                                mui.AppTerminal(),
                                icon=mui.IconType.Terminal,
                                tooltip="app terminal (read only)"),
-                    mui.TabDef("",
-                               "3",
-                               self.fast_layout_container,
-                               icon=mui.IconType.ManageAccounts,
-                               tooltip="custom layout (appctx.inspector.set_custom_layout_sync)"),
-
-                ], init_value="2").prop(panelProps=mui.FlexBoxProps(width="100%", padding=0),
-                        orientation="vertical",
-                        borderRight=1,
-                        borderColor='divider',
-                        tooltipPlacement="right")
+                    mui.TabDef(
+                        "",
+                        "3",
+                        self.fast_layout_container,
+                        icon=mui.IconType.ManageAccounts,
+                        tooltip=
+                        "custom layout (appctx.inspector.set_custom_layout_sync)"
+                    ),
+                ],
+                         init_value="2").prop(panelProps=mui.FlexBoxProps(
+                             width="100%", padding=0),
+                                              orientation="vertical",
+                                              borderRight=1,
+                                              borderColor='divider',
+                                              tooltipPlacement="right")
             ], tab_theme)
         ])
         if use_allotment:
             self.detail_container.prop(height="100%")
         else:
             self.detail_container.prop(flex=1)
         self._cached_preview_layouts: Dict[str, Tuple[mui.FlexBox, int]] = {}
@@ -174,28 +179,30 @@
         self._type_to_handler_object: Dict[Type[Any],
                                            ObjectPreviewHandler] = {}
         self._current_preview_layout: Optional[mui.FlexBox] = None
 
     async def get_object_by_uid(self, uid: str):
         return await self.tree.get_object_by_uid(uid)
 
-    async def _on_select(self, uid_list: Union[List[str], str, Dict[str, bool]]):
+    async def _on_select(self, uid_list: Union[List[str], str, Dict[str,
+                                                                    bool]]):
         if isinstance(uid_list, list):
             # node id list may empty
             if not uid_list:
                 return
             uid = uid_list[0]
         elif isinstance(uid_list, dict):
             if not uid_list:
                 return
             uid = list(uid_list.keys())[0]
         else:
             uid = uid_list
         uid_obj = UniqueTreeIdForTree(uid)
-        nodes = self.tree._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
+        nodes = self.tree._objinspect_root._get_node_by_uid_trace(
+            uid_obj.parts)
         node = nodes[-1]
         if node.type in FOLDER_TYPES:
             await self.preview_container.set_new_layout([])
             return
         obj, found = await self.tree._get_obj_by_uid_with_folder(uid, nodes)
         if not found:
             raise ValueError(
@@ -266,36 +273,48 @@
         if preview_layout is not None:
             if root is not None:
                 preview_layout.set_flow_event_context_creator(
                     lambda: root.enter_context(root))
             # preview_layout.event_emitter.remove_listener()
             if self._current_preview_layout is None:
                 get_editable_app().observe_layout(
-                    preview_layout, partial(self._on_preview_layout_reload, uid=uid, obj_id=id(obj)))
+                    preview_layout,
+                    partial(self._on_preview_layout_reload,
+                            uid=uid,
+                            obj_id=id(obj)))
             else:
                 # get_app()._get_self_as_editable_app()._flowapp_remove_observer(
                 #     self._current_preview_layout)
                 get_editable_app().observe_layout(
-                    preview_layout, partial(self._on_preview_layout_reload, uid=uid, obj_id=id(obj)))
+                    preview_layout,
+                    partial(self._on_preview_layout_reload,
+                            uid=uid,
+                            obj_id=id(obj)))
             self._current_preview_layout = preview_layout
             self._cached_preview_layouts[uid] = (preview_layout, id(obj))
 
             # self.__install_preview_event_listeners(preview_layout)
             await self.preview_container.set_new_layout([preview_layout])
         else:
             childs = list(self.preview_container._child_comps.values())
             if not childs or childs[0] is not handler:
                 await self.preview_container.set_new_layout([handler])
             await handler.bind(obj, uid)
 
     async def _on_preview_layout_reload(self, layout: mui.FlexBox,
-                                        create_layout: ServFunctionMeta, uid: str, obj_id: int):
-        layout_flex = await preview_layout_reload(lambda x: self.preview_container.set_new_layout([x]), layout, create_layout)
+                                        create_layout: ServFunctionMeta,
+                                        uid: str, obj_id: int):
+        layout_flex = await preview_layout_reload(
+            lambda x: self.preview_container.set_new_layout([x]), layout,
+            create_layout)
         if layout_flex is not None:
-            get_editable_app().observe_layout(layout_flex, partial(self._on_preview_layout_reload, uid=uid, obj_id=obj_id))
+            get_editable_app().observe_layout(
+                layout_flex,
+                partial(self._on_preview_layout_reload, uid=uid,
+                        obj_id=obj_id))
             self._cached_preview_layouts[uid] = (layout_flex, obj_id)
             return layout_flex
 
     async def set_object(self,
                          obj,
                          key: str = _DEFAULT_OBJ_NAME,
                          expand_level: int = 0):
@@ -382,25 +401,24 @@
             # return fut
         else:
             # we can wait fut here.
             fut = asyncio.run_coroutine_threadsafe(
                 self.set_object(obj, key, expand_level), loop)
 
             return fut.result()
-        
-    async def set_custom_layout(self,
-                        layout: mui.FlexBox):
+
+    async def set_custom_layout(self, layout: mui.FlexBox):
         """set object in sync manner, usually used on non-sync code via appctx.
         """
         await self.fast_layout_container.set_new_layout([layout])
 
-
-    def set_custom_layout_sync(self,
-                        layout: mui.FlexBox,
-                        loop: Optional[asyncio.AbstractEventLoop] = None):
+    def set_custom_layout_sync(
+            self,
+            layout: mui.FlexBox,
+            loop: Optional[asyncio.AbstractEventLoop] = None):
         """set object in sync manner, usually used on non-sync code via appctx.
         """
         if loop is None:
             loop = asyncio.get_running_loop()
         if get_app()._flowapp_thread_id == threading.get_ident():
             # we can't wait fut here
             task = asyncio.create_task(self.set_custom_layout(layout))
@@ -470,15 +488,15 @@
                    traced_types: Optional[Tuple[Type]] = None,
                    traced_names: Optional[Set[str]] = None,
                    traced_folders: Optional[Set[str]] = None,
                    trace_return: bool = True,
                    depth: int = 5,
                    use_return_locals: bool = False,
                    ignored_names: Optional[Set[str]] = None,
-                    use_profile: bool = False,
+                   use_profile: bool = False,
                    *,
                    _frame_cnt=3,
                    loop: Optional[asyncio.AbstractEventLoop] = None):
         if traced_folders is None:
             traced_folders = set()
         traced_folders.update(_parse_trace_modules(traced_locs))
         trace_res: List[FrameResult] = []
@@ -510,15 +528,15 @@
                           key: str = "trace",
                           traced_types: Optional[Tuple[Type]] = None,
                           traced_names: Optional[Set[str]] = None,
                           traced_folders: Optional[Set[str]] = None,
                           trace_return: bool = True,
                           depth: int = 5,
                           ignored_names: Optional[Set[str]] = None,
-                            use_profile: bool = False,
+                          use_profile: bool = False,
                           *,
                           _frame_cnt: int = 4,
                           loop: Optional[asyncio.AbstractEventLoop] = None):
         return self.trace_sync(traced_locs,
                                key,
                                traced_types,
                                traced_names,
@@ -560,8 +578,8 @@
         try:
             with tracer:
                 yield
         finally:
             tree_items = parse_frame_result_to_trace_item(
                 trace_res, use_return_locals)
             show_dict = {v.get_uid(): v for v in tree_items}
-            await self.set_object(show_dict, key)
+            await self.set_object(show_dict, key)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/inspectpanel.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/inspectpanel.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,15 +9,17 @@
 
     def __init__(self,
                  obj: Any,
                  init_layout: Optional[FlexLayoutInitType] = None,
                  use_fast_tree: bool = False,
                  fixed_size: bool = False):
         self.anylayout = AnyFlexLayout(init_layout)
-        self.inspector = ObjectInspector(obj, use_fast_tree=use_fast_tree, fixed_size=fixed_size)
+        self.inspector = ObjectInspector(obj,
+                                         use_fast_tree=use_fast_tree,
+                                         fixed_size=fixed_size)
         self.inspector.prop(width="100%", height="100%", overflow="hidden")
         child = mui.Allotment([
             self.inspector,
             mui.HBox([
                 self.anylayout,
             ]).prop(width="100%", height="100%", overflow="hidden")
         ]).prop(defaultSizes=[1, 3], width="100%", height="100%")
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/layout.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/layout.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,19 +21,22 @@
     ALL_OBJECT_LAYOUT_HANDLERS, ObjectLayoutCreator)
 from tensorpc.flow.flowapp.core import (AppEditorFrontendEvent,
                                         FlowSpecialMethods, FrontendEventType,
                                         _get_obj_def_path)
 from tensorpc.flow.flowapp.coretypes import TreeDragTarget
 
 FlexLayoutInitType: TypeAlias = Union[List[Union[mui.FlexLayout.Row,
-                                                     mui.FlexLayout.TabSet]],
-                                          mui.FlexLayout.Row,
-                                          mui.FlexLayout.TabSet,
-                                          mui.FlexLayout.Tab, mui.FlexLayout.HBox,
-                                          mui.FlexLayout.VBox, mui.MUIComponentType]
+                                                 mui.FlexLayout.TabSet]],
+                                      mui.FlexLayout.Row,
+                                      mui.FlexLayout.TabSet,
+                                      mui.FlexLayout.Tab, mui.FlexLayout.HBox,
+                                      mui.FlexLayout.VBox,
+                                      mui.MUIComponentType]
+
+
 class AnyFlexLayout(mui.FlexLayout):
 
     def __init__(self,
                  children: Optional[FlexLayoutInitType] = None,
                  use_app_editor: bool = True) -> None:
         if children is None:
             children = []
@@ -95,23 +98,25 @@
             elif isinstance(obj, ObjectLayoutCreator):
                 obj_is_anylayout = False
                 obj = obj.create()
                 wrapped_obj = obj
             else:
                 if not isinstance(obj, mui.Component):
                     if obj_is_anylayout:
-                        wrapped_obj = mui.flex_wrapper(obj, reload_mgr=self.flow_app_comp_core.reload_mgr)
+                        wrapped_obj = mui.flex_wrapper(
+                            obj, reload_mgr=self.flow_app_comp_core.reload_mgr)
                     else:
                         if type(obj) in ALL_OBJECT_LAYOUT_HANDLERS:
                             handler_cls = ALL_OBJECT_LAYOUT_HANDLERS[type(obj)]
                             wrapped_obj = handler_cls.from_object(obj)
                             assert type(obj) == handler_cls and isinstance(
                                 wrapped_obj, mui.FlexBox)
                         else:
-                            raise ValueError("this shouldn't happen", obj, type(obj))
+                            raise ValueError("this shouldn't happen", obj,
+                                             type(obj))
                 else:
                     wrapped_obj = obj
             wrapped_obj.set_flow_event_context_creator(target.context_creator)
             if obj_is_anylayout:
                 await self._bind_code_editor(obj, wrapped_obj, uid)
             await self.update_childs({uid: wrapped_obj})
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/reload.py` & `tensorpc-0.11.0/tensorpc/flow/init_langserv/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/tree.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/tree.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,22 +19,23 @@
 from tensorpc.flow.flowapp.components.plus.collection import SimpleFileReader, ScriptExecutor
 from tensorpc.flow.flowapp.components.plus.scriptmgr import ScriptManager
 
 from tensorpc.flow.flowapp.components.plus.monitor import \
     ComputeResourceMonitor
 from tensorpc.flow.flowapp.components.plus.core import (
     ALL_OBJECT_LAYOUT_HANDLERS, ALL_OBJECT_PREVIEW_HANDLERS,
-    USER_OBJ_TREE_TYPES, ContextMenuType, CustomTreeItemHandler, ObjectLayoutCreator,
-    ObjectLayoutHandler)
+    USER_OBJ_TREE_TYPES, ContextMenuType, CustomTreeItemHandler,
+    ObjectLayoutCreator, ObjectLayoutHandler)
 from tensorpc.flow.flowapp.core import FlowSpecialMethods, FrontendEventType
 from tensorpc.flow.flowapp.coretypes import TreeDragTarget
 from tensorpc.flow.flowapp.objtree import UserObjTree, UserObjTreeProtocol
 from tensorpc.flow.flowapp.reload import reload_object_methods
 from tensorpc.flow.jsonlike import (CommonQualNames, ContextMenuData,
-                                    IconButtonData, parse_obj_to_jsonlike, TreeItem)
+                                    IconButtonData, parse_obj_to_jsonlike,
+                                    TreeItem)
 from tensorpc.flow.marker import mark_did_mount, mark_will_unmount
 from .controllers import CallbackSlider, ThreadLocker, MarkdownViewer
 from .analysis import ObjectTreeParser, TreeContext, enter_tree_conetxt
 
 _DEFAULT_OBJ_NAME = "default"
 
 _DEFAULT_BUILTINS_NAME = "builtins"
@@ -59,29 +60,29 @@
 
 class ButtonType(enum.Enum):
     Reload = "reload"
     Delete = "delete"
     Watch = "watch"
     Record = "record"
 
+
 # _SHOULD_EXPAND_TYPES = {mui.JsonLikeType.List.value, mui.JsonLikeType.Tuple.value,
 #                         mui.JsonLikeType.Dict.value, mui.JsonLikeType.Object.value,
 #                             mui.JsonLikeType.ListFolder.value, mui.JsonLikeType.DictFolder.value,
 #                             mui.JsonLikeType.Layout.value}
 
 # def _check_is_valid(obj_type, cared_types: Set[Type],
 #                     ignored_types: Set[Type]):
 #     valid = True
 #     if len(cared_types) != 0:
 #         valid &= obj_type in cared_types
 #     if len(ignored_types) != 0:
 #         valid &= obj_type not in ignored_types
 #     return valid
 
-
 # def _get_root_tree(obj,
 #                    checker: Callable[[Type], bool],
 #                    key: str,
 #                    obj_meta_cache=None):
 #     obj_dict = get_obj_dict(obj, checker)
 #     root_node = parse_obj_item(obj, key, key, checker, obj_meta_cache)
 #     root_node.children = parse_obj_dict_to_nodes(obj_dict, key, checker, obj_meta_cache)
@@ -89,23 +90,22 @@
 #         obj_child_dict = get_obj_dict(o, checker)
 #         c.drag = False
 #         c.children = parse_obj_dict_to_nodes(obj_child_dict, c.id, checker,
 #                                     obj_meta_cache)
 #     root_node.cnt = len(obj_dict)
 #     return root_node
 
-
 # async def _parse_obj_to_node(obj,
 #                        node: mui.JsonLikeNode,
 #                        checker: Callable[[Type], bool],
 #                        cached_lazy_expand_ids: Set[str],
 #                        obj_meta_cache=None,
 #                        total_expand_level: int = 0):
 #     if node.type not in _SHOULD_EXPAND_TYPES:
-#         return 
+#         return
 #     if isinstance(obj, TreeItem):
 #         obj_dict = await obj.get_child_desps(node.id)  # type: ignore
 #         tree_children = list(obj_dict.values())
 #     else:
 #         obj_dict = get_obj_dict(obj, checker)
 #         tree_children = parse_obj_dict_to_nodes(obj_dict, node.id, checker, obj_meta_cache)
 #     node.children = tree_children
@@ -114,24 +114,23 @@
 #         should_expand = child_node.id in cached_lazy_expand_ids or total_expand_level > 0
 #         if isinstance(v, TreeItem) and v.default_expand():
 #             should_expand = True
 #         if should_expand:
 #             await _parse_obj_to_node(v, child_node, checker, cached_lazy_expand_ids,
 #                                obj_meta_cache, total_expand_level - 1)
 
-
 # async def _get_obj_tree(obj,
 #                   checker: Callable[[Type], bool],
 #                   key: str,
 #                   parent_id: str,
 #                   obj_meta_cache=None,
 #                   cached_lazy_expand_ids: Optional[List[str]] = None,
 #                   total_expand_level: int = 0):
 #     if parent_id == "":
-#         obj_id = key 
+#         obj_id = key
 #     else:
 #         obj_id = f"{parent_id}{GLOBAL_SPLIT}{key}"
 #     assert total_expand_level >= 0
 #     root_node = parse_obj_item(obj, key, obj_id, checker, obj_meta_cache)
 #     if cached_lazy_expand_ids is None:
 #         cached_lazy_expand_ids = []
 #     cached_lazy_expand_ids_set = set(cached_lazy_expand_ids)
@@ -158,15 +157,17 @@
 class DataStorageTreeItem(TreeItem):
 
     def __init__(self, node_id: str, readable_id: str) -> None:
         super().__init__()
         self.node_id = node_id
         self.readable_id = readable_id
 
-    async def get_child_desps(self, parent_ns: UniqueTreeIdForTree) -> Dict[str, mui.JsonLikeNode]:
+    async def get_child_desps(
+            self,
+            parent_ns: UniqueTreeIdForTree) -> Dict[str, mui.JsonLikeNode]:
         metas = await appctx.list_data_storage(self.node_id)
         for m in metas:
             m.id = parent_ns + m.id
             userdata = {
                 "type": ContextMenuType.DataStorageItemDelete.value,
             }
             userdata_cpycmd = {
@@ -188,15 +189,16 @@
             #                         mui.IconType.Delete.value)]
         return {m.last_part(): m for m in metas}
 
     async def get_child(self, key: str) -> Any:
         res = await appctx.read_data_storage(key, self.node_id)
         return res
 
-    def get_json_like_node(self, id: UniqueTreeIdForTree) -> Optional[mui.JsonLikeNode]:
+    def get_json_like_node(
+            self, id: UniqueTreeIdForTree) -> Optional[mui.JsonLikeNode]:
         btns = [
             IconButtonData(ButtonType.Delete.value, mui.IconType.Delete.value)
         ]
         return mui.JsonLikeNode(id + self.readable_id,
                                 self.readable_id,
                                 mui.JsonLikeType.Object.value,
                                 typeStr="DataStorageTreeItem",
@@ -224,33 +226,39 @@
     async def handle_child_context_menu(self, child_key: str,
                                         userdata: Dict[str, Any]):
         type = ContextMenuType(userdata["type"])
         if type == ContextMenuType.DataStorageItemDelete:
             await appctx.remove_data_storage(child_key, self.node_id)
             return True  # tell outside update childs
         if type == ContextMenuType.DataStorageItemCommand:
-            await appctx.get_app().copy_text_to_clipboard(f"await appctx.read_data_storage('{child_key}', '{self.node_id}')")
+            await appctx.get_app().copy_text_to_clipboard(
+                f"await appctx.read_data_storage('{child_key}', '{self.node_id}')"
+            )
 
     async def handle_context_menu(self, userdata: Dict[str, Any]):
         return
 
 
 class ObservedFunctionTree(TreeItem):
 
     def __init__(self) -> None:
         super().__init__()
         self._watched_funcs: Set[str] = set()
 
     # async def get_childs(self):
 
-    async def get_child_desps(self, parent_ns: UniqueTreeIdForTree) -> Dict[str, mui.JsonLikeNode]:
+    async def get_child_desps(
+            self,
+            parent_ns: UniqueTreeIdForTree) -> Dict[str, mui.JsonLikeNode]:
         metas: Dict[str, mui.JsonLikeNode] = {}
         for k, v in appctx.get_app().get_observed_func_registry().items():
-            node = mui.JsonLikeNode(parent_ns + k, v.name,
-                                    mui.JsonLikeType.Function.value, alias=v.name)
+            node = mui.JsonLikeNode(parent_ns + k,
+                                    v.name,
+                                    mui.JsonLikeType.Function.value,
+                                    alias=v.name)
             node.iconBtns = [
                 IconButtonData(ButtonType.Watch.value,
                                mui.IconType.Visibility.value, "Watch"),
                 IconButtonData(ButtonType.Record.value, mui.IconType.Mic.value,
                                "Record")
             ]
             value = ""
@@ -263,16 +271,18 @@
             node.value = value
             metas[k] = node
         return metas
 
     async def get_child(self, key: str) -> Any:
         return appctx.get_app().get_observed_func_registry()[key]
 
-    def get_json_like_node(self, id: UniqueTreeIdForTree) -> Optional[mui.JsonLikeNode]:
-        return mui.JsonLikeNode(UniqueTreeIdForTree.from_parts(["observedFunc"]),
+    def get_json_like_node(
+            self, id: UniqueTreeIdForTree) -> Optional[mui.JsonLikeNode]:
+        return mui.JsonLikeNode(UniqueTreeIdForTree.from_parts(
+            ["observedFunc"]),
                                 "observedFunc",
                                 mui.JsonLikeType.Object.value,
                                 typeStr="ObservedFunctions",
                                 cnt=1,
                                 drag=False)
 
     async def handle_child_button(self, button_key: str, child_key: str):
@@ -295,22 +305,25 @@
 
 
 class SimpleCanvasCreator(ObjectLayoutCreator):
 
     def create(self):
         return SimpleCanvas()
 
+
 class BasicTreeEventType(enum.IntEnum):
     SelectSingle = 0
 
+
 @dataclasses.dataclass
 class SelectSingleEvent:
     nodes: List[mui.JsonLikeNode]
     objs: Optional[List[Any]] = None
 
+
 class BasicObjectTree(mui.FlexBox):
     """basic object tree, contains enough features
     to analysis python object.
     TODO auto expand child when you expand a node.
     """
 
     def __init__(self,
@@ -328,27 +341,31 @@
             self.tree = mui.TanstackJsonLikeTree()
         else:
             self.tree = mui.JsonLikeTree()
         super().__init__([
             self.tree.prop(ignoreRoot=True, fixedSize=fixed_size),
         ])
         self.prop(overflow="auto")
-        self._tree_parser = ObjectTreeParser(cared_types, ignored_types, custom_tree_item_handler=custom_tree_handler)
+        self._tree_parser = ObjectTreeParser(
+            cared_types,
+            ignored_types,
+            custom_tree_item_handler=custom_tree_handler)
         self._uid_to_node: Dict[str, mui.JsonLikeNode] = {}
         if cared_types is None:
             cared_types = set()
         if ignored_types is None:
             ignored_types = set()
         self._cared_types = cared_types
         self._ignored_types = ignored_types
         self._obj_meta_cache = {}
         self._auto_lazy_expand = auto_lazy_expand
         # self._cached_lazy_expand_uids: List[str] = []
-        self._cared_dnd_uids: Dict[UniqueTreeIdForTree, Callable[[UniqueTreeIdForTree, Any],
-                                                 mui.CORO_NONE]] = {}
+        self._cared_dnd_uids: Dict[UniqueTreeIdForTree,
+                                   Callable[[UniqueTreeIdForTree, Any],
+                                            mui.CORO_NONE]] = {}
         self.limit = limit
         if init is None:
             self.root = {}
             # self.tree.props.tree = mui.JsonLikeNode(
             #     _ROOT, _ROOT, mui.JsonLikeType.Dict.value)
         else:
             if use_init_as_root:
@@ -364,29 +381,33 @@
         self.tree.register_event_handler(
             FrontendEventType.TreeItemButton.value, self._on_custom_button)
         self.tree.register_event_handler(
             FrontendEventType.ContextMenuSelect.value, self._on_contextmenu)
         self.tree.register_event_handler(
             FrontendEventType.TreeItemRename.value, self._on_rename)
         self.tree.register_event_handler(
-            FrontendEventType.TreeItemSelectChange.value, self._on_select_single)
+            FrontendEventType.TreeItemSelectChange.value,
+            self._on_select_single)
 
         self.tree.register_event_handler(FrontendEventType.DragCollect.value,
                                          self._on_drag_collect,
                                          backend_only=True)
         self.default_expand_level = default_expand_level
 
-        self.event_async_select_single = self._create_emitter_event_slot(BasicTreeEventType.SelectSingle)
+        self.event_async_select_single = self._create_emitter_event_slot(
+            BasicTreeEventType.SelectSingle)
 
     @mark_did_mount
     async def _on_mount(self):
         # self.tree.props.tree = await _get_root_tree_async(
         #     self.root, self._valid_checker, _ROOT, self._obj_meta_cache)
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            root_node = await self._tree_parser.get_root_tree(self.root, _ROOT, self.default_expand_level)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            root_node = await self._tree_parser.get_root_tree(
+                self.root, _ROOT, self.default_expand_level)
         # self.tree.props.tree = await _get_obj_tree(
         #     self.root, self._valid_checker, _ROOT, "", self._obj_meta_cache, total_expand_level=self.default_expand_level)
         self.tree.props.tree = root_node
         await self.tree.send_and_wait(
             self.tree.update_event(tree=self.tree.props.tree))
 
     # def _checker(self, obj):
@@ -406,36 +427,41 @@
         real_uids: List[str] = []
         real_keys: List[Union[Hashable, mui.Undefined]] = []
         for i in range(len(tree_node_trace)):
             k = tree_node_trace[i].get_dict_key()
             if not tree_node_trace[i].is_folder():
                 real_keys.append(k)
                 real_uids.append(uids[i])
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            return await self._tree_parser.get_obj_by_uid(self.root,
-                                        UniqueTreeIdForTree.from_parts(real_uids),
-                                        real_keys=real_keys)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            return await self._tree_parser.get_obj_by_uid(
+                self.root,
+                UniqueTreeIdForTree.from_parts(real_uids),
+                real_keys=real_keys)
 
     async def _get_obj_by_uid_trace(self, uid: UniqueTreeIdForTree,
                                     tree_node_trace: List[mui.JsonLikeNode]):
         uids = uid.parts
         real_uids: List[str] = []
         real_keys: List[Union[Hashable, mui.Undefined]] = []
         for i in range(len(tree_node_trace)):
             k = tree_node_trace[i].get_dict_key()
             if not tree_node_trace[i].is_folder():
                 real_keys.append(k)
                 real_uids.append(uids[i])
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            return await self._tree_parser.get_obj_by_uid_trace(self.root,
-                                            UniqueTreeIdForTree.from_parts(real_uids),
-                                            real_keys=real_keys)
-
-    def _register_dnd_uid(self, uid: UniqueTreeIdForTree, cb: Callable[[UniqueTreeIdForTree, Any],
-                                                       mui.CORO_NONE]):
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            return await self._tree_parser.get_obj_by_uid_trace(
+                self.root,
+                UniqueTreeIdForTree.from_parts(real_uids),
+                real_keys=real_keys)
+
+    def _register_dnd_uid(self, uid: UniqueTreeIdForTree,
+                          cb: Callable[[UniqueTreeIdForTree, Any],
+                                       mui.CORO_NONE]):
         self._cared_dnd_uids[uid] = cb
 
     def _unregister_dnd_uid(self, uid: UniqueTreeIdForTree):
         if uid in self._cared_dnd_uids:
             self._cared_dnd_uids.pop(uid)
 
     def _unregister_all_dnd_uid(self):
@@ -460,131 +486,147 @@
                         await res
         for d in deleted:
             self._cared_dnd_uids.pop(d)
 
     async def _on_drag_collect(self, data):
         uid: str = data["id"]
         uid_obj = UniqueTreeIdForTree(uid)
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            objs, found = await self._tree_parser.get_obj_by_uid_trace(self.root, uid_obj)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            objs, found = await self._tree_parser.get_obj_by_uid_trace(
+                self.root, uid_obj)
         if not found:
             return None
         tab_id = ""
         if "complexLayoutTabNodeId" in data:
             # for complex layout UI: FlexLayout
             tab_id = data["complexLayoutTabNodeId"]
         root: Optional[UserObjTreeProtocol] = None
         for obj in objs:
             if isinstance(obj, tuple(USER_OBJ_TREE_TYPES)):
                 root = obj
                 break
         if root is not None:
-            return TreeDragTarget(objs[-1], uid, tab_id, self._flow_uid_encoded,
+            return TreeDragTarget(objs[-1], uid, tab_id,
+                                  self._flow_uid_encoded,
                                   lambda: root.enter_context(root))
         return TreeDragTarget(objs[-1], uid, tab_id, self._flow_uid_encoded)
-    
-    async def _on_select_single(self, uid_list: Union[List[str], str, Dict[str, bool]]):
+
+    async def _on_select_single(self, uid_list: Union[List[str], str,
+                                                      Dict[str, bool]]):
         if isinstance(uid_list, list):
             # node id list may empty
             if not uid_list:
                 return
             uid = uid_list[0]
         elif isinstance(uid_list, dict):
             if not uid_list:
                 return
             uid = list(uid_list.keys())[0]
         else:
             uid = uid_list
         uid_obj = UniqueTreeIdForTree(uid)
         nodes = self._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
         objs, found = await self._get_obj_by_uid_trace(uid_obj, nodes)
-        return await self.flow_event_emitter.emit_async(BasicTreeEventType.SelectSingle, 
-            mui.Event(BasicTreeEventType.SelectSingle, SelectSingleEvent(nodes, objs if found else None)))
+        return await self.flow_event_emitter.emit_async(
+            BasicTreeEventType.SelectSingle,
+            mui.Event(BasicTreeEventType.SelectSingle,
+                      SelectSingleEvent(nodes, objs if found else None)))
 
     async def _on_expand(self, uid_encoded: str):
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
             uid = UniqueTreeIdForTree(uid_encoded)
             node = self._objinspect_root._get_node_by_uid(uid_encoded)
             nodes, node_found = self._objinspect_root._get_node_by_uid_trace_found(
                 uid.parts)
             assert node_found, "can't find your node via uid"
             node = nodes[-1]
             obj_dict: Dict[Hashable, Any] = {}
             if self._auto_lazy_expand:
                 self._tree_parser.update_lazy_expand_uids(uid)
             if node.type in FOLDER_TYPES:
                 assert not isinstance(node.start, mui.Undefined)
                 assert not isinstance(node.realId, mui.Undefined)
                 if node._is_divisible(self.limit):
-                    node.children = node._get_divided_tree(self.limit, node.start)
+                    node.children = node._get_divided_tree(
+                        self.limit, node.start)
                     upd = self.tree.update_event(tree=self._objinspect_root)
                     return await self.tree.send_and_wait(upd)
                 # real_node = self._objinspect_root._get_node_by_uid(node.realId)
-                real_obj, found = await self._get_obj_by_uid(node.realId, nodes)
+                real_obj, found = await self._get_obj_by_uid(
+                    node.realId, nodes)
                 if node.type == mui.JsonLikeType.ListFolder.value:
                     data = real_obj[node.start:node.start + node.cnt]
                 else:
                     assert not isinstance(node.keys, mui.Undefined)
                     data = {k: real_obj[k] for k in node.keys.data}
                 obj_dict = {**(await self._tree_parser.expand_object(data))}
-                tree = await self._tree_parser.parse_obj_dict_to_nodes(obj_dict, node.id)
+                tree = await self._tree_parser.parse_obj_dict_to_nodes(
+                    obj_dict, node.id)
                 node.children = tree
                 upd = self.tree.update_event(tree=self._objinspect_root)
                 return await self.tree.send_and_wait(upd)
             obj, found = await self._get_obj_by_uid(uid, nodes)
             if node.type in CONTAINER_TYPES and node._is_divisible(self.limit):
                 if node.type == mui.JsonLikeType.Dict.value:
                     node.keys = mui.BackendOnlyProp(list(obj.keys()))
                 node.children = node._get_divided_tree(self.limit, 0)
                 upd = self.tree.update_event(tree=self._objinspect_root)
                 return await self.tree.send_and_wait(upd)
             # if not found, we expand (update) the deepest valid object.
             # if the object is special (extend TreeItem), we use used-defined
             # function instead of analysis it.
             if isinstance(obj, TreeItem):
-                obj_dict_desp = await obj.get_child_desps(uid)  
+                obj_dict_desp = await obj.get_child_desps(uid)
                 obj_dict = {**obj_dict_desp}
                 tree = list(obj_dict_desp.values())
             else:
                 obj_dict = {**(await self._tree_parser.expand_object(obj))}
-                tree = await self._tree_parser.parse_obj_dict_to_nodes(obj_dict, node.id)
+                tree = await self._tree_parser.parse_obj_dict_to_nodes(
+                    obj_dict, node.id)
             node.children = tree
             upd = self.tree.update_event(tree=self._objinspect_root)
             return await self.tree.send_and_wait(upd)
 
     async def _on_rename(self, uid_newname: Tuple[str, str]):
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
 
             uid = uid_newname[0]
             uid_obj = UniqueTreeIdForTree(uid)
             uid_parts = uid_obj.parts
 
-            obj_trace, found = await self._tree_parser.get_obj_by_uid_trace(self.root, uid_obj)
+            obj_trace, found = await self._tree_parser.get_obj_by_uid_trace(
+                self.root, uid_obj)
             if not found:
                 return
             # if object is TreeItem or parent is TreeItem,
             # the button/contextmenu event will be handled in TreeItem instead of common
             # handler.
             if len(obj_trace) >= 2:
                 parent = obj_trace[-2]
                 if isinstance(parent, TreeItem):
-                    nodes = self._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
+                    nodes = self._objinspect_root._get_node_by_uid_trace(
+                        uid_obj.parts)
                     parent_node = nodes[-2]
-                    res = await parent.handle_child_rename(uid_parts[-1],
-                                                        uid_newname[1])
+                    res = await parent.handle_child_rename(
+                        uid_parts[-1], uid_newname[1])
                     if res == True:
                         await self._on_expand(parent_node.id.uid_encoded)
                     return
 
     async def _on_custom_button(self, uid_btn: Tuple[str, str]):
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
             uid = uid_btn[0]
             uid_obj = UniqueTreeIdForTree(uid)
             uid_parts = uid_obj.parts
-            obj_trace, found = await self._tree_parser.get_obj_by_uid_trace(self.root, uid_obj)
+            obj_trace, found = await self._tree_parser.get_obj_by_uid_trace(
+                self.root, uid_obj)
             if not found:
                 return
 
             obj = obj_trace[-1]
             # if object is TreeItem or parent is TreeItem,
             # the button/contextmenu event will be handled in TreeItem instead of common
             # handler.
@@ -595,33 +637,35 @@
                     await self._on_expand(uid)
                 return
             nodes = self._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
             if len(obj_trace) >= 2:
                 parent = obj_trace[-2]
                 if isinstance(parent, TreeItem):
                     parent_node = nodes[-2]
-                    res = await parent.handle_child_button(uid_btn[1],
-                                                        uid_parts[-1])
+                    res = await parent.handle_child_button(
+                        uid_btn[1], uid_parts[-1])
                     if res == True:
                         await self._on_expand(parent_node.id.uid_encoded)
                     return
 
             if uid_btn[1] == ButtonType.Reload.value:
                 metas, is_reload = reload_object_methods(
                     obj, reload_mgr=self.flow_app_comp_core.reload_mgr)
                 if metas is not None:
                     special_methods = FlowSpecialMethods(metas)
                 else:
                     print("reload failed.")
             if self._tree_parser.custom_tree_item_handler is not None:
-                await self._tree_parser.custom_tree_item_handler.handle_button(obj_trace, nodes, uid_btn[1])
+                await self._tree_parser.custom_tree_item_handler.handle_button(
+                    obj_trace, nodes, uid_btn[1])
 
     async def _on_contextmenu(self, uid_menuid_data: Tuple[str, str,
                                                            Optional[Any]]):
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
 
             uid = uid_menuid_data[0]
             uid_obj = UniqueTreeIdForTree(uid)
             uid_parts = uid_obj.parts
             userdata = uid_menuid_data[2]
             if userdata is not None:
                 obj_trace, found = await self._tree_parser.get_obj_by_uid_trace(
@@ -632,37 +676,46 @@
                     # handle tree item first
                     if isinstance(obj, TreeItem):
                         res = await obj.handle_context_menu(userdata)
                         if res == True:
                             # update this node
                             await self._on_expand(uid)
                         return
-                    nodes = self._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
+                    nodes = self._objinspect_root._get_node_by_uid_trace(
+                        uid_obj.parts)
                     if len(obj_trace) >= 2:
                         parent = obj_trace[-2]
                         parent_node = nodes[-2]
                         if isinstance(parent, TreeItem):
                             res = await parent.handle_child_context_menu(
                                 uid_parts[-1], userdata)
                             if res == True:
                                 # update this node
-                                await self._on_expand(parent_node.id.uid_encoded)
+                                await self._on_expand(
+                                    parent_node.id.uid_encoded)
                             return
                     if self._tree_parser.custom_tree_item_handler is not None:
-                        await self._tree_parser.custom_tree_item_handler.handle_button(obj_trace, nodes, userdata)
+                        await self._tree_parser.custom_tree_item_handler.handle_button(
+                            obj_trace, nodes, userdata)
 
     def has_object(self, key: str):
         return key in self.root
 
-    async def set_object(self, obj, key: str = _DEFAULT_OBJ_NAME, expand_level: int = 1):
+    async def set_object(self,
+                         obj,
+                         key: str = _DEFAULT_OBJ_NAME,
+                         expand_level: int = 1):
         key_in_root = key in self.root
         self.root[key] = obj
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            obj_tree = await self._tree_parser.get_root_tree(obj, key, expand_level, ns=self.tree.props.tree.id)
-            await self._tree_parser.parse_obj_to_tree(obj, obj_tree, expand_level)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            obj_tree = await self._tree_parser.get_root_tree(
+                obj, key, expand_level, ns=self.tree.props.tree.id)
+            await self._tree_parser.parse_obj_to_tree(obj, obj_tree,
+                                                      expand_level)
         # obj_tree = await _get_obj_tree(obj, self._checker, key,
         #                          self.tree.props.tree.id, self._obj_meta_cache,
         #                          self._cached_lazy_expand_uids,
         #                          total_expand_level=expand_level)
         if key_in_root:
             for i, node in enumerate(self.tree.props.tree.children):
                 if node.name == key:
@@ -672,18 +725,23 @@
             self.tree.props.tree.children.append(obj_tree)
         # self.tree.props.tree = _get_root_tree(self.root, self._valid_checker,
         #                                       _ROOT, self._obj_meta_cache)
         await self.tree.send_and_wait(
             self.tree.update_event(tree=self.tree.props.tree))
         await self._do_when_tree_updated(obj_tree.id)
 
-    async def update_tree(self, wait: bool = True, update_tree: bool = True, update_iff_change: bool = False):
+    async def update_tree(self,
+                          wait: bool = True,
+                          update_tree: bool = True,
+                          update_iff_change: bool = False):
         t = time.time()
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            new_tree = await self._tree_parser.get_root_tree(self.root, _ROOT, self.default_expand_level)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            new_tree = await self._tree_parser.get_root_tree(
+                self.root, _ROOT, self.default_expand_level)
         # print(0, time.time() - t)
         if update_tree:
             if update_iff_change:
                 # send tree to frontend is greatly slower than compare.
                 if new_tree != self.tree.props.tree:
                     await self.tree.send_and_wait(
                         self.tree.update_event(tree=new_tree), wait=wait)
@@ -693,31 +751,35 @@
             self.tree.props.tree = new_tree
         # print(1, time.time() - t)
 
         await self._do_when_tree_updated(self.tree.props.tree.id)
         # print(2, time.time() - t)
 
     async def update_tree_event(self):
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            self.tree.props.tree = await self._tree_parser.get_root_tree(self.root, _ROOT, self.default_expand_level)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            self.tree.props.tree = await self._tree_parser.get_root_tree(
+                self.root, _ROOT, self.default_expand_level)
         return self.tree.update_event(tree=self.tree.props.tree)
 
     async def remove_object(self, key: str):
         key_in_root = key in self.root
         if not key_in_root:
             return
         new_child = []
         for i, node in enumerate(self.tree.props.tree.children):
             if node.name != key:
                 new_child.append(node)
         self.tree.props.tree.children = new_child
         await self.tree.send_and_wait(
             self.tree.update_event(tree=self.tree.props.tree))
 
-    async def _get_obj_by_uid_with_folder(self, uid_may_obj: Union[str, UniqueTreeIdForTree], nodes: List[mui.JsonLikeNode]):
+    async def _get_obj_by_uid_with_folder(
+            self, uid_may_obj: Union[str, UniqueTreeIdForTree],
+            nodes: List[mui.JsonLikeNode]):
         if isinstance(uid_may_obj, str):
             uid_obj = UniqueTreeIdForTree(uid_may_obj)
             uid = uid_may_obj
         else:
             uid_obj = uid_may_obj
             uid = uid_may_obj.uid_encoded
 
@@ -750,15 +812,17 @@
                         obj = real_obj[key]
             else:
                 obj, found = await self._get_obj_by_uid(uid_obj, nodes)
         else:
             obj, found = await self._get_obj_by_uid(uid_obj, nodes)
         return obj, found
 
-    async def _get_obj_by_uid_with_folder_trace(self, uid_may_obj: Union[str, UniqueTreeIdForTree], nodes: List[mui.JsonLikeNode]):
+    async def _get_obj_by_uid_with_folder_trace(
+            self, uid_may_obj: Union[str, UniqueTreeIdForTree],
+            nodes: List[mui.JsonLikeNode]):
         # TODO merge this with _get_obj_by_uid_with_folder
         node = nodes[-1]
         if isinstance(uid_may_obj, str):
             uid_obj = UniqueTreeIdForTree(uid_may_obj)
             uid = uid_may_obj
         else:
             uid_obj = uid_may_obj
@@ -791,20 +855,22 @@
                                           mui.BackendOnlyProp)
                         key = node.name
                         if not isinstance(node.get_dict_key(), mui.Undefined):
                             key = node.get_dict_key()
                         obj = real_obj[key]
                     obj_trace.append(obj)
             else:
-                obj_trace, found = await self._get_obj_by_uid_trace(uid_obj, nodes)
+                obj_trace, found = await self._get_obj_by_uid_trace(
+                    uid_obj, nodes)
         else:
             obj_trace, found = await self._get_obj_by_uid(uid_obj, nodes)
         return obj_trace, found
 
-    async def get_object_by_uid(self, uid_may_obj: Union[str, UniqueTreeIdForTree]):
+    async def get_object_by_uid(self, uid_may_obj: Union[str,
+                                                         UniqueTreeIdForTree]):
         if isinstance(uid_may_obj, str):
             uid_obj = UniqueTreeIdForTree(uid_may_obj)
             uid = uid_may_obj
         else:
             uid_obj = uid_may_obj
             uid = uid_may_obj.uid_encoded
 
@@ -841,26 +907,29 @@
         else:
             obj, found = await self._get_obj_by_uid(uid_obj, nodes)
         if not found:
             raise ValueError(
                 f"your object {uid} is invalid, may need to reflesh")
         return obj
 
+
 class ObjectTree(BasicObjectTree):
     """object tree for object inspector.
     """
 
-    def __init__(self,
-                 init: Optional[Any] = None,
-                 cared_types: Optional[Set[Type]] = None,
-                 ignored_types: Optional[Set[Type]] = None,
-                 use_fast_tree: bool = False,
-                 limit: int = 50,
-                 fixed_size: bool = False,
-                 custom_tree_handler: Optional[CustomTreeItemHandler] = None) -> None:
+    def __init__(
+            self,
+            init: Optional[Any] = None,
+            cared_types: Optional[Set[Type]] = None,
+            ignored_types: Optional[Set[Type]] = None,
+            use_fast_tree: bool = False,
+            limit: int = 50,
+            fixed_size: bool = False,
+            custom_tree_handler: Optional[CustomTreeItemHandler] = None
+    ) -> None:
         self._default_data_storage_nodes: Dict[str, DataStorageTreeItem] = {}
         self._default_obs_funcs = ObservedFunctionTree()
         default_builtins = {
             _DEFAULT_BUILTINS_NAME: {
                 "script": ScriptManager(),
                 "monitor": ComputeResourceMonitor(),
                 "appTerminal": mui.AppTerminal(),
@@ -870,16 +939,23 @@
                 "callbackSlider": CallbackSlider(),
                 "threadLocker": ThreadLocker(),
                 "markdown": MarkdownViewer(),
             },
             _DEFAULT_DATA_STORAGE_NAME: self._default_data_storage_nodes,
             _DEFAULT_OBSERVED_FUNC_NAME: self._default_obs_funcs,
         }
-        self._data_storage_uid = UniqueTreeIdForTree.from_parts([_ROOT, _DEFAULT_DATA_STORAGE_NAME])
-        super().__init__(init, cared_types, ignored_types, limit, use_fast_tree, fixed_size=fixed_size, custom_tree_handler=custom_tree_handler)
+        self._data_storage_uid = UniqueTreeIdForTree.from_parts(
+            [_ROOT, _DEFAULT_DATA_STORAGE_NAME])
+        super().__init__(init,
+                         cared_types,
+                         ignored_types,
+                         limit,
+                         use_fast_tree,
+                         fixed_size=fixed_size,
+                         custom_tree_handler=custom_tree_handler)
         self.root.update(default_builtins)
 
     @mark_did_mount
     async def _on_mount(self):
         all_data_nodes = await appctx.list_all_data_storage_nodes()
         userdata = {
             "type": ContextMenuType.CopyReadItemCode.value,
@@ -901,16 +977,18 @@
                                     icon=mui.IconType.DataObject.value,
                                     userdata=userdata))
 
             self._default_data_storage_nodes[readable_n] = DataStorageTreeItem(
                 n, readable_n)
         # self.tree.props.tree = await _get_root_tree_async(
         #     self.root, self._valid_checker, _ROOT, self._obj_meta_cache)
-        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree, self)):
-            self.tree.props.tree = await self._tree_parser.get_root_tree(self.root, _ROOT, self.default_expand_level)
+        with enter_tree_conetxt(TreeContext(self._tree_parser, self.tree,
+                                            self)):
+            self.tree.props.tree = await self._tree_parser.get_root_tree(
+                self.root, _ROOT, self.default_expand_level)
         if context_menus:
             await self.tree.send_and_wait(
                 self.tree.update_event(tree=self.tree.props.tree,
                                        contextMenus=context_menus))
         else:
             await self.tree.send_and_wait(
                 self.tree.update_event(tree=self.tree.props.tree))
@@ -934,15 +1012,14 @@
                     await self.run_callback(entry.run_function_with_record,
                                             sync_status_first=False,
                                             change_status=False)
 
     async def _sync_data_storage_node(self):
         await self._on_expand(self._data_storage_uid.uid_encoded)
 
-
     async def _on_contextmenu(self, uid_menuid_data: Tuple[str, str,
                                                            Optional[Any]]):
         uid = uid_menuid_data[0]
         uid_obj = UniqueTreeIdForTree(uid)
         uid_parts = uid_obj.parts
         userdata = uid_menuid_data[2]
         if userdata is not None:
@@ -953,33 +1030,34 @@
                 obj = obj_trace[-1]
                 # handle tree item first
                 if isinstance(obj, TreeItem):
                     res = await obj.handle_context_menu(userdata)
                     if res == True:
                         # update this node
                         await self._on_expand(uid)
-                    if res is not None: # handle this event
+                    if res is not None:  # handle this event
                         return
                 if len(obj_trace) >= 2:
                     parent = obj_trace[-2]
-                    nodes = self._objinspect_root._get_node_by_uid_trace(uid_obj.parts)
+                    nodes = self._objinspect_root._get_node_by_uid_trace(
+                        uid_obj.parts)
                     parent_node = nodes[-2]
 
                     if isinstance(parent, TreeItem):
                         res = await parent.handle_child_context_menu(
                             uid_parts[-1], userdata)
                         if res == True:
                             # update this node
                             await self._on_expand(parent_node.id.uid_encoded)
-                        if res is not None: # handle this event
+                        if res is not None:  # handle this event
                             return
                 # handle regular objects
                 menu_type = ContextMenuType(userdata["type"])
                 if menu_type == ContextMenuType.DataStorageStore:
                     node_id = userdata["node_id"]
                     if not found:
                         return
                     await appctx.save_data_storage(uid_parts[-1], obj, node_id)
                     await self._sync_data_storage_node()
                 if menu_type == ContextMenuType.CopyReadItemCode:
-                    await appctx.get_app().copy_text_to_clipboard(f"await appctx.inspector.read_item('{uid}')")
-
+                    await appctx.get_app().copy_text_to_clipboard(
+                        f"await appctx.inspector.read_item('{uid}')")
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/objinspect/treeitems.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/objinspect/treeitems.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,20 +18,23 @@
 import datetime as dt
 
 _DELTA_SHORTCUTS = [
     ("microseconds", "microsecond", "us"),
     ("milliseconds", "millisecond", "ms"),
 ]
 
+
 def _delta_shortcut(string: str):
     for ss, s, sh in _DELTA_SHORTCUTS:
         string = string.replace(ss, sh).replace(s, sh)
     return string
 
-def parse_frame_result_to_trace_item(frame_results: List[FrameResult], use_return_locals: bool = False):
+
+def parse_frame_result_to_trace_item(frame_results: List[FrameResult],
+                                     use_return_locals: bool = False):
     fr_stack: List[Tuple[FrameResult, TraceTreeItem]] = []
     res: List[TraceTreeItem] = []
     # print([(x.qualname, x.type, x.depth) for x in frame_results])
     for fr in frame_results:
         if fr.type == TraceType.Call:
             item = TraceTreeItem(fr)
             if fr_stack:
@@ -49,14 +52,15 @@
                 res.append(poped[1])
             else:
                 fr_stack[-1][1].append_child(poped[1])
     return res
 
 
 class TraceTreeItem(TreeItem):
+
     def __init__(self, frame_res: FrameResult) -> None:
         super().__init__()
         self.set_return_frame_result(frame_res)
         self.depth = frame_res.depth
         self.call_var_names: List[str] = list(frame_res.local_vars.keys())
         self.start_ts = frame_res.timestamp
         self.end_ts = -1
@@ -80,21 +84,23 @@
     def get_display_name(self):
         # if method, use "self.xxx" instead of full qualname
         if self.is_method and self.parent_cls_name == self.cls_name:
             return f"self.{self.name}"
         else:
             return self.qname
 
-    async def get_child_desps(self, parent_ns: UniqueTreeIdForTree) -> Dict[str, JsonLikeNode]:
+    async def get_child_desps(
+            self, parent_ns: UniqueTreeIdForTree) -> Dict[str, JsonLikeNode]:
         res: Dict[str, JsonLikeNode] = {}
         for v in self.child_trace_res:
             id = parent_ns.append_part(v.get_uid())
             node = v.get_json_like_node(id)
             res[v.get_uid()] = node
-        res_list = await get_tree_context_noexcept().parser.parse_obj_dict_to_nodes(self.local_vars, parent_ns)
+        res_list = await get_tree_context_noexcept(
+        ).parser.parse_obj_dict_to_nodes(self.local_vars, parent_ns)
         res.update({x.name: x for x in res_list})
         return res
 
     async def get_child(self, key: str) -> Any:
         child_trace_keys = [x.get_uid() for x in self.child_trace_res]
         if key in child_trace_keys:
             return self.child_trace_res[child_trace_keys.index(key)]
@@ -119,15 +125,17 @@
     def get_uid(self):
         return f"{self.filename}:{self.lineno}@{self.qname}"
 
     def get_delta_str(self):
         delta = 0
         if self.end_ts != -1 and self.start_ts != -1:
             delta = (self.end_ts - self.start_ts) / 1e6
-        delta_str = _delta_shortcut(humanize.naturaldelta(dt.timedelta(milliseconds=delta), minimum_unit="milliseconds"))
+        delta_str = _delta_shortcut(
+            humanize.naturaldelta(dt.timedelta(milliseconds=delta),
+                                  minimum_unit="milliseconds"))
         if delta == 0:
             delta_str = "undefined"
         return delta_str
 
     @mark_create_preview_layout
     def preview_layout(self):
         btn = mui.Button("Run Frame", self._on_run_frame)
@@ -142,35 +150,35 @@
             mui.Typography(f"Time: {self.get_delta_str()}").prop(
                 **font, variant="caption"),
             mui.HBox([btn, reload_btn]),
         ]).prop(flex=1)
 
     def _get_qname(self):
         if sys.version_info[:2] >= (3, 11):
-            return self.qname 
+            return self.qname
         else:
-            # use ast parse 
+            # use ast parse
             with tokenize.open(self.filename) as f:
                 data = f.read()
             tree = ast.parse(data)
             res = find_toplevel_func_node_by_lineno(tree, self.lineno)
             if res is None:
-                return None 
+                return None
             if res[0].name != self.name:
-                return None 
+                return None
             ns = ".".join([x.name for x in res[1]])
             return f"{ns}.{res[0]}"
 
     def _get_static_method(self):
         qname = self._get_qname()
         if qname is None:
-            return None 
+            return None
         module = sys.modules.get(self.module_qname)
         if module is None:
-            return None 
+            return None
         parts = qname.split(".")
         obj = module.__dict__[parts[0]]
         for part in parts[1:]:
             obj = getattr(obj, part)
         return obj
 
     async def _on_run_frame(self):
@@ -179,21 +187,27 @@
         if "self" not in self.local_vars:
             # try find method via qualname
             method = self._get_static_method()
             if method is None:
                 raise ValueError(
                     "self not in local vars, currently only support run frame with self"
                 )
-            async with appctx.inspector.trace([], f"trace-{self.name}", traced_names=set([self.name]), use_return_locals=True):
+            async with appctx.inspector.trace([],
+                                              f"trace-{self.name}",
+                                              traced_names=set([self.name]),
+                                              use_return_locals=True):
                 method(**self.local_vars)
         else:
             local_vars = {k: v for k, v in self.local_vars.items()}
             local_vars.pop("self")
             fn = getattr(self.local_vars["self"], self.name)
-            async with appctx.inspector.trace([], f"trace-{self.name}", traced_names=set([self.name]), use_return_locals=True):
+            async with appctx.inspector.trace([],
+                                              f"trace-{self.name}",
+                                              traced_names=set([self.name]),
+                                              use_return_locals=True):
                 fn(**local_vars)
 
     def _on_reload_self(self):
         if "self" not in self.local_vars:
             raise ValueError(
                 "self not in local vars, currently only support reload object with self"
             )
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/options.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/options.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,35 +1,38 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing_extensions import TypedDict
+
+
 class CreatableAutocompleteType(TypedDict):
-    selectOnFocus: bool 
+    selectOnFocus: bool
     clearOnBlur: bool
     handleHomeEndKeys: bool
     freeSolo: bool
     addOption: bool
 
+
 class CommonOptions:
 
     AddableAutocomplete: CreatableAutocompleteType = {
         "selectOnFocus": True,
         "clearOnBlur": True,
         "handleHomeEndKeys": True,
         "freeSolo": True,
         "addOption": True,
     }
     FitParent = {
         "width": "100%",
         "height": "100%",
         "overflow": "auto",
-    }
+    }
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/reload_utils.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/reload_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,20 @@
-
 from typing import Any, Callable, Coroutine
 from tensorpc.core.serviceunit import AppFuncType, ServFunctionMeta
 from tensorpc.flow.flowapp.components import mui, three
 from tensorpc.flow.flowapp.core import FlowSpecialMethods, FrontendEventType, _get_obj_def_path
 from tensorpc.flow.flowapp.appcore import Event, get_app, get_editable_app
 from functools import partial
 
 
-async def preview_layout_reload(layout_setter: Callable[[mui.FlexBox], Coroutine[Any, Any, None]], layout: mui.FlexBox,
-                                    create_layout: ServFunctionMeta):
+async def preview_layout_reload(layout_setter: Callable[[mui.FlexBox],
+                                                        Coroutine[Any, Any,
+                                                                  None]],
+                                layout: mui.FlexBox,
+                                create_layout: ServFunctionMeta):
     if create_layout.user_app_meta is not None and create_layout.user_app_meta.type == AppFuncType.CreatePreviewLayout:
         if layout._wrapped_obj is not None:
             layout_flex = create_layout.get_binded_fn()()
             assert isinstance(
                 layout_flex, mui.FlexBox
             ), f"create_layout must return a flexbox when use anylayout"
             layout_flex._flow_comp_def_path = _get_obj_def_path(
@@ -26,8 +28,8 @@
         else:
             layout_flex = create_layout.get_binded_fn()()
             layout_flex.set_flow_event_context_creator(
                 layout._flow_event_context_creator)
             # self.__install_preview_event_listeners(layout_flex)
             await layout.set_new_layout(layout_flex)
         return layout_flex
-    return None 
+    return None
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/scheduler.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/scheduler.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,20 +50,20 @@
                                             TaskStatus.Canceled: ("Canceled",
                                                                   "warning"),
                                             TaskStatus.Failed: ("Failed",
                                                                 "error"),
                                             TaskStatus.Finished: ("Finished",
                                                                   "success"),
                                             TaskStatus.Booting: ("Finished",
-                                                                  "secondary"),
-
+                                                                 "secondary"),
                                         }
 
 
 class TaskCard(mui.FlexBox):
+
     def __init__(self, client: SchedulerClient, task: Task) -> None:
         self.task_id = task.id
         self.task = task
         self.client = client
         self.name = mui.Typography(task.name if task.name else task.id)
         self.status = mui.Typography("unknown")
         status_name_color = _TASK_STATUS_TO_UI_TEXT_AND_COLOR[
@@ -75,25 +75,27 @@
             variant="indeterminate" if progress_0 else "determinate")
         self.collapse_btn = mui.IconButton(mui.IconType.ExpandMore,
                                            self._on_expand_more).prop(
                                                tooltip="Show Detail",
                                                size="small")
         self.command = mui.Typography(task.command).prop(
             fontSize="14px", fontFamily="monospace", wordBreak="break-word")
-        self.tmux_pane = mui.Typography("").prop(
-            fontSize="12px", fontFamily="monospace", whiteSpace="pre-line", border="1px solid #ccc", padding="5px")
+        self.tmux_pane = mui.Typography("").prop(fontSize="12px",
+                                                 fontFamily="monospace",
+                                                 whiteSpace="pre-line",
+                                                 border="1px solid #ccc",
+                                                 padding="5px")
 
         self.detail = mui.Collapse([
-            mui.VBox([self.command, self.tmux_pane]).prop(maxHeight="300px", overflow="auto"),
+            mui.VBox([self.command, self.tmux_pane]).prop(maxHeight="300px",
+                                                          overflow="auto"),
         ]).prop(timeout="auto", unmountOnExit=True)
         self._expanded = False
-        self.gpu_tag = mui.Chip(f"{task.num_gpu_used} gpus").prop(color="green",
-                                    size="small",
-                                    margin="0 3px 0 3px",
-                                    clickable=False)
+        self.gpu_tag = mui.Chip(f"{task.num_gpu_used} gpus").prop(
+            color="green", size="small", margin="0 3px 0 3px", clickable=False)
         layout = [
             mui.VBox([
                 mui.HBox([
                     mui.FlexBox([
                         mui.Icon(mui.IconType.DragIndicator).prop(),
                     ]).prop(takeDragRef=True, cursor="move"),
                     self.name,
@@ -146,15 +148,14 @@
         self.prop(draggable=True,
                   dragType="TaskCard",
                   dragInChild=True,
                   sxOverDrop={
                       "opacity": "0.5",
                   })
 
-
     async def _on_expand_more(self):
         self._expanded = not self._expanded
         icon = mui.IconType.ExpandLess if self._expanded else mui.IconType.ExpandMore
         await self.send_and_wait(
             self.collapse_btn.update_event(icon=icon.value) +
             self.detail.update_event(triggered=self._expanded))
 
@@ -195,39 +196,46 @@
             ev += self.progress.update_event(
                 value=task.state.progress * 100,
                 variant="indeterminate" if progress_0 else "determinate")
         else:
             ev += self.progress.update_event(value=task.state.progress * 100,
                                              variant="determinate")
         if task.state.tmux_pane_last_lines:
-            ev += self.tmux_pane.update_event(value=task.state.tmux_pane_last_lines)
+            ev += self.tmux_pane.update_event(
+                value=task.state.tmux_pane_last_lines)
         self.task = task
 
         return ev
 
     async def update_task_data(self, task: Task):
         await self.send_and_wait(self.update_task_data_event(task))
 
     def update_tmux_pane_event(self, data: str):
         return self.tmux_pane.update_event(value=data)
 
+
 class TmuxScheduler(mui.FlexBox):
+
     def __init__(
-        self, ssh_target: Optional[Union[SSHTarget, Callable[[], Coroutine[None, None,
-                                                                  SSHTarget]]]] = None
+        self,
+        ssh_target: Optional[Union[SSHTarget,
+                                   Callable[[], Coroutine[None, None,
+                                                          SSHTarget]]]] = None
     ) -> None:
         ssh_target_creator: Optional[Callable[[], Coroutine[None, None,
                                                             SSHTarget]]] = None
         if ssh_target is None:
             ssh_target = SSHTarget.create_fake_target()
         if isinstance(ssh_target, SSHTarget):
             ssh_desp = f"SSH: {ssh_target.username}@{ssh_target.hostname}:{ssh_target.port}"
             if ssh_target.is_localhost():
                 ssh_desp = f"SSH: localhost"
-            self.info = mui.Typography(ssh_desp).prop(margin="5px", fontSize="14px", fontFamily="monospace")
+            self.info = mui.Typography(ssh_desp).prop(margin="5px",
+                                                      fontSize="14px",
+                                                      fontFamily="monospace")
         else:
             ssh_target_creator = ssh_target
             ssh_target = SSHTarget.create_fake_target()
 
             self.info = mui.Typography(f"SSH: ").prop(margin="5px",
                                                       fontSize="14px",
                                                       fontFamily="monospace")
@@ -286,15 +294,16 @@
         # self.period_check_task.cancel()
         pass
 
     async def _period_check_task(self):
         num_tmux_pane_capture_lines = 5
         try:
             await asyncio.sleep(1)
-            updated, deleted = await self.client.update_tasks(num_tmux_pane_capture_lines)
+            updated, deleted = await self.client.update_tasks(
+                num_tmux_pane_capture_lines)
             await self.info.write(
                 self._get_info(await self._get_resource_info()))
             ev = mui.AppEvent("", {})
             new_task_cards = {}
             for updated_task in updated:
                 # print(updated_task.id, updated_task.state.status)
                 if updated_task.id not in self.task_cards:
@@ -304,19 +313,22 @@
                     # await self.task_cards[updated_task.id].update_task_data(updated_task)
                     ev += self.task_cards[
                         updated_task.id].update_task_data_event(updated_task)
             task_card_detail_expand: List[TaskCard] = []
             for task_card in self.task_cards.values():
                 if task_card._expanded:
                     task_card_detail_expand.append(task_card)
-            tmux_pane_captures = await self.client.query_tmux_panes([x.task.id for x in task_card_detail_expand], num_tmux_pane_capture_lines)
+            tmux_pane_captures = await self.client.query_tmux_panes(
+                [x.task.id for x in task_card_detail_expand],
+                num_tmux_pane_capture_lines)
 
             for task_card in task_card_detail_expand:
                 if task_card.task.id in tmux_pane_captures:
-                    ev += task_card.update_tmux_pane_event(tmux_pane_captures[task_card.task.id])
+                    ev += task_card.update_tmux_pane_event(
+                        tmux_pane_captures[task_card.task.id])
             if updated:
                 await self.send_and_wait(ev)
                 await self.tasks.update_childs(new_task_cards)
                 self.task_cards.update(new_task_cards)
             if deleted:
                 await self.tasks.remove_childs_by_keys(deleted)
                 for delete in deleted:
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/scriptmgr.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/scriptmgr.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,45 +30,49 @@
 from tensorpc.flow.flowapp.components import mui, three
 from tensorpc.flow.flowapp.components.plus.tutorials import AppInMemory
 from tensorpc.flow.flowapp.core import FrontendEventType
 from .options import CommonOptions
 
 from tensorpc.flow.client import MasterMeta
 
+
 @dataclasses.dataclass
 class Script:
     label: str
     code: Union[str, Dict[str, str]]
     lang: str
 
     def get_code(self):
         if isinstance(self.code, dict):
             return self.code.get(self.lang, "")
         else:
             return self.code
 
+
 _LANG_TO_VSCODE_MAPPING = {
     "python": "python",
     "cpp": "cpp",
     "bash": "shell",
     "app": "python",
 }
 
-async def _read_stream(stream, cb):  
+
+async def _read_stream(stream, cb):
     while True:
         line = await stream.readline()
         if line:
             try:
                 line_print = line.decode().rstrip()
             except UnicodeDecodeError:
                 line_print = line
             cb(line_print)
         else:
             break
 
+
 _INITIAL_SCRIPT_PER_LANG = {
     "python": """
 from tensorpc.flow import appctx
 import asyncio
 async def main():
     pass
 asyncio.get_running_loop().create_task(main())
@@ -91,41 +95,47 @@
 
     """,
     "bash": """
 echo "Hello World"
     """,
 }
 
+
 class ScriptManager(mui.FlexBox):
 
-    def __init__(
-        self,
-        storage_node_rid: Optional[str] = None,
-        graph_id: Optional[str] = None,
-        init_scripts: Optional[Dict[str, str]] = None
-    ):
+    def __init__(self,
+                 storage_node_rid: Optional[str] = None,
+                 graph_id: Optional[str] = None,
+                 init_scripts: Optional[Dict[str, str]] = None):
         """when storage_node_rid is None, use app node storage, else use the specified node storage
         """
         super().__init__()
         if storage_node_rid is None:
             storage_node_rid = MasterMeta().node_id
         if graph_id is None:
             graph_id = MasterMeta().graph_id
         self._storage_node_rid = storage_node_rid
 
         self._graph_id = graph_id
         self.code_editor = mui.MonacoEditor("", "python",
-                                            "default").prop(flex=1, minHeight=0, minWidth=0)
-        self.app_editor = AppInMemory("scriptmgr", "").prop(flex=1, minHeight=0, minWidth=0)
-        self.app_show_box = mui.FlexBox() # .prop(flex=1)
+                                            "default").prop(flex=1,
+                                                            minHeight=0,
+                                                            minWidth=0)
+        self.app_editor = AppInMemory("scriptmgr", "").prop(flex=1,
+                                                            minHeight=0,
+                                                            minWidth=0)
+        self.app_show_box = mui.FlexBox()  # .prop(flex=1)
 
         self.code_editor_container = mui.HBox({
-            "editor": self.code_editor,
-            "divider":  mui.Divider("horizontal"),
-            "app_show_box": self.app_show_box
+            "editor":
+            self.code_editor,
+            "divider":
+            mui.Divider("horizontal"),
+            "app_show_box":
+            self.app_show_box
         }).prop(flex=1)
         self.scripts = mui.Autocomplete(
             "Scripts",
             [],
             self._on_script_select,
         ).prop(size="small",
                muiMargin="dense",
@@ -138,30 +148,33 @@
             mui.ToggleButton("app", name="APP"),
         ], True, self._on_lang_select).prop(value="python",
                                             enforceValueSet=True)
         # self._enable_save_watch = mui.ToggleButton(
         #             "value",
         #             mui.IconType.Visibility).prop(muiColor="secondary", size="small")
         self._run_button = mui.IconButton(
-                    mui.IconType.PlayArrow,
-                    self._on_run_script).prop(progressColor="primary")
+            mui.IconType.PlayArrow,
+            self._on_run_script).prop(progressColor="primary")
         self._delete_button = mui.IconButton(
-                    mui.IconType.Delete,
-                    self._on_script_delete).prop(progressColor="primary", confirmTitle="Warning", confirmMessage="Are you sure to delete this script?")
+            mui.IconType.Delete, self._on_script_delete).prop(
+                progressColor="primary",
+                confirmTitle="Warning",
+                confirmMessage="Are you sure to delete this script?")
 
         self.init_add_layout({
-            "header": mui.HBox([
+            "header":
+            mui.HBox([
                 self.scripts.prop(flex=1),
                 self._run_button,
                 # self._enable_save_watch,
                 self.langs,
                 self._delete_button,
-
             ]).prop(alignItems="center"),
-            "editor": self.code_editor_container,
+            "editor":
+            self.code_editor_container,
         })
         self._init_scripts = _INITIAL_SCRIPT_PER_LANG.copy()
         if init_scripts is not None:
             self._init_scripts.update(init_scripts)
         self.prop(flex=1,
                   flexDirection="column",
                   width="100%",
@@ -171,36 +184,39 @@
             FrontendEventType.EditorSave.value, self._on_editor_save)
         self.scripts.register_event_handler(
             FrontendEventType.SelectNewItem.value, self._on_new_script)
         self.code_editor.register_event_handler(
             FrontendEventType.EditorReady.value, self._on_editor_ready)
 
     async def _on_editor_ready(self):
-        items = await appctx.list_data_storage(self._storage_node_rid, self._graph_id)
+        items = await appctx.list_data_storage(self._storage_node_rid,
+                                               self._graph_id)
         items.sort(key=lambda x: x.userdata["timestamp"]
                    if not isinstance(x.userdata, mui.Undefined) else 0,
                    reverse=True)
         options: List[Dict[str, Any]] = []
         for item in items:
             if item.typeStr == Script.__name__:
                 options.append({"label": item.name})
         if options:
             await self.scripts.update_options(options, 0)
             await self._on_script_select(options[0])
         else:
             await self._on_new_script({
-                    "label": "example",
-                }, init_str=self._init_scripts["python"])
+                "label": "example",
+            },
+                                      init_str=self._init_scripts["python"])
 
     async def _on_run_script(self):
         await self.code_editor.save()
         if self.scripts.value is not None:
             label = self.scripts.value["label"]
             item = await appctx.read_data_storage(label,
-                                                  self._storage_node_rid, self._graph_id)
+                                                  self._storage_node_rid,
+                                                  self._graph_id)
             assert isinstance(item, Script)
             item_uid = f"{self._graph_id}@{self._storage_node_rid}@{item.label}"
             fname = f"<{TENSORPC_FILE_NAME_PREFIX}-scripts-{item_uid}>"
             if isinstance(item.code, dict):
                 code = item.code.get(item.lang, "")
             else:
                 code = item.code
@@ -232,117 +248,131 @@
                 # if stdout:
                 #     print(f'[stdout]\n{stdout.decode()}')
                 # if stderr:
                 #     print(f'[stderr]\n{stderr.decode()}')
             elif item.lang == "cpp":
                 import ccimport
                 from ccimport.utils import tempdir
-                from pathlib import Path 
+                from pathlib import Path
                 import subprocess
 
                 with tempdir() as tempd:
                     path = Path(tempd) / "source.cc"
                     exec_path = Path(tempd) / "executable"
                     with open(path, "w") as f:
                         f.write(code)
                     sources = [
                         path,
                     ]
                     build_meta = ccimport.BuildMeta()
                     source = ccimport.ccimport(sources,
-                                            exec_path,
-                                            build_meta,
-                                            shared=False,
-                                            load_library=False,
-                                            verbose=False)
+                                               exec_path,
+                                               build_meta,
+                                               shared=False,
+                                               load_library=False,
+                                               verbose=False)
                     subprocess.check_call([str(source)])
             if item.lang == "app":
                 mod_dict = {}
                 code_comp = compile(code, fname, "exec")
                 exec(code_comp, mod_dict)
                 app_cls = mod_dict["App"]
                 layout = mui.flex_wrapper(app_cls())
                 await self.app_show_box.set_new_layout({"layout": layout})
 
     async def _on_lang_select(self, value):
         if value != "app":
             await self.app_show_box.set_new_layout({})
-        await self.send_and_wait(self.app_show_box.update_event(flex=1 if value == "app" else mui.undefined))
+        await self.send_and_wait(
+            self.app_show_box.update_event(
+                flex=1 if value == "app" else mui.undefined))
 
         if self.scripts.value is not None:
             label = self.scripts.value["label"]
 
             item = await appctx.read_data_storage(label,
-                                                  self._storage_node_rid, self._graph_id)
+                                                  self._storage_node_rid,
+                                                  self._graph_id)
             assert isinstance(item, Script)
             item.lang = value
             await self.send_and_wait(
                 self.code_editor.update_event(
-                    language=_LANG_TO_VSCODE_MAPPING[value], value=item.get_code()))
+                    language=_LANG_TO_VSCODE_MAPPING[value],
+                    value=item.get_code()))
             if value == "app":
                 # TODO add better option
                 await self._on_run_script()
-            await appctx.save_data_storage(label, item, self._storage_node_rid, self._graph_id)
+            await appctx.save_data_storage(label, item, self._storage_node_rid,
+                                           self._graph_id)
         else:
             await self.send_and_wait(
                 self.code_editor.update_event(
                     language=_LANG_TO_VSCODE_MAPPING[value]))
 
     async def _on_editor_save(self, value: str):
         if self.scripts.value is not None:
             label = self.scripts.value["label"]
             item = await appctx.read_data_storage(label,
-                                                  self._storage_node_rid, self._graph_id)
+                                                  self._storage_node_rid,
+                                                  self._graph_id)
             assert isinstance(item, Script)
             # compact new code dict
             if not isinstance(item.code, dict):
                 item.code = self._init_scripts.copy()
             item.code[item.lang] = value
-            await appctx.save_data_storage(label, item, self._storage_node_rid, self._graph_id)
+            await appctx.save_data_storage(label, item, self._storage_node_rid,
+                                           self._graph_id)
             if item.lang == "app":
                 await self._on_run_script()
             # if self._enable_save_watch.checked:
             #     await self._run_button.headless_click()
 
     async def _on_new_script(self, value, init_str: Optional[str] = None):
 
         new_item_name = value["label"]
         await self.scripts.update_options([*self.scripts.props.options, value],
                                           -1)
         lang = self.langs.props.value
         assert isinstance(lang, str)
         script = Script(new_item_name, self._init_scripts, lang)
-        await appctx.save_data_storage(new_item_name, script, self._storage_node_rid,
-                                       self._graph_id)
-        await self.send_and_wait(self.app_show_box.update_event(flex=1 if lang == "app" else mui.undefined))
+        await appctx.save_data_storage(new_item_name, script,
+                                       self._storage_node_rid, self._graph_id)
+        await self.send_and_wait(
+            self.app_show_box.update_event(
+                flex=1 if lang == "app" else mui.undefined))
         await self.send_and_wait(
             self.code_editor.update_event(
                 language=_LANG_TO_VSCODE_MAPPING[lang],
                 value=script.get_code(),
                 path=script.label))
 
     async def _on_script_delete(self):
         if self.scripts.value is not None:
             label = self.scripts.value["label"]
-            await appctx.remove_data_storage(label, self._storage_node_rid, self._graph_id)
-            new_options =  [x for x in self.scripts.props.options if x["label"] != label]
-            await self.scripts.update_options(
-            new_options, 0)
+            await appctx.remove_data_storage(label, self._storage_node_rid,
+                                             self._graph_id)
+            new_options = [
+                x for x in self.scripts.props.options if x["label"] != label
+            ]
+            await self.scripts.update_options(new_options, 0)
             if new_options:
                 await self._on_script_select(new_options[0])
 
     async def _on_script_select(self, value):
         label = value["label"]
-        item = await appctx.read_data_storage(label, self._storage_node_rid, self._graph_id)
+        item = await appctx.read_data_storage(label, self._storage_node_rid,
+                                              self._graph_id)
         assert isinstance(item, Script)
-        await self.send_and_wait(self.app_show_box.update_event(flex=1 if item.lang == "app" else mui.undefined))
-            
+        await self.send_and_wait(
+            self.app_show_box.update_event(
+                flex=1 if item.lang == "app" else mui.undefined))
+
         await self.langs.set_value(item.lang)
         await self.send_and_wait(
             self.code_editor.update_event(
                 language=_LANG_TO_VSCODE_MAPPING[item.lang],
                 value=item.get_code(),
                 path=item.label))
         if item.lang != "app":
             await self.app_show_box.set_new_layout({})
         else:
-            await self._on_run_script()
+            await self._on_run_script()
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/sliders.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/sliders.py`

 * *Ordering differences only*

 * *Files 1% similar despite different names*

```diff
@@ -49,14 +49,15 @@
                 return
             obj = self.obj_list[index]
             for handler in handlers.handlers:
                 coro = handler.cb(obj)
                 if inspect.iscoroutine(coro):
                     await coro
 
+
 class BlenderListSlider(mui.BlenderSlider, Generic[T]):
     """a slider that used for list.
     """
 
     def __init__(self,
                  callback: Callable[[T], mui._CORO_NONE],
                  init: Optional[List[T]] = None) -> None:
@@ -82,8 +83,7 @@
             if index >= len(self.obj_list):
                 return
             obj = self.obj_list[index]
             for handler in handlers.handlers:
                 coro = handler.cb(obj)
                 if inspect.iscoroutine(coro):
                     await coro
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/tutorials.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/tutorials.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,75 +13,97 @@
 from tensorpc.flow.flowapp.components import mui, three
 
 from tensorpc.constants import TENSORPC_FILE_NAME_PREFIX
 from tensorpc.flow.marker import mark_did_mount, mark_will_unmount
 from tensorpc.flow.flowapp.core import (_get_obj_def_path)
 import yaml
 
+
 @dataclasses.dataclass
 class MarkdownMetadata:
     type: Optional[Literal["Canvas", "Notebook"]] = None
 
+
 @dataclasses.dataclass
 class MarkdownBlock:
     content: str
     type: Literal["markdown", "code"] = "markdown"
 
+
 def _parse_markdown_very_trivial(content: str):
     """this function only check ```Python ``` block, then split
     markdown into several markdown blocks and code blocks.
     """
     # find comment firstly
     comment_prefix = "<!--"
     comment_suffix = "-->"
     comment_start = content.find(comment_prefix)
-    comment_end = content.find(comment_suffix, comment_start + len(comment_prefix))
+    comment_end = content.find(comment_suffix,
+                               comment_start + len(comment_prefix))
     metadata = MarkdownMetadata()
     if comment_start != -1 and comment_end != -1:
         yaml_str = content[comment_start + len(comment_prefix):comment_end]
         yaml_data = yaml.safe_load(yaml_str)
         metadata = MarkdownMetadata(**yaml_data)
         # remove comment
-        content = content[:comment_start] + content[comment_end + len(comment_suffix):]
+        content = content[:comment_start] + content[comment_end +
+                                                    len(comment_suffix):]
     res_blocks: List[MarkdownBlock] = []
     remain_code_index = 0
     code_block_prefix = "```Python"
     code_block_suffix = "```"
     while True:
         code_block_start = content.find(code_block_prefix, remain_code_index)
         if code_block_start == -1:
-            res_blocks.append(MarkdownBlock(content[remain_code_index:], "markdown"))
+            res_blocks.append(
+                MarkdownBlock(content[remain_code_index:], "markdown"))
             break
-        code_block_end = content.find(code_block_suffix, code_block_start + len(code_block_prefix))
+        code_block_end = content.find(
+            code_block_suffix, code_block_start + len(code_block_prefix))
         if code_block_end == -1:
-            res_blocks.append(MarkdownBlock(content[remain_code_index:], "markdown"))
+            res_blocks.append(
+                MarkdownBlock(content[remain_code_index:], "markdown"))
             break
-        res_blocks.append(MarkdownBlock(content[remain_code_index:code_block_start], "markdown"))
-        res_blocks.append(MarkdownBlock(content[code_block_start + len(code_block_prefix):code_block_end], "code"))
+        res_blocks.append(
+            MarkdownBlock(content[remain_code_index:code_block_start],
+                          "markdown"))
+        res_blocks.append(
+            MarkdownBlock(
+                content[code_block_start +
+                        len(code_block_prefix):code_block_end], "code"))
         remain_code_index = code_block_end + len(code_block_suffix)
     return res_blocks, metadata
 
 
 class AppInMemory(mui.FlexBox):
     """app with editor (app must be anylayout)
     """
+
     # @dataclasses.dataclass
     class Config:
         is_horizontal: bool = True
         height: Union[mui.ValueType, mui.Undefined] = mui.undefined
 
-    def __init__(self, path: str, code: str, is_horizontal: bool = True,
-            external_onsave: Optional[Callable[[str], Coroutine[Any, Any, None]]] = None):
+    def __init__(self,
+                 path: str,
+                 code: str,
+                 is_horizontal: bool = True,
+                 external_onsave: Optional[Callable[[str],
+                                                    Coroutine[Any, Any,
+                                                              None]]] = None):
         wrapped_path = f"<{TENSORPC_FILE_NAME_PREFIX}-{path}>"
-        self.editor = mui.MonacoEditor(code, "python", wrapped_path).prop(minWidth=0, minHeight=0)
-        self.path = wrapped_path 
-        self.code = code 
+        self.editor = mui.MonacoEditor(code, "python",
+                                       wrapped_path).prop(minWidth=0,
+                                                          minHeight=0)
+        self.path = wrapped_path
+        self.code = code
         self.app_cls_name = "App"
         self.show_box = mui.FlexBox().prop(overflowY="auto")
-        self.divider = mui.Divider("horizontal" if is_horizontal else "vertical")
+        self.divider = mui.Divider(
+            "horizontal" if is_horizontal else "vertical")
         super().__init__([
             self.editor.prop(flex=1),
             self.divider,
             self.show_box.prop(flex=1),
         ])
         self._layout_for_reload: Optional[mui.FlexBox] = None
         self.prop(flexFlow="row" if is_horizontal else "column")
@@ -95,28 +117,35 @@
         reload_mgr.in_memory_fs.add_file(self.path, self.code)
         mod = reload_mgr.in_memory_fs.load_in_memory_module(self.path)
         app_cls = mod.__dict__[self.app_cls_name]
         if hasattr(app_cls, "Config"):
             cfg_cls = getattr(app_cls, "Config")
             assert issubclass(cfg_cls, AppInMemory.Config)
             if cfg_cls.is_horizontal:
-                await self.send_and_wait(self.update_event(flexFlow="row") + self.divider.update_event(orientation="horizontal"))
+                await self.send_and_wait(
+                    self.update_event(flexFlow="row") +
+                    self.divider.update_event(orientation="horizontal"))
             else:
-                await self.send_and_wait(self.update_event(flexFlow="column") + self.divider.update_event(orientation="vertical"))
+                await self.send_and_wait(
+                    self.update_event(flexFlow="column") +
+                    self.divider.update_event(orientation="vertical"))
             if cfg_cls.height is not mui.undefined:
-                await self.send_and_wait(self.update_event(height=cfg_cls.height))
+                await self.send_and_wait(
+                    self.update_event(height=cfg_cls.height))
         layout = mui.flex_wrapper(app_cls())
         self._layout_for_reload = layout
         await self.show_box.update_childs({"layout": layout})
-        appctx.get_editable_app()._flowapp_observe(layout, self._handle_reload_layout)
+        appctx.get_editable_app()._flowapp_observe(layout,
+                                                   self._handle_reload_layout)
 
     @mark_will_unmount
     async def _on_unmount(self):
         if self._layout_for_reload is not None:
-            appctx.get_editable_app()._flowapp_remove_observer(self._layout_for_reload)
+            appctx.get_editable_app()._flowapp_remove_observer(
+                self._layout_for_reload)
 
     async def _handle_reload_layout(self, layout: mui.FlexBox,
                                     create_layout: ServFunctionMeta):
         # if create_layout.user_app_meta is not None and create_layout.user_app_meta.type == AppFuncType.CreateLayout:
         layout_flex = create_layout.get_binded_fn()()
         assert isinstance(
             layout_flex, mui.FlexBox
@@ -126,68 +155,85 @@
         await self.show_box.update_childs({"layout": layout_flex})
 
     async def _on_editor_save(self, value: str):
         reload_mgr = self.flow_app_comp_core.reload_mgr
         reload_mgr.in_memory_fs.modify_file(self.path, value)
         if self._external_onsave is not None:
             await self._external_onsave(value)
-        await appctx.get_editable_app()._reload_object_with_new_code(self.path, value)
+        await appctx.get_editable_app()._reload_object_with_new_code(
+            self.path, value)
+
 
 class CodeBlock(mui.FlexBox):
     """app with editor (app must be anylayout)
     """
+
     # @dataclasses.dataclass
     class Config:
         is_horizontal: bool = True
         height: Union[mui.ValueType, mui.Undefined] = mui.undefined
 
     def __init__(self, code: str):
-        self.editor = mui.MonacoEditor(code, "python", "").prop(minWidth=0, minHeight=0)
-        self.code = code 
+        self.editor = mui.MonacoEditor(code, "python", "").prop(minWidth=0,
+                                                                minHeight=0)
+        self.code = code
         super().__init__([
             mui.Button("run", self._on_run),
             self.editor.prop(flex=1),
         ])
         self._layout_for_reload: Optional[mui.FlexBox] = None
         self.prop(flexFlow="column")
         self.editor.event_editor_save.on(self._on_editor_save)
 
     async def _on_editor_save(self, value: str):
-        self.code = value 
+        self.code = value
 
     async def _on_run(self):
         code_comp = compile(self.code, f"", "exec")
         exec(code_comp)
 
+
 class MarkdownTutorial(mui.FlexBox):
     """ this component parse markdowns in a very simple way, don't use it in production, it's only for tutorials.
     """
+
     def __init__(self, md_content: str, path_uid: str):
         res_blocks, metadata = _parse_markdown_very_trivial(md_content)
         layout: mui.LayoutType = []
         if metadata.type == "Canvas":
-            from tensorpc.flow import plus 
-            complex_canvas = plus.ComplexCanvas(init_enable_grid=False).prop(width="100%", flex=1)
+            from tensorpc.flow import plus
+            complex_canvas = plus.ComplexCanvas(init_enable_grid=False).prop(
+                width="100%", flex=1)
             blocks: mui.LayoutType = []
             for i, block in enumerate(res_blocks):
                 if block.type == "markdown":
                     if block.content.strip() == "":
                         continue
                     blocks.append(mui.Markdown(block.content))
                 elif block.type == "code":
-                    blocks.append(CodeBlock(block.content.lstrip()).prop(height="200px", padding="10px"))
+                    blocks.append(
+                        CodeBlock(block.content.lstrip()).prop(height="200px",
+                                                               padding="10px"))
             book = mui.VirtualizedBox(blocks).prop(overflow="auto", flex=1)
             layout = [complex_canvas, book]
         else:
             blocks: mui.LayoutType = []
             for i, block in enumerate(res_blocks):
                 if block.type == "markdown":
                     if block.content.strip() == "":
                         continue
                     blocks.append(mui.Markdown(block.content))
                 elif block.type == "code":
-                    blocks.append(AppInMemory(f"{path_uid}-{i}", block.content.lstrip()).prop(minHeight="400px", padding="10px"))
+                    blocks.append(
+                        AppInMemory(f"{path_uid}-{i}",
+                                    block.content.lstrip()).prop(
+                                        minHeight="400px", padding="10px"))
             book = mui.VirtualizedBox(blocks)
             layout = [book]
         super().__init__(layout)
-        self.prop(flexFlow="column nowrap", padding="10px", overflow="hidden", minHeight=0, minWidth=0, height="100%", width="100%")
-
+        self.prop(flexFlow="column nowrap",
+                  padding="10px",
+                  overflow="hidden",
+                  minHeight=0,
+                  minWidth=0,
+                  height="100%",
+                  width="100%")
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/canvas.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/canvas.py`

 * *Files 1% similar despite different names*

```diff
@@ -218,27 +218,28 @@
     V.points("point_key", limit=5000).array(points)
     box = V.bounding_box((3, 1, 1), pos=(0, 3, 0))
 """
 
 
 class ComplexCanvasView(three.View):
 
-    def __init__(
-            self,
-            key: str,
-            childs: Optional[three.ThreeLayoutType] = None,
-            transparent_canvas: bool = False,
-            custom_effect: Optional[three.EffectComposer] = None,
-            camera: Optional[Union[three.PerspectiveCamera, three.OrthographicCamera]] = None,
-            control: Optional[three.CameraControl] = None,
-            enable_gizmo: bool = True):
+    def __init__(self,
+                 key: str,
+                 childs: Optional[three.ThreeLayoutType] = None,
+                 transparent_canvas: bool = False,
+                 custom_effect: Optional[three.EffectComposer] = None,
+                 camera: Optional[Union[three.PerspectiveCamera,
+                                        three.OrthographicCamera]] = None,
+                 control: Optional[three.CameraControl] = None,
+                 enable_gizmo: bool = True):
         if camera is None:
             camera = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
         self.camera = camera
-        self.camera.prop(layers=1 | (1 << 31)) # only objects in layer 0 and 31 is visible.
+        self.camera.prop(
+            layers=1 | (1 << 31))  # only objects in layer 0 and 31 is visible.
         if control is None:
             control = three.CameraControl().prop(makeDefault=True)
         self.ctrl = control
         self._infgrid = three.InfiniteGridHelper(5, 50, "gray")
         self._axis_helper = three.AxesHelper(20)
 
         self._dynamic_grid = three.Group([self._infgrid, self._axis_helper])
@@ -246,15 +247,15 @@
         init_layout = {
             # "camera": self.camera,
             "grid": self._dynamic_grid,
             # "utree": self._user_obj_tree_group,
             "effects": custom_effect,
         }
         gizmo_helper = three.GizmoHelper().prop(alignment="bottom-right",
-                                                      renderPriority=2)
+                                                renderPriority=2)
         if enable_gizmo:
             init_layout["gizmo"] = gizmo_helper
         self._lock = asyncio.Lock()
         for comp in init_layout.values():
             lock_component(comp)
         reserved_group = three.Group(init_layout)
         lock_component(self._axis_helper)
@@ -263,66 +264,69 @@
         layout: three.ThreeLayoutType = {
             "reserved": reserved_group,
         }
         self.reserved_group = reserved_group
         if childs is not None:
             layout["init"] = three.Group(childs)
         self._init_layout = layout
-        self._item_root = three.SelectionContext(layout,
-                                                 self._on_3d_object_select).prop(useOutline=True)
+        self._item_root = three.SelectionContext(
+            layout, self._on_3d_object_select).prop(useOutline=True)
 
         super().__init__({
             "root": self._item_root,
             "camera": self.camera,
             "control": self.ctrl,
         })
-        self.prop(flex=1, allowKeyboardEvent=True, menuItems=[
-                    mui.MenuItem("reset", "reset"),
-                    mui.MenuItem("clear", "clear"),
-                ])
+        self.prop(flex=1,
+                  allowKeyboardEvent=True,
+                  menuItems=[
+                      mui.MenuItem("reset", "reset"),
+                      mui.MenuItem("clear", "clear"),
+                  ])
         self.event_context_menu.on(self._on_menu_select)
 
     def _set_selection_callback(self):
-        pass 
+        pass
 
     async def _on_menu_select(self, value: str):
         if value == "reset":
             await self._on_reset_cam()
         elif value == "clear":
             await self.clear()
 
     async def clear(self):
         await self._item_root.set_new_layout({**self._init_layout})
         await self.item_tree.update_tree()
 
     async def _on_reset_cam(self):
         await self.ctrl.reset_camera()
 
+
 class ComplexCanvas(mui.FlexBox):
     """
     a blender-like canvas
     Design:
         * put controls to left as toggle buttons
         * put canvas object tree view to right
         * support helpers such as light, camera, etc.
         * support switch to camera view
     """
 
     def __init__(
-            self,
-            init_canvas_childs: Optional[three.ThreeLayoutType] = None,
-            transparent_canvas: bool = False,
-            init_tree_root: Optional[Any] = None,
-            init_tree_child_accessor: Optional[Callable[[Any],
-                                                        Dict[str,
-                                                             Any]]] = None,
-            key: str = "canvas",
-            custom_effect: Optional[three.EffectComposer] = None,
-            init_enable_grid: bool = True,
-            camera: Optional[Union[three.PerspectiveCamera, three.OrthographicCamera]] = None):
+        self,
+        init_canvas_childs: Optional[three.ThreeLayoutType] = None,
+        transparent_canvas: bool = False,
+        init_tree_root: Optional[Any] = None,
+        init_tree_child_accessor: Optional[Callable[[Any], Dict[str,
+                                                                Any]]] = None,
+        key: str = "canvas",
+        custom_effect: Optional[three.EffectComposer] = None,
+        init_enable_grid: bool = True,
+        camera: Optional[Union[three.PerspectiveCamera,
+                               three.OrthographicCamera]] = None):
 
         super().__init__()
         self.component_tree = three.Fragment([])
         if camera is None:
             camera = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
         self.camera = camera
         self._init_enable_grid = init_enable_grid
@@ -423,16 +427,16 @@
         layout: three.ThreeLayoutType = {
             "reserved": reserved_group,
         }
         self.reserved_group = reserved_group
         if init_canvas_childs is not None:
             layout["init"] = three.Group(init_canvas_childs)
         self._init_layout = layout
-        self._item_root = three.SelectionContext(layout,
-                                                 self._on_3d_object_select).prop(useOutline=False)
+        self._item_root = three.SelectionContext(
+            layout, self._on_3d_object_select).prop(useOutline=False)
         # self._item_root = three.Group(layout)
         self.key = key
         self.prop_container = mui.HBox([]).prop(overflow="auto",
                                                 padding="3px",
                                                 flex=1,
                                                 width="100%",
                                                 height="100%")
@@ -447,18 +451,21 @@
                                                     width="100%",
                                                     height="100%")
 
         self.canvas = three.Canvas({
             "root": self._item_root,
             "camera": self.camera,
             "control": self.ctrl,
-        }).prop(flex=1, allowKeyboardEvent=True, shadows=True, menuItems=[
-            mui.MenuItem("reset", "reset"),
-            mui.MenuItem("clear", "clear"),
-        ])
+        }).prop(flex=1,
+                allowKeyboardEvent=True,
+                shadows=True,
+                menuItems=[
+                    mui.MenuItem("reset", "reset"),
+                    mui.MenuItem("clear", "clear"),
+                ])
         self.canvas.event_context_menu.on(self._on_menu_select)
         self.custom_tree_handler = CanvasTreeItemHandler()
         self.item_tree = BasicObjectTree(
             self._item_root,
             use_init_as_root=True,
             custom_tree_handler=self.custom_tree_handler,
             default_expand_level=1000,
@@ -467,15 +474,14 @@
         self.init_add_layout([*self._layout_func()])
         self.prop(
             droppable=True,
             border="4px solid transparent",
             sxOverDrop={"border": "4px solid green"},
         )
 
-
     @marker.mark_create_layout
     def _layout_func(self):
         help_string = (f"Keyboard\n"
                        f"WSAD: move camera\n"
                        f"Z: descend camera\n"
                        f"SpaceBar: ascend camera\n"
                        f"use dolly (wheel) to\n"
@@ -521,18 +527,15 @@
                     mui.IconButton(mui.IconType.Refresh,
                                    callback=self._on_reset_cam).prop(
                                        tooltip="Reset Camera",
                                        tooltipPlacement="right"),
                 ]).prop(backgroundColor="lavender", borderRadius="4px"),
                 # self._cfg_panel,
                 self._cfg_container,
-            ]).prop(position="absolute",
-                    top=3,
-                    left=3,
-                    zIndex=5),
+            ]).prop(position="absolute", top=3, left=3, zIndex=5),
             mui.HBox([
                 mui.IconButton(mui.IconType.Help, lambda: None).prop(
                     tooltip=help_string,
                     tooltipMultiline=True,
                 ),
             ]).prop(top=3, position="absolute", right=3, zIndex=5),
             self.background_img.prop(position="absolute",
@@ -615,23 +618,20 @@
                                icon=mui.IconType.DashboardCustomize,
                                tooltip="Custom Grid"),
                     mui.TabDef("",
                                "5",
                                self.array_table,
                                icon=mui.IconType.Dataset,
                                tooltip="numpy array table"),
-
-                    mui.TabDef(
-                        "",
-                        "6",
-                        ScriptManager(init_scripts={
-                            "python": _EXAMPLE_SCRIPT
-                        }),
-                        icon=mui.IconType.Code,
-                        tooltip="python script playground"),
+                    mui.TabDef("",
+                               "6",
+                               ScriptManager(
+                                   init_scripts={"python": _EXAMPLE_SCRIPT}),
+                               icon=mui.IconType.Code,
+                               tooltip="python script playground"),
                 ], "2").prop(panelProps=mui.FlexBoxProps(width="100%",
                                                          padding=0),
                              orientation="vertical",
                              borderRight=1,
                              tooltipPlacement="right",
                              borderColor='divider')
             ], tab_theme)
@@ -684,27 +684,33 @@
         await self._user_obj_tree_group.set_new_layout({**groups})
         self.gv_tree_layout.set_tree_root(tree_root)
         await self.gv_tree_layout.set_new_items({
             ".".join(k): v
             for k, v in flatted_tree_nodes.items()
         })
 
-    async def set_new_grid_items(self, items: Dict[str, Any], is_local: bool = False):
+    async def set_new_grid_items(self,
+                                 items: Dict[str, Any],
+                                 is_local: bool = False):
         if is_local:
             await self.gv_locals_layout.set_new_items(items)
         else:
             await self.gv_custom_layout.set_new_items(items)
 
-    async def update_grid_items(self, items: Dict[str, Any], is_local: bool = False):
+    async def update_grid_items(self,
+                                items: Dict[str, Any],
+                                is_local: bool = False):
         if is_local:
             await self.gv_locals_layout.update_items(items)
         else:
             await self.gv_custom_layout.update_items(items)
 
-    async def delete_grid_items(self, items: List[str], is_local: bool = False):
+    async def delete_grid_items(self,
+                                items: List[str],
+                                is_local: bool = False):
         if is_local:
             await self.gv_locals_layout.delete_items(items)
         else:
             await self.gv_custom_layout.delete_items(items)
 
     async def clear_grid(self, is_local: bool = False):
         if is_local:
@@ -791,15 +797,14 @@
                                  partial(self._gv_cards_callback,
                                          group=group)))
         return groups, obj_to_item_meta
 
     def _get_tdata_container_pane(self, tdata_table: mui.DataGrid):
         return mui.Allotment.Pane(tdata_table, preferredSize=1)
 
-
     async def _on_pan_to_fwd(self, selected):
         await self.ctrl.send_and_wait(
             self.ctrl.update_event(verticalDragToForward=not selected))
 
     async def _on_enable_grid(self, selected):
         if selected:
             await self._dynamic_grid.set_new_layout(
@@ -1076,40 +1081,43 @@
                 await self.item_tree.tree.update_subtree(cfg.node)
         await obj.send_and_wait(
             obj.create_update_event({
                 uid: value,
             }, validate=True))
 
     def _get_local_uid_of_object(self, uid: UniqueTreeId):
-        assert self._item_root._flow_uid is not None 
+        assert self._item_root._flow_uid is not None
         assert uid.startswith(self._item_root._flow_uid)
-        return UniqueTreeIdForTree.from_parts(uid.parts[uid.common_prefix_index(self._item_root._flow_uid):])
+        return UniqueTreeIdForTree.from_parts(
+            uid.parts[uid.common_prefix_index(self._item_root._flow_uid):])
 
     def _convert_tree_node_uid_to_local_uid(self, uid: UniqueTreeId):
         assert uid.parts[0] == "root"
         return UniqueTreeIdForTree.from_parts(uid.parts[1:])
 
     async def _on_3d_object_select(self, selected: list):
         if not selected:
             await self.item_tree.tree.select([])
             return
         assert len(selected) == 1
         select = selected[0]
         selected_uid = select["userData"]["uid"]
         # print(self.item_tree.tree.props.tree)
-        selected_uid_local_uid = self._get_local_uid_of_object(UniqueTreeIdForTree(selected_uid))
+        selected_uid_local_uid = self._get_local_uid_of_object(
+            UniqueTreeIdForTree(selected_uid))
 
         container_parents, remain_keys, _ = find_component_trace_by_uid_with_not_exist_parts(
             self._item_root, selected_uid_local_uid)
         if len(remain_keys) == 0:
             obj = container_parents[-1]
             # we need to convert object component uid to tree node uid.
             # tree node uid always start with "root"
             # tree_node_uid = f"root::{selected_uid_local_uid.replace('.', '::')}"
-            tree_node_uid = UniqueTreeIdForTree.from_parts(["root"] + selected_uid_local_uid.parts)
+            tree_node_uid = UniqueTreeIdForTree.from_parts(
+                ["root"] + selected_uid_local_uid.parts)
             await self.item_tree.tree.select([tree_node_uid.uid_encoded])
             await self.item_tree._on_select_single(tree_node_uid.uid_encoded)
             # print(selected_uid_local_uid, obj, obj._flow_uid)
 
     async def _unknown_visualization(self,
                                      tree_id: str,
                                      obj: Any,
@@ -1144,24 +1152,22 @@
                 self._random_colors[tree_id_replaced] = pick
             with V.enter_v_conetxt(V.VContext(self, unk_container)):
                 points = V.points(tree_id_replaced, pc_obj.shape[0]).array(
                     pc_obj.astype(np.float32))
                 if pc_obj.shape[1] == 3:
                     points.prop(colors=pick)
 
-            await V._draw_all_in_vctx(vctx_unk,
-                                      unk_container._flow_uid)
+            await V._draw_all_in_vctx(vctx_unk, unk_container._flow_uid)
             return True
         img_obj = _try_cast_to_image(obj)
         if img_obj is not None:
             with V.enter_v_conetxt(V.VContext(self, unk_container)):
                 V.image(img_obj, name=tree_id_replaced,
                         pos=(0, 0, 0.1)).prop(scale=(3, 3, 3))
-            await V._draw_all_in_vctx(vctx_unk,
-                                      unk_container._flow_uid)
+            await V._draw_all_in_vctx(vctx_unk, unk_container._flow_uid)
             return True
         b3d_obj = _try_cast_to_box3d(obj)
         if b3d_obj is not None:
             if tree_id in self._random_colors:
                 pick = self._random_colors[tree_id]
             else:
                 random_colors = colors.RANDOM_COLORS_FOR_UI
@@ -1170,16 +1176,15 @@
                 self._random_colors[tree_id] = pick
             with V.enter_v_conetxt(V.VContext(self, unk_container)):
 
                 with V.group(tree_id_replaced):
                     for box in b3d_obj:
                         V.bounding_box(box[3:6], (0, 0, box[6]),
                                        box[:3]).prop(color=pick)
-            await V._draw_all_in_vctx(vctx_unk,
-                                      unk_container._flow_uid)
+            await V._draw_all_in_vctx(vctx_unk, unk_container._flow_uid)
             return True
         line_obj = _try_cast_to_lines(obj)
         if line_obj is not None:
             if tree_id in self._random_colors:
                 pick = self._random_colors[tree_id]
             else:
                 random_colors = colors.RANDOM_COLORS_FOR_UI
@@ -1187,16 +1192,15 @@
                     unk_container, three.Segments) % len(random_colors)]
                 self._random_colors[tree_id] = pick
             with V.enter_v_conetxt(V.VContext(self, unk_container)):
 
                 points = V.lines(tree_id_replaced, line_obj.shape[0]).array(
                     line_obj.astype(np.float32))
 
-            await V._draw_all_in_vctx(vctx_unk,
-                                      unk_container._flow_uid)
+            await V._draw_all_in_vctx(vctx_unk, unk_container._flow_uid)
             return True
         return False
 
     async def _on_drop(self, data):
         from tensorpc.flow.flowapp.components.plus import BasicObjectTree
         if isinstance(data, TreeDragTarget):
             obj = data.obj
@@ -1204,15 +1208,16 @@
 
             success = await self._unknown_visualization(data.tree_id, obj)
             if success:
                 # register to tree
                 tree = find_component_by_uid_with_type_check(
                     data.source_comp_uid, BasicObjectTree)
                 if tree is not None:
-                    tree._register_dnd_uid(UniqueTreeIdForTree(data.tree_id), self._dnd_cb)
+                    tree._register_dnd_uid(UniqueTreeIdForTree(data.tree_id),
+                                           self._dnd_cb)
                     self._dnd_trees.add(data.source_comp_uid)
 
     async def _dnd_cb(self, uid: UniqueTreeIdForTree, data: Any):
         await self._unknown_visualization(uid.uid_encoded, data)
 
     async def register_cam_control_event_handler(
             self,
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/core.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,50 +1,53 @@
-
 import json
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple
 from tensorpc.core.dataclass_dispatch import dataclass, field
 from pydantic_core import PydanticCustomError, core_schema
 from pydantic import (
-    GetCoreSchemaHandler,
-)
+    GetCoreSchemaHandler, )
 from tensorpc.core.tree_id import UniqueTreeId, UniqueTreeIdForTree
 from tensorpc.flow.jsonlike import JsonLikeNode
 from ... import mui, three
 if TYPE_CHECKING:
     from .canvas import ComplexCanvas
 from tensorpc.utils.uniquename import UniqueNamePool
 
 UNKNOWN_VIS_KEY = "unknown_vis"
 UNKNOWN_KEY_SPLIT = "%"
 RESERVED_NAMES = set([UNKNOWN_VIS_KEY, "reserved"])
 
+
 def is_reserved_name(name: str):
     parts = name.split(".")
     return parts[0] in RESERVED_NAMES
 
+
 def is_reserved_uid(uid: UniqueTreeId):
     parts = uid.parts
     return parts[0] in RESERVED_NAMES
 
+
 class CanvasItemProxy:
+
     def __init__(self) -> None:
         super().__init__()
         self._detail_layout: Optional[mui.FlexBox] = None
 
         self._tdata: Optional[Dict[str, Any]] = None
 
     # def update_event(self, comp: three.Component):
-    #     pass 
+    #     pass
 
     # def detail_layout(self, layout: mui.FlexBox):
     #     self._detail_layout = layout
     #     return self
 
     @classmethod
-    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler: GetCoreSchemaHandler):
+    def __get_pydantic_core_schema__(cls, _source_type: Any,
+                                     _handler: GetCoreSchemaHandler):
         return core_schema.no_info_after_validator_function(
             cls.validate,
             core_schema.any_schema(),
         )
 
     @classmethod
     def validate(cls, v):
@@ -54,44 +57,49 @@
 
     # def tdata(self, data: Dict[str, Any]):
     #     # make sure data is json serializable
     #     json.dumps(data)
     #     self._tdata = data
     #     return self
 
+
 @dataclass
 class CanvasItemCfg:
     lock: bool = False
     visible: bool = True
-    proxy: Optional[CanvasItemProxy] = None 
+    proxy: Optional[CanvasItemProxy] = None
     is_vapi: bool = False
     # if exists, will use it as detail layout
     detail_layout: Optional[mui.Component] = None
     type_str_override: Optional[str] = None
     alias: Optional[str] = None
     node: Optional[JsonLikeNode] = None
     tdata: Optional[Dict[str, Any]] = None
 
+
 def get_canvas_item_cfg(comp: three.Component) -> Optional[CanvasItemCfg]:
     res = comp.find_user_meta_by_type(CanvasItemCfg)
     if res is not None:
         return res
-    return None 
+    return None
 
-def get_or_create_canvas_item_cfg(comp: three.Component, is_vapi: Optional[bool] = None):
+
+def get_or_create_canvas_item_cfg(comp: three.Component,
+                                  is_vapi: Optional[bool] = None):
     res = comp.find_user_meta_by_type(CanvasItemCfg)
     if res is None:
         res = CanvasItemCfg()
         comp._flow_user_datas.append(res)
     if is_vapi is not None:
         res.is_vapi = is_vapi
     return res
 
 
 class VContext:
+
     def __init__(self,
                  canvas: "ComplexCanvas",
                  root: Optional[three.ContainerBase] = None):
         self.stack = []
         self.canvas = canvas
         self.name_stack: List[str] = []
         self.exist_name_stack: List[str] = []
@@ -117,34 +125,35 @@
         self.stack.clear()
         self.name_stack.clear()
         self.exist_name_stack.clear()
         self._name_to_group.clear()
         self._name_to_group[""] = self.root
         self._group_assigns.clear()
 
-
     @classmethod
-    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler: GetCoreSchemaHandler):
+    def __get_pydantic_core_schema__(cls, _source_type: Any,
+                                     _handler: GetCoreSchemaHandler):
         return core_schema.no_info_after_validator_function(
             cls.validate,
             core_schema.any_schema(),
         )
 
     @classmethod
     def validate(cls, v):
         if not isinstance(v, VContext):
             raise ValueError('VContext required')
         return v
 
 
-
 class ContainerProxy(CanvasItemProxy):
     pass
 
+
 class GroupProxy(ContainerProxy):
+
     def __init__(self, uid: str) -> None:
         super().__init__()
         self.uid = uid
 
         self.childs: Dict[str, three.Component] = {}
 
         self.namepool = UniqueNamePool()
@@ -154,36 +163,45 @@
 
     def clear(self):
         self.childs.clear()
         self.namepool.clear()
 
 
 class UserTreeItemCard(mui.FlexBox):
-    def __init__(self, name: str, type_str: str, callback: Callable[[bool], Any], init_width: mui.ValueType = "240px"):
+
+    def __init__(self,
+                 name: str,
+                 type_str: str,
+                 callback: Callable[[bool], Any],
+                 init_width: mui.ValueType = "240px"):
         self.preview_layout = mui.VBox([])
         self._is_selected = mui.Checkbox(callback=callback).prop(checked=True)
-        self._print_blocks = mui.DataFlexBox(mui.Markdown().set_override_props(value="data")).prop(flexFlow="column nowrap", overflowY="auto", height="100%")
+        self._print_blocks = mui.DataFlexBox(mui.Markdown().set_override_props(
+            value="data")).prop(flexFlow="column nowrap",
+                                overflowY="auto",
+                                height="100%")
         layout: mui.LayoutType = [
             mui.HBox([
-                mui.Typography(f"{name}@{type_str}").prop(variant="caption", flex=1),
+                mui.Typography(f"{name}@{type_str}").prop(variant="caption",
+                                                          flex=1),
                 self._is_selected.prop(size="small"),
             ]),
             mui.HDivider(),
             self.preview_layout,
             self._print_blocks,
         ]
         super().__init__(layout)
         self.prop(flexFlow="column nowrap", height="100%", width=init_width)
         self.name = name
         self.type_str = type_str
 
     @property
     def is_selected(self):
         return self._is_selected.checked
-    
+
     def print_blocks_event(self, data: List[str]):
         data_list = [{"id": i, "data": d} for i, d in enumerate(data)]
         return self._print_blocks.update_event(dataList=data_list)
 
 
 @dataclass
 class CanvasUserTreeItem:
@@ -194,9 +212,10 @@
     # childs will be filled from vctx when top vctx exit
     childs: Dict[str, three.ThreeComponentBase] = field(default_factory=dict)
 
     md_prints: List[str] = field(default_factory=list)
 
 
 class _VapiObjects:
+
     def prepare_vapi_props(self) -> None:
         raise NotImplementedError
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/treeview.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/treeview.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,63 +9,69 @@
 from tensorpc.flow.flowapp.components import mui, three
 from .core import CanvasItemCfg, CanvasItemProxy
 
 
 def lock_component(comp: mui.Component):
     user_meta = comp.find_user_meta_by_type(CanvasItemCfg)
     if user_meta is not None:
-        user_meta.lock = True 
+        user_meta.lock = True
     else:
         user_meta = CanvasItemCfg(lock=True)
         comp._flow_user_datas.append(user_meta)
     return comp
 
+
 def set_component_visible(comp: mui.Component, visible: bool):
     user_meta = comp.find_user_meta_by_type(CanvasItemCfg)
     if user_meta is not None:
-        user_meta.visible = visible 
+        user_meta.visible = visible
     else:
         user_meta = CanvasItemCfg(visible=visible)
         comp._flow_user_datas.append(user_meta)
     return comp
 
 
 class CanvasButtonType(enum.Enum):
     Visibility = "visibility"
     Delete = "delete"
 
+
 class CanvasTreeItemHandler(CustomTreeItemHandler):
 
     def _get_icon_button(self, obj: mui.Component) -> List[IconButtonData]:
         res = [
-            IconButtonData(CanvasButtonType.Visibility.value, mui.IconType.Visibility, "toggle visibility"),
-            IconButtonData(CanvasButtonType.Delete.value, mui.IconType.Delete, "toggle visibility"),
+            IconButtonData(CanvasButtonType.Visibility.value,
+                           mui.IconType.Visibility, "toggle visibility"),
+            IconButtonData(CanvasButtonType.Delete.value, mui.IconType.Delete,
+                           "toggle visibility"),
         ]
         user_meta = obj.find_user_meta_by_type(CanvasItemCfg)
         if user_meta is not None:
             if user_meta.lock:
                 res.pop()
             if not user_meta.visible:
                 res[0].icon = mui.IconType.VisibilityOff
-            if not isinstance(obj, (three.Object3dBase, three.Object3dContainerBase)):
+            if not isinstance(
+                    obj, (three.Object3dBase, three.Object3dContainerBase)):
                 res.pop(0)
-        return res 
+        return res
 
     async def get_childs(self, obj: Any) -> Optional[Dict[str, Any]]:
         """if return None, we will use default method to extract childs
         of object.
         """
         # print(obj, isinstance(obj, mui.Component) and three.is_three_component(obj), "WTF")
         if isinstance(obj, mui.Component) and three.is_three_component(obj):
             if isinstance(obj, mui.ContainerBase):
                 return obj._child_comps
             return {}
         return {}
 
-    def patch_node(self, obj: Any, node: mui.JsonLikeNode) -> Optional[mui.JsonLikeNode]:
+    def patch_node(self, obj: Any,
+                   node: mui.JsonLikeNode) -> Optional[mui.JsonLikeNode]:
         """modify/patch node created from `parse_obj_to_tree_node`
         """
         if isinstance(obj, mui.Component) and three.is_three_component(obj):
             # buttons: visibility, delete
             user_meta = obj.find_user_meta_by_type(CanvasItemCfg)
             if user_meta is None:
                 user_meta = CanvasItemCfg()
@@ -73,34 +79,41 @@
             if isinstance(user_meta, CanvasItemCfg):
                 if user_meta.type_str_override is not None:
                     node.typeStr = user_meta.type_str_override
                 if user_meta.alias is not None:
                     node.alias = user_meta.alias
                 user_meta.node = node
             node.fixedIconBtns = self._get_icon_button(obj)
-        return None 
+        return None
 
-    async def handle_button(self, obj_trace: List[Any], node_trace: List[mui.JsonLikeNode], button_id: str) -> Optional[bool]:
+    async def handle_button(self, obj_trace: List[Any],
+                            node_trace: List[mui.JsonLikeNode],
+                            button_id: str) -> Optional[bool]:
         obj = obj_trace[-1]
         node = node_trace[-1]
         if isinstance(obj, mui.Component):
             item_cfg = obj.find_user_meta_by_type(CanvasItemCfg)
             if item_cfg is None:
                 item_cfg = CanvasItemCfg()
                 obj._flow_user_datas.append(item_cfg)
             if button_id == CanvasButtonType.Visibility.value:
                 item_cfg.visible = not item_cfg.visible
                 node.fixedIconBtns = self._get_icon_button(obj)
                 await get_tree_context_noexcept().tree.update_subtree(node)
-                if isinstance(obj, (three.Object3dBase, three.Object3dContainerBase)):
+                if isinstance(
+                        obj,
+                    (three.Object3dBase, three.Object3dContainerBase)):
                     await obj.update_object3d(visible=item_cfg.visible)
             elif button_id == CanvasButtonType.Delete.value:
                 if len(obj_trace) > 1 and not item_cfg.lock:
                     obj_container = obj_trace[-2]
                     if isinstance(obj_container, mui.ContainerBase):
                         await obj_container.remove_childs_by_keys([node.name])
-                        await get_tree_context_noexcept().get_tree_instance(BasicObjectTree).update_tree()
+                        await get_tree_context_noexcept().get_tree_instance(
+                            BasicObjectTree).update_tree()
                         return True
         return None
-    
-    async def handle_context_menu(self, obj_trace: List[Any], node_trace: List[mui.JsonLikeNode], userdata: Dict[str, Any]) -> Optional[bool]:
+
+    async def handle_context_menu(self, obj_trace: List[Any],
+                                  node_trace: List[mui.JsonLikeNode],
+                                  userdata: Dict[str, Any]) -> Optional[bool]:
         return None
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/plus/vis/vapi_core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/plus/vis/vapi_core.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,178 +50,214 @@
                           annotated_function_to_dataclass)
 from .canvas import ComplexCanvas, find_component_trace_by_uid_with_not_exist_parts
 from .core import CanvasItemCfg, CanvasItemProxy, is_reserved_name, VContext, _VapiObjects
 import numpy as np
 from .core import get_canvas_item_cfg, get_or_create_canvas_item_cfg, ContainerProxy, GroupProxy
 
 
-
 class Points(three.Points, _VapiObjects):
+
     def __init__(self, limit: int) -> None:
         super().__init__(limit)
         self._points: List[three.Vector3Type] = []
         self._points_arr: List[np.ndarray] = []
         self.prop(layers=31)
-    
+
     @property
     def _count(self):
-        return len(self._points) + sum([arr.shape[0] for arr in self._points_arr])
+        return len(self._points) + sum(
+            [arr.shape[0] for arr in self._points_arr])
 
     def p(self, x: float, y: float, z: float):
         assert self._count + 1 <= self.props.limit, f"points count exceed limit {self.props.limit}"
         self._points.append((x, y, z))
         return self
 
     def array(self, data: np.ndarray):
-        assert data.ndim == 2 and data.shape[1] in [3, 4], "points dim must be 3 or 4 (with intensity)"
-        assert self._count + data.shape[0] <= self.props.limit, f"points count exceed limit {self.props.limit}"
+        assert data.ndim == 2 and data.shape[1] in [
+            3, 4
+        ], "points dim must be 3 or 4 (with intensity)"
+        assert self._count + data.shape[
+            0] <= self.props.limit, f"points count exceed limit {self.props.limit}"
         if self._points_arr:
-            assert data.shape[1] == self._points_arr[0].shape[1], "points dim must be same as first array"
+            assert data.shape[1] == self._points_arr[0].shape[
+                1], "points dim must be same as first array"
         if data.dtype != np.float32:
             data = data.astype(np.float32)
         self._points_arr.append(data)
         return self
-    
+
     def prepare_vapi_props(self):
         # TODO global config
-        points_nparray = np.array(self._points, dtype=np.float32).reshape(-1, 3)
+        points_nparray = np.array(self._points,
+                                  dtype=np.float32).reshape(-1, 3)
         if self._points_arr:
             has_intensity = self._points_arr[0].shape[1] == 4
             if has_intensity:
-                points_nparray = np.concatenate([points_nparray, np.full((points_nparray.shape[0], 1), 255.0, dtype=np.float32)], axis=-1)
+                points_nparray = np.concatenate([
+                    points_nparray,
+                    np.full(
+                        (points_nparray.shape[0], 1), 255.0, dtype=np.float32)
+                ],
+                                                axis=-1)
             points_nparray = np.concatenate(self._points_arr +
                                             [points_nparray])
         self.prop(points=points_nparray)
 
+
 class ColoredPoints(three.Points, _VapiObjects):
+
     def __init__(self, limit: int) -> None:
         super().__init__(limit)
         self._points: List[three.Vector3Type] = []
         self._colors: List[Tuple[int, int, int]] = []
 
         self._points_arr: List[np.ndarray] = []
         self._colors_arr: List[np.ndarray] = []
 
     @property
     def _count(self):
-        return len(self._points) + sum([arr.shape[0] for arr in self._points_arr])
+        return len(self._points) + sum(
+            [arr.shape[0] for arr in self._points_arr])
 
     def p(self, x: float, y: float, z: float, r: int, g: int, b: int):
         assert r >= 0 and r <= 255
         assert g >= 0 and g <= 255
         assert b >= 0 and b <= 255
         assert self._count + 1 <= self.props.limit, f"points count exceed limit {self.props.limit}"
         self._points.append((x, y, z))
         self._colors.append((r, g, b))
         return self
 
     def array(self, data: np.ndarray, color: np.ndarray):
         assert color.dtype == np.uint8, "color must be uint8 array"
-        assert data.ndim == 2 and data.shape[1] == 3, "points with color dim must be 3"
-        assert color.ndim == 2 and color.shape[1] == 3, "points with color dim must be 3"
-        assert self._count + data.shape[0] <= self.props.limit, f"points count exceed limit {self.props.limit}"
+        assert data.ndim == 2 and data.shape[
+            1] == 3, "points with color dim must be 3"
+        assert color.ndim == 2 and color.shape[
+            1] == 3, "points with color dim must be 3"
+        assert self._count + data.shape[
+            0] <= self.props.limit, f"points count exceed limit {self.props.limit}"
         if self._points_arr:
-            assert data.shape[1] == self._points_arr[0].shape[1], "points dim must be same as first array"
+            assert data.shape[1] == self._points_arr[0].shape[
+                1], "points dim must be same as first array"
         if data.dtype != np.float32:
             data = data.astype(np.float32)
         self._points_arr.append(data)
         self._colors_arr.append(color)
         return self
-    
+
     def prepare_vapi_props(self):
         # TODO global config
-        points_nparray = np.array(self._points, dtype=np.float32).reshape(-1, 3)
+        points_nparray = np.array(self._points,
+                                  dtype=np.float32).reshape(-1, 3)
         color_nparray = np.array(self._colors, dtype=np.uint8).reshape(-1, 3)
         if self._points_arr:
             points_nparray = np.concatenate(self._points_arr +
                                             [points_nparray])
-            color_nparray = np.concatenate(self._colors_arr +
-                                            [color_nparray])
+            color_nparray = np.concatenate(self._colors_arr + [color_nparray])
         self.prop(points=points_nparray, colors=color_nparray)
 
 
 class _Polygon:
+
     def __init__(self, start: three.Vector3Type, closed: bool,
                  line_proxy: "Lines") -> None:
         self.line_proxy = line_proxy
         self.closed = closed
         self.start = start
 
     def to(self, x: float, y: float, z: float):
         self.line_proxy.p(self.start[0], self.start[1], self.start[2], x, y, z)
         self.start = (x, y, z)
         return self
 
+
 class Lines(three.Segments, _VapiObjects):
-    def __init__(self, limit: int,
+
+    def __init__(self,
+                 limit: int,
                  line_width: float = 1.0,
                  color: Union[str, mui.Undefined] = mui.undefined) -> None:
         super().__init__(limit, line_width, color)
         self._point_pairs: List[Tuple[three.Vector3Type,
                                       three.Vector3Type]] = []
         self._lines_arr: List[np.ndarray] = []
         self.prop(layers=31)
 
     @property
     def _count(self):
-        return len(self._point_pairs) + sum([arr.shape[0] for arr in self._lines_arr])
+        return len(self._point_pairs) + sum(
+            [arr.shape[0] for arr in self._lines_arr])
 
     def p(self, x1: float, y1: float, z1: float, x2: float, y2: float,
           z2: float):
         assert self._count + 1 <= self.props.limit, f"lines count exceed limit {self.props.limit}"
         self._point_pairs.append(((x1, y1, z1), (x2, y2, z2)))
         return self
 
     def array(self, data: np.ndarray):
-        assert self._count + data.shape[0] <= self.props.limit, f"lines count exceed limit {self.props.limit}"
+        assert self._count + data.shape[
+            0] <= self.props.limit, f"lines count exceed limit {self.props.limit}"
         if data.dtype != np.float32:
             data = data.astype(np.float32)
         three.SegmentsProps.lines_validator(data)
         self._lines_arr.append(data)
         return self
 
     def prepare_vapi_props(self):
-        lines_array = np.array(self._point_pairs, dtype=np.float32).reshape(-1, 2, 3)
+        lines_array = np.array(self._point_pairs,
+                               dtype=np.float32).reshape(-1, 2, 3)
         if self._lines_arr:
             lines_array = np.concatenate(self._lines_arr + [lines_array])
         self.prop(lines=lines_array)
 
     def polygon(self, x: float, y: float, z: float, closed: bool = False):
         return _Polygon((x, y, z), closed, self)
 
     def closed_polygon(self, x: float, y: float, z: float):
         return _Polygon((x, y, z), True, self)
 
+
 class BoundingBox(three.BoundingBox, _VapiObjects):
+
     def prepare_vapi_props(self):
         self.prop(enableSelect=True)
 
+
 class Image(three.Image, _VapiObjects):
+
     def __init__(self, img: np.ndarray, use_datatex: bool = False) -> None:
         super().__init__()
         self._img = img
         self._use_datatex = use_datatex
         assert img.dtype == np.uint8
 
     def prepare_vapi_props(self):
         # TODO currently use texture loader (webimage) in frontend cause problem,
         # so we use data texture for now.
         use_datatex = self._use_datatex
         img = self._img
         if use_datatex:
             if img.ndim == 3 and img.shape[-1] == 3:
-                img = np.concatenate([img, np.full((*img.shape[:-1], 1), 255, dtype=np.uint8)], axis=-1)
+                img = np.concatenate(
+                    [img,
+                     np.full((*img.shape[:-1], 1), 255, dtype=np.uint8)],
+                    axis=-1)
             elif img.ndim == 2:
                 # gray to rgba
                 img = img.reshape((*img.shape, 1))
                 img = np.tile(img, (1, 1, 3))
-                img = np.concatenate([img, np.full((*img.shape[:-1], 1), 255, dtype=np.uint8)], axis=-1)
+                img = np.concatenate(
+                    [img,
+                     np.full((*img.shape[:-1], 1), 255, dtype=np.uint8)],
+                    axis=-1)
             self.prop(image=img, enableSelect=True)
         else:
-            self.prop(image=mui.Image.encode_image_bytes(img), enableSelect=True)
+            self.prop(image=mui.Image.encode_image_bytes(img),
+                      enableSelect=True)
 
 
 V_CONTEXT_VAR: contextvars.ContextVar[
     Optional[VContext]] = contextvars.ContextVar("v_context", default=None)
 
 GROUP_CONTEXT_VAR: contextvars.ContextVar[
     Optional[GroupProxy]] = contextvars.ContextVar("group_context",
@@ -237,19 +273,20 @@
     token = V_CONTEXT_VAR.set(robj)
     try:
         yield robj
     finally:
         V_CONTEXT_VAR.reset(token)
 
 
-async def _draw_all_in_vctx(vctx: VContext,
-                            detail_update_prefix: Optional[UniqueTreeId] = None,
-                            app_event: Optional[AppEvent] = None,
-                            update_iff_change: bool = False):
-    # import rich 
+async def _draw_all_in_vctx(
+        vctx: VContext,
+        detail_update_prefix: Optional[UniqueTreeId] = None,
+        app_event: Optional[AppEvent] = None,
+        update_iff_change: bool = False):
+    # import rich
     # rich.print("?", vctx._group_assigns)
     # rich.print(vctx._name_to_group)
     vctx.canvas._tree_collect_in_vctx()
     for k, v in vctx._name_to_group.items():
         cfg = get_canvas_item_cfg(v)
         # print(k, cfg )
         if cfg is not None:
@@ -275,15 +312,16 @@
     for container, (group, name) in vctx._group_assigns.items():
         assert isinstance(group, three.Group)
         # print(group)
         if container.is_mounted():
             await container.update_childs({name: group})
     await vctx.canvas._show_visible_groups_of_objtree()
 
-    await vctx.canvas.item_tree.update_tree(wait=True, update_iff_change=update_iff_change)
+    await vctx.canvas.item_tree.update_tree(
+        wait=True, update_iff_change=update_iff_change)
     if detail_update_prefix is not None:
         await vctx.canvas.update_detail_layout(detail_update_prefix)
     if app_event is not None:
         await vctx.canvas.send_and_wait(app_event)
 
 
 # @contextlib.contextmanager
@@ -341,15 +379,16 @@
 
 
 @contextlib.contextmanager
 def group(name: str,
           pos: Optional[three.Vector3Type] = None,
           rot: Optional[three.Vector3Type] = None,
           canvas: Optional[ComplexCanvas] = None,
-          variant: Optional[Literal["default", "faceToCamera", "relativeToCamera"]] = None,
+          variant: Optional[Literal["default", "faceToCamera",
+                                    "relativeToCamera"]] = None,
           loop: Optional[asyncio.AbstractEventLoop] = None):
     if canvas is None:
         canvas = appctx.find_component(ComplexCanvas)
         assert canvas is not None, "you must add complex canvas before using vapi"
 
     name_parts = name.split(".")
     name_obj = UniqueTreeIdForTree.from_parts(name_parts)
@@ -367,15 +406,16 @@
     # if obj_self is not None:
     #     obj_self_id = id(obj_self)
     #     if obj_self_id in canvas._user_obj_tree_item_to_meta:
     #         v_ctx = canvas._user_obj_tree_item_to_meta[obj_self_id].vctx
     #         v_ctx.canvas = canvas
     #         is_objtree_ctx = True
     if v_ctx is None:
-        assert not is_reserved_name(name), f"{name} should not be reserved name"
+        assert not is_reserved_name(
+            name), f"{name} should not be reserved name"
         v_ctx = VContext(canvas)
     #     token = V_CONTEXT_VAR.set(v_ctx)
     # else:
     if is_first_ctx:
         token = V_CONTEXT_VAR.set(v_ctx)
     # v_ctx = get_v_context()
     # assert v_ctx is not None
@@ -505,71 +545,91 @@
 
             if loop is None:
                 loop = app._loop
             if loop is None:
                 loop = asyncio.get_running_loop()
             if app._flowapp_thread_id == threading.get_ident():
                 # we can't wait fut here
-                task = asyncio.create_task(_draw_all_in_vctx(v_ctx, UniqueTreeId(""), ev))
+                task = asyncio.create_task(
+                    _draw_all_in_vctx(v_ctx, UniqueTreeId(""), ev))
                 # we can't wait fut here
                 # return task
                 # return fut
             else:
                 # we can wait fut here.
                 fut = asyncio.run_coroutine_threadsafe(
                     _draw_all_in_vctx(v_ctx, UniqueTreeId(""), ev), loop)
                 fut.result()
 
-async def _uninstall_detail_when_unmount(ev: three.Event, obj: three.Component, canvas: ComplexCanvas):
+
+async def _uninstall_detail_when_unmount(ev: three.Event, obj: three.Component,
+                                         canvas: ComplexCanvas):
     object_pyid = obj._flow_uid
     cur_detail_obj_pyid = canvas._cur_detail_layout_object_id
     if object_pyid == cur_detail_obj_pyid:
-        await canvas._uninstall_detail_layout() 
+        await canvas._uninstall_detail_layout()
+
 
-async def _install_detail_before_mount(ev: three.Event, obj: three.Component, canvas: ComplexCanvas):
+async def _install_detail_before_mount(ev: three.Event, obj: three.Component,
+                                       canvas: ComplexCanvas):
     object_pyid = obj._flow_uid
     cur_detail_obj_pyid = canvas._cur_detail_layout_object_id
     if object_pyid == cur_detail_obj_pyid:
-        await canvas._install_detail_layout(obj) 
+        await canvas._install_detail_layout(obj)
 
-async def _uninstall_table_when_unmount(ev: three.Event, obj: three.Component, canvas: ComplexCanvas):
+
+async def _uninstall_table_when_unmount(ev: three.Event, obj: three.Component,
+                                        canvas: ComplexCanvas):
     object_pyid = obj._flow_uid
     cur_detail_obj_pyid = canvas._cur_table_object_id
     if object_pyid == cur_detail_obj_pyid:
-        await canvas._uninstall_table_layout() 
+        await canvas._uninstall_table_layout()
+
 
-async def _install_table_before_mount(ev: three.Event, obj: three.Component, canvas: ComplexCanvas):
+async def _install_table_before_mount(ev: three.Event, obj: three.Component,
+                                      canvas: ComplexCanvas):
     object_pyid = obj._flow_uid
     cur_detail_obj_pyid = canvas._cur_table_object_id
     if object_pyid == cur_detail_obj_pyid:
-        await canvas._install_table_layout(obj) 
+        await canvas._install_table_layout(obj)
+
 
 def _install_obj_event_handlers(obj: three.Component, canvas: ComplexCanvas):
     if isinstance(obj, three.Group):
-        obj.event_before_mount.on(partial(_install_table_before_mount, obj=obj, canvas=canvas))
-        obj.event_before_unmount.on(partial(_uninstall_table_when_unmount, obj=obj, canvas=canvas))
-    obj.event_before_mount.on(partial(_install_detail_before_mount, obj=obj, canvas=canvas))
-    obj.event_before_unmount.on(partial(_uninstall_detail_when_unmount, obj=obj, canvas=canvas))
+        obj.event_before_mount.on(
+            partial(_install_table_before_mount, obj=obj, canvas=canvas))
+        obj.event_before_unmount.on(
+            partial(_uninstall_table_when_unmount, obj=obj, canvas=canvas))
+    obj.event_before_mount.on(
+        partial(_install_detail_before_mount, obj=obj, canvas=canvas))
+    obj.event_before_unmount.on(
+        partial(_uninstall_detail_when_unmount, obj=obj, canvas=canvas))
+
 
-def _find_frame_self(*, _frame_cnt: int=1):
+def _find_frame_self(*, _frame_cnt: int = 1):
     cur_frame = inspect.currentframe()
     assert cur_frame is not None
     frame = cur_frame
     while _frame_cnt > 0:
         frame = cur_frame.f_back
         assert frame is not None
         cur_frame = frame
         _frame_cnt -= 1
     local_vars = cur_frame.f_locals
     if "self" in local_vars:
         obj = local_vars["self"]
         return obj
-    return None 
- 
-def _create_vapi_three_obj_pcfg(obj: three.Component, name: Optional[str], default_name_prefix: str, *, _frame_cnt: int=1):
+    return None
+
+
+def _create_vapi_three_obj_pcfg(obj: three.Component,
+                                name: Optional[str],
+                                default_name_prefix: str,
+                                *,
+                                _frame_cnt: int = 1):
     v_ctx = get_v_context()
     assert v_ctx is not None
     # if v_ctx.canvas._user_obj_tree_item_to_meta:
     #     # write to standalone vctx for tree item
     #     obj_self = _find_frame_self(_frame_cnt=_frame_cnt + 1)
     #     if obj_self is not None:
     #         if id(obj_self) in v_ctx.canvas._user_obj_tree_item_to_meta:
@@ -581,78 +641,99 @@
     if name is None:
         name = proxy.namepool(default_name_prefix)
     proxy.childs[name] = obj
     _install_obj_event_handlers(obj, v_ctx.canvas)
     pcfg = get_or_create_canvas_item_cfg(obj, True)
     return pcfg
 
+
 def points(name: str, limit: int):
     point = Points(limit)
     pcfg = _create_vapi_three_obj_pcfg(point, name, "points", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return point
 
+
 def colored_points(name: str, limit: int):
     point = ColoredPoints(limit)
     pcfg = _create_vapi_three_obj_pcfg(point, name, "points", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return point
 
+
 def lines(name: str, limit: int):
     point = Lines(limit)
     pcfg = _create_vapi_three_obj_pcfg(point, name, "lines", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return point
 
-def bounding_box(dim: three.Vector3Type, rot: Optional[three.Vector3Type] = None, pos: Optional[three.Vector3Type] = None, name: Optional[str] = None):
+
+def bounding_box(dim: three.Vector3Type,
+                 rot: Optional[three.Vector3Type] = None,
+                 pos: Optional[three.Vector3Type] = None,
+                 name: Optional[str] = None):
     obj = BoundingBox(dim)
     if rot is not None:
         obj.prop(rotation=rot)
     if pos is not None:
         obj.prop(position=pos)
     pcfg = _create_vapi_three_obj_pcfg(obj, name, "box", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return obj
 
-def text(text: str, rot: Optional[three.Vector3Type] = None, pos: Optional[three.Vector3Type] = None, name: Optional[str] = None):
+
+def text(text: str,
+         rot: Optional[three.Vector3Type] = None,
+         pos: Optional[three.Vector3Type] = None,
+         name: Optional[str] = None):
     obj = three.Text(text)
     if rot is not None:
         obj.prop(rotation=rot)
     if pos is not None:
         obj.prop(position=pos)
     pcfg = _create_vapi_three_obj_pcfg(obj, name, "text", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return obj
 
-def image(img: np.ndarray, rot: Optional[three.Vector3Type] = None, pos: Optional[three.Vector3Type] = None, name: Optional[str] = None, use_datatex: bool = False):
+
+def image(img: np.ndarray,
+          rot: Optional[three.Vector3Type] = None,
+          pos: Optional[three.Vector3Type] = None,
+          name: Optional[str] = None,
+          use_datatex: bool = False):
     assert img.dtype == np.uint8 and (img.ndim == 3 or img.ndim == 2)
     obj = Image(img, use_datatex)
     if rot is not None:
         obj.prop(rotation=rot)
     if pos is not None:
         obj.prop(position=pos)
     pcfg = _create_vapi_three_obj_pcfg(obj, name, "img", _frame_cnt=2)
     pcfg.proxy = CanvasItemProxy()
     return obj
 
+
 def three_ui(comp: three.ThreeComponentType, name: Optional[str] = None):
     # FIXME better way to handle cast layer
-    if isinstance(comp, (three.Points, three.Segments, three.BufferMesh, three.InstancedMesh, three.VoxelMesh)):
+    if isinstance(comp, (three.Points, three.Segments, three.BufferMesh,
+                         three.InstancedMesh, three.VoxelMesh)):
         comp.props.layers = 31
     _create_vapi_three_obj_pcfg(comp, name, "obj3d", _frame_cnt=2)
-    return 
+    return
+
 
 def set_tdata(obj: three.Component, tdata: Dict[str, Any]):
     cfg = get_or_create_canvas_item_cfg(obj)
     cfg.tdata = tdata
 
+
 def set_detail_layout(obj: three.Component, layout: mui.FlexBox):
     cfg = get_or_create_canvas_item_cfg(obj)
     cfg.detail_layout = layout
 
+
 def program(name: str, func: Callable):
     # raise NotImplementedError
     group = three.Group([])
     func_dcls = annotated_function_to_dataclass(func)
     func_dcls_obj = func_dcls()
     v_ctx = get_v_context()
     assert v_ctx is not None
@@ -680,12 +761,14 @@
                 kwargs = {}
                 for field in dataclasses.fields(func_dcls_obj):
                     kwargs[field.name] = getattr(func_dcls_obj, field.name)
                 res = func(**kwargs)
                 if inspect.iscoroutine(res):
                     await res
             # we need to update tree iff tree change because update tree is very slow.
-            await _draw_all_in_vctx(vctx_program, group._flow_uid, update_iff_change=True)
+            await _draw_all_in_vctx(vctx_program,
+                                    group._flow_uid,
+                                    update_iff_change=True)
 
     pcfg.detail_layout = ConfigPanelV2(func_dcls_obj, callback)
     pcfg.proxy = GroupProxy("")
     return
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/three.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/three.py`

 * *Files 0% similar despite different names*

```diff
@@ -335,15 +335,14 @@
                              FrontendEventType.Leave.value,
                              FrontendEventType.Over.value,
                              FrontendEventType.Out.value,
                              FrontendEventType.Up.value,
                              FrontendEventType.Down.value,
                              FrontendEventType.ContextMenu.value,
                              FrontendEventType.Change.value,
-
                          ] + list(allowed_events))
         self.event_double_click = self._create_event_slot(
             FrontendEventType.DoubleClick)
         self.event_click = self._create_event_slot(FrontendEventType.Click)
         self.event_enter = self._create_event_slot(FrontendEventType.Enter)
         self.event_leave = self._create_event_slot(FrontendEventType.Leave)
         self.event_over = self._create_event_slot(FrontendEventType.Over)
@@ -921,14 +920,15 @@
             self.props.checked = data
         elif isinstance(data, dict):
             assert "position" in data
             assert "rotation" in data
             self.props.position = data["position"]
             self.props.rotation = data["rotation"]
 
+
 @dataclasses.dataclass
 class AxesHelperProps(Object3dBaseProps):
     length: NumberType = 10
 
 
 class AxesHelper(ThreeComponentBase[AxesHelperProps]):
 
@@ -1291,15 +1291,16 @@
 
     infinityDolly: Union[bool, Undefined] = undefined
     makeDefault: Union[bool, Undefined] = undefined
     mouseButtons: Union[MouseButtonConfig, Undefined] = undefined
     # used to sync object 3ds based on camera position and rotation.
     # keep in mind that this won't affact position/rotation of those objects in BACKEND.
     # you need to due with them in backend.
-    syncObject3ds: Union[List[Union[Object3dBase, Object3dContainerBase]], Undefined] = undefined
+    syncObject3ds: Union[List[Union[Object3dBase, Object3dContainerBase]],
+                         Undefined] = undefined
 
 
 class MapControl(ThreeComponentBase[OrbitControlProps]):
 
     def __init__(self) -> None:
         super().__init__(UIType.ThreeMapControl, OrbitControlProps)
         self.props.enableDamping = True
@@ -1738,15 +1739,18 @@
 
     def __init__(self,
                  children: Union[List[ThreeComponentType],
                                  Dict[str, ThreeComponentType]],
                  background: Union[str, Undefined] = undefined) -> None:
         if isinstance(children, list):
             children = {str(i): v for i, v in enumerate(children)}
-        super().__init__(UIType.ThreeCanvas, ThreeCanvasProps, children, allowed_events=[
+        super().__init__(UIType.ThreeCanvas,
+                         ThreeCanvasProps,
+                         children,
+                         allowed_events=[
                              FrontendEventType.ContextMenuSelect.value,
                          ])
         self.props.threeBackgroundColor = background
         self.event_context_menu = self._create_event_slot(
             FrontendEventType.ContextMenuSelect)
 
     @property
@@ -1758,14 +1762,15 @@
     def update_event(self):
         propcls = self.propcls
         return self._update_props_base(propcls)
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
         return await handle_standard_event(self, ev, is_sync)
 
+
 class View(MUIContainerBase[ThreeViewProps, ThreeComponentType]):
 
     def __init__(
         self, children: Union[List[ThreeComponentType],
                               Dict[str, ThreeComponentType]]
     ) -> None:
         if isinstance(children, list):
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/threecore.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/threecore.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-import enum 
+import enum
+
 
 class TextureFormat(enum.IntEnum):
     AlphaFormat = 0
     RedFormat = 1
     RedIntegerFormat = 2
     RGFormat = 3
     RGIntegerFormat = 4
@@ -11,41 +12,45 @@
     RGBAFormat = 7
     RGBAIntegerFormat = 8
     LuminanceFormat = 9
     LuminanceAlphaFormat = 10
     DepthFormat = 11
     DepthStencilFormat = 12
 
+
 class TextureType(enum.IntEnum):
     UnsignedByteType = 0
     ByteType = 1
     ShortType = 2
     UnsignedShortType = 3
     IntType = 4
     UnsignedIntType = 5
     FloatType = 6
     HalfFloatType = 7
     UnsignedShort4444Type = 8
     UnsignedShort5551Type = 9
     UnsignedShort565Type = 10
     UnsignedInt248Type = 11
 
+
 class TextureMappingType(enum.Enum):
     UVMapping = 0
     CubeReflectionMapping = 1
     CubeRefractionMapping = 2
     EquirectangularReflectionMapping = 3
     EquirectangularRefractionMapping = 4
     CubeUVReflectionMapping = 5
 
+
 class TextureWrappingMode(enum.IntEnum):
     RepeatWrapping = 0
     ClampToEdgeWrapping = 1
     MirroredRepeatWrapping = 2
-    
+
+
 class TextureFilterType(enum.IntEnum):
     NearestFilter = 0
     NearestMipmapNearestFilter = 1
     NearestMipmapLinearFilter = 2
     LinearFilter = 3
     LinearMipmapNearestFilter = 4
     LinearMipmapLinearFilter = 5
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/components/typemetas.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/components/typemetas.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,57 +1,64 @@
 import inspect
 from typing import Callable, List, Optional, Tuple, Any, Union
 from typing_extensions import Annotated
 from tensorpc.core.dataclass_dispatch import dataclass
 from typing_extensions import TypeAlias, get_type_hints
 from dataclasses import Field, make_dataclass, field
 
+
 @dataclass
 class CommonObject:
-    alias: Optional[str] = None 
+    alias: Optional[str] = None
     default: Optional[Any] = None
 
+
 @dataclass
 class RangedInt:
     lo: int
     hi: int
     step: Optional[int] = None
-    alias: Optional[str] = None 
+    alias: Optional[str] = None
     default: Optional[int] = None
 
+
 @dataclass
 class RangedFloat:
     lo: float
     hi: float
     step: Optional[float] = None
-    alias: Optional[str] = None 
+    alias: Optional[str] = None
     default: Optional[float] = None
 
+
 @dataclass
 class ColorRGB:
     value_is_string: bool = True
     default: Optional[Union[int, str]] = None
 
+
 @dataclass
 class ColorRGBA:
     value_is_string: bool = True
     default: Optional[Union[int, str]] = None
 
+
 @dataclass
 class RangedVector3:
     lo: float
     hi: float
     step: Optional[float] = None
-    alias: Optional[str] = None 
+    alias: Optional[str] = None
     default: Optional[Tuple[float, float, float]] = None
 
+
 @dataclass
 class Vector3:
     step: Optional[float] = None
-    alias: Optional[str] = None 
+    alias: Optional[str] = None
     default: Optional[Tuple[float, float, float]] = None
 
 
 Vector2Type: TypeAlias = Tuple[float, float]
 
 Vector3Type: TypeAlias = Tuple[float, float, float]
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/core.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/core.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,16 +23,16 @@
 import io
 import re
 import sys
 import threading
 import traceback
 from pathlib import Path
 from typing import (Any, AsyncGenerator, Awaitable, Callable, Coroutine, Dict,
-                    Generic, Iterable, List, Optional, Set, Tuple, Type,
-                    Union, ClassVar, Dict, Protocol)
+                    Generic, Iterable, List, Optional, Set, Tuple, Type, Union,
+                    ClassVar, Dict, Protocol)
 
 from typing_extensions import (Concatenate, ContextManager, Literal, ParamSpec,
                                Protocol, Self, TypeAlias, TypeVar)
 
 from tensorpc.core.event_emitter.aio import AsyncIOEventEmitter
 from pydantic import (
     BaseModel,
@@ -55,24 +55,26 @@
 from tensorpc.flow.flowapp.reload import AppReloadManager, FlowSpecialMethods
 from tensorpc.utils.registry import HashableRegistry
 from tensorpc.utils.uniquename import UniqueNamePool
 
 from tensorpc.core import dataclass_dispatch as dataclasses_strict
 
 from ..jsonlike import (BackendOnlyProp, DataClassWithUndefined, Undefined,
-                        as_dict_no_undefined, asdict_no_deepcopy, camel_to_snake, snake_to_camel,
-                        split_props_to_undefined, undefined, undefined_dict_factory)
-from .appcore import SimpleEventType, NumberType, ValueType, get_app, Event, EventDataType
+                        as_dict_no_undefined, asdict_no_deepcopy,
+                        camel_to_snake, snake_to_camel,
+                        split_props_to_undefined, undefined,
+                        undefined_dict_factory)
+from .appcore import SimpleEventType, NumberType, ValueType, enter_event_handling_conetxt, get_app, Event, EventDataType, get_event_handling_context
 from tensorpc.flow.constants import TENSORPC_FLOW_COMP_UID_STRUCTURE_SPLIT
 
 
 class DataclassType(Protocol):
     # as already noted in comments, checking for this attribute is currently
     # the most reliable way to ascertain that something is a dataclass
-    __dataclass_fields__: ClassVar[Dict] 
+    __dataclass_fields__: ClassVar[Dict]
 
 
 ALL_APP_EVENTS = HashableRegistry()
 
 _CORO_NONE = Union[Coroutine[None, None, None], None]
 
 _CORO_ANY: TypeAlias = Union[Coroutine[None, None, Any], Any]
@@ -277,14 +279,22 @@
     LeafletTooltip = 0x2008
     LeafletCircleMarker = 0x2009
 
     # composite elements
     # a poly line and lots of circle markers/tooltips (typo) in single flowapp element.
     LeafletTracklet = 0x2100
 
+    MASK_FLOW_COMPONENTS = 0x8000
+    Flow = 0x8001
+    FlowMiniMap = 0x8002
+    FlowControls = 0x8003
+    FlowNodeResizer = 0x8004
+    FlowNodeToolBar = 0x8005
+    FlowBackground = 0x8006
+
 
 class AppEventType(enum.IntEnum):
     # layout events
     UpdateLayout = 0
     UpdateComponents = 1
     DeleteComponents = 2
 
@@ -344,15 +354,14 @@
     # modal close: include dialog and drawer (modal based).
     ModalClose = 23
     Drag = 24
     Drop = 25
     SelectNewItem = 26
     ContextMenuSelect = 28
 
-
     TreeLazyExpand = 30
     TreeItemSelectChange = 31
 
     TreeItemToggle = 32
     TreeItemFocus = 33
     TreeItemButton = 34
     TreeItemRename = 36
@@ -377,27 +386,31 @@
     # data grid events
     DataGridRowSelection = 70
     DataGridFetchDetail = 71
     DataGridFetchInf = 72
     DataGridRowRangeChanged = 73
     DataGridProxyLazyLoadRange = 74
 
+    FlowSelectionChange = 80
+    FlowNodesInitialized = 81
+    FlowEdgeConnection = 82
+    FlowEdgeDelete = 83
+    FlowNodeDelete = 84
+
     PlotlyClickData = 100
     PlotlyClickAnnotation = 101
 
 
 UI_TYPES_SUPPORT_DATACLASS: Set[UIType] = {
-    UIType.DataGrid, UIType.MatchCase,
-    UIType.DataFlexBox,
-    UIType.Tabs, UIType.Allotment,
-    UIType.GridLayout,
-    UIType.MenuList,
-    UIType.MatrixDataGrid
+    UIType.DataGrid, UIType.MatchCase, UIType.DataFlexBox, UIType.Tabs,
+    UIType.Allotment, UIType.GridLayout, UIType.MenuList,
+    UIType.MatrixDataGrid, UIType.Flow
 }
 
+
 class AppDraggableType(enum.Enum):
     JsonLikeTreeItem = "JsonLikeTreeItem"
 
 
 ALL_POINTER_EVENTS = [
     FrontendEventType.Down.value,
     FrontendEventType.Up.value,
@@ -405,15 +418,14 @@
     FrontendEventType.Enter.value,
     FrontendEventType.Leave.value,
     FrontendEventType.Over.value,
     FrontendEventType.Out.value,
     FrontendEventType.Click.value,
     FrontendEventType.DoubleClick.value,
     FrontendEventType.ContextMenu.value,
-
 ]
 
 
 class UIRunStatus(enum.IntEnum):
     Stop = 0
     Running = 1
     Pause = 2
@@ -766,35 +778,34 @@
     def from_dict(cls, data: Dict[str, Any]):
         return cls(data["text"])
 
     def merge_new(self, new):
         assert isinstance(new, CopyToClipboardEvent)
         return new
 
+
 @ALL_APP_EVENTS.register(key=AppEventType.InitLSPClient.value)
 class InitLSPClientEvent:
 
     def __init__(self, port: int, init_cfg: dict) -> None:
         self.port = port
         self.init_cfg = init_cfg
 
     def to_dict(self):
-        return {
-            "port": self.port,
-            "initConfig": self.init_cfg
-        }
+        return {"port": self.port, "initConfig": self.init_cfg}
 
     @classmethod
     def from_dict(cls, data: Dict[str, Any]):
         return cls(data["port"], data["initConfig"])
 
     def merge_new(self, new):
         assert isinstance(new, InitLSPClientEvent)
         return new
 
+
 @ALL_APP_EVENTS.register(key=AppEventType.ScheduleNext.value)
 class ScheduleNextForApp:
 
     def __init__(self, data) -> None:
         self.data = data
 
     def to_dict(self):
@@ -897,14 +908,15 @@
 @dataclasses_strict.dataclass
 class BasicProps(DataClassWithUndefined):
     status: int = UIRunStatus.Stop.value
     tensorpc_dynamic_eval: Union[Undefined, Dict[str, Any]] = undefined
     # used for template component
     override_props: Union[Dict[str, str], Undefined] = undefined
 
+
 @dataclasses_strict.dataclass
 class ContainerBaseProps(BasicProps):
     childs: List[str] = dataclasses_strict.field(default_factory=list)
 
 
 T_base_props = TypeVar("T_base_props", bound=BasicProps)
 T_container_props = TypeVar("T_container_props", bound=ContainerBaseProps)
@@ -934,93 +946,117 @@
 
 def _get_obj_def_path(obj):
     is_dynamic_path = False
     try:
         path = inspect.getfile(builtins.type(obj))
         is_dynamic_path = is_tensorpc_dynamic_path(path)
         if is_dynamic_path:
-            _flow_comp_def_path = path 
+            _flow_comp_def_path = path
         else:
             _flow_comp_def_path = str(
                 Path(inspect.getfile(builtins.type(obj))).resolve())
     except:
         traceback.print_exc()
         _flow_comp_def_path = ""
     if is_dynamic_path:
         return _flow_comp_def_path
     path = Path(_flow_comp_def_path)
     if not path.exists() or path.suffix != ".py":
         _flow_comp_def_path = ""
     return _flow_comp_def_path
 
+
 class _EventSlot:
+
     def __init__(self, event_type: EventDataType, comp: "Component"):
         self.event_type = event_type
         self.comp = comp
 
     def on_standard(self, handler: Callable[[Event], Any]) -> "_EventSlot":
         """standard event means the handler must be a function with one argument of Event.
         this must be used to get template key
         if you use template layout such as table column def.
         """
-        self.comp.register_event_handler(self.event_type, handler, simple_event=False)
-        return self 
-    
+        self.comp.register_event_handler(self.event_type,
+                                         handler,
+                                         simple_event=False)
+        return self
+
     def on(self, handler: Callable) -> "_EventSlot":
         """simple event means the event data isn't Event, but the data of Event, or none for no-arg event
         such as click.
         """
-        self.comp.register_event_handler(self.event_type, handler, simple_event=True)
-        return self 
+        self.comp.register_event_handler(self.event_type,
+                                         handler,
+                                         simple_event=True)
+        return self
 
     def off(self, handler: Callable) -> "_EventSlot":
         self.comp.remove_event_handler(self.event_type, handler)
-        return self 
+        return self
 
-    def configure(self, stop_propagation: bool = False,
-            throttle: Optional[NumberType] = None,
-            debounce: Optional[NumberType] = None) -> "_EventSlot":
+    def configure(self,
+                  stop_propagation: bool = False,
+                  throttle: Optional[NumberType] = None,
+                  debounce: Optional[NumberType] = None) -> "_EventSlot":
         self.comp.configure_event_handlers(self.event_type, stop_propagation,
-                throttle, debounce)
+                                           throttle, debounce)
         return self
 
+
 class _EmitterEventSlot:
     # TODO remove this
-    def __init__(self, event_type: EventDataType, emitter: "AsyncIOEventEmitter[EventDataType, Event]"):
+    def __init__(self, event_type: EventDataType,
+                 emitter: "AsyncIOEventEmitter[EventDataType, Event]"):
         self.event_type = event_type
         self.emitter = emitter
 
     def on(self, handler: Callable[[Event], Any]) -> "_EmitterEventSlot":
         """simple event means the event data isn't Event, but the data of Event, or none for no-arg event
         such as click.
         """
         self.emitter.on(self.event_type, handler)
-        return self 
+        return self
 
     def off(self, handler: Callable) -> "_EmitterEventSlot":
         self.emitter.remove_listener(self.event_type, handler)
-        return self 
-    
-T_child_structure = TypeVar("T_child_structure", default=Any, bound=DataclassType)
+        return self
+
+
+T_child_structure = TypeVar("T_child_structure",
+                            default=Any,
+                            bound=DataclassType)
+
 
 class _ComponentEffects:
-    def __init__(self) -> None:
-        self._flow_effects: Dict[str, List[Callable[[], Union[Callable[[], Any], None, Coroutine[None, None, Union[Callable[[], Any], None]]]]]] = {}
-        self._flow_unmounted_effects: Dict[str, List[Callable[[], _CORO_NONE]]] = {}
 
-    def use_effect(self, effect: Callable[[], Union[Optional[Callable[[], Any]], Coroutine[None, None, Optional[Callable[[], Any]]]]], key: str = ""):
+    def __init__(self) -> None:
+        self._flow_effects: Dict[str, List[Callable[[], Union[Callable[
+            [], Any], None, Coroutine[None, None, Union[Callable[[], Any],
+                                                        None]]]]]] = {}
+        self._flow_unmounted_effects: Dict[str,
+                                           List[Callable[[],
+                                                         _CORO_NONE]]] = {}
+
+    def use_effect(self,
+                   effect: Callable[[],
+                                    Union[Optional[Callable[[], Any]],
+                                          Coroutine[None, None,
+                                                    Optional[Callable[[],
+                                                                      Any]]]]],
+                   key: str = ""):
         if key not in self._flow_effects:
             self._flow_effects[key] = []
             self._flow_unmounted_effects[key] = []
 
         self._flow_effects[key].append(effect)
 
     def has_effect_key(self, key: str):
         return key in self._flow_effects
-    
+
     def remove_effect_key(self, key: str):
         self._flow_effects.pop(key)
         self._flow_unmounted_effects.pop(key)
 
 
 class Component(Generic[T_base_props, T_child]):
 
@@ -1038,18 +1074,22 @@
         # if previous control callback hasn't finished yet,
         # the new control event will be IGNORED
         self._task: Optional[asyncio.Task] = None
         self._parent = ""
         self.__props = prop_cls()
         self.__prop_cls = prop_cls
         self._prop_validator = TypeAdapter(self.__prop_cls)
-        self._prop_field_names: Set[str] = set([x.name for x in dataclasses.fields(prop_cls)])
+        self._prop_field_names: Set[str] = set(
+            [x.name for x in dataclasses.fields(prop_cls)])
         self._mounted_override = False
         self.__sx_props: Dict[str, Any] = {}
-        self._flow_allowed_events: Set[EventDataType] = set([FrontendEventType.BeforeMount.value, FrontendEventType.BeforeUnmount.value])
+        self._flow_allowed_events: Set[EventDataType] = set([
+            FrontendEventType.BeforeMount.value,
+            FrontendEventType.BeforeUnmount.value
+        ])
         if allowed_events is not None:
             self._flow_allowed_events.update(allowed_events)
         self._flow_user_datas: List[Any] = []
         self._flow_comp_def_path = _get_obj_def_path(self)
         self._flow_reference_count = 0
         # tensorpc will scan your prop dict to find
         # np.ndarray and bytes by default.
@@ -1061,47 +1101,61 @@
         self._flow_unmount_effect_objects: List[Callable[[], _CORO_NONE]] = []
 
         self._flow_event_context_creator: Optional[Callable[
             [], ContextManager]] = None
         # flow event handlers is used for frontend events
         self._flow_event_handlers: Dict[EventDataType, EventHandlers] = {}
         # event emitter is used for backend events, e.g. mount, unmount
-        self._flow_event_emitter: AsyncIOEventEmitter[EventDataType, Event] = AsyncIOEventEmitter()
-        self._flow_event_emitter.add_exception_listener(self.__event_emitter_on_exc)
-        self.event_before_mount = self._create_emitter_event_slot(FrontendEventType.BeforeMount)
-        self.event_before_unmount = self._create_emitter_event_slot(FrontendEventType.BeforeUnmount)
-    
-    def use_effect(self, effect: Callable[[], Union[Optional[Callable[[], Any]], Coroutine[None, None, Optional[Callable[[], Any]]]]], key: str = ""):
+        self._flow_event_emitter: AsyncIOEventEmitter[
+            EventDataType, Event] = AsyncIOEventEmitter()
+        self._flow_event_emitter.add_exception_listener(
+            self.__event_emitter_on_exc)
+        self.event_before_mount = self._create_emitter_event_slot(
+            FrontendEventType.BeforeMount)
+        self.event_before_unmount = self._create_emitter_event_slot(
+            FrontendEventType.BeforeUnmount)
+
+    def use_effect(self,
+                   effect: Callable[[],
+                                    Union[Optional[Callable[[], Any]],
+                                          Coroutine[None, None,
+                                                    Optional[Callable[[],
+                                                                      Any]]]]],
+                   key: str = ""):
         return self.effects.use_effect(effect, key)
 
     @classmethod
-    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler: GetCoreSchemaHandler):
+    def __get_pydantic_core_schema__(cls, _source_type: Any,
+                                     _handler: GetCoreSchemaHandler):
         return core_schema.no_info_after_validator_function(
             cls.validate,
             core_schema.any_schema(),
         )
 
     @classmethod
     def validate(cls, v):
         if not isinstance(v, Component):
             raise ValueError('Component required')
         return v
 
-    def _create_event_slot(self, event_type: Union[FrontendEventType, EventDataType]):
+    def _create_event_slot(self, event_type: Union[FrontendEventType,
+                                                   EventDataType]):
         if isinstance(event_type, FrontendEventType):
             event_type_value = event_type.value
         else:
             event_type_value = event_type
         return _EventSlot(event_type_value, self)
-    
-    def _create_emitter_event_slot(self, event_type: Union[FrontendEventType, EventDataType]):
+
+    def _create_emitter_event_slot(self, event_type: Union[FrontendEventType,
+                                                           EventDataType]):
         if isinstance(event_type, FrontendEventType):
             event_type_value = event_type.value
             assert event_type.value < 0, "only support backend events"
-            return _EmitterEventSlot(event_type_value, self._flow_event_emitter)
+            return _EmitterEventSlot(event_type_value,
+                                     self._flow_event_emitter)
         else:
             event_type_value = event_type
         return _EmitterEventSlot(event_type_value, self._flow_event_emitter)
 
     @property
     def flow_event_emitter(self) -> AsyncIOEventEmitter[EventDataType, Event]:
         return self._flow_event_emitter
@@ -1122,15 +1176,15 @@
 
     @property
     def props(self) -> T_base_props:
         return self.__props
 
     @property
     def _flow_uid_encoded(self) -> str:
-        assert self._flow_uid is not None 
+        assert self._flow_uid is not None
         return self._flow_uid.uid_encoded
 
     @property
     def propcls(self) -> Type[T_base_props]:
         return self.__prop_cls
 
     def merge_prop(self, prop: T_base_props):
@@ -1140,81 +1194,88 @@
             setattr(self.__props, k, v)
 
     def _attach(self, uid: UniqueTreeId, comp_core: AppComponentCore) -> dict:
         if self._flow_reference_count == 0:
             self._flow_uid = uid
             self._flow_comp_core = comp_core
             self._flow_reference_count += 1
-            self.flow_event_emitter.emit(FrontendEventType.BeforeMount.value, Event(FrontendEventType.BeforeMount.value, None))
+            self.flow_event_emitter.emit(
+                FrontendEventType.BeforeMount.value,
+                Event(FrontendEventType.BeforeMount.value, None))
             return {uid: self}
         self._flow_reference_count += 1
         return {}
 
     def _detach(self) -> Dict[UniqueTreeId, "Component"]:
         self._flow_reference_count -= 1
         if self._flow_reference_count == 0:
-            self.flow_event_emitter.emit(FrontendEventType.BeforeUnmount.value, Event(FrontendEventType.BeforeUnmount.value, None))
+            self.flow_event_emitter.emit(
+                FrontendEventType.BeforeUnmount.value,
+                Event(FrontendEventType.BeforeUnmount.value, None))
             res_uid = self._flow_uid
-            assert res_uid is not None 
+            assert res_uid is not None
             self._flow_uid = None
             self._flow_comp_core = None
             return {res_uid: self}
         return {}
 
     def is_mounted(self):
         return self._flow_comp_core is not None
 
     def _prop_base(self, prop: Callable[P, Any], this: T3) -> Callable[P, T3]:
         """set prop by keyword arguments
         this function is used to provide intellisense result for all props.
         """
+
         def wrapper(*args: P.args, **kwargs: P.kwargs):
             # do validation on changed props only
             # self.__prop_cls(**kwargs)
             # TypeAdapter(self.__prop_cls).validate_python(kwargs)
             for k, v in kwargs.items():
                 setattr(self.__props, k, v)
             # do validation for all props (call model validator)
             self._prop_validator.validate_python(self.__props)
             return this
+
         return wrapper
 
     def _update_props_base(self,
                            prop: Callable[P, Any],
                            json_only: bool = False):
         """create prop update event by keyword arguments
         this function is used to provide intellisense result for all props.
         """
+
         def wrapper(*args: P.args, **kwargs: P.kwargs):
             # do validation on changed props only
             # self.__prop_cls(**kwargs)
             # TypeAdapter(self.__prop_cls).validate_python(kwargs)
             for k, v in kwargs.items():
                 setattr(self.__props, k, v)
             # do validation for all props (call model validator)
             self._prop_validator.validate_python(self.__props)
             return self.create_update_event(kwargs, json_only)
 
         return wrapper
 
     async def handle_event(self, ev: Event, is_sync: bool = False):
         pass
-    
+
     def __repr__(self):
         res = f"{self.__class__.__name__}({self._flow_uid_encoded})"
         # if self._flow_user_data is not None:
         #     res += f"({self._flow_user_data})"
-        return res 
+        return res
 
     def find_user_meta_by_type(self, type: Type[T]) -> Optional[T]:
         for x in self._flow_user_datas:
             if isinstance(x, type):
                 return x
-        return None 
-    
+        return None
+
     def set_user_meta_by_type(self, obj: Any):
         obj_type = type(obj)
         for i, x in self._flow_user_datas:
             if isinstance(x, obj_type):
                 self._flow_user_datas[i] = obj
                 return self
         self._flow_user_datas.append(obj)
@@ -1244,15 +1305,15 @@
             except:
                 traceback.print_exc()
             self._task = None
 
     def update_sx_props(self, sx_props: Dict[str, Any]):
         self.__sx_props.update(sx_props)
         return self
-    
+
     def get_sx_props(self):
         return self.__sx_props
 
     def to_dict(self):
         """undefined will be removed here.
         if you reimplement to_dict, you need to use 
         camel name, no conversion provided.
@@ -1271,15 +1332,16 @@
         if self._flow_json_only:
             res["props"] = JsonOnlyData(props)
         return res
 
     def _get_used_events_dict(self):
         evs = []
         for k, v in self._flow_event_handlers.items():
-            if not isinstance(v, Undefined) and not v.backend_only and v.handlers:
+            if not isinstance(v,
+                              Undefined) and not v.backend_only and v.handlers:
                 d = v.to_dict()
                 d["type"] = k
                 evs.append(d)
         return evs
 
     def _to_dict_with_sync_props(self):
         props = self.get_sync_props()
@@ -1304,15 +1366,17 @@
     def get_persist_props(self) -> Optional[Dict[str, Any]]:
         return None
 
     async def set_persist_props_async(self, state: Dict[str, Any]) -> None:
         return
 
     def get_props(self) -> Dict[str, Any]:
-        return self.__props.get_dict(dict_factory=_undefined_comp_dict_factory, obj_factory=_undefined_comp_obj_factory)  # type: ignore
+        return self.__props.get_dict(
+            dict_factory=_undefined_comp_dict_factory,
+            obj_factory=_undefined_comp_obj_factory)  # type: ignore
 
     def validate_props(self, props: Dict[str, Any]):
         """use this function to validate props before call
         set props.
         """
         return True
 
@@ -1322,18 +1386,20 @@
             name_to_fields = {f.name: f for f in fields}
             for name, value in props.items():
                 if name in name_to_fields:
                     setattr(self.__props, name, value)
 
     async def put_loopback_ui_event(self, ev: SimpleEventType):
         if self.is_mounted():
-            assert self._flow_uid is not None 
+            assert self._flow_uid is not None
             return await self.queue.put(
-                AppEvent("",
-                         {AppEventType.UIEvent: UIEvent({self._flow_uid.uid_encoded: ev})},
+                AppEvent("", {
+                    AppEventType.UIEvent:
+                    UIEvent({self._flow_uid.uid_encoded: ev})
+                },
                          is_loopback=True))
 
     async def put_app_event(self, ev: AppEvent):
         if self.is_mounted():
             return await self.queue.put(ev)
 
     def set_override_props(self, **kwargs: str):
@@ -1353,15 +1419,15 @@
         for k, v in kwargs.items():
             new_kwargs[snake_to_camel(k)] = v
         if isinstance(self.props.override_props, Undefined):
             self.props.override_props = new_kwargs
         else:
             self.props.override_props.update(new_kwargs)
         return self
-    
+
     def set_override_props_unchecked_dict(self, kwargs: Dict[str, str]):
         new_kwargs: Dict[str, str] = {}
         for k, v in kwargs.items():
             new_kwargs[snake_to_camel(k)] = v
         if isinstance(self.props.override_props, Undefined):
             self.props.override_props = new_kwargs
         else:
@@ -1374,65 +1440,66 @@
         return self._flow_comp_core.queue
 
     @property
     def flow_app_comp_core(self):
         assert self._flow_comp_core is not None, f"you must add ui by flexbox.add_xxx"
         return self._flow_comp_core
 
-    def configure_event_handlers(self, 
-            type: Union[FrontendEventType, EventDataType],
-            stop_propagation: bool = False,
-            throttle: Optional[NumberType] = None,
-            debounce: Optional[NumberType] = None,
-            backend_only: bool = False):
+    def configure_event_handlers(self,
+                                 type: Union[FrontendEventType, EventDataType],
+                                 stop_propagation: bool = False,
+                                 throttle: Optional[NumberType] = None,
+                                 debounce: Optional[NumberType] = None,
+                                 backend_only: bool = False):
         if isinstance(type, FrontendEventType):
             type_value = type.value
         else:
             type_value = type
         if type_value not in self._flow_event_handlers:
             self._flow_event_handlers[type_value] = EventHandlers([])
         handlers = self._flow_event_handlers[type_value]
         handlers.stop_propagation = stop_propagation
         handlers.throttle = throttle
         handlers.debounce = debounce
         handlers.backend_only = backend_only
-        return 
+        return
 
-    
     def register_event_handler(self,
                                type: Union[FrontendEventType, EventDataType],
                                cb: Callable,
                                stop_propagation: bool = False,
                                throttle: Optional[NumberType] = None,
                                debounce: Optional[NumberType] = None,
                                backend_only: bool = False,
                                simple_event: bool = True):
         if self._flow_allowed_events:
             if not backend_only:
                 assert type in self._flow_allowed_events, f"only support events: {self._flow_allowed_events}"
-        
+
         evh = EventHandler(cb, simple_event)
         if isinstance(type, FrontendEventType):
             type_value = type.value
         else:
             type_value = type
         if type_value not in self._flow_event_handlers:
             self._flow_event_handlers[type_value] = EventHandlers([])
         handlers = self._flow_event_handlers[type_value]
         if type == FrontendEventType.DragCollect:
-            assert len(handlers.handlers) == 0, "DragCollect only support one handler"
-        self.configure_event_handlers(type_value, stop_propagation, throttle, debounce, backend_only)
+            assert len(
+                handlers.handlers) == 0, "DragCollect only support one handler"
+        self.configure_event_handlers(type_value, stop_propagation, throttle,
+                                      debounce, backend_only)
         handlers.handlers.append(evh)
         # self._flow_event_handlers[type_value] = evh
         # if once:
         #     self._flow_event_emitter.once(type_value, self.handle_event)
         # else:
         #     self._flow_event_emitter.once(type_value, self.handle_event)
         return evh
-    
+
     def remove_event_handler(self, type: EventDataType, handler: Callable):
         if type in self._flow_event_handlers:
             return self._flow_event_handlers[type].remove_handler(handler)
         return False
 
     def remove_event_handlers(self, type: EventDataType):
         if type in self._flow_event_handlers:
@@ -1465,17 +1532,17 @@
         data_unds = []
         for k, v in data.items():
             # k = snake_to_camel(k)
             if isinstance(v, Undefined):
                 data_unds.append(k)
             else:
                 data_no_und[k] = as_dict_no_undefined(v)
-        assert self._flow_uid is not None 
-        ev = UIUpdateEvent({self._flow_uid.uid_encoded: (data_no_und, data_unds)},
-                           json_only)
+        assert self._flow_uid is not None
+        ev = UIUpdateEvent(
+            {self._flow_uid.uid_encoded: (data_no_und, data_unds)}, json_only)
         # uid is set in flowapp service later.
         return AppEvent("", {AppEventType.UIUpdateEvent: ev})
 
     def create_update_used_events_event(self):
         used_events = self._get_used_events_dict()
         return self.create_update_event({"usedEvents": used_events}, True)
 
@@ -1487,25 +1554,27 @@
         data_unds = []
         for k, v in data.items():
             k = snake_to_camel(k)
             if isinstance(v, Undefined):
                 data_unds.append(k)
             else:
                 data_no_und[k] = v
-        assert self._flow_uid is not None 
-        ev = UIUpdateEvent({self._flow_uid.uid_encoded: (data_no_und, data_unds)})
+        assert self._flow_uid is not None
+        ev = UIUpdateEvent(
+            {self._flow_uid.uid_encoded: (data_no_und, data_unds)})
         # uid is set in flowapp service later.
         return AppEvent("", {AppEventType.UIUpdatePropsEvent: ev})
 
     def create_comp_event(self, data: Dict[str, Any]):
         """create component control event for
         backend -> frontend direct communication
         """
-        assert self._flow_uid is not None 
-        ev = ComponentEvent({self._flow_uid.uid_encoded: as_dict_no_undefined(data)})
+        assert self._flow_uid is not None
+        ev = ComponentEvent(
+            {self._flow_uid.uid_encoded: as_dict_no_undefined(data)})
         # uid is set in flowapp service later.
         return AppEvent("", {AppEventType.ComponentEvent: ev})
 
     async def send_and_wait(self, ev: AppEvent, wait: bool = True):
         if ev.sent_event is None:
             ev.sent_event = asyncio.Event()
         await self.put_app_event(ev)
@@ -1535,23 +1604,22 @@
         return AppEvent("", {AppEventType.AppEditor: ev})
 
     async def __event_emitter_on_exc(self, exc_param: ExceptionParam):
         traceback.print_exc()
         e = exc_param.exc
         ss = io.StringIO()
         traceback.print_exc(file=ss)
-        assert self._flow_uid is not None 
-        user_exc = UserMessage.create_error(self._flow_uid.uid_encoded, repr(e),
-                                            ss.getvalue())
+        assert self._flow_uid is not None
+        user_exc = UserMessage.create_error(self._flow_uid.uid_encoded,
+                                            repr(e), ss.getvalue())
         await self.put_app_event(self.create_user_msg_event(user_exc))
         app = get_app()
         if app._flowapp_enable_exception_inspect:
             await app._inspect_exception()
 
-
     async def run_callback(self,
                            cb: Callable[[], _CORO_ANY],
                            sync_state: bool = False,
                            sync_status_first: bool = False,
                            res_callback: Optional[Callable[[Any],
                                                            _CORO_ANY]] = None,
                            change_status: bool = True) -> Optional[Any]:
@@ -1581,50 +1649,57 @@
         # only ui with loading support need sync first.
         # otherwise don't use this because slow
         if sync_status_first:
             ev = asyncio.Event()
             await self.sync_status(sync_state, ev)
             await ev.wait()
         res = None
-        try:
-            coro = cb()
-            if inspect.iscoroutine(coro):
-                res = await coro
-            else:
-                res = coro
-            if res_callback is not None:
-                res_coro = res_callback(res)
-                if inspect.iscoroutine(res_coro):
-                    await res_coro
-
-        except Exception as e:
-            traceback.print_exc()
-            ss = io.StringIO()
-            traceback.print_exc(file=ss)
-            assert self._flow_uid is not None 
-
-            user_exc = UserMessage.create_error(self._flow_uid.uid_encoded, repr(e),
-                                                ss.getvalue())
-            await self.put_app_event(self.create_user_msg_event(user_exc))
-            app = get_app()
-            if app._flowapp_enable_exception_inspect:
-                await app._inspect_exception()
-        finally:
-            if change_status:
-                self.props.status = UIRunStatus.Stop.value
-                await self.sync_status(sync_state)
+        assert self._flow_uid is not None 
+        with enter_event_handling_conetxt(self._flow_uid) as evctx:
+            try:
+                coro = cb()
+                if inspect.iscoroutine(coro):
+                    res = await coro
+                else:
+                    res = coro
+                if res_callback is not None:
+                    res_coro = res_callback(res)
+                    if inspect.iscoroutine(res_coro):
+                        await res_coro
+
+            except Exception as e:
+                traceback.print_exc()
+                ss = io.StringIO()
+                traceback.print_exc(file=ss)
+                assert self._flow_uid is not None
+
+                user_exc = UserMessage.create_error(self._flow_uid.uid_encoded,
+                                                    repr(e), ss.getvalue())
+                await self.put_app_event(self.create_user_msg_event(user_exc))
+                app = get_app()
+                if app._flowapp_enable_exception_inspect:
+                    await app._inspect_exception()
+            finally:
+                if change_status:
+                    self.props.status = UIRunStatus.Stop.value
+                    await self.sync_status(sync_state)
+                if evctx.delayed_callbacks:
+                    for cb in evctx.delayed_callbacks:
+                        coro = cb()
+                        if inspect.iscoroutine(coro):
+                            await coro
         return res
-    
-    async def run_callbacks(self,
-                           cbs: List[Callable[[], _CORO_NONE]],
-                           sync_state: bool = False,
-                           sync_status_first: bool = False,
-                           res_callback: Optional[Callable[[Any],
-                                                           _CORO_NONE]] = None,
-                           change_status: bool = True):
+
+    async def run_callbacks(
+            self,
+            cbs: List[Callable[[], _CORO_NONE]],
+            sync_state: bool = False,
+            sync_status_first: bool = False,
+            res_callback: Optional[Callable[[Any], _CORO_NONE]] = None,
+            change_status: bool = True):
         """
         Runs the given callback function and handles its result and potential exceptions.
 
         Args:
             cbs: The callback functions to run.
             sync_state: Whether to synchronize the component's state before and after running the callback.
                 this is required for components which can change state
@@ -1647,43 +1722,50 @@
         # only ui with loading support need sync first.
         # otherwise don't use this because slow
         if sync_status_first:
             ev = asyncio.Event()
             await self.sync_status(sync_state, ev)
             await ev.wait()
         res = None
-        for cb in cbs:
-            try:
-                coro = cb()
-                if inspect.iscoroutine(coro):
-                    res = await coro
-                else:
-                    res = coro
-                if res_callback is not None:
-                    res_coro = res_callback(res)
-                    if inspect.iscoroutine(res_coro):
-                        await res_coro
-            except Exception as e:
-                traceback.print_exc()
-                ss = io.StringIO()
-                traceback.print_exc(file=ss)
-                assert self._flow_uid is not None 
-                user_exc = UserMessage.create_error(self._flow_uid.uid_encoded, repr(e),
-                                                    ss.getvalue())
-                await self.put_app_event(self.create_user_msg_event(user_exc))
-                app = get_app()
-                if app._flowapp_enable_exception_inspect:
-                    await app._inspect_exception()
-        # finally:
-        if change_status:
-            self.props.status = UIRunStatus.Stop.value
-            await self.sync_status(sync_state)
-        else:
-            if sync_state:
-                await self.sync_state()
+        assert self._flow_uid is not None 
+        with enter_event_handling_conetxt(self._flow_uid) as evctx:
+            for cb in cbs:
+                try:
+                    coro = cb()
+                    if inspect.iscoroutine(coro):
+                        res = await coro
+                    else:
+                        res = coro
+                    if res_callback is not None:
+                        res_coro = res_callback(res)
+                        if inspect.iscoroutine(res_coro):
+                            await res_coro
+                except Exception as e:
+                    traceback.print_exc()
+                    ss = io.StringIO()
+                    traceback.print_exc(file=ss)
+                    assert self._flow_uid is not None
+                    user_exc = UserMessage.create_error(self._flow_uid.uid_encoded,
+                                                        repr(e), ss.getvalue())
+                    await self.put_app_event(self.create_user_msg_event(user_exc))
+                    app = get_app()
+                    if app._flowapp_enable_exception_inspect:
+                        await app._inspect_exception()
+            # finally:
+            if change_status:
+                self.props.status = UIRunStatus.Stop.value
+                await self.sync_status(sync_state)
+            else:
+                if sync_state:
+                    await self.sync_state()
+            if evctx.delayed_callbacks:
+                for cb in evctx.delayed_callbacks:
+                    coro = cb()
+                    if inspect.iscoroutine(coro):
+                        await coro
         return res
 
     async def sync_status(self,
                           sync_state: bool = False,
                           sent_event: Optional[asyncio.Event] = None):
         if sync_state:
             sync_props = self.get_sync_props()
@@ -1701,81 +1783,99 @@
 
     def get_sync_event(self, sync_state: bool = False):
         if sync_state:
             return self.create_update_event(self.get_sync_props())
         else:
             return self.create_update_event({"status": self.props.status})
 
+
 class ForEachResult(enum.Enum):
     Continue = 0
     Return = 1
 
+
 def _find_comps_in_dc(obj):
     "(list[tuple[str, Any]]) -> dict[str, Any]"
-    
     """same as dataclasses.asdict except that this function
     won't recurse into nested container.
     """
     res_comp_localids: List[Tuple[Component, str]] = []
     if not dataclasses.is_dataclass(obj):
         raise TypeError("asdict() should be called on dataclass instances")
     _find_comps_in_dc_inner(obj, res_comp_localids, "")
     return res_comp_localids
 
-def _find_comps_in_dc_inner(obj, res_comp_localids: List[Tuple[Component, str]], comp_local_id: str):
+
+def _find_comps_in_dc_inner(obj, res_comp_localids: List[Tuple[Component,
+                                                               str]],
+                            comp_local_id: str):
     if comp_local_id == "":
         local_id_prefix = ""
     else:
         local_id_prefix = f"{comp_local_id}{TENSORPC_FLOW_COMP_UID_STRUCTURE_SPLIT}"
     if isinstance(obj, Component):
         res_comp_localids.append((obj, comp_local_id))
         return
     if dataclasses.is_dataclass(obj):
         for f in dataclasses.fields(obj):
             local_id = local_id_prefix + f.name
-            _find_comps_in_dc_inner(getattr(obj, f.name), res_comp_localids, local_id)
+            _find_comps_in_dc_inner(getattr(obj, f.name), res_comp_localids,
+                                    local_id)
     elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
-        return type(obj)(*[_find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix + str(i)) for i, v in enumerate(obj)])
+        return type(obj)(*[
+            _find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix +
+                                    str(i)) for i, v in enumerate(obj)
+        ])
     elif isinstance(obj, (list, tuple)):
         # Assume we can create an object of this type by passing in a
         # generator (which is not true for namedtuples, handled
         # above).
-        return type(obj)(_find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix + str(i)) for i, v in enumerate(obj))
+        return type(obj)(
+            _find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix +
+                                    str(i)) for i, v in enumerate(obj))
     elif isinstance(obj, dict):
         # TODO validate that all keys are number or letters
         for k in obj.keys():
-            assert isinstance(k, str) and k.isalnum(), f"key {k} must be string and alphanumeric"
-        return type(obj)((k,
-                          _find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix + k))
-                         for k, v in obj.items())
+            assert isinstance(k,
+                              str), f"key {k} must be string and alphanumeric"
+        return type(obj)(
+            (k,
+             _find_comps_in_dc_inner(v, res_comp_localids, local_id_prefix +
+                                     k)) for k, v in obj.items())
+
 
 def _undefined_comp_dict_factory(x: List[Tuple[str, Any]]):
     res: Dict[str, Any] = {}
     for k, v in x:
         if isinstance(v, Component):
-            assert v.is_mounted(), f"you must ensure component is inside comp tree if you add it to props, {k}, {type(v)}"
+            assert v.is_mounted(
+            ), f"you must ensure component is inside comp tree if you add it to props, {k}, {type(v)}"
             res[k] = v._flow_uid_encoded
         elif isinstance(v, UniqueTreeId):
             res[k] = v.uid_encoded
         elif not isinstance(v, (Undefined, BackendOnlyProp)):
             res[k] = v
     return res
 
+
 def _undefined_comp_obj_factory(x: Any):
     if isinstance(x, Component):
-        assert x.is_mounted(), f"you must ensure component is inside comp tree if you add it to props, {type(x)}"
+        assert x.is_mounted(
+        ), f"you must ensure component is inside comp tree if you add it to props, {type(x)}"
         return x._flow_uid_encoded
     return x
 
+
 class ContainerBase(Component[T_container_props, T_child]):
 
     def __init__(self,
                  base_type: UIType,
                  prop_cls: Type[T_container_props],
-                 _children: Optional[Union[Dict[str, T_child], DataclassType]] = None,
+                 _children: Optional[Union[Dict[str, T_child],
+                                           DataclassType]] = None,
                  inited: bool = False,
                  allowed_events: Optional[Iterable[EventDataType]] = None,
                  uid: Optional[UniqueTreeId] = None,
                  app_comp_core: Optional[AppComponentCore] = None) -> None:
         super().__init__(base_type, prop_cls, allowed_events, uid)
         self._flow_comp_core = app_comp_core
         if inited:
@@ -1794,27 +1894,30 @@
                 assert isinstance(v, Component)
                 self._child_comps[k] = v
         else:
             assert base_type in UI_TYPES_SUPPORT_DATACLASS
             assert dataclasses.is_dataclass(_children)
             # parse dataclass, get components, save structure
             self._child_structure = _children
-            children_dict = _find_comps_in_dc(_children)
+            children_dict = self._find_comps_in_dataclass(_children)
             for comp, local_id in children_dict:
                 self._child_comps[local_id] = comp
 
         # self.props.childs: List[str] = []
         self.inited = inited
         self._prevent_add_layout = False
-        
+
     def __repr__(self):
         res = super().__repr__()
         if self._child_comps:
             res += f"({','.join(self._child_comps.keys())})"
-        return res 
+        return res
+
+    def _find_comps_in_dataclass(self, _children: DataclassType):
+        return _find_comps_in_dc(_children)
 
     def _get_comp_by_uid(self, uid: str):
         uid_obj = UniqueTreeId(uid)
         parts = uid_obj.parts
         # uid contains root, remove it at first.
         return self._get_comp_by_uid_resursive(parts[1:])
 
@@ -1851,28 +1954,28 @@
         for k, v in self._child_comps.items():
             child_uid = child_ns.append_part(k)
             if isinstance(v, ContainerBase):
                 res = handler(child_uid, v)
                 if res is None:
                     res_foreach.append((child_uid, v))
                 elif res == ForEachResult.Continue:
-                    continue 
+                    continue
                 elif res == ForEachResult.Return:
-                    return 
+                    return
             else:
                 res = handler(child_uid, v)
                 if res == ForEachResult.Continue:
-                    continue 
+                    continue
                 elif res == ForEachResult.Return:
-                    return 
+                    return
         for child_uid, v in res_foreach:
             v._foreach_comp_recursive(child_uid, handler)
 
-    def _foreach_comp(self, handler: Callable[[UniqueTreeId, Component], Union[ForEachResult,
-                                                                      None]]):
+    def _foreach_comp(self, handler: Callable[[UniqueTreeId, Component],
+                                              Union[ForEachResult, None]]):
         assert self._flow_uid is not None, f"_flow_uid must be set before modify_comp, {type(self)}, {self._flow_reference_count}, {id(self)}"
         handler(self._flow_uid, self)
         self._foreach_comp_recursive(self._flow_uid, handler)
 
     def _update_uid(self):
 
         def handler(uid: UniqueTreeId, v: Component):
@@ -1895,27 +1998,29 @@
             disposed_uids.update(v._detach())
         return disposed_uids
 
     def _attach_child(self,
                       comp_core: AppComponentCore,
                       childs: Optional[List[str]] = None):
         atached_uids: Dict[str, Component] = {}
-        assert self._flow_uid is not None 
+        assert self._flow_uid is not None
         if childs is None:
             childs = list(self._child_comps.keys())
         for k in childs:
             v = self._child_comps[k]
-            atached_uids.update(v._attach(self._flow_uid.append_part(k), comp_core))
+            atached_uids.update(
+                v._attach(self._flow_uid.append_part(k), comp_core))
         return atached_uids
 
     def _attach(self, uid: UniqueTreeId, comp_core: AppComponentCore):
         attached: Dict[str, Component] = super()._attach(uid, comp_core)
-        assert self._flow_uid is not None 
+        assert self._flow_uid is not None
         for k, v in self._child_comps.items():
-            attached.update(v._attach(self._flow_uid.append_part(k), comp_core))
+            attached.update(v._attach(self._flow_uid.append_part(k),
+                                      comp_core))
         return attached
 
     def _get_uid_encoded_to_comp_dict(self):
         res: Dict[str, Component] = {}
 
         def handler(uid: UniqueTreeId, v: Component):
             res[uid.uid_encoded] = v
@@ -1994,17 +2099,22 @@
         #     v._flow_name = k
         if self._prevent_add_layout:
             raise ValueError("you must init layout in app_create_layout")
         self._child_comps.update(layout)
 
     def get_props(self):
         state = super().get_props()
-        state["childs"] = [self[n]._flow_uid_encoded for n in self._child_comps]
+        state["childs"] = [
+            self[n]._flow_uid_encoded for n in self._child_comps
+        ]
         if self._child_structure is not None:
-            state["childsComplex"] = asdict_no_deepcopy(self._child_structure, dict_factory=_undefined_comp_dict_factory, obj_factory=_undefined_comp_obj_factory)
+            state["childsComplex"] = asdict_no_deepcopy(
+                self._child_structure,
+                dict_factory=_undefined_comp_dict_factory,
+                obj_factory=_undefined_comp_obj_factory)
         return state
 
     async def _run_special_methods(
             self,
             attached: List[Component],
             detached: List[Component],
             reload_mgr: Optional[AppReloadManager] = None):
@@ -2014,112 +2124,174 @@
         for deleted in detached:
             special_methods = deleted.get_special_methods(reload_mgr)
             if special_methods.will_unmount is not None:
                 await self.run_callback(
                     special_methods.will_unmount.get_binded_fn(),
                     sync_status_first=False,
                     change_status=False)
-            for k, unmount_effects in deleted.effects._flow_unmounted_effects.items():
+            for k, unmount_effects in deleted.effects._flow_unmounted_effects.items(
+            ):
                 for unmount_effect in unmount_effects:
-                    await self.run_callback(
-                        unmount_effect,
-                        sync_status_first=False,
-                        change_status=False)
+                    await self.run_callback(unmount_effect,
+                                            sync_status_first=False,
+                                            change_status=False)
                 unmount_effects.clear()
         for attach in attached:
             special_methods = attach.get_special_methods(reload_mgr)
             if special_methods.did_mount is not None:
                 await self.run_callback(
                     special_methods.did_mount.get_binded_fn(),
                     sync_status_first=False,
                     change_status=False)
-            # run effects 
+            # run effects
             for k, effects in attach.effects._flow_effects.items():
                 for effect in effects:
-                    res = await self.run_callback(
-                        effect,
-                        sync_status_first=False,
-                        change_status=False)
+                    res = await self.run_callback(effect,
+                                                  sync_status_first=False,
+                                                  change_status=False)
                     if res is not None:
                         # res is effect
                         attach.effects._flow_unmounted_effects[k].append(res)
 
-    def set_new_layout_locally(self, layout: Union[Dict[str, Component], T_child_structure]):
+    def set_new_layout_locally(self, layout: Union[Dict[str, Component],
+                                                   T_child_structure]):
         detached_uid_to_comp = self._detach_child()
         if isinstance(layout, dict):
             self._child_comps = layout
         else:
             assert dataclasses.is_dataclass(layout)
-            assert type(layout) == type(self._child_structure), f"{type(layout)}, {type(self._child_structure)}"
+            assert type(layout) == type(
+                self._child_structure
+            ), f"{type(layout)}, {type(self._child_structure)}"
             self._child_comps.clear()
             # parse dataclass, get components, save structure
             self._child_structure = layout
-            children_dict = _find_comps_in_dc(layout)
+            children_dict = self._find_comps_in_dataclass(layout)
             for comp, local_id in children_dict:
                 self._child_comps[local_id] = comp
         attached = self._attach_child(self.flow_app_comp_core)
         # update all childs of this component
         comps_frontend = {
             c._flow_uid_encoded: c
             for c in self._get_all_nested_childs()
         }
         comps_frontend_dict = {
             k: v.to_dict()
             for k, v in comps_frontend.items()
         }
         child_uids = [self[c]._flow_uid_encoded for c in self._child_comps]
-        update_msg: Dict[str, Any] = {
-            "childs": child_uids
-        }
+        update_msg: Dict[str, Any] = {"childs": child_uids}
         if self._child_structure is not None:
-            update_msg["childsComplex"] = asdict_no_deepcopy(self._child_structure, dict_factory=_undefined_comp_dict_factory, obj_factory=_undefined_comp_obj_factory)
+            update_msg["childsComplex"] = asdict_no_deepcopy(
+                self._child_structure,
+                dict_factory=_undefined_comp_dict_factory,
+                obj_factory=_undefined_comp_obj_factory)
         update_ev = self.create_update_event(update_msg)
         deleted = [x.uid_encoded for x in detached_uid_to_comp.keys()]
         return update_ev + self.create_update_comp_event(
-            comps_frontend_dict, deleted), list(
-                attached.values()), list(detached_uid_to_comp.values())
+            comps_frontend_dict, deleted), list(attached.values()), list(
+                detached_uid_to_comp.keys()), list(
+                detached_uid_to_comp.values())
 
     async def set_new_layout(self, layout: Union[Dict[str, Component],
                                                  List[Component],
                                                  T_child_structure]):
         if isinstance(layout, list):
             layout = {str(i): v for i, v in enumerate(layout)}
-        new_ev, attached, removed = self.set_new_layout_locally(layout)
-        for deleted in removed:
+
+        self_to_be_removed = self._check_ctx_contains_self(list(self._child_comps.keys()))
+        evctx = get_event_handling_context()
+        if evctx is not None and self_to_be_removed:
+            evctx.delayed_callbacks.append(lambda: self._set_new_layout_delay(layout, comp_dont_need_cancel=evctx.comp_uid))
+        else:
+            await self._set_new_layout_delay(layout)
+
+    async def _set_new_layout_delay(self, layout: Union[Dict[str, Component],
+                                                 T_child_structure],
+                                                 comp_dont_need_cancel: Optional[UniqueTreeId] = None):
+        new_ev, attached, removed_uids, removed = self.set_new_layout_locally(layout)
+        for deleted, deleted_uid in zip(removed, removed_uids):
+            if comp_dont_need_cancel is not None and comp_dont_need_cancel == deleted_uid:
+                continue
             await deleted._cancel_task()
         await self.put_app_event(new_ev)
         await self._run_special_methods(attached, removed)
 
-    async def remove_childs_by_keys(self, keys: List[str]):
-        self.__check_child_structure_is_none()
+    def _check_ctx_contains_self(self, keys: Union[List[str], Set[str]]):
+        evctx = get_event_handling_context()
+        self_to_be_removed = False
+        if evctx is not None:
+            for k in keys:
+                if k in self._child_comps:
+                    comp = self._child_comps[k]
+                    if comp._flow_uid is not None and evctx.comp_uid.startswith(comp._flow_uid):
+                        self_to_be_removed = True 
+                        break 
+        return self_to_be_removed 
+
+    async def remove_childs_by_keys(
+            self,
+            keys: List[str],
+            update_child_complex: bool = True,
+            additional_ev_creator: Optional[Callable[[], AppEvent]] = None):
+        if update_child_complex:
+            self.__check_child_structure_is_none()
+        self_to_be_removed = self._check_ctx_contains_self(keys)
+        evctx = get_event_handling_context()
+        if evctx is not None and self_to_be_removed:
+            evctx.delayed_callbacks.append(lambda: self._remove_childs_by_keys_delay(keys, additional_ev_creator, comp_dont_need_cancel=evctx.comp_uid))
+        else:
+            await self._remove_childs_by_keys_delay(keys, additional_ev_creator)
+
+    async def _remove_childs_by_keys_delay(
+            self,
+            keys: List[str],
+            additional_ev_creator: Optional[Callable[[], AppEvent]] = None,
+            comp_dont_need_cancel: Optional[UniqueTreeId] = None):
         detached_uid_to_comp = self._detach_child(keys)
         for k, comp in detached_uid_to_comp.items():
+            if comp_dont_need_cancel is not None and comp_dont_need_cancel == k:
+                continue
             await comp._cancel_task()
         for k in keys:
             self._child_comps.pop(k)
         if not detached_uid_to_comp:
             return
         deleted = [x.uid_encoded for x in detached_uid_to_comp.keys()]
-
-        await self.put_app_event(
-            self.create_delete_comp_event(deleted))
+        ev = self.create_delete_comp_event(deleted)
+        if additional_ev_creator is not None:
+            ev = ev + additional_ev_creator()
+        await self.put_app_event(ev)
         await self._run_special_methods([],
                                         list(detached_uid_to_comp.values()))
 
     def update_childs_complex_event(self):
         update_msg: Dict[str, Any] = {}
-        update_msg["childsComplex"] = asdict_no_deepcopy(self._child_structure, dict_factory=_undefined_comp_dict_factory, obj_factory=_undefined_comp_obj_factory)
+        update_msg["childsComplex"] = asdict_no_deepcopy(
+            self._child_structure,
+            dict_factory=_undefined_comp_dict_factory,
+            obj_factory=_undefined_comp_obj_factory)
         update_ev = self.create_update_event(update_msg)
         return update_ev
-    
+
     async def update_childs_complex(self):
         await self.send_and_wait(self.update_childs_complex_event())
 
-    def update_childs_locally(self, layout: Dict[str, Component]):
-        self.__check_child_structure_is_none()
+    def update_childs_locally(self,
+                              layout: Dict[str, Component],
+                              update_child_complex: bool = True):
+        """update child components locally, without sending event to frontend.
+        
+        Args:
+            layout: new layout
+            update_child_complex: whether to update child complex structure. only 
+                for advanced usage.
+        """
+        if update_child_complex:
+            self.__check_child_structure_is_none()
         intersect = set(layout.keys()).intersection(self._child_comps.keys())
         detached = self._detach_child(list(intersect))
         self._child_comps.update(layout)
         attached = self._attach_child(self.flow_app_comp_core,
                                       list(layout.keys()))
         # remove replaced components first.
         comps_frontend = {
@@ -2127,34 +2299,64 @@
             for c in self._get_all_nested_childs(list(layout.keys()))
         }
         comps_frontend_dict = {
             k: v.to_dict()
             for k, v in comps_frontend.items()
         }
         child_uids = [self[c]._flow_uid_encoded for c in self._child_comps]
-        update_msg: Dict[str, Any] = {
-            "childs": child_uids
-        }
-        if self._child_structure is not None:
-            update_msg["childsComplex"] = asdict_no_deepcopy(self._child_structure, dict_factory=_undefined_comp_dict_factory, obj_factory=_undefined_comp_obj_factory)
+        update_msg: Dict[str, Any] = {"childs": child_uids}
+        if update_child_complex and self._child_structure is not None:
+            update_msg["childsComplex"] = asdict_no_deepcopy(
+                self._child_structure,
+                dict_factory=_undefined_comp_dict_factory,
+                obj_factory=_undefined_comp_obj_factory)
         update_ev = self.create_update_event(update_msg)
         deleted = [x.uid_encoded for x in detached.keys()]
 
         return update_ev + self.create_update_comp_event(
-            comps_frontend_dict, deleted), list(
-                attached.values()), list(detached.values())
+            comps_frontend_dict, deleted), list(attached.values()), list(detached.keys()), list(
+                detached.values())
 
-    async def update_childs(self, layout: Union[Dict[str, Component],
-                                                 List[Component]]):
+    async def update_childs(
+            self,
+            layout: Union[Dict[str, Component], List[Component]],
+            update_child_complex: bool = True,
+            additional_ev_creator: Optional[Callable[[], AppEvent]] = None):
+        """update child components locally, without sending event to frontend.
+        
+        Args:
+            layout: new layout
+            update_child_complex: whether to update child complex structure. only 
+                for advanced usage.
+            additional_ev: additional event to send
+        """
         if isinstance(layout, list):
             layout = {str(i): v for i, v in enumerate(layout)}
-        self.__check_child_structure_is_none()
-        new_ev, attached, removed = self.update_childs_locally(layout)
-        for deleted in removed:
+        if update_child_complex:
+            self.__check_child_structure_is_none()
+        intersect = set(layout.keys()).intersection(self._child_comps.keys())
+        evctx = get_event_handling_context()
+        self_to_be_removed = self._check_ctx_contains_self(intersect)
+        if evctx is not None and self_to_be_removed:
+            evctx.delayed_callbacks.append(lambda: self._update_childs_delay(layout, update_child_complex, additional_ev_creator, evctx.comp_uid))
+        else:
+            await self._update_childs_delay(layout, update_child_complex, additional_ev_creator)
+
+    async def _update_childs_delay(self, layout: Dict[str, Component],
+                                    update_child_complex: bool = True, 
+                                   additional_ev_creator: Optional[Callable[[], AppEvent]] = None,
+                                   comp_dont_need_cancel: Optional[UniqueTreeId] = None):
+        new_ev, attached, removed_uids, removed = self.update_childs_locally(
+            layout, update_child_complex)
+        for deleted, deleted_uid in zip(removed, removed_uids):
+            if comp_dont_need_cancel == deleted_uid:
+                continue
             await deleted._cancel_task()
+        if additional_ev_creator is not None:
+            new_ev = new_ev + additional_ev_creator()
         await self.put_app_event(new_ev)
         await self._run_special_methods(attached, removed)
 
     async def replace_childs(self, layout: Dict[str, Component]):
         self.__check_child_structure_is_none()
         for k in layout.keys():
             assert k in self._child_comps
@@ -2169,52 +2371,55 @@
 class Fragment(ContainerBase[FragmentProps, Component]):
 
     def __init__(self,
                  children: Union[List[Component], Dict[str, Component]],
                  inited: bool = False) -> None:
         if isinstance(children, list):
             children = {str(i): v for i, v in enumerate(children)}
-        super().__init__(UIType.Fragment, FragmentProps, children,
-                         inited)
+        super().__init__(UIType.Fragment, FragmentProps, children, inited)
 
     @property
     def prop(self):
         propcls = self.propcls
         return self._prop_base(propcls, self)
 
     @property
     def update_event(self):
         propcls = self.propcls
         return self._update_props_base(propcls)
 
     async def set_disabled(self, disabled: bool):
         await self.send_and_wait(self.update_event(disabled=disabled))
 
-# FIXME use dataclasses_strict
+
 @dataclasses.dataclass
 class MatchCaseProps(ContainerBaseProps):
     condition: Union[Undefined, ValueType] = undefined
 
+
 @dataclasses.dataclass
 class MatchCaseItem:
     # if value is undefined, it is default case
     value: Union[ValueType, Undefined]
     child: Component
     isExpr: Union[bool, Undefined] = undefined
 
+
 @dataclasses.dataclass
 class ExprCaseItem:
     value: str
     child: Component
     isExpr: bool = True
 
+
 @dataclasses.dataclass
 class MatchCaseChildDef:
     items: List["Union[MatchCaseItem, ExprCaseItem]"]
 
+
 class MatchCase(ContainerBase[MatchCaseProps, Component]):
     """special container for extended switch case. (implemented by if/else)
     It is not a real container, but a component with children. 
     It is used to implement switch case in frontend.
     this can be used to implement tab.
 
     when you use ExprCaseItem, you need to specify a filter expr with "x"
@@ -2245,16 +2450,18 @@
 
     """
     Case = MatchCaseItem
     ExprCase = ExprCaseItem
     ChildDef = MatchCaseChildDef
 
     def __init__(self,
-                 children: List[Union[MatchCaseItem, ExprCaseItem]], init_value: Union[ValueType, Undefined] = undefined) -> None:
-        super().__init__(UIType.MatchCase, MatchCaseProps, MatchCaseChildDef(items=children))
+                 children: List[Union[MatchCaseItem, ExprCaseItem]],
+                 init_value: Union[ValueType, Undefined] = undefined) -> None:
+        super().__init__(UIType.MatchCase, MatchCaseProps,
+                         MatchCaseChildDef(items=children))
         self.props.condition = init_value
 
     @property
     def prop(self):
         propcls = self.propcls
         return self._prop_base(propcls, self)
 
@@ -2262,29 +2469,30 @@
     def update_event(self):
         propcls = self.propcls
         return self._update_props_base(propcls)
 
     async def set_condition(self, condition: Union[ValueType, Undefined]):
         assert isinstance(self._child_structure, MatchCaseChildDef)
         if isinstance(condition, Undefined):
-            return await self.send_and_wait(self.update_event(condition=condition))
+            return await self.send_and_wait(
+                self.update_event(condition=condition))
         has_expr_case = False
         for item in self._child_structure.items:
             if item.value == condition:
-                await self.send_and_wait(self.update_event(condition=condition))
+                await self.send_and_wait(self.update_event(condition=condition)
+                                         )
                 return
             if item.isExpr:
-                has_expr_case = True 
+                has_expr_case = True
         if not has_expr_case:
             raise ValueError(f"Condition {condition} not found in MatchCase")
         else:
             await self.send_and_wait(self.update_event(condition=condition))
 
 
-
 def create_ignore_usr_msg(comp: Component):
     msg = comp.create_user_msg_event((UserMessage.create_warning(
         comp._flow_uid_encoded, "UI Running",
         f"UI {comp._flow_uid_encoded}@{str(type(comp).__name__)} is still running, so ignore your control"
     )))
     return msg
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/objtree.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/objtree.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,49 +12,50 @@
 
 """
 
 import asyncio
 import contextlib
 import contextvars
 import inspect
-from typing import (Any, Callable, Coroutine, Dict, Generator, Iterator, List, Optional, Protocol, Set, Tuple,
-                    Type, TypeVar, Union)
-import uuid 
+from typing import (Any, Callable, Coroutine, Dict, Generator, Iterator, List,
+                    Optional, Protocol, Set, Tuple, Type, TypeVar, Union)
+import uuid
 from typing_extensions import ContextManager
 from tensorpc.core.moduleid import get_qualname_of_type, get_mro_qualnames_of_type
 
 T = TypeVar("T")
 
+
 class ObjTreeContextProtocol(Protocol):
     nodes: Dict[str, "UserObjTreeProtocol"]
 
 
 class UserObjTreeProtocol(Protocol):
 
     def get_childs(self) -> Dict[str, Union[Any, "UserObjTreeProtocol"]]:
         ...
 
     def enter_context(
-            self, node: "UserObjTreeProtocol"
+        self, node: "UserObjTreeProtocol"
     ) -> ContextManager["ObjTreeContextProtocol"]:
         ...
 
-    
     def default_expand(self) -> bool:
         ...
 
     # def update_tree(self) -> None:
     #     ...
 
     # async def update_tree_async(self) -> None:
     #     ...
 
     # def attach_update_tree_callback(self, func: Callable[[], Union[Coroutine[None, None, None], None]]):
     #     ...
 
+
 class ObjTreeContext:
 
     def __init__(self, nodes: Dict[str, "UserObjTreeProtocol"]) -> None:
         self.nodes: Dict[str, "UserObjTreeProtocol"] = nodes
 
 
 T_treeitem = TypeVar("T_treeitem", bound=UserObjTreeProtocol)
@@ -63,27 +64,30 @@
     Optional[ObjTreeContextProtocol]] = contextvars.ContextVar(
         "objtree_context", default=None)
 
 
 def get_objtree_context() -> Optional[ObjTreeContextProtocol]:
     return OBJ_TREE_CONTEXT_VAR.get()
 
+
 # def enter_obj_tree_context(node: UserObjTreeProtocol) -> Generator["ObjTreeContextProtocol", None, None]:
 #     ctx = ObjTreeContext(node)
 #     token = OBJ_TREE_CONTEXT_VAR.set(ctx)
 #     try:
 #         yield ctx
 #     finally:
 #         OBJ_TREE_CONTEXT_VAR.reset(token)
 
+
 class UserObjTree:
 
     def __init__(self) -> None:
         self._childs: Dict[str, Union[Any, "UserObjTreeProtocol"]] = {}
-        self._objtree_update_tree_callback: Optional[Callable[[], Union[Coroutine[None, None, None], None]]] = None
+        self._objtree_update_tree_callback: Optional[Callable[[], Union[
+            Coroutine[None, None, None], None]]] = None
 
     def get_childs(self) -> Dict[str, Union[Any, "UserObjTreeProtocol"]]:
         return self._childs
 
     @contextlib.contextmanager
     def enter_context(
         self, node: "UserObjTreeProtocol"
@@ -97,163 +101,199 @@
         ctx = ObjTreeContext({key: node, **nodes_prev})
         token = OBJ_TREE_CONTEXT_VAR.set(ctx)
         try:
             yield ctx
         finally:
             OBJ_TREE_CONTEXT_VAR.reset(token)
 
-
-    
     def default_expand(self) -> bool:
         return True
 
-    def find_may_exist(self, obj_type: Type[T], validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
+    def find_may_exist(
+            self,
+            obj_type: Type[T],
+            validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
         """find a child object of self node by type of obj.
         if not exist, return None.
         """
         if _check_node(self, obj_type, validator):
-            return self # type: ignore
+            return self  # type: ignore
 
-        return find_tree_child_item_may_exist(self, obj_type, UserObjTree, validator)
+        return find_tree_child_item_may_exist(self, obj_type, UserObjTree,
+                                              validator)
 
-    def find(self, obj_type: Type[T], validator: Optional[Callable[[T], bool]] = None) -> T:
+    def find(self,
+             obj_type: Type[T],
+             validator: Optional[Callable[[T], bool]] = None) -> T:
         """find a child object of self by type of obj.
         if not exist, raise an error.
         """
         if _check_node(self, obj_type, validator):
-            return self # type: ignore
+            return self  # type: ignore
 
         return find_tree_child_item(self, obj_type, UserObjTree, validator)
-    
+
     def update_tree(self):
         # TODO if we run in executor, we need to get loop in main thread.
-        asyncio.run_coroutine_threadsafe(self.update_tree_async(), asyncio.get_running_loop())
+        asyncio.run_coroutine_threadsafe(self.update_tree_async(),
+                                         asyncio.get_running_loop())
 
     async def update_tree_async(self):
         if self._objtree_update_tree_callback is not None:
             res = self._objtree_update_tree_callback()
             if inspect.iscoroutine(res):
                 await res
 
-    def attach_update_tree_callback(self, func: Callable[[], Union[Coroutine[None, None, None], None]]):
+    def attach_update_tree_callback(self,
+                                    func: Callable[[],
+                                                   Union[Coroutine[None, None,
+                                                                   None],
+                                                         None]]):
         self._objtree_update_tree_callback = func
 
 
-def find_tree_child_item_may_exist(root: UserObjTreeProtocol, obj_type: Type[T],
-                                   node_type: Type[T_treeitem], 
-                                   validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
+def find_tree_child_item_may_exist(
+        root: UserObjTreeProtocol,
+        obj_type: Type[T],
+        node_type: Type[T_treeitem],
+        validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
     obj_type_qnames = get_mro_qualnames_of_type(obj_type)
 
     childs_dict = root.get_childs()
     res_foreach: List[UserObjTreeProtocol] = []
 
     for k, v in childs_dict.items():
         v_type_qname = get_qualname_of_type(type(v))
         if v_type_qname in obj_type_qnames:
-            if validator is None or (validator is not None and validator(v)): # type: ignore
-                return v # type: ignore
+            if validator is None or (validator is not None
+                                     and validator(v)):  # type: ignore
+                return v  # type: ignore
         if isinstance(v, node_type):
             res_foreach.append(v)
             # res = find_tree_child_item_may_exist(v, obj_type, node_type)
             # if res is not None:
             #     return res
     for v in res_foreach:
         res = find_tree_child_item_may_exist(v, obj_type, node_type)
         if res is not None:
             return res
     return None
 
 
-def _get_tree_child_items_recursive(root: UserObjTreeProtocol, obj_type: Type[T],
-                         node_type: Type[T_treeitem],
-                         validator: Optional[Callable[[T], bool]] = None) -> List[T]:
+def _get_tree_child_items_recursive(
+        root: UserObjTreeProtocol,
+        obj_type: Type[T],
+        node_type: Type[T_treeitem],
+        validator: Optional[Callable[[T], bool]] = None) -> List[T]:
     childs_dict = root.get_childs()
     res: List[T] = []
     # we use qualname to compare type, because type may be different
     # when we reload module which is quite often in GUI.
     obj_type_qnames = get_mro_qualnames_of_type(obj_type)
     for k, v in childs_dict.items():
         v_type_qname = get_qualname_of_type(type(v))
         if v_type_qname in obj_type_qnames:
-            if validator is None or (validator is not None and validator(v)): # type: ignore
-                res.append(v) # type: ignore
+            if validator is None or (validator is not None
+                                     and validator(v)):  # type: ignore
+                res.append(v)  # type: ignore
         elif isinstance(v, node_type):
             res.extend(_get_tree_child_items_recursive(v, obj_type, node_type))
     return res
 
-def _check_node(node: UserObjTreeProtocol, obj_type: Type[T], validator: Optional[Callable[[T], bool]] = None):
+
+def _check_node(node: UserObjTreeProtocol,
+                obj_type: Type[T],
+                validator: Optional[Callable[[T], bool]] = None):
     obj_type_qnames = get_mro_qualnames_of_type(obj_type)
     # check root
     v_type_qname = get_qualname_of_type(type(node))
     if v_type_qname in obj_type_qnames:
-        if validator is None or (validator is not None and validator(node)): # type: ignore
+        if validator is None or (validator is not None
+                                 and validator(node)):  # type: ignore
             return True
     return False
 
-def get_tree_child_items(root: UserObjTreeProtocol, obj_type: Type[T],
-                         node_type: Type[T_treeitem],
-                         validator: Optional[Callable[[T], bool]] = None) -> List[T]:
+
+def get_tree_child_items(
+        root: UserObjTreeProtocol,
+        obj_type: Type[T],
+        node_type: Type[T_treeitem],
+        validator: Optional[Callable[[T], bool]] = None) -> List[T]:
     res: List[T] = []
     if _check_node(root, obj_type, validator):
-        res.append(root) # type: ignore
-    res += _get_tree_child_items_recursive(root, obj_type, node_type, validator)
+        res.append(root)  # type: ignore
+    res += _get_tree_child_items_recursive(root, obj_type, node_type,
+                                           validator)
     return res
 
-def find_tree_child_item(root: UserObjTreeProtocol, obj_type: Type[T],
+
+def find_tree_child_item(root: UserObjTreeProtocol,
+                         obj_type: Type[T],
                          node_type: Type[T_treeitem],
                          validator: Optional[Callable[[T], bool]] = None) -> T:
     res = find_tree_child_item_may_exist(root, obj_type, node_type, validator)
     assert res is not None, f"can't find type {obj_type} in root."
     return res
 
 
-def find(obj_type: Type[T], validator: Optional[Callable[[T], bool]] = None) -> T:
+def find(obj_type: Type[T],
+         validator: Optional[Callable[[T], bool]] = None) -> T:
     """find a child object of current context node by type of obj.
     if not exist, raise an error.
     """
     ctx = get_objtree_context()
     assert ctx is not None
     res: Optional[T] = None
     for node in ctx.nodes.values():
         if _check_node(node, obj_type, validator):
-            return node # type: ignore
+            return node  # type: ignore
 
-        res = find_tree_child_item_may_exist(node, obj_type, UserObjTree, validator)
+        res = find_tree_child_item_may_exist(node, obj_type, UserObjTree,
+                                             validator)
         if res is not None:
             break
-    assert res is not None, f"can't find type {obj_type} in root." 
-    return res 
+    assert res is not None, f"can't find type {obj_type} in root."
+    return res
 
 
-def find_may_exist(obj_type: Type[T], validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
+def find_may_exist(
+        obj_type: Type[T],
+        validator: Optional[Callable[[T], bool]] = None) -> Optional[T]:
     """find a child object of current context node by type of obj.
     if not exist, return None.
     """
     ctx = get_objtree_context()
     assert ctx is not None
     res: Optional[T] = None
     for node in ctx.nodes.values():
         if _check_node(node, obj_type, validator):
-            return node # type: ignore
+            return node  # type: ignore
 
-        res = find_tree_child_item_may_exist(node, obj_type, UserObjTree, validator)
+        res = find_tree_child_item_may_exist(node, obj_type, UserObjTree,
+                                             validator)
         if res is not None:
             break
-    return res 
+    return res
 
-def _apply_tree_child_items_recursive(root: UserObjTree, name_parts: Tuple[str, ...], apply_fn: Callable[[Tuple[str, ...], Any], None]) -> None:
+
+def _apply_tree_child_items_recursive(
+        root: UserObjTree, name_parts: Tuple[str, ...],
+        apply_fn: Callable[[Tuple[str, ...], Any], None]) -> None:
     childs_dict = root.get_childs()
     for k, v in childs_dict.items():
         if isinstance(v, UserObjTree):
             apply_fn((*name_parts, k), v)
             _apply_tree_child_items_recursive(v, (*name_parts, k), apply_fn)
         else:
             apply_fn((*name_parts, k), v)
 
-def apply_tree_child_items(root: UserObjTree, apply_fn: Callable[[Tuple[str, ...], Any], None]) -> None:
+
+def apply_tree_child_items(
+        root: UserObjTree, apply_fn: Callable[[Tuple[str, ...], Any],
+                                              None]) -> None:
     childs_dict = root.get_childs()
     for k, v in childs_dict.items():
         if isinstance(v, UserObjTree):
-            apply_fn((k,), v)
-            _apply_tree_child_items_recursive(v, (k,), apply_fn)
+            apply_fn((k, ), v)
+            _apply_tree_child_items_recursive(v, (k, ), apply_fn)
         else:
-            apply_fn((k,), v)
+            apply_fn((k, ), v)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/flowapp/reload.py` & `tensorpc-0.11.0/tensorpc/flow/flowapp/reload.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 from dataclasses import dataclass
 from typing import (Any, Callable, Dict, Generic, Hashable, List, Optional,
                     Type, TypeVar)
 
 from tensorpc.core.moduleid import (TypeMeta, get_obj_type_meta,
                                     get_qualname_of_type)
-from tensorpc.core.serviceunit import (AppFuncType, AppFunctionMeta, ObjectReloadManager,
+from tensorpc.core.serviceunit import (AppFuncType, AppFunctionMeta,
+                                       ObjectReloadManager,
                                        ObservedFunctionRegistry,
                                        ObservedFunctionRegistryProtocol,
                                        ReloadableDynamicClass,
                                        ServFunctionMeta)
-from tensorpc.flow.constants import (TENSORPC_ANYLAYOUT_EFFECT_FUNC_NAME, TENSORPC_ANYLAYOUT_FUNC_NAME,
+from tensorpc.flow.constants import (TENSORPC_ANYLAYOUT_EFFECT_FUNC_NAME,
+                                     TENSORPC_ANYLAYOUT_FUNC_NAME,
                                      TENSORPC_ANYLAYOUT_PREVIEW_FUNC_NAME,
                                      TENSORPC_LEGACY_LAYOUT_FUNC_NAME)
 
 T = TypeVar("T")
 
+
 class FlowSpecialMethods:
 
     def __init__(self, metas: List[ServFunctionMeta]) -> None:
         self.create_layout: Optional[ServFunctionMeta] = None
         self.auto_runs: List[ServFunctionMeta] = []
         self.effects: List[ServFunctionMeta] = []
 
@@ -42,15 +45,16 @@
                 if m.user_app_meta is None:
                     m.user_app_meta = AppFunctionMeta(AppFuncType.CreateLayout)
 
             elif m.name == TENSORPC_ANYLAYOUT_PREVIEW_FUNC_NAME:
                 self.create_preview_layout = m
                 # handle legacy name-based layout function
                 if m.user_app_meta is None:
-                    m.user_app_meta = AppFunctionMeta(AppFuncType.CreatePreviewLayout)
+                    m.user_app_meta = AppFunctionMeta(
+                        AppFuncType.CreatePreviewLayout)
             elif m.name == TENSORPC_ANYLAYOUT_EFFECT_FUNC_NAME:
                 self.create_preview_layout = m
                 # handle legacy name-based layout function
                 if m.user_app_meta is None:
                     m.user_app_meta = AppFunctionMeta(AppFuncType.Effect)
                     self.effects.append(m)
 
@@ -67,23 +71,23 @@
                     self.create_preview_layout = m
                 elif m.user_app_meta.type == AppFuncType.AutoRun:
                     self.auto_runs.append(m)
                 elif m.user_app_meta.type == AppFuncType.Effect:
                     self.effects.append(m)
 
     def contains_special_method(self):
-        res =  self.create_layout is not None
+        res = self.create_layout is not None
         res |= self.did_mount is not None
         res |= self.will_unmount is not None
         res |= self.create_object is not None
         res |= self.create_preview_layout is not None
         res |= bool(self.auto_runs)
         res |= bool(self.effects)
-        return res 
-    
+        return res
+
     def collect_all_special_meta(self):
         res: List[ServFunctionMeta] = []
         if self.create_layout is not None:
             res.append(self.create_layout)
         for r in self.auto_runs:
             res.append(r)
         for r in self.effects:
@@ -93,15 +97,15 @@
             res.append(self.did_mount)
         if self.will_unmount is not None:
             res.append(self.will_unmount)
         if self.create_object is not None:
             res.append(self.create_object)
         if self.create_preview_layout is not None:
             res.append(self.create_preview_layout)
-        return res 
+        return res
 
     def contains_autorun(self):
         return bool(self.auto_runs)
 
     def bind(self, obj):
         if self.create_layout is not None:
             self.create_layout.bind(obj)
@@ -162,17 +166,16 @@
             # if new_meta.code != meta.code:
             #     code_changed_metas.append(new_meta)
         else:
             setattr(obj, new_meta.name, new_method)
             # code_changed_metas.append(new_meta)
     return new_metas, is_reload
 
-def bind_and_reset_object_methods(
-        obj: Any,
-        new_metas: List[ServFunctionMeta]):
+
+def bind_and_reset_object_methods(obj: Any, new_metas: List[ServFunctionMeta]):
     for new_meta in new_metas:
         new_method = new_meta.bind(obj)
         setattr(obj, new_meta.name, new_method)
     return
 
 
 @dataclass
@@ -181,15 +184,18 @@
 
 
 class AppReloadManager(ObjectReloadManager):
     """to resolve some side effects, users should
     always use reload manager defined in app.
     """
 
-    def __init__(self, observed_registry: Optional[ObservedFunctionRegistryProtocol] = None) -> None:
+    def __init__(
+        self,
+        observed_registry: Optional[ObservedFunctionRegistryProtocol] = None
+    ) -> None:
         super().__init__(observed_registry)
         self.obj_layout_meta_cache: Dict[Any, AppObjectMeta] = {}
 
     def query_obj_is_anylayout(self, obj):
         obj_type = type(obj)
         if obj_type in self.obj_layout_meta_cache:
             return self.obj_layout_meta_cache[obj_type].is_anylayout
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/init_langserv/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/collection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright 2023 Yan Yan
-# 
+#
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
-# 
+#
 #     http://www.apache.org/licenses/LICENSE-2.0
-# 
+#
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/jsonlike.py` & `tensorpc-0.11.0/tensorpc/flow/jsonlike.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,14 +34,15 @@
             items.extend(flatten_dict(v, new_key, sep=sep).items())
         else:
             items.append((new_key, v))
     return dict(items)
 
 
 class Undefined:
+
     def __repr__(self) -> str:
         return "undefined"
 
     @classmethod
     def __get_pydantic_core_schema__(cls, _source_type: Any,
                                      _handler: GetCoreSchemaHandler):
         return core_schema.no_info_after_validator_function(
@@ -65,14 +66,15 @@
         # for python 3.11
         return 0
 
 
 class BackendOnlyProp(Generic[T]):
     """when wrap a property with this class, it will be ignored when serializing to frontend
     """
+
     def __init__(self, data: T) -> None:
         super().__init__()
         self.data = data
 
     def __repr__(self) -> str:
         return "BackendOnlyProp"
 
@@ -282,14 +284,15 @@
         if obj_factory is not None:
             obj = obj_factory(obj)
         return obj
 
 
 @dataclasses.dataclass
 class DataClassWithUndefined:
+
     def get_dict_and_undefined(
             self,
             state: Dict[str, Any],
             dict_factory: Callable[[List[Tuple[str, Any]]],
                                    Dict[str, Any]] = undefined_dict_factory,
             obj_factory: Optional[Callable[[Any], Any]] = None):
         this_type = type(self)
@@ -664,16 +667,18 @@
                             name,
                             t.value,
                             value=value,
                             typeStr=obj_type.__qualname__)
 
 
 class TreeItem(abc.ABC):
+
     @abc.abstractmethod
-    async def get_child_desps(self, parent_ns: UniqueTreeIdForTree) -> Dict[str, JsonLikeNode]:
+    async def get_child_desps(
+            self, parent_ns: UniqueTreeIdForTree) -> Dict[str, JsonLikeNode]:
         raise NotImplementedError
 
     @abc.abstractmethod
     async def get_child(self, key: str) -> Any:
         raise NotImplementedError
 
     def get_json_like_node(self) -> Optional[JsonLikeNode]:
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/langserv/core.py` & `tensorpc-0.11.0/tensorpc/flow/langserv/core.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,60 +1,69 @@
-
 import subprocess
 import time
 from typing import List, Tuple
 import fire
 import libtmux
 from tensorpc.utils.wait_tools import get_free_ports
 from tensorpc.constants import TENSORPC_SPLIT
 from tensorpc.flow.constants import TENSORPC_FLOW_LANG_SERVER_NAME_SPLIT, TENSORPC_FLOW_LANG_SERVER_PREFIX
 
 _SPLIT = TENSORPC_FLOW_LANG_SERVER_NAME_SPLIT
 
+
 def get_tmux_lang_server_info_may_create(ls_type: str, uid: str, port: int):
     # TODO jedi support
     assert ls_type in ["pyright", "pylsp", "clangd"]
     if ls_type == "pyright":
         window_command_fmt = "python -m tensorpc.cli.pyls --port={}"
         try:
             import pyright
         except ImportError:
-            raise Exception("pyright not installed, you can install by pip install pyright")
+            raise Exception(
+                "pyright not installed, you can install by pip install pyright"
+            )
     elif ls_type == "pylsp":
         window_command_fmt = "pylsp --ws --port {}"
         try:
             subprocess.check_call(["pylsp", "--version"])
         except Exception:
-            raise Exception("pylsp not installed, you can install by pip install python-lsp-server[websockets] yapf")
+            raise Exception(
+                "pylsp not installed, you can install by pip install python-lsp-server[websockets] yapf"
+            )
     else:
         window_command_fmt = "python -m tensorpc.cli.cppls --port={}"
         try:
             subprocess.check_call(["clangd", "--version"])
         except Exception:
-            raise Exception("clangd not installed, you can install by sudo apt install clangd")
+            raise Exception(
+                "clangd not installed, you can install by sudo apt install clangd"
+            )
 
     s = libtmux.Server()
     sessions = s.sessions
     sess_names = [sess.name for sess in sessions]
     scheduler_sess_names = [
-        sess_name for sess_name in sess_names if sess_name.startswith(TENSORPC_FLOW_LANG_SERVER_PREFIX)]
+        sess_name for sess_name in sess_names
+        if sess_name.startswith(TENSORPC_FLOW_LANG_SERVER_PREFIX)
+    ]
     found = False
     for sess_name in scheduler_sess_names:
         sess_parts = sess_name.split(_SPLIT)
         port_candidate = int(sess_parts[1])
         uid_candidate = sess_parts[2]
         if uid != uid_candidate:
-            continue 
+            continue
         found = True
         # print(port, port_candidate, port_candidate != port)
         if port_candidate != port:
             close_tmux_lang_server(uid)
             window_command = window_command_fmt.format(port)
             scheduler_sess_name = f"{TENSORPC_FLOW_LANG_SERVER_PREFIX}{_SPLIT}{port}{_SPLIT}{uid}"
-            sess = s.new_session(scheduler_sess_name, window_command=window_command)
+            sess = s.new_session(scheduler_sess_name,
+                                 window_command=window_command)
             # pane: libtmux.Pane = sess.windows[0].panes[0]
             # pane.send_keys(window_command)
 
             return port
         else:
             assert port_candidate == port
             scheduler_sess_name = scheduler_sess_names[0]
@@ -64,32 +73,36 @@
     # if port == -1:
     #     port = get_free_ports(1)[0]
     if not found:
         window_command = window_command_fmt.format(port)
         print(window_command)
 
         scheduler_sess_name = f"{TENSORPC_FLOW_LANG_SERVER_PREFIX}{_SPLIT}{port}{_SPLIT}{uid}"
-        sess = s.new_session(scheduler_sess_name, window_command=window_command)
+        sess = s.new_session(scheduler_sess_name,
+                             window_command=window_command)
 
         # pane: libtmux.Pane = sess.windows[0].panes[0]
         # pane.send_keys(window_command)
 
     return port
 
+
 def close_tmux_lang_server(uid: str):
     _prefix = TENSORPC_FLOW_LANG_SERVER_PREFIX
     # print("CLOSE CLANG", uid)
     # raise NotImplementedError
 
     # TODO pyright support
     s = libtmux.Server()
     sessions = s.sessions
     sess_names = [sess.name for sess in sessions]
     scheduler_sess_names = [
-        sess_name for sess_name in sess_names if sess_name.startswith(_prefix) and sess_name.endswith(uid)]
+        sess_name for sess_name in sess_names
+        if sess_name.startswith(_prefix) and sess_name.endswith(uid)
+    ]
     if len(scheduler_sess_names) != 0:
         scheduler_sess_name = scheduler_sess_names[0]
         sess_parts = scheduler_sess_name.split(_SPLIT)
         port = int(sess_parts[1])
         sess = s.sessions.get(session_name=scheduler_sess_name)
         assert isinstance(sess, libtmux.Session)
         pane: libtmux.Pane = sess.windows[0].panes[0]
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/langserv/pyls.py` & `tensorpc-0.11.0/tensorpc/flow/langserv/pyls.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 """
 use subprocess to check language server stdio.
 references:
 https://github.com/windmill-labs/windmill/blob/v1.101.1/lsp/pyls_launcher.py
 https://github.com/python-lsp/python-lsp-jsonrpc/blob/v1.0.0/pylsp_jsonrpc/streams.py
 """
 
-
 import asyncio
 import json
 
 import logging
 import subprocess
 import threading
 import os
@@ -17,19 +16,20 @@
 
 import aiohttp
 from aiohttp import web
 import ssl
 
 from tensorpc.core.asynctools import cancel_task
 
-
 log = logging.getLogger(__name__)
 logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))
 
+
 class AsyncJsonRpcStreamReader:
+
     def __init__(self, reader: asyncio.StreamReader):
         self._rfile = reader
 
     # def close(self):
     #     self._rfile.close()
 
     async def listen(self, message_consumer):
@@ -56,19 +56,22 @@
         """Extract the content length from an input line."""
         if line.startswith(b'Content-Length: '):
             _, value = line.split(b'Content-Length: ')
             value = value.strip()
             try:
                 return int(value)
             except ValueError as e:
-                raise ValueError("Invalid Content-Length header: {}".format(value)) from e
+                raise ValueError(
+                    "Invalid Content-Length header: {}".format(value)) from e
 
         return None
 
+
 class AsyncJsonRpcStreamWriter:
+
     def __init__(self, wfile: asyncio.StreamWriter, **json_dumps_args):
         self._wfile = wfile
         self._wfile_lock = asyncio.Lock()
         self._json_dumps_args = json_dumps_args
 
     async def close(self):
         async with self._wfile_lock:
@@ -79,27 +82,29 @@
             if self._wfile.is_closing():
                 return
             try:
                 # print("JSONRPC OUT", message)
                 body = json.dumps(message, **self._json_dumps_args)
 
                 # Ensure we get the byte length, not the character length
-                content_length = len(body) if isinstance(body, bytes) else len(body.encode('utf-8'))
+                content_length = len(body) if isinstance(body, bytes) else len(
+                    body.encode('utf-8'))
                 response = (
                     "Content-Length: {}\r\n"
                     "Content-Type: application/vscode-jsonrpc; charset=utf8\r\n\r\n"
-                    "{}".format(content_length, body)
-                )
+                    "{}".format(content_length, body))
                 self._wfile.write(response.encode('utf-8'))
                 await self._wfile.drain()
             except Exception:  # pylint: disable=broad-except
-                log.exception("Failed to write message to output file %s", message)
+                log.exception("Failed to write message to output file %s",
+                              message)
 
 
 class LanguageServerHandler:
+
     def __init__(self, st_ev: asyncio.Event, ls_cmd: List[str]) -> None:
         self.ls_cmd = ls_cmd
         self._num_conn = 0
         self._shutdown_delay_task: Optional[asyncio.Task] = None
         self.st_ev = st_ev
 
     async def handle_ls_open(self, request):
@@ -110,75 +115,81 @@
             await cancel_task(self._shutdown_delay_task)
             self._shutdown_delay_task = None
 
         self._num_conn += 1
         try:
             ws = web.WebSocketResponse()
             await ws.prepare(request)
-            aproc = await asyncio.create_subprocess_exec(*self.ls_cmd, env=os.environ,
+            aproc = await asyncio.create_subprocess_exec(
+                *self.ls_cmd,
+                env=os.environ,
                 stdin=subprocess.PIPE,
                 stdout=subprocess.PIPE)
-            assert aproc.stdout is not None 
-            assert aproc.stdin is not None 
+            assert aproc.stdout is not None
+            assert aproc.stdin is not None
             # proc = subprocess.Popen(["python", "-m", "tensorpc.cli.pyright_launch", "--stdio"], env=os.environ,
             #     stdin=subprocess.PIPE,
             #     stdout=subprocess.PIPE)
             # Create a writer that formats json messages with the correct LSP headers
             writer = AsyncJsonRpcStreamWriter(aproc.stdin)
             reader = AsyncJsonRpcStreamReader(aproc.stdout)
+
             async def cosumer(msg):
                 print("[JSONRPC OUT]", msg)
                 await ws.send_json(msg)
+
             task = asyncio.create_task(reader.listen(cosumer))
             # Create a reader for consuming stdout of the language server. We need to
             # consume this in another thread
             async for ws_msg in ws:
                 if ws_msg.type == aiohttp.WSMsgType.TEXT:
-                    print("[JSONRPC IN]", ws_msg.json())  
+                    print("[JSONRPC IN]", ws_msg.json())
                     await writer.write(ws_msg.json())
-            
+
                 elif ws_msg.type == aiohttp.WSMsgType.ERROR:
                     print("ERROR", ws_msg)
                     print("ERROR", ws_msg.data)
                 else:
                     raise NotImplementedError
             print("LANGUAGE CLIENT CONN SHUTDOWN", request)
         finally:
             self._num_conn -= 1
             if self._num_conn == 0:
                 print("SHUTDOWN LANGUAGE SERVER AFTER 1 Minute")
-                self._shutdown_delay_task = asyncio.create_task(self._shutdown_task_delayed(60))
+                self._shutdown_delay_task = asyncio.create_task(
+                    self._shutdown_task_delayed(60))
         return ws
 
     async def _shutdown_task_delayed(self, delay: float):
         await asyncio.sleep(delay)
         self.st_ev.set()
 
+
 async def serve_app(app,
                     port,
                     async_shutdown_ev: asyncio.Event,
                     url=None,
                     ssl_context=None):
     runner = web.AppRunner(app)
     await runner.setup()
     site = web.TCPSite(runner, host=url, port=port, ssl_context=ssl_context)
     await site.start()
     await async_shutdown_ev.wait()
     await runner.cleanup()
 
+
 async def serve_ls(port: int,
                    ls_cmd: List[str],
-                    ssl_key_path: str = "",
-                    ssl_crt_path: str = ""):
+                   ssl_key_path: str = "",
+                   ssl_crt_path: str = ""):
     async_shutdown_ev = asyncio.Event()
     app = web.Application()
     handler = LanguageServerHandler(async_shutdown_ev, ls_cmd)
     app.router.add_get("", handler.handle_ls_open)
     ssl_context = None
     if ssl_key_path != "" and ssl_key_path != "":
         ssl_context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
         ssl_context.load_cert_chain(ssl_crt_path, ssl_key_path)
     return await serve_app(app,
-                    port,
-                    async_shutdown_ev,
-                    ssl_context=ssl_context)
-
+                           port,
+                           async_shutdown_ev,
+                           ssl_context=ssl_context)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/langserv/pyrightcfg.py` & `tensorpc-0.11.0/tensorpc/flow/langserv/pyrightcfg.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,45 @@
 import dataclasses
 from typing import Any, Dict, List, Union
 from typing_extensions import Literal
 from tensorpc.flow.jsonlike import DataClassWithUndefined, Undefined, undefined
 
+
 @dataclasses.dataclass
 class PyrightConfig:
     disableLanguageServices: Union[bool, Undefined] = undefined
     disableOrganizeImports: Union[bool, Undefined] = undefined
     openFilesOnly: Union[bool, Undefined] = undefined
     useLibraryCodeForTypes: Union[bool, Undefined] = undefined
 
+
 @dataclasses.dataclass
 class PythonAnalysisConfig:
     autoImportCompletions: Union[bool, Undefined] = undefined
     autoSearchPaths: Union[bool, Undefined] = undefined
-    diagnosticMode: Union[Literal["openFilesOnly", "workspace"], Undefined] = undefined
+    diagnosticMode: Union[Literal["openFilesOnly", "workspace"],
+                          Undefined] = undefined
     diagnosticSeverityOverrides: Union[Dict[str, Any], Undefined] = undefined
     extraPaths: Union[List[str], Undefined] = undefined
-    logLevel: Union[Literal["Error", "Warning", "Information", "Trace"], Undefined] = undefined
+    logLevel: Union[Literal["Error", "Warning", "Information", "Trace"],
+                    Undefined] = undefined
     stubPath: Union[str, Undefined] = undefined
-    typeCheckingMode: Union[Literal["basic", "strict", "off"], Undefined] = undefined
+    typeCheckingMode: Union[Literal["basic", "strict", "off"],
+                            Undefined] = undefined
     typeshedPaths: Union[List[str], Undefined] = undefined
     useLibraryCodeForTypes: Union[bool, Undefined] = undefined
     pythonPath: Union[str, Undefined] = undefined
     venvPath: Union[str, Undefined] = undefined
     include: Union[List[str], Undefined] = undefined
     exclude: Union[List[str], Undefined] = undefined
 
+
 @dataclasses.dataclass
 class PythonConfig:
-    analysis: PythonAnalysisConfig = dataclasses.field(default_factory=PythonAnalysisConfig)
+    analysis: PythonAnalysisConfig = dataclasses.field(
+        default_factory=PythonAnalysisConfig)
+
 
 @dataclasses.dataclass
 class LanguageServerConfig(DataClassWithUndefined):
     pyright: PyrightConfig = dataclasses.field(default_factory=PyrightConfig)
-    python: PythonConfig = dataclasses.field(default_factory=PythonConfig)
+    python: PythonConfig = dataclasses.field(default_factory=PythonConfig)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/marker.py` & `tensorpc-0.11.0/tensorpc/flow/marker.py`

 * *Files 0% similar despite different names*

```diff
@@ -55,14 +55,15 @@
     return meta_decorator(func, meta)
 
 
 def mark_create_layout(func=None):
     meta = AppFunctionMeta(AppFuncType.CreateLayout)
     return meta_decorator(func, meta)
 
+
 def mark_create_preview_layout(func=None):
     meta = AppFunctionMeta(AppFuncType.CreatePreviewLayout)
     return meta_decorator(func, meta)
 
 
 def mark_create_object(func=None):
     meta = AppFunctionMeta(AppFuncType.CreateObject)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/runapp/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/runapp/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/runapp/__main__.py` & `tensorpc-0.11.0/tensorpc/flow/runapp/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/__init__.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/app.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/app.py`

 * *Files 2% similar despite different names*

```diff
@@ -53,15 +53,17 @@
                                                   VList)
 from tensorpc.flow.flowapp.components import typemetas
 from tensorpc.flow.flowapp.components.plus.config import ConfigPanel
 from tensorpc.flow.sampleapp.sample_reload_fn import func_support_reload
 from tensorpc.flow.flowapp.objtree import get_objtree_context
 from tensorpc.flow.sampleapp.sample_preview import TestPreview0
 
+
 class SampleApp(App):
+
     def __init__(self) -> None:
         super().__init__()
         self.img_ui = mui.Image()
         self.task_loop = mui.TaskLoop("Test", self.on_task_loop)
         self.swi = mui.Switch("Switch Dynamic Layout", self.on_switch)
         self.swi_box = mui.FlexBox()
         self.root.add_layout({
@@ -197,14 +199,15 @@
             dura = time.time() - t
             t = time.time()
             # await asyncio.sleep(0)
             # print(cnt, len(img_str), (time.time() - t) / cnt)
 
 
 class SampleDictApp(App):
+
     def __init__(self) -> None:
         super().__init__()
         self.vlist = VList({
             "text0": ListItemText("0"),
             "text1": ListItemText("1"),
         })
         self.vlist.prop(flex=1)
@@ -234,14 +237,15 @@
     async def _ppend_list(self):
         await self.vlist.update_childs(
             {f"text{self.cnt}": ListItemText(str(self.cnt))})
         self.cnt += 1
 
 
 class SamplePlotMetricApp(App):
+
     def __init__(self) -> None:
         super().__init__()
         self.plots = plus.HomogeneousMetricFigure(300, 300)
 
         self.root.add_layout({
             "plot0":
             self.plots,
@@ -275,28 +279,30 @@
 
     async def _mask_first_trace(self):
         await self.plots.set_trace_visible("x", not self.visible_test)
         self.visible_test = not self.visible_test
 
 
 class SampleFlowApp(App):
+
     def __init__(self) -> None:
         super().__init__()
         self.text = mui.Typography("")
         self.root.add_layout({
             "text": self.text,
         })
         self.set_init_window_size([480, 320])
 
     async def flow_run(self, ev: ScheduleEvent):
         await self.text.write(str(b"\n".join(ev.data)))
         return None
 
 
 class SampleEditorApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__()
         self.text = mui.Typography("WTF")
         self.root.add_layout({
             "text": self.text,
             "btn": Button("runCB", self.example_cb),
             "btn2": Button("ShowTS", self.show_ts),
@@ -314,14 +320,15 @@
         await self.text.write(str(time.time_ns()))
 
     def new_method(self):
         print("new method")
 
 
 class SampleEditorAppV2(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.text = mui.Typography("WTF")
         # self.root.add_layout({
         #     "text": self.text,
         #     "btn": Button("runCB", self.example_cb),
         #     "btn2": Button("ShowTS", self.show_ts),
@@ -346,14 +353,15 @@
         await self.text.write(str(time.time_ns()))
 
     def new_method(self):
         print("new method")
 
 
 class SampleThreeApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.set_init_window_size([800, 600])
         # makesure three canvas size fit parent.
         self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
@@ -418,17 +426,15 @@
         #     pc[i] = i
         # print(pc)
         # print(pc.shape)
         # attrs = [str(i) for i in range(num)]
         attrs = pc
         attrFields = ["x", "y", "z"]
         # print("???", pc.size * pc.itemsize)
-        await self.points.update_points(pc,
-                                        attrs=attrs,
-                                        attrFields=attrFields)
+        await self.points.update_points(pc, attrs=attrs, attrFields=attrFields)
 
         random_lines = np.random.uniform(-5, 5, size=[5, 2,
                                                       3]).astype(np.float32)
         await self.lines.update_lines(random_lines)
         # print("???????", random_lines)
         # with open("/home/yy/Pictures/Screenshot from 2022-02-11 15-10-06.png", "rb") as f:
         #     await self.img.show_raw(f.read(), "png")
@@ -465,16 +471,16 @@
             pc = np.ascontiguousarray(pc[:, :3])[is_not_nan_mask]
         await self.points.update_points(pc,
                                         intensity=intensity,
                                         attrs=attrs[is_not_nan_mask],
                                         attrFields=attrFields)
 
 
-
 class SampleThreePointsApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.set_init_window_size([800, 600])
         # makesure three canvas size fit parent.
         self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
@@ -521,28 +527,30 @@
 
     @mark_autorun
     def wtf(self):
         print("RTD?")
 
 
 class SampleTestApp(App):
+
     def __init__(self) -> None:
         super().__init__()
         self.root.add_layout({
             "plot0":
             VBox({
                 "asd": mui.Typography("Hello"),
             }).prop(flex=1),
             "btn":
             Button("Show", lambda: print("?"))
         })
         self.set_init_window_size([480, 320])
 
 
 class SampleThreeHudApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.set_init_window_size([800, 600])
         # makesure three canvas size fit parent.
         self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
@@ -558,15 +566,15 @@
         ctrl = three.MapControl()
         # ctrl = three.FirstPersonControl()
 
         # ctrl = three.OrbitControl()
         infgrid = three.InfiniteGridHelper(5, 50, "gray")
         self.b2d = three.Boxes2D(1000)
         mesh = three.MeshV1(three.RoundedRectGeometry(2, 1.5, 0.5),
-                          three.MeshBasicMaterial().prop(color="#393939"))
+                            three.MeshBasicMaterial().prop(color="#393939"))
         mesh.set_pointer_callback(
             on_click=three.EventHandler(lambda x: print(1), True))
         mesh.prop(hover_color="#222222", click_color="#009A63")
         text = three.Text("WTF")
         text.prop(color="red", fontSize=2)
         text.set_pointer_callback(
             on_click=three.EventHandler(lambda x: print(2)))
@@ -700,14 +708,15 @@
                 "mesh0":
                 three.Button("RTX2", 2, 1, lambda x: print("HELLO")),
             }).prop(centerAnchor=True),
         })
 
 
 class SampleThree2DApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.set_init_window_size([800, 600])
         # makesure three canvas size fit parent.
         self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
@@ -824,15 +833,17 @@
     i: V.Annotated[int, V.RangedInt(0, 10)] = 1
     j: TestEnumInt = TestEnumInt.C
     wtf: V.Annotated[float, V.RangedFloat(0, 1, 0.05, "ftw")] = 0.5
     wtfcolor: V.Annotated[str, V.ColorRGB()] = "red"
     v3: three.Vector3Type = (1, 2, 3)
     v4: V.Annotated[three.Vector3Type, V.Vector3(1.0)] = (1, 2, 3)
 
+
 class SampleConfigApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         self.set_init_window_size([800, 600])
         # makesure three canvas size fit parent.
         # self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
@@ -843,14 +854,15 @@
         return {
             "control": plus.ConfigPanel(self.cfg),
             "check": mui.Button("Check Config", lambda: print(self.cfg))
         }
 
 
 class SampleDataControlApp(EditableApp):
+
     def __init__(self) -> None:
         super().__init__(reloadable_layout=True)
         # makesure three canvas size fit parent.
         # self.root.props.minHeight = 0
         # store components here if you want to keep
         # data after reload layout.
         self.root.props.flexFlow = "row nowrap"
@@ -866,14 +878,15 @@
                                      np.zeros((500, 3)))
 
     async def read_data(self):
         print(await self.read_data_storage("Data.arr0"))
 
 
 class AutoComputeApp:
+
     @mark_create_layout
     def create_layout(self):
 
         self.options = [
             {
                 "label": 'The Shawshank Redemption',
                 "year": 1994
@@ -892,22 +905,23 @@
             },
             {
                 "label": 'Monty Python and the Holy Grail',
                 "year": 1975
             },
         ]
         ac = mui.MultipleAutocomplete("Movies", self.options).prop(
-                variant="checkbox", disableCloseOnSelect=True)
-        
+            variant="checkbox", disableCloseOnSelect=True)
+
         return mui.VBox([
             ac,
         ]).prop(width=640, height=480)
 
 
 class AnyLayout:
+
     def __init__(self) -> None:
         super().__init__()
 
     @marker.mark_create_layout
     def my_layout(self):
         return mui.FlexBox([mui.Button("Hi2345", self.handle_click)])
 
@@ -916,14 +930,15 @@
 
     def handle_click(self):
         print("???22X???")
         self.reload_wtf()
 
 
 class ObjectInspectApp:
+
     @marker.mark_create_layout
     def my_latout(self):
         self.array = np.random.uniform(-1, 1, size=[500])
         self.non_contig_arr = np.random.uniform(-1, 1, size=[500, 3])[:, 1:]
 
         try:
             import torch
@@ -935,27 +950,27 @@
             pass
 
         return mui.VBox([plus.ObjectInspector(self)]).prop(width=640,
                                                            height=480)
 
 
 class PointCloudApp:
+
     @mark_create_layout
     def my_layout(self):
         cam = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
         self.wtfobj = UserObjTree()
 
         self.canvas = plus.SimpleCanvas(cam, self._on_video_save)
-        self.slider = mui.Slider(0,
-                                 1,
-                                 1,
-                                 callback=self._on_slider_select)
+        self.slider = mui.Slider(0, 1, 1, callback=self._on_slider_select)
 
         res = mui.VBox([
-            mui.Markdown("PointCloud **:red[App]** :dog: :+1: :green[$\\sqrt{3}$]").prop(padding="10px", katex=True, emoji=True),
+            mui.Markdown(
+                "PointCloud **:red[App]** :dog: :+1: :green[$\\sqrt{3}$]").
+            prop(padding="10px", katex=True, emoji=True),
             mui.Input("hello world"),
             mui.HBox([
                 mui.Button("Change Slider Range",
                            self._on_slider_range_change),
                 mui.Button("Video", self._on_save_video),
                 self.slider.prop(flex=1),
             ]),
@@ -1012,49 +1027,53 @@
         # 3. a color string, e.g. red, green
         colors = np.random.uniform(0, 255, size=[1000]).astype(np.uint8)
         # print(colors)
         # colors = np.random.uniform(254, 255, size=[1000]).astype(np.uint8)
         sizes = np.random.uniform(0.5, 10.5, size=[1000]).astype(
             np.float32) * 1
 
-        await self.canvas.show_points("key0",
-                                      points,
-                                      limit=100000,
-                                    #   colors=colors,
-                                    #   attrs=points,
-                                    #   attr_fields=["x", "y", "z"]
-                                      )
+        await self.canvas.show_points(
+            "key0",
+            points,
+            limit=100000,
+            #   colors=colors,
+            #   attrs=points,
+            #   attr_fields=["x", "y", "z"]
+        )
         # lines = np.random.uniform(-10, 10, size=[1000, 2, 3]).astype(np.float32)
-        
+
         # await self.canvas.show_lines("lkey0",
         #                               lines,
         #                               limit=800000)
 
         # boxes: dims, locs, rots, colors (string list, don't support ndarray currently)
         dims = np.random.uniform(1, 2, size=[5, 3])
         locs = np.random.uniform(-5, 5, size=[5, 3])
         rots = np.random.uniform(-1, 1, size=[5, 3])
         rots[:, :2] = 0
         colors = ["red", "yellow", "red", "blue", "yellow"]
         await self.canvas.show_boxes("key0", dims, locs, rots, colors)
-        
+
         voxel_size = 0.1
         size = np.random.randint(100, 300)
         pcs = np.random.randint(-10, 10, size=[size, 3]) * voxel_size
         pcs[:, 0] += 3
         pcs = pcs.astype(np.float32)
         pc_colors = np.random.uniform(0, 255, size=[pcs.shape[0],
                                                     3]).astype(np.uint8)
 
         await self.canvas.show_voxels("vox0", pcs, pc_colors, voxel_size, 1000)
         random_img = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
 
-        await self.canvas.show_image("img0", random_img, (0, 0, 0), (0, 0, 0), 3)
+        await self.canvas.show_image("img0", random_img, (0, 0, 0), (0, 0, 0),
+                                     3)
+
 
 class PlotApp:
+
     @mark_create_layout
     def my_layout(self):
         self.plot = plotly.Plotly().prop(
             data=[
                 plotly.Trace(x=[1, 2, 3],
                              y=[2, 7, 3],
                              type="scatter",
@@ -1085,211 +1104,258 @@
             autosize=True,
             margin=plotly.Margin(l=0, r=0, b=0, t=0),
             xaxis=plotly.Axis(automargin=True),
             yaxis=plotly.Axis(automargin=True),
         )
         await self.plot.show_raw(data, layout)
 
+
 class ThreadLockerApp:
+
     @mark_create_layout
     def my_layout(self):
         return mui.VBox([
             Button("enter lock", self._enter_lock),
         ]).prop(width="100%")
 
     async def _enter_lock(self):
         return await appctx.run_in_executor(self.long_process)
-    
+
     def long_process(self):
 
         for i in range(100):
             print(i)
             appctx.thread_locker_wait_sync()
 
 
 class MatchCaseAppBase:
+
     @marker.mark_create_layout
     def my_layout(self):
         self.switchcase = mui.MatchCase([
             mui.MatchCase.Case("1", mui.Typography("1")),
             mui.MatchCase.Case("2", mui.Typography("2")),
             mui.MatchCase.Case("3", mui.Typography("3")),
             mui.MatchCase.Case(mui.undefined, mui.Typography("default")),
         ])
         self.switchcase_fp = mui.MatchCase([
             mui.MatchCase.ExprCase("x <= 0.2", mui.Typography("1")),
-            mui.MatchCase.ExprCase("x >= 0.2 and x < 0.6", mui.Typography("2")),
+            mui.MatchCase.ExprCase("x >= 0.2 and x < 0.6",
+                                   mui.Typography("2")),
             mui.MatchCase.ExprCase("x >= 0.6", mui.Typography("3")),
             mui.MatchCase.Case(mui.undefined, mui.Typography("default")),
         ])
 
         return mui.VBox([
             mui.RadioGroup(["1", "2", "3"], self._on_select),
             mui.Divider(),
             self.switchcase,
             mui.Divider(),
             mui.Slider(0, 1.0, 0.01, self._on_slider),
             self.switchcase_fp,
         ])
-    
+
     async def _on_slider(self, value):
         print(value, 4)
         await self.switchcase_fp.set_condition(value)
 
     async def _on_select(self, value):
         print(3)
         await self.switchcase.set_condition(value)
 
+
 class MatchCaseApp(MatchCaseAppBase):
+
     async def _on_slider(self, value):
         print(value, 2)
         await self.switchcase_fp.set_condition(value)
 
+
 class DataListApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         dataList = [
             {
                 "id": "1",
                 "name": "name1",
                 "isCheck": True,
                 "tags": [{
                     "id": "0",
                     "tag": "good",
                 }],
             },
             {
-                "id": "2",
-                "name": "name2",
-                "isCheck": True,
+                "id":
+                "2",
+                "name":
+                "name2",
+                "isCheck":
+                True,
                 "tags": [{
                     "id": "0",
                     "tag": "good",
                 }, {
                     "id": "1",
                     "tag": "small",
                 }],
-
             },
             {
                 "id": "3",
                 "name": "name3",
                 "isCheck": False,
                 "tags": [{
                     "id": "0",
                     "tag": "fat",
                 }],
-
             },
-
         ]
         datalist_checkbox = mui.Checkbox()
         datalist_comp = mui.HBox([
             mui.Typography().set_override_props(value="name"),
             datalist_checkbox,
-            mui.DataFlexBox(mui.Chip().prop(size="small").set_override_props(label="tag")).set_override_props(dataList="tags").prop(flexFlow="row", virtualized=False),
+            mui.DataFlexBox(mui.Chip().prop(size="small").set_override_props(
+                label="tag")).set_override_props(dataList="tags").prop(
+                    flexFlow="row", virtualized=False),
         ]).prop(alignItems="center")
-        datalist = mui.DataFlexBox(datalist_comp).prop(flexFlow="column", dataList=dataList)
+        datalist = mui.DataFlexBox(datalist_comp).prop(flexFlow="column",
+                                                       dataList=dataList)
         datalist.bind_prop(datalist_checkbox, "isCheck")
         return mui.VBox([
             datalist,
         ])
 
+
 class DataGridApp:
-    def create_data(self, index: int, name: str, calories: float,
-        fat: float,
-        carbs: float,
-        protein: bool):
+
+    def create_data(self, index: int, name: str, calories: float, fat: float,
+                    carbs: float, protein: bool):
         return {
-            "id": str(index),
-            "name": name,
-            "calories": calories,
-            "fat": fat,
-            "carbs": carbs,
-            "protein": protein,
-            "nested": [
-                {
-                    "id": str(i),
-                    "iq": random.randint(0, 100),
-                } for i in range(random.randint(2, 6))
-            ]
+            "id":
+            str(index),
+            "name":
+            name,
+            "calories":
+            calories,
+            "fat":
+            fat,
+            "carbs":
+            carbs,
+            "protein":
+            protein,
+            "nested": [{
+                "id": str(i),
+                "iq": random.randint(0, 100),
+            } for i in range(random.randint(2, 6))]
         }
-    
+
     def create_many_datas(self, count: int):
         fake = Faker()
         for i in range(count):
-            yield self.create_data(i, fake.name(), random.randint(100, 300), random.randint(1, 25), random.randint(22, 44), i % 2 == 0)
-
+            yield self.create_data(i, fake.name(), random.randint(100, 300),
+                                   random.randint(1, 25),
+                                   random.randint(22, 44), i % 2 == 0)
 
     @marker.mark_create_layout
     def my_layout(self):
         rows = list(self.create_many_datas(10))
         btn = mui.Button("Edit").prop(loading=False)
         btn.event_click.on_standard(lambda x: print(x.keys)).configure(True)
         cbox = mui.Checkbox("")
         input_cell = mui.Input("dev")
         fat_cell = mui.Slider(0, 100, 1)
 
         column_defs = [
-            mui.DataGrid.ColumnDef("special", specialType=mui.DataGridColumnSpecialType.MasterDetail.value),
+            mui.DataGrid.ColumnDef(
+                "special",
+                specialType=mui.DataGridColumnSpecialType.MasterDetail.value),
             mui.DataGrid.ColumnDef("id", accessorKey="id"),
-            mui.DataGrid.ColumnDef("name", accessorKey="name", width=120, editCell=input_cell),
+            mui.DataGrid.ColumnDef("name",
+                                   accessorKey="name",
+                                   width=120,
+                                   editCell=input_cell),
             mui.DataGrid.ColumnDef("calories", accessorKey="calories"),
-            mui.DataGrid.ColumnDef("fat", accessorKey="fat", editCell=fat_cell),
+            mui.DataGrid.ColumnDef("fat", accessorKey="fat",
+                                   editCell=fat_cell),
             mui.DataGrid.ColumnDef("carbs", accessorKey="carbs"),
-            mui.DataGrid.ColumnDef("protein", accessorKey="protein", align="right", cell=cbox),
+            mui.DataGrid.ColumnDef("protein",
+                                   accessorKey="protein",
+                                   align="right",
+                                   cell=cbox),
             mui.DataGrid.ColumnDef("btn", cell=btn),
         ]
         master_detail = mui.JsonViewer().set_override_props(data=".")
         master_detail = mui.VBox([
             mui.Typography("Master Detail").prop(variant="h4"),
             mui.DataGrid([
                 mui.DataGrid.ColumnDef("id", accessorKey="id"),
                 mui.DataGrid.ColumnDef("iq", accessorKey="iq"),
-            ]).prop(idKey="id", rowHover=True, stickyHeader=False, virtualized=False, size="small", enableFilter=False).set_override_props(dataList="nested")
+            ]).prop(idKey="id",
+                    rowHover=True,
+                    stickyHeader=False,
+                    virtualized=False,
+                    size="small",
+                    enableFilter=False).set_override_props(dataList="nested")
         ]).prop(width="100%", alignItems="center")
         # master_detail = mui.DataFlexBox(mui.HBox([
         #     mui.Typography().set_override_props(value="id"),
         #     mui.Divider(orientation="vertical"),
         #     mui.Typography().set_override_props(value="iq"),
         # ])).set_override_props(dataList="nested").prop(flexFlow="column")
         btn = mui.Button("NAME!")
         md_root = mui.Markdown("")
         btn.event_click.on(lambda: md_root.write("FOOT!"))
 
-        dgrid = mui.DataGrid(column_defs, rows, master_detail, customHeaders=[
-            mui.MatchCase([
-                mui.MatchCase.Case("name", btn),
-                mui.MatchCase.Case(mui.undefined, mui.Typography("Other H!")),
-            ]).set_override_props(condition="condition")
-        ], customFooters=[
-            mui.MatchCase([
-                mui.MatchCase.Case("name", md_root),
-                mui.MatchCase.Case(mui.undefined, mui.Typography("Other F!")),
-            ]).set_override_props(condition="condition")
-        ]).prop(idKey="id", rowHover=True, virtualized=True, enableFilter=True)
+        dgrid = mui.DataGrid(
+            column_defs,
+            rows,
+            master_detail,
+            customHeaders=[
+                mui.MatchCase([
+                    mui.MatchCase.Case("name", btn),
+                    mui.MatchCase.Case(mui.undefined,
+                                       mui.Typography("Other H!")),
+                ]).set_override_props(condition="condition")
+            ],
+            customFooters=[
+                mui.MatchCase([
+                    mui.MatchCase.Case("name", md_root),
+                    mui.MatchCase.Case(mui.undefined,
+                                       mui.Typography("Other F!")),
+                ]).set_override_props(condition="condition")
+            ]).prop(idKey="id",
+                    rowHover=True,
+                    virtualized=True,
+                    enableFilter=True)
         # dgrid.event_fetch_detail.on(self._fetch_detail)
         dgrid.bind_prop(cbox, "protein")
         dgrid.bind_prop(input_cell, "name")
         dgrid.bind_prop(fat_cell, "fat")
 
         return mui.VBox([
-            dgrid.prop(stickyHeader=False, virtualized=False, size="small", tableLayout="fixed"),
+            dgrid.prop(stickyHeader=False,
+                       virtualized=False,
+                       size="small",
+                       tableLayout="fixed"),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
     def _fetch_detail(self, key: str):
         print("WTF", key)
         return {"key": key}
 
+
 class NumpyDataGridProxy(mui.DataGridProxy):
+
     def __init__(self, obj: np.ndarray):
         assert obj.ndim == 2
         self.obj = obj
         default_data = {f"{c}": 0 for c in range(obj.shape[1])}
-        super().__init__(numRows=obj.shape[0], numColumns=obj.shape[1], defaultData=default_data)
+        super().__init__(numRows=obj.shape[0],
+                         numColumns=obj.shape[1],
+                         defaultData=default_data)
 
     async def fetch_data(self, start: int, end: int):
         print("fetch", start, end)
         subarr = self.obj[start:end]
         data_list: List[Dict[str, Any]] = []
         for row in range(subarr.shape[0]):
             col = {f"{c}": subarr[row, c] for c in range(subarr.shape[1])}
@@ -1304,65 +1370,93 @@
         data_list: List[Dict[str, Any]] = []
         for row in range(subarr.shape[0]):
             col = {f"{c}": subarr[row, c] for c in range(subarr.shape[1])}
             col["id"] = str(start + row)
             data_list.append(col)
         return data_list
 
-class DataGridProxyApp:    
+
+class DataGridProxyApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         arr = np.random.uniform(0, 1, size=[1000, 3])
         data_list: List[Dict[str, Any]] = []
         for row in range(arr.shape[0]):
             col = {f"{c}": arr[row, c] for c in range(arr.shape[1])}
             col["id"] = str(row)
             data_list.append(col)
 
         column_defs = [
             mui.DataGrid.ColumnDef(id=f"{c}") for c in range(arr.shape[1])
         ]
-        dgrid = mui.DataGrid(column_defs, NumpyDataGridProxy(arr)).prop(idKey="id", rowHover=True, virtualized=True, enableFilter=True)
+        dgrid = mui.DataGrid(column_defs,
+                             NumpyDataGridProxy(arr)).prop(idKey="id",
+                                                           rowHover=True,
+                                                           virtualized=True,
+                                                           enableFilter=True)
         return mui.VBox([
             dgrid.prop(stickyHeader=False, virtualized=True, size="small"),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
-class MatrixDataGridApp:    
+
+class MatrixDataGridApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         arr = np.random.uniform(0, 1, size=[100, 3])
         arr2 = np.random.randint(0, 100, size=[100, 1]).astype(np.int64)
-        column_def = mui.DataGrid.ColumnDef(id=f"unused", specialType=mui.DataGridColumnSpecialType.Number, width=80, specialProps=mui.DataGridColumnSpecialProps(mui.DataGridNumberCell(fixed=8)))
+        column_def = mui.DataGrid.ColumnDef(
+            id=f"unused",
+            specialType=mui.DataGridColumnSpecialType.Number,
+            width=80,
+            specialProps=mui.DataGridColumnSpecialProps(
+                mui.DataGridNumberCell(fixed=8)))
         custom_footers = [
             mui.MatchCase([
                 mui.MatchCase.Case("index", mui.Typography("Max")),
-                mui.MatchCase.Case(mui.undefined, mui.Typography().set_override_props(value="data").prop(enableTooltipWhenOverflow=True, tooltipEnterDelay=400, fontSize="12px")),
+                mui.MatchCase.Case(
+                    mui.undefined,
+                    mui.Typography().set_override_props(value="data").prop(
+                        enableTooltipWhenOverflow=True,
+                        tooltipEnterDelay=400,
+                        fontSize="12px")),
             ]).set_override_props(condition="condition")
         ]
         custom_footer_datas = [{
             "a-0": str(arr.max(0)[0]),
             "a-1": str(arr.max(0)[1]),
             "a-2": str(arr.max(0)[2]),
             "b-0": str(arr2.max(0)[0]),
         }]
-        dgrid = mui.MatrixDataGrid(column_def, {"a": arr, "b": arr2}, 
-            customFooters=custom_footers, 
+        dgrid = mui.MatrixDataGrid(
+            column_def,
+            {
+                "a": arr,
+                "b": arr2
+            },
+            customFooters=custom_footers,
             customFooterDatas=custom_footer_datas,
         )
-        dgrid.prop(rowHover=True, virtualized=True, enableFilter=True, tableLayout="fixed")
+        dgrid.prop(rowHover=True,
+                   virtualized=True,
+                   enableFilter=True,
+                   tableLayout="fixed")
         dgrid.prop(tableSxProps={
             '& .MuiTableCell-sizeSmall': {
                 "padding": '2px 2px',
             },
         })
         return mui.VBox([
             dgrid.prop(stickyHeader=False, virtualized=True, size="small"),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
+
 class TutorialApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         code = f"""
 from tensorpc.flow.flowapp import appctx
 from tensorpc.flow.flowapp.components import mui, three, plus
 from tensorpc.flow import mark_create_layout
 class App:
@@ -1378,43 +1472,51 @@
         pyright_setting.python.analysis.extraPaths = [
             str(PACKAGE_ROOT.parent),
         ]
 
         return mui.VBox([
             plus.AppInMemory("sample_code", code).prop(flex=1),
         ])
-    
+
+
 class LinkDownloadApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         appctx.get_app().add_file_resource("sample.py", self.sample_file)
         return mui.VBox([
             mui.Markdown("## WTF"),
             mui.Link.app_download_link("click to download", "sample.py"),
         ])
 
     def sample_file(self):
         return Path(__file__).read_bytes()
 
+
 class VirtualizedBoxApp:
-    
+
     def create_many_datas(self, count: int):
         fake = Faker()
         for i in range(count):
             yield fake.text()
 
     @marker.mark_create_layout
     def my_layout(self):
         rows = list(self.create_many_datas(10))
-        row_elems = [mui.HBox([mui.Typography(row).prop(variant="body1")]) for row in rows]
+        row_elems = [
+            mui.HBox([mui.Typography(row).prop(variant="body1")])
+            for row in rows
+        ]
         return mui.VBox([
             mui.VirtualizedBox([*row_elems]),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
+
 class CollectionApp:
+
     @mark_create_layout
     def my_layout(self):
         self.anylayout = AnyLayout()
         self.monitor = plus.ComputeResourceMonitor()
         self.example_draggable_pc = np.random.uniform(-3, 3, size=[1000, 3])
         self.example_3d_canvas = PointCloudApp()
         self.example_preview_layout = TestPreview0()
@@ -1464,36 +1566,36 @@
         pyright_setting.python.analysis.pythonPath = sys.executable
         pyright_setting.python.analysis.extraPaths = [
             str(PACKAGE_ROOT.parent),
         ]
 
         res = mui.HBox([
             mui.Allotment([
-                plus.ObjectInspector(self, use_fast_tree=True).prop(width="100%",
-                                                height="100%",
-                                                overflow="hidden"),
+                plus.ObjectInspector(self, use_fast_tree=True).prop(
+                    width="100%", height="100%", overflow="hidden"),
                 mui.HBox([
                     plus.AnyFlexLayout(
-                        mui.FlexLayout.VBox([self.sm, mui.FlexLayout.HBox([
-                            mui.Markdown("1"),
-                            mui.Markdown("2"),
-                        ])])),
+                        mui.FlexLayout.VBox([
+                            self.sm,
+                            mui.FlexLayout.HBox([
+                                mui.Markdown("1"),
+                                mui.Markdown("2"),
+                            ])
+                        ])),
                 ]).prop(width="100%", height="100%", overflow="hidden")
             ]).prop(defaultSizes=[1, 3], width="100%", height="100%")
         ]).prop(flexFlow="row nowrap")
 
         return res
 
     @mark_autorun
     async def _autorun_dev(self):
-        
 
         return await self._autorun_dev2()
 
-
     @staticmethod
     @observe_autorun_function
     async def _autorun_dev2():
         print("X2ss25sad")
 
     @staticmethod
     @observe_autorun_script
@@ -1503,22 +1605,25 @@
         #%% block split
         print("BLOCK2s asfasf asf")
 
     @staticmethod
     @observe_autorun_script
     def _autorun_dev4():
         print("BLOCK0 asasff WTF")
-        appctx.inspector.set_custom_layout_sync(mui.VBox([
-            mui.Markdown("## :red[WTF2]"),
-        ]))
+        appctx.inspector.set_custom_layout_sync(
+            mui.VBox([
+                mui.Markdown("## :red[WTF2]"),
+            ]))
         a = 5
         #%% block split
         print("BLOCK2s asfasf asf")
 
+
 class SchedulerTest:
+
     @mark_create_layout
     def my_layout(self):
 
         return mui.VBox([
             Button("Show", self._submit_simple_task),
         ])
 
@@ -1548,14 +1653,15 @@
         # task1.keep_tmux_session = False
         await schr.submit_task(task1)
         await schr.submit_task(task2)
         await schr.submit_task(task3)
 
 
 class SchedulerApp:
+
     @mark_create_layout
     def my_layout(self):
 
         # use a function to protect your password (only stored in master disk).
         self.scheduler = plus.TmuxScheduler(
             lambda: appctx.get_app().get_ssh_node_data("Local"))
         self.scheduler_test = mui.flex_wrapper(SchedulerTest())
@@ -1565,21 +1671,21 @@
                 mui.FlexLayout.Tab(self.scheduler),
                 mui.FlexLayout.Tab(self.scheduler_test),
             ]))
         return res
 
 
 class CameraBenchmarkApp:
+
     @mark_create_layout
     def my_layout(self):
         self.img_ui = mui.Image()
         self.task = None
         return mui.VBox([
-            mui.Button("OpenCam",
-                        self.on_button_click),
+            mui.Button("OpenCam", self.on_button_click),
             self.img_ui,
         ]).prop(width="400px", height="400px")
 
     async def on_button_click(self):
         if self.task is None:
             loop = asyncio.get_running_loop()
             self.task = asyncio.create_task(self._video_task())
@@ -1618,60 +1724,60 @@
             dura_encode = time.time() - t2
 
             dura = time.time() - t
             t = time.time()
             # await asyncio.sleep(0)
             # print(dura, dura_encode, dura_cv, len(img_str), frame.shape)
 
+
 class TestNodeNode0(UserObjTree):
+
     def __init__(self) -> None:
         super().__init__()
 
     @marker.mark_create_preview_layout
     def layout_func(self):
-        return mui.VBox([
-            mui.Button("WTF"),
-            mui.Markdown("## 6")
-        ])
+        return mui.VBox([mui.Button("WTF"), mui.Markdown("## 6")])
+
 
 class TestNodeRoot(UserObjTree):
+
     def __init__(self) -> None:
         super().__init__()
         self.node0 = TestNodeNode0()
         self._childs["node0"] = self.node0
 
     @marker.mark_create_preview_layout
     def layout_func(self):
-        return mui.VBox([
-            mui.Button("ROOT"),
-            mui.Markdown("## ROOT132")
-        ])
+        return mui.VBox([mui.Button("ROOT"), mui.Markdown("## ROOT132")])
+
 
 class GridPreviewLayoutApp:
+
     @mark_create_layout
     def my_layout(self):
         root = TestNodeRoot()
         reload_mgr = appctx.get_reload_manager()
 
-        return mui.HBox([plus.GridPreviewLayout({
-            "root": root,
-            "root.node0": root.get_childs()["node0"]
-        })]).prop(width="100%")
+        return mui.HBox([
+            plus.GridPreviewLayout({
+                "root": root,
+                "root.node0": root.get_childs()["node0"]
+            })
+        ]).prop(width="100%")
 
 
 if __name__ == "__main__":
     from pydantic import (
         BaseModel,
         GetCoreSchemaHandler,
         GetJsonSchemaHandler,
         TypeAdapter,
         ValidationError,
     )
 
     props = mui.MultipleAutocompleteProps()
-    props.variant = "wtf" 
+    props.variant = "wtf"
 
     TypeAdapter(mui.MultipleAutocompleteProps).validate_python(props)
     # ac = mui.MultipleAutocomplete("Movies", []).prop(
     #             variant="checkbox", disableCloseOnSelect=True)
-
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/arraygrid.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/arraygrid.py`

 * *Files 4% similar despite different names*

```diff
@@ -54,82 +54,101 @@
 from tensorpc.flow.flowapp.components import typemetas
 from tensorpc.flow.flowapp.components.plus.config import ConfigPanel
 from tensorpc.flow.sampleapp.sample_reload_fn import func_support_reload
 from tensorpc.flow.flowapp.objtree import get_objtree_context
 from tensorpc.flow.sampleapp.sample_preview import TestPreview0
 from tensorpc.flow.flowapp.components.plus.arraygrid import NumpyArrayGrid, NumpyArrayGridTable
 
-class MatrixDataGridAppV1:    
+
+class MatrixDataGridAppV1:
+
     @marker.mark_create_layout
     def my_layout(self):
         arr = np.random.uniform(0, 1, size=[100, 3])
         arr2 = np.random.randint(0, 100, size=[100, 1]).astype(np.int64)
-        column_def = mui.DataGrid.ColumnDef(id=f"unused", specialType=mui.DataGridColumnSpecialType.Number, width=80, specialProps=mui.DataGridColumnSpecialProps(mui.DataGridNumberCell(fixed=8)))
+        column_def = mui.DataGrid.ColumnDef(
+            id=f"unused",
+            specialType=mui.DataGridColumnSpecialType.Number,
+            width=80,
+            specialProps=mui.DataGridColumnSpecialProps(
+                mui.DataGridNumberCell(fixed=8)))
         custom_footers = [
             mui.MatchCase([
                 mui.MatchCase.Case("index", mui.Typography("Max")),
-                mui.MatchCase.Case(mui.undefined, mui.Typography().set_override_props(value="data").prop(enableTooltipWhenOverflow=True, tooltipEnterDelay=400, fontSize="12px")),
+                mui.MatchCase.Case(
+                    mui.undefined,
+                    mui.Typography().set_override_props(value="data").prop(
+                        enableTooltipWhenOverflow=True,
+                        tooltipEnterDelay=400,
+                        fontSize="12px")),
             ]).set_override_props(condition="condition")
         ]
         custom_footer_datas = [{
             "a-0": str(arr.max(0)[0]),
             "a-1": str(arr.max(0)[1]),
             "a-2": str(arr.max(0)[2]),
             "b-0": str(arr2.max(0)[0]),
         }]
-        dgrid = mui.MatrixDataGrid(column_def, {"a": arr, "b": arr2}, 
-            customFooters=custom_footers, 
+        dgrid = mui.MatrixDataGrid(
+            column_def,
+            {
+                "a": arr,
+                "b": arr2
+            },
+            customFooters=custom_footers,
             customFooterDatas=custom_footer_datas,
         )
-        dgrid.prop(rowHover=True, virtualized=True, enableFilter=True, tableLayout="fixed")
+        dgrid.prop(rowHover=True,
+                   virtualized=True,
+                   enableFilter=True,
+                   tableLayout="fixed")
         dgrid.prop(tableSxProps={
             '& .MuiTableCell-sizeSmall': {
                 "padding": '2px 2px',
             },
         })
         return mui.VBox([
             dgrid.prop(stickyHeader=False, virtualized=True, size="small"),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
-class MatrixDataGridApp:    
+
+class MatrixDataGridApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         arr = np.random.uniform(0, 1, size=[1, 3, 20000, 3])
         arr2 = np.random.randint(0, 1000, size=[1, 3, 20000, 4])
-        arr3 = np.random.randint(0, 254, size=[1, 3, 20000, 1]).astype(np.uint8)
+        arr3 = np.random.randint(0, 254, size=[1, 3, 20000,
+                                               1]).astype(np.uint8)
         arr.reshape(-1)[-2] = np.nan
         arr.reshape(-1)[-1] = np.inf
 
         grid = NumpyArrayGrid({
             "a": arr,
             "b": arr2,
             "c": arr3,
         })
         return mui.VBox([
             grid.prop(flex=1),
         ]).prop(width="100%", height="100%", overflow="hidden")
 
 
-class MatrixDataGridContainerApp:    
+class MatrixDataGridContainerApp:
+
     @marker.mark_create_layout
     def my_layout(self):
         arr = np.random.uniform(0, 1, size=[1, 3, 20000, 3])
         arr2 = np.random.randint(0, 1000, size=[1, 3, 20000, 4])
-        arr3 = np.random.randint(0, 254, size=[1, 3, 20000, 1]).astype(np.uint8)
+        arr3 = np.random.randint(0, 254, size=[1, 3, 20000,
+                                               1]).astype(np.uint8)
         arr.reshape(-1)[-2] = np.nan
         arr.reshape(-1)[-1] = np.inf
         arr_item1 = {
             "a": arr,
             "b": arr2,
             "c": arr3,
         }
         arr4 = np.random.uniform(0, 1, size=[100, 3])
 
         arr_item2 = arr4
-        
 
-        return NumpyArrayGridTable({
-            "a": arr_item1,
-            "b": arr_item2,
-            "d": 5
-
-        })
+        return NumpyArrayGridTable({"a": arr_item1, "b": arr_item2, "d": 5})
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/collection.py` & `tensorpc-0.11.0/tensorpc/flow/serv/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 Yan Yan
+# Copyright 2024 Yan Yan
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/d3.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/d3.py`

 * *Files 2% similar despite different names*

```diff
@@ -149,38 +149,44 @@
 
 class BufferMeshDevApp:
 
     @mark_create_layout
     def my_layout(self):
         self.limit = 1000
         initial_num_pts = 500
-        cam = three.PerspectiveCamera(fov=75, near=0.1, far=1000, children=[
-            three.PointLight(intensity=8).prop(castShadow=True, position=(0, 0, 1)),
-            # three.SpotLight(position=(0, 0, -1), target_position=(0, 0, -3)).prop(angle=0.25,
-            #                                 penumbra=0.5,
-            #                                 castShadow=True,
-            #                                 intensity=8,
-            #                                 helperColor=0x555555,
-            #                                 distance=50),
-        ])
+        cam = three.PerspectiveCamera(
+            fov=75,
+            near=0.1,
+            far=1000,
+            children=[
+                three.PointLight(intensity=8).prop(castShadow=True,
+                                                   position=(0, 0, 1)),
+                # three.SpotLight(position=(0, 0, -1), target_position=(0, 0, -3)).prop(angle=0.25,
+                #                                 penumbra=0.5,
+                #                                 castShadow=True,
+                #                                 intensity=8,
+                #                                 helperColor=0x555555,
+                #                                 distance=50),
+            ])
         random_pcs = np.random.randint(1, 20, size=[initial_num_pts, 3])
         random_pc_colors = np.random.uniform(0,
                                              255,
                                              size=[random_pcs.shape[0],
                                                    3]).astype(np.uint8)
         voxel_size = 0.1
         self.voxel_size = voxel_size
         voxel_mesh = three.VoxelMesh(
             random_pcs.astype(np.float32) * voxel_size,
             voxel_size,
             self.limit,
             [
                 # three.MeshPhongMaterial().prop(vertexColors=True, color="aqua", specular="#ffffff", shininess=250, transparent=False),
                 # three.MeshStandardMaterial().prop(vertexColors=True),
-                three.MeshBasicMaterial().prop(vertexColors=False, color="red"),
+                three.MeshBasicMaterial().prop(vertexColors=False,
+                                               color="red"),
                 # three.Edges(),
                 # three.Wireframe(),
             ],
             colors=random_pc_colors).prop(receiveShadow=True, castShadow=True)
         instanced_voxel_mesh = three.InstancedMesh(
             random_pcs.astype(np.float32) * voxel_size,
             random_pcs.shape[0],
@@ -218,16 +224,15 @@
                 #     # ]).prop(castShadow=True),
                 # ]).prop(variant="relativeToCamera", position=(0, 0.1, -1)),
                 three.Sky().prop(sunPosition=(1, 1, 1),
                                  distance=450000,
                                  inclination=0,
                                  azimuth=0.25),
                 three.AmbientLight(),
-                three.DirectionalLight((10, 10, 10)).prop(
-                                                   castShadow=True),
+                three.DirectionalLight((10, 10, 10)).prop(castShadow=True),
                 # three.HemisphereLight(color=0xffffff, ground_color=0xb9b9b9, intensity=0.85).prop(position=(-7, 25, 13)),
                 # three.PointLight(intensity=0.8).prop(position=(100, 100, 100),
                 #                                    castShadow=True),
                 # buffer_mesh,
                 # voxel_mesh,
                 instanced_voxel_mesh,
                 three.Mesh([
@@ -844,80 +849,78 @@
 class ViewDevApp:
 
     @mark_create_layout
     def my_layout(self):
         cam = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
         cam2 = three.PerspectiveCamera(fov=75, near=0.1, far=1000)
 
-        btns = [
-            mui.MenuItem("Button 1"),
-            mui.MenuItem("Button 2")
-        ]
+        btns = [mui.MenuItem("Button 1"), mui.MenuItem("Button 2")]
         view1 = three.View([
-                cam,
-                three.CameraControl().prop(makeDefault=True, syncObject3ds=[cam2]),
-                # three.Mesh([
-                #     three.BoxGeometry(),
-                #     three.MeshBasicMaterial().prop(color="orange",
-                #                                     transparent=True),
-                # ]),
-                three.SelectionContext([
-                    three.EffectComposer([
-                        three.Outline().prop(
-                            blur=True,
-                            edgeStrength=100,
-                            width=1000,
-                            visibleEdgeColor=0xddd,
-                            hiddenEdgeColor=0xddd,
-                            blendFunction=three.BlendFunction.ALPHA),
-                        # three.Bloom(),
-                        # three.GammaCorrection(),
-                        # three.ToneMapping().prop(mode=three.ToneMapppingMode.ACES_FILMIC),
-                    ]).prop(autoClear=False),
-                    three.Mesh([
-                        three.BoxGeometry(),
-                        three.Edges(),
-                        three.MeshBasicMaterial().prop(color="orange",
-                                                        transparent=True),
-                    ]).prop(
-                        enableSelect=True,
+            cam,
+            three.CameraControl().prop(makeDefault=True, syncObject3ds=[cam2]),
+            # three.Mesh([
+            #     three.BoxGeometry(),
+            #     three.MeshBasicMaterial().prop(color="orange",
+            #                                     transparent=True),
+            # ]),
+            three.SelectionContext([
+                three.EffectComposer([
+                    three.Outline().prop(
+                        blur=True,
+                        edgeStrength=100,
+                        width=1000,
+                        visibleEdgeColor=0xddd,
+                        hiddenEdgeColor=0xddd,
+                        blendFunction=three.BlendFunction.ALPHA),
+                    # three.Bloom(),
+                    # three.GammaCorrection(),
+                    # three.ToneMapping().prop(mode=three.ToneMapppingMode.ACES_FILMIC),
+                ]).prop(autoClear=False),
+                three.Mesh([
+                    three.BoxGeometry(),
+                    three.Edges(),
+                    three.MeshBasicMaterial().prop(color="orange",
+                                                   transparent=True),
+                ]).prop(enableSelect=True,
                         castShadow=True,
                         position=(0, 0, 0),
                         enableHover=True,
                         enablePivotControl=True,
                         enablePivotOnSelected=True,
                         pivotControlProps=three.PivotControlsCommonProps(
-                            depthTest=False, annotations=True, anchor=(0, 0, 0))),
-                ]),
-
-            ]).prop(flex=2,
-                    overflow="hidden",
-                    index=1,
-                    border="1px solid red",
-                    allowKeyboardEvent=True, 
-                    menuItems=btns)
+                            depthTest=False,
+                            annotations=True,
+                            anchor=(0, 0, 0))),
+            ]),
+        ]).prop(flex=2,
+                overflow="hidden",
+                index=1,
+                border="1px solid red",
+                allowKeyboardEvent=True,
+                menuItems=btns)
         view1.event_context_menu.on(lambda x: print(x))
         canvas = three.ViewCanvas([
-                            mui.VBox([
-                                view1,
-            three.View([
-                cam2,
-                # three.CameraControl().prop(makeDefault=True),
-                three.Mesh([
-                    three.BoxGeometry(),
-                    three.MeshBasicMaterial().prop(color="orange",
-                                                    transparent=True),
-                ]),
-            ]).prop(flex=1,
-                    overflow="hidden",
-                    index=2)
-                            ]).prop(width="100%", height="100%" , overflow="hidden")
-
-
-        ]).prop(display="flex", flexDirection="row", width="100%", height="100%" , overflow="hidden", enablePerf=True)
+            mui.VBox([
+                view1,
+                three.View([
+                    cam2,
+                    # three.CameraControl().prop(makeDefault=True),
+                    three.Mesh([
+                        three.BoxGeometry(),
+                        three.MeshBasicMaterial().prop(color="orange",
+                                                       transparent=True),
+                    ]),
+                ]).prop(flex=1, overflow="hidden", index=2)
+            ]).prop(width="100%", height="100%", overflow="hidden")
+        ]).prop(display="flex",
+                flexDirection="row",
+                width="100%",
+                height="100%",
+                overflow="hidden",
+                enablePerf=True)
         # canvas.update_sx_props({
         #     "grid-template-columns": "1fr 1fr",
         # })
         # canvas = three.ViewCanvas([
         #         three.View([
         #             three.PerspectiveCamera(fov=75, near=0.1, far=1000, make_default=True),
         #             three.CameraControl().prop(makeDefault=True),
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/file.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/file.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,25 +30,22 @@
 
         self.ssh_files = mui.TextField("Files").prop(multiline=True)
         self.target_loc = mui.TextField("Target")
         self.run = mui.Button("Run", self.download_files)
 
         return {
             "container":
-            mui.VBox(
-                {
-                    "url": self.ssh_url,
-                    "ssh_files": self.ssh_files,
-                    "ssh_username": self.ssh_username,
-                    "ssh_password": self.ssh_password,
-                    "target_loc": self.target_loc,
-                    "run": self.run,
-                }
-                ).prop(width=480,
-                height=480)
+            mui.VBox({
+                "url": self.ssh_url,
+                "ssh_files": self.ssh_files,
+                "ssh_username": self.ssh_username,
+                "ssh_password": self.ssh_password,
+                "target_loc": self.target_loc,
+                "run": self.run,
+            }).prop(width=480, height=480)
         }
 
     async def download_files(self):
         client = SSHClient(self.ssh_url.value, self.ssh_username.value,
                            self.ssh_password.value, None)
         assert Path(self.target_loc.value).exists()
         async with client.simple_connect() as conn:
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/sample_reload_fn.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/sample_reload_fn.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,40 +1,37 @@
-
-from tensorpc.flow import appctx 
+from tensorpc.flow import appctx
 
 
 @appctx.observe_function
 def func_support_reload(a, b):
     print("hi6", a, b)
 
-
-
     return a + b
 
+
 # from cumm.common import TensorViewNVRTC
 # from cumm.inliner import NVRTCInlineBuilder
-# import pccm 
-# from cumm import tensorview as tv 
+# import pccm
+# from cumm import tensorview as tv
 # from pccm.builder.inliner import PreCaptureFunctionCode
 
 # class Dev:
 #     def __init__(self) -> None:
 #         self.inliner = NVRTCInlineBuilder([TensorViewNVRTC], reload_when_code_change=True)
 
 #         self.a = 1
 
 #     def prepare_params(self, code: PreCaptureFunctionCode):
 #         with code.capture_vars():
 #             code.raw(f"""
 #             int A = $(self.a);
 #             """)
-        
+
 #     @appctx.observe_function
 #     def dev(self):
 #         a = tv.zeros([1], tv.float32, 0)
 #         code = PreCaptureFunctionCode()
 #         self.prepare_params(code)
 #         code.raw(f"""
 #         tv::printf2(A, "RTX2411214");
 #         """)
 #         self.inliner.kernel_1d("dev", 1, 0, code)
-
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/sampleapp/v_nextgen.py` & `tensorpc-0.11.0/tensorpc/flow/sampleapp/v_nextgen.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,136 +11,152 @@
 from tensorpc.flow.flowapp.components.plus.core import ObjectGridItemConfig
 
 from tensorpc.flow.marker import mark_did_mount
 from tensorpc import prim
 from tensorpc.flow.flowapp.objtree import UserObjTree, find
 from tensorpc.flow import observe_function
 
+
 class TestNodeNode0(UserObjTree):
+
     def __init__(self, wh: Tuple[float, float], uid: str = "0") -> None:
         super().__init__()
         self.uid = uid
         self.wh = wh
 
     def func(self, a, b):
         V.points("points0", 1000).p(a, b, 1).prop(colors="red", size=5)
 
     @mark_create_preview_layout
     def layout_func(self):
-        res = mui.VBox([
-            mui.Markdown(f"{self.uid}|`{self.wh}`")
-        ])
+        res = mui.VBox([mui.Markdown(f"{self.uid}|`{self.wh}`")])
         res.set_user_meta_by_type(ObjectGridItemConfig(self.wh[0], self.wh[1]))
-        return res 
+        return res
 
 
 class TestNodeRoot(UserObjTree):
+
     def __init__(self) -> None:
         super().__init__()
         self.node0 = TestNodeNode0((0.5, 0.5), "0")
         for i in range(20):
             random_w = 1.0
             random_h = np.random.randint(1, 3) * 2 / 4
 
-            self._childs[f"node{i}"] = TestNodeNode0((random_w, random_h), str(i))
+            self._childs[f"node{i}"] = TestNodeNode0((random_w, random_h),
+                                                     str(i))
         # self._childs["node1"] = TestNodeNode0("1")
         # self._childs["node2"] = TestNodeNode0("2")
         # self._childs["node3"] = TestNodeNode0("3")
 
     def func(self, a, b):
         with V.group("dev"):
             V.bounding_box((a, b, 2))
             self._childs[f"node{0}"].func(a, b)
 
     async def on_task_loop(self):
         async for x in self.task_loop.task_loop(list(range(10))):
             await asyncio.sleep(0.1)
             print(find(TestNodeNode0))
 
-
     @mark_create_preview_layout
     def layout_func(self):
         self.task_loop = mui.TaskLoop("dev", self.on_task_loop)
         return mui.VBox([
             mui.Button("ROOT"),
             mui.Markdown("## ROOT1"),
             self.task_loop,
         ])
 
+
 class DevApp:
 
     @mark_create_layout
     def my_layout(self):
         root = TestNodeRoot()
         self.root = root
-        canvas = plus.ComplexCanvas([
-        ], init_tree_root=root, init_tree_child_accessor=lambda x: x.get_childs())
+        canvas = plus.ComplexCanvas(
+            [],
+            init_tree_root=root,
+            init_tree_child_accessor=lambda x: x.get_childs())
         canvas.canvas.prop(flat=True, shadows=True)
         self.canvas = canvas
-        self.random_img = np.random.randint(0, 255, (128 * 16, 128 * 16, 3), dtype=np.uint8)
+        self.random_img = np.random.randint(0,
+                                            255, (128 * 16, 128 * 16, 3),
+                                            dtype=np.uint8)
         return mui.VBox([
             mui.HBox([
                 mui.Button("Test V", self.on_click),
                 mui.Button("Test Tree", self.on_test_tree),
                 mui.Button("Test custom layout", self.on_custom_gv_layout),
                 mui.Button("Test gv locals", self.on_gv_locals_layout),
-
             ]),
             plus.InspectPanel(self, canvas).prop(width="100%", height="100%"),
         ])
-    
+
     async def on_test_tree(self):
         self.root.func(3, 4)
 
     async def on_custom_gv_layout(self):
         items = {}
         for k in range(3):
             half = random.random() > 0.5
             items[f"name{k}"] = mui.FlexBox([
                 mui.Markdown(f"## hello world {k}"),
-            ]).set_user_meta_by_type(ObjectGridItemConfig(1, 0.5 if half else 1))
+            ]).set_user_meta_by_type(
+                ObjectGridItemConfig(1, 0.5 if half else 1))
         await self.canvas.set_new_grid_items(items, False)
 
     async def on_click(self):
         print("clicked")
         # with V.ctx():
         random_img = np.random.randint(0, 255, (100, 100, 4), dtype=np.uint8)
         random_img[:, :, -1] = 255
 
-        random_img_rgb = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)
+        random_img_rgb = np.random.randint(0,
+                                           255, (100, 100, 3),
+                                           dtype=np.uint8)
 
         # await self.canvas.canvas.update_childs([
         #     three.Group([
         #         three.Image().prop(image=random_img)
         #     ])
         # ])
         # random_img = np.random.randint(0, 255, (128 * 16, 128 * 16, 3), dtype=np.uint8)
 
         with V.group("debug"):
             V.image(random_img, pos=(5, 5, 2), use_datatex=True)
         with V.group("debugX"):
 
             V.image(random_img_rgb, pos=(0, 5, 2), use_datatex=False)
 
-            points = np.random.uniform(-1, 1, size=[1000, 3]).astype(np.float32)
+            points = np.random.uniform(-1, 1, size=[1000,
+                                                    3]).astype(np.float32)
             # V.bounding_box((1, 1, 1))
             mesh = three.Mesh([
                 three.BoxGeometry(1, 1, 1),
                 three.MeshBasicMaterial().prop(color="red")
-            ]).prop(position=(2, 0, 0), enableSelect=True, enablePivotControl=True, pivotControlProps=three.PivotControlsCommonProps(
+            ]).prop(position=(2, 0, 0),
+                    enableSelect=True,
+                    enablePivotControl=True,
+                    pivotControlProps=three.PivotControlsCommonProps(
                         depthTest=False, annotations=True, anchor=(0, 0, 0)))
             mesh.event_change.on(lambda x: print(x))
 
             V.three_ui(mesh)
-            box = three.BoundingBox([2, 2, 2]).prop(position=(4, 0, 0), enableSelect=True, enablePivotControl=True, pivotControlProps=three.PivotControlsCommonProps(
-                        depthTest=False, annotations=True, anchor=(0, 0, 0)))
+            box = three.BoundingBox([2, 2, 2]).prop(
+                position=(4, 0, 0),
+                enableSelect=True,
+                enablePivotControl=True,
+                pivotControlProps=three.PivotControlsCommonProps(
+                    depthTest=False, annotations=True, anchor=(0, 0, 0)))
             box.event_change.on(lambda x: print(x))
 
             V.three_ui(box)
 
         # await self.canvas._unknown_visualization("foo.bar", points)
 
     async def on_gv_locals_layout(self):
         a = np.array([1, 2, 3])
         b = np.array([1, np.nan, 3])
         c = np.zeros([1, 3, 224, 224], np.int64)
-        await self.canvas.update_locals()
+        await self.canvas.update_locals()
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/serv/__init__.py` & `tensorpc-0.11.0/tensorpc/serve/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/serv/core.py` & `tensorpc-0.11.0/tensorpc/flow/serv/core.py`

 * *Files 2% similar despite different names*

```diff
@@ -53,18 +53,19 @@
 from tensorpc.flow import constants as flowconstants
 from tensorpc.flow.constants import (
     FLOW_DEFAULT_GRAPH_ID, FLOW_FOLDER_PATH, TENSORPC_FLOW_DEFAULT_TMUX_NAME,
     TENSORPC_FLOW_GRAPH_ID, TENSORPC_FLOW_MASTER_GRPC_PORT,
     TENSORPC_FLOW_MASTER_HTTP_PORT, TENSORPC_FLOW_NODE_ID,
     TENSORPC_FLOW_NODE_READABLE_ID, TENSORPC_FLOW_NODE_UID,
     TENSORPC_FLOW_USE_REMOTE_FWD)
-from tensorpc.flow.coretypes import (
-    Message, MessageEvent, MessageEventType, MessageLevel,
-    ScheduleEvent, SessionStatus, StorageDataItem, UserContentEvent,
-    UserDataUpdateEvent, UserEvent, UserStatusEvent, get_uid)
+from tensorpc.flow.coretypes import (Message, MessageEvent, MessageEventType,
+                                     MessageLevel, ScheduleEvent,
+                                     SessionStatus, StorageDataItem,
+                                     UserContentEvent, UserDataUpdateEvent,
+                                     UserEvent, UserStatusEvent, get_uid)
 from tensorpc.flow.flowapp.core import (AppEvent, AppEventType, ComponentEvent,
                                         FrontendEventType, NotifyEvent,
                                         NotifyType, ScheduleNextForApp,
                                         UIEvent, UISaveStateEvent,
                                         app_event_from_data)
 from tensorpc.flow.jsonlike import JsonLikeNode
 from tensorpc.flow.serv_names import serv_names
@@ -107,15 +108,16 @@
     def empty():
         return NodeStatus(CommandEventType.PROMPT_END, SessionStatus.Stop)
 
 
 ENCODING = "utf-8"
 ENCODING = None
 USE_APP_HTTP_PORT = True
-USE_LANG_SERVER_PORT = True 
+USE_LANG_SERVER_PORT = True
+
 
 def _extract_graph_node_id(uid: str):
     parts = uid.split("@")
     return parts[0], parts[1]
 
 
 def _get_uid(graph_id: str, node_id: str):
@@ -263,15 +265,15 @@
     @property
     def readable_id(self) -> str:
         return self._flow_data["data"]["readableNodeId"]
 
     @property
     def raw_data(self) -> Dict[str, Any]:
         return self._flow_data
-    
+
     def update_data(self, update_data: Dict[str, Any]):
         self._flow_data["data"].update(update_data)
 
     def set_data(self, graph_id: str, flow_data: Dict[str, Any]):
         self._flow_data = flow_data
         # graph id may change due to rename
         self.inputs: Dict[str, List[Handle]] = {}
@@ -769,30 +771,30 @@
     def key(self):
         return self.node_data["key"]
 
     @property
     def value(self):
         return self.node_data["value"]
 
+
 class DataStorageNodeBase(abc.ABC):
     """storage: 
     """
 
     def __init__(self) -> None:
         self.stored_data: Dict[str, StorageDataItem] = {}
 
     @abc.abstractmethod
     def get_node_id(self) -> str:
         raise NotImplementedError
-    
+
     @abc.abstractmethod
     def get_in_memory_limit(self) -> int:
         raise NotImplementedError
 
-
     def get_data_attrs(self):
         items = self.get_item_metas()
         res = []
         for item in items:
             data = self.read_meta_dict(item)
             res.append(data)
         return res
@@ -852,97 +854,98 @@
             data_item = self.stored_data[key]
             return data_item.get_meta_dict()
         meta_path = self.get_meta_path(key)
         if meta_path.exists():
             with meta_path.open("r") as f:
                 meta_dict = json.load(f)
             if "|" not in meta_dict["id"]:
-                meta_dict["id"] = UniqueTreeId.from_parts([meta_dict["id"]]).uid_encoded
+                meta_dict["id"] = UniqueTreeId.from_parts([meta_dict["id"]
+                                                           ]).uid_encoded
                 with meta_path.open("w") as f:
                     json.dump(meta_dict, f)
             return meta_dict
         raise FileNotFoundError(f"{meta_path} not exists")
-    
 
     def remove_data(self, key: Optional[str]):
         if key is None:
             items = self.get_items()
             for k in items:
                 self.remove_data(k)
-            return 
+            return
         if key in self.stored_data:
             self.stored_data.pop(key)
         meta_path = self.get_meta_path(key)
         if meta_path.exists():
             meta_path.unlink()
         path = self.get_save_path(key)
         if path.exists():
             path.unlink()
 
     def rename_data(self, key: str, new_name: str):
         if key == new_name:
-            return False 
+            return False
         if key not in self.stored_data:
             return False
         if new_name in self.stored_data:
             return False
         item = self.stored_data[key]
-        # rename data 
+        # rename data
         path = self.get_save_path(key)
         if path.exists():
             path.rename(self.get_save_path(new_name))
         # self.remove_data(key)
-        item.meta.name = new_name 
+        item.meta.name = new_name
         item.meta.id = UniqueTreeIdForTree.from_parts([new_name])
         meta_path = self.get_meta_path(key)
         if meta_path.exists():
             meta_path.unlink()
         with self.get_meta_path(new_name).open("w") as f:
             json.dump(item.get_meta_dict(), f)
         # print(item.meta)
         # self.save_data(new_name, item.data, item.meta, item.timestamp)
-        return True 
+        return True
 
     def read_data(self, key: str) -> StorageDataItem:
         if key in self.stored_data:
             data_item = self.stored_data[key]
             if len(data_item.data) > 0:
                 return data_item
         path = self.get_save_path(key)
         meta_path = self.get_meta_path(key)
 
         if path.exists() and meta_path.exists():
             with meta_path.open("r") as f:
                 meta_dict = json.load(f)
-            
+
             # TODO remove this (old style uid compat)
             if "|" not in meta_dict["id"]:
-                meta_dict["id"] = UniqueTreeId.from_parts([meta_dict["id"]]).uid_encoded
+                meta_dict["id"] = UniqueTreeId.from_parts([meta_dict["id"]
+                                                           ]).uid_encoded
                 with meta_path.open("w") as f:
                     json.dump(meta_dict, f)
             meta = JsonLikeNode(**meta_dict)
             with path.open("rb") as f:
                 data: bytes = pickle.load(f)
                 data_item = StorageDataItem(data, meta)
                 if len(data) <= self.get_in_memory_limit():
                     self.stored_data[key] = data_item
                 else:
-                    self.stored_data[key] = StorageDataItem(
-                        bytes(), meta)
+                    self.stored_data[key] = StorageDataItem(bytes(), meta)
                 return data_item
         raise FileNotFoundError(f"{path} not exists")
 
     def need_update(self, key: str, timestamp: int):
         return self.stored_data[key].timestamp != timestamp
 
     def read_data_if_need_update(self, key: str, timestamp: int):
         if self.need_update(key, timestamp):
             return self.read_data(key)
         return None
 
+
 @ALL_NODES.register
 class DataStorageNode(Node, DataStorageNodeBase):
     """storage: 
     """
 
     def __init__(self,
                  flow_data: Dict[str, bytes],
@@ -1134,22 +1137,26 @@
                                env: Dict[str, str]):
         if fports:
             self.fwd_grpc_port = fports[0]
             self.fwd_http_port = fports[1]
             print(self.lang_server_port, fports[2])
             if self.lang_server_port != -1:
                 self.fwd_lang_server_port = fports[2]
-                env[flowconstants.TENSORPC_FLOW_APP_LANG_SERVER_FWD_PORT] = str(
-                    self.fwd_lang_server_port)
+                env[flowconstants.
+                    TENSORPC_FLOW_APP_LANG_SERVER_FWD_PORT] = str(
+                        self.fwd_lang_server_port)
 
         super()._env_port_modifier(fports, rfports, env)
 
-    async def stop_language_server(self, enable_port_forward: bool, url: str,
-                            username: str,
-                            password: str, init_cmds: str = ""):
+    async def stop_language_server(self,
+                                   enable_port_forward: bool,
+                                   url: str,
+                                   username: str,
+                                   password: str,
+                                   init_cmds: str = ""):
         if enable_port_forward:
             await _close_lang_serv(self.id, url, username, password, init_cmds)
         else:
             close_tmux_lang_server(self.id)
 
     async def start_session(self,
                             callback: Callable[[Event], Awaitable[None]],
@@ -1179,15 +1186,16 @@
         #         langserv_port = -1
         # else:
         #     langserv_port = get_tmux_lang_server_info_may_create("pyright", self.id)
         # print("APP", url, client.url_no_port, client.port)
         num_port = 3
         if not is_worker:
             # query two free port in target via ssh, then use them as app ports
-            ports = await _get_free_port(num_port, url, username, password, init_cmds)
+            ports = await _get_free_port(num_port, url, username, password,
+                                         init_cmds)
         else:
             # query two local ports in flow remote worker, then use them as app ports
             ports = get_free_ports(num_port)
         if len(ports) != num_port:
             raise ValueError("get free port failed. exit.")
         # if langserv_port != -1:
         #     ports.append(langserv_port)
@@ -1204,14 +1212,15 @@
         self.running_driver_id = running_driver_id
 
         async def exit_callback():
             self.task = None
             self.last_event = CommandEventType.PROMPT_END
             self.set_stop_status()
             self.running_driver_id = ""
+
         envs.update({
             flowconstants.TENSORPC_FLOW_APP_GRPC_PORT:
             str(self.grpc_port),
             flowconstants.TENSORPC_FLOW_APP_HTTP_PORT:
             str(self.http_port),
             flowconstants.TENSORPC_FLOW_APP_MODULE_NAME:
             f"\"{self.module_name}\"",
@@ -1576,33 +1585,36 @@
             print(e.stdout)
             print("-----------")
             print(e.stderr)
             stderr = e.stderr
             raise ValueError(e.stderr)
     return ports
 
-async def _query_lang_serv_port_and_init(uid: str, url: str,
-                         username: str,
-                         password: str,
-                         init_cmds: str = ""):
+
+async def _query_lang_serv_port_and_init(uid: str,
+                                         url: str,
+                                         username: str,
+                                         password: str,
+                                         init_cmds: str = ""):
     client = SSHClient(url, username, password, None, "", "utf-8")
     port = -1
     # res = await client.simple_run_command(f"python -m tensorpc.cli.free_port {count}")
     # print(res)
     stderr = ""
     async with client.simple_connect() as conn:
         try:
             if init_cmds:
                 cmd = (
                     f"bash -i -c "
                     f'"{init_cmds} && python -m tensorpc.flow.init_langserv pyright {uid}"'
                 )
             else:
-                cmd = (f"bash -i -c "
-                       f'"python -m tensorpc.flow.init_langserv pyright {uid}"')
+                cmd = (
+                    f"bash -i -c "
+                    f'"python -m tensorpc.flow.init_langserv pyright {uid}"')
             result = await conn.run(cmd, check=True)
             stdout = result.stdout
             if stdout is not None:
                 if isinstance(stdout, bytes):
                     stdout = stdout.decode("utf-8")
                 port_strs = stdout.strip().split("\n")[-1]
                 port = int(port_strs)
@@ -1612,18 +1624,20 @@
             print(e.stdout)
             print("-----------")
             print(e.stderr)
             stderr = e.stderr
             raise ValueError(e.stderr)
     return port
 
-async def _close_lang_serv(uid: str, url: str,
-                         username: str,
-                         password: str,
-                         init_cmds: str = ""):
+
+async def _close_lang_serv(uid: str,
+                           url: str,
+                           username: str,
+                           password: str,
+                           init_cmds: str = ""):
     client = SSHClient(url, username, password, None, "", "utf-8")
     port = -1
     # res = await client.simple_run_command(f"python -m tensorpc.cli.free_port {count}")
     # print(res)
     stderr = ""
     async with client.simple_connect() as conn:
         try:
@@ -1684,29 +1698,30 @@
                     _empty_flow_graph(FLOW_DEFAULT_GRAPH_ID).to_dict(), f)
         self.flow_dict: Dict[str, FlowGraph] = {}
         for flow_path in self.root.glob("*.json"):
             with flow_path.open("r") as f:
                 flow_data = json.load(f)
             self.flow_dict[flow_path.stem] = FlowGraph(flow_data,
                                                        flow_path.stem)
-            
+
         self._prev_ssh_q_task: Optional[asyncio.Task[Event]] = None
 
     def _get_node_desp(self, graph_id: str, node_id: str):
         assert graph_id in self.flow_dict, f"can't find graph {graph_id}"
         gh = self.flow_dict[graph_id]
         assert gh.node_exists(node_id), f"can't find node {node_id}"
         node = gh.get_node_by_id(node_id)
         driver: Optional[Node] = None
         if node.driver_id != "":
             if gh.node_exists(node.driver_id):
                 driver = gh.get_node_by_id(node.driver_id)
         return NodeDesp(node, gh, driver)
 
-    @marker.mark_server_event(event_type=marker.ServiceEventType.WebSocketOnDisConnect)
+    @marker.mark_server_event(
+        event_type=marker.ServiceEventType.WebSocketOnDisConnect)
     def _on_client_disconnect(self, cl):
         # TODO when all client closed instead of one client, close all terminal
         for g in self.flow_dict.values():
             for n in g.nodes:
                 if isinstance(n, NodeWithSSHBase):
                     if n.terminal_close_ts < 0:
                         n.terminal_close_ts = time.time_ns()
@@ -1848,18 +1863,19 @@
 
         if isinstance(driver, RemoteSSHNode):
             if use_grpc:
                 return await driver.simple_grpc_remote_call(
                     worker_key, graph_id, node_id, type, ui_ev_dict, is_sync)
             else:
                 return await driver.http_remote_call(worker_key, graph_id,
-                                                     node_id, type, ui_ev_dict, is_sync)
+                                                     node_id, type, ui_ev_dict,
+                                                     is_sync)
         else:
             # if node.last_event == CommandEventType.COMMAND_COMPLETE:
-            #     return 
+            #     return
             if use_grpc:
                 grpc_port = node.grpc_port
                 durl, _ = get_url_port(driver.url)
                 if driver.enable_port_forward:
                     app_url = get_grpc_url("localhost", node.fwd_grpc_port)
                 else:
                     app_url = get_grpc_url(durl, grpc_port)
@@ -1872,19 +1888,24 @@
                 if driver.enable_port_forward:
                     app_url = get_http_url("localhost", node.fwd_http_port)
                 else:
                     app_url = get_http_url(durl, http_port)
                 return await http_remote_call(sess, app_url, app_key, type,
                                               ui_ev_dict, is_sync)
 
-    async def run_ui_event(self, graph_id: str, node_id: str,
-                           ui_ev_dict: Dict[str, Any], is_sync: bool = False):
-        return await self.run_single_event(graph_id, node_id,
+    async def run_ui_event(self,
+                           graph_id: str,
+                           node_id: str,
+                           ui_ev_dict: Dict[str, Any],
+                           is_sync: bool = False):
+        return await self.run_single_event(graph_id,
+                                           node_id,
                                            AppEventType.UIEvent.value,
-                                           ui_ev_dict, is_sync=is_sync)
+                                           ui_ev_dict,
+                                           is_sync=is_sync)
 
     async def run_app_editor_event(self, graph_id: str, node_id: str,
                                    ui_ev_dict: Dict[str, Any]):
         return await self.run_single_event(graph_id, node_id,
                                            AppEventType.AppEditor.value,
                                            ui_ev_dict)
 
@@ -1916,18 +1937,18 @@
             grpc_port = node.grpc_port
             durl, _ = get_url_port(driver.url)
             if driver.enable_port_forward:
                 app_url = get_grpc_url("localhost", node.fwd_grpc_port)
             else:
                 app_url = get_grpc_url(durl, grpc_port)
             async with AsyncRemoteManager(app_url) as robj:
-                async for x in robj.remote_generator(serv_names.APP_GET_FILE, file_key):
+                async for x in robj.remote_generator(serv_names.APP_GET_FILE,
+                                                     file_key):
                     yield x
 
-
     async def query_app_state(self,
                               graph_id: str,
                               node_id: str,
                               editor_only: bool = False):
         node, driver = self._get_app_node_and_driver(graph_id, node_id)
         if not node.is_session_started():
             return None
@@ -1943,17 +1964,15 @@
             if driver.enable_port_forward:
                 app_url = get_grpc_url("localhost", node.fwd_grpc_port)
             else:
                 app_url = get_grpc_url(durl, grpc_port)
             return await tensorpc.simple_chunk_call_async(
                 app_url, serv_names.APP_GET_LAYOUT, editor_only)
 
-    async def stop_app_node(self,
-                              graph_id: str,
-                              node_id: str):
+    async def stop_app_node(self, graph_id: str, node_id: str):
         node, driver = self._get_app_node_and_driver(graph_id, node_id)
         if not node.is_session_started():
             return None
         # if isinstance(driver, DirectSSHNode):
         #     # TODO if we stop and start a node in a short time, language server may not be able to start
         #     await node.stop_language_server(True, driver.url, driver.username, driver.password, driver.init_commands)
 
@@ -2101,15 +2120,16 @@
 
             elif isinstance(event, (CommandEvent)):
                 node.last_event = event.type
                 if event.type == CommandEventType.CURRENT_COMMAND:
                     if isinstance(node, CommandNode):
                         if event.arg is not None:
                             current_cmd = event.arg.decode("utf-8")
-                            if node._previous_cmd.strip() == current_cmd.strip():
+                            if node._previous_cmd.strip() == current_cmd.strip(
+                            ):
                                 node._start_record_stdout = True
                 if event.type == CommandEventType.COMMAND_COMPLETE:
                     if isinstance(node, CommandNode):
                         if node._start_record_stdout:
                             res = node.get_previous_cmd_result()
                             if event.arg is not None:
                                 res.return_code = int(event.arg)
@@ -2154,24 +2174,23 @@
         # TODO query status in remote
         if not self._node_exists(graph_id, node_id):
             return UserStatusEvent.empty().to_dict()
         node_desp = self._get_node_desp(graph_id, node_id)
         if isinstance(node_desp.node, (NodeWithSSHBase)):
             return node_desp.node.get_node_status().to_dict()
         return UserStatusEvent.empty().to_dict()
-    
+
     def _get_all_node_status(self, graphs: List[FlowGraph]):
         res = {}
         for graph in graphs:
             graph_id = graph.graph_id
             for node in graph.nodes:
                 res[node.id] = self.query_node_status(graph_id, node.id)
         return res
 
-
     async def save_terminal_state(self, graph_id: str, node_id: str, state,
                                   timestamp_ms: int):
         if len(state) > 0:
             node_desp = self._get_node_desp(graph_id, node_id)
             node = node_desp.node
             assert isinstance(node, (NodeWithSSHBase))
             if node_desp.has_remote_driver:
@@ -2302,37 +2321,39 @@
         # TODO do we need a async lock here?
         flow_path = self.root / "dockview" / f"favorite_nodes.json"
         if not flow_path.parent.exists():
             flow_path.parent.mkdir(0o755, True, True)
         with flow_path.open("w") as f:
             json.dump(nodeIds, f)
 
-    async def update_node_data_and_save_graph(self, graph_id: str, node_id: str, update_node_data: Dict[str, Any]):
+    async def update_node_data_and_save_graph(self, graph_id: str,
+                                              node_id: str,
+                                              update_node_data: Dict[str,
+                                                                     Any]):
         graph = self.flow_dict[graph_id]
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         node.update_data(update_node_data)
         self._save_graph_content_only(graph_id, graph.get_save_data())
 
-
     async def load_default_graph_object(self):
         final_res = [
             await self.load_graph(FLOW_DEFAULT_GRAPH_ID, force_reload=False)
         ]
         for k, v in self.flow_dict.items():
             if k != FLOW_DEFAULT_GRAPH_ID:
                 res = await self.load_graph(k, force_reload=False)
                 final_res.append(res)
         return final_res
 
     async def load_default_graph(self):
         dock_state = self.load_dock_state()
         favorites = self.load_favorite_nodes()
         final_res = await self.load_default_graph_object()
-        # dockLayoutModel, 
+        # dockLayoutModel,
         return {
             "flows": [g.to_dict() for g in final_res],
             **dock_state,
             "nodeStatus": self._get_all_node_status(final_res),
             "favoriteNodes": favorites,
         }
 
@@ -2364,15 +2385,14 @@
                               current_key: str):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, MarkdownNode)
         node.set_page(page, current_key)
         node.set_current_key(current_key)
 
-
     async def load_graph(self, graph_id: str, force_reload: bool = False):
         flow_path = self.root / f"{graph_id}.json"
         with flow_path.open("r") as f:
             flow_data = json.load(f)
         for n in flow_data["nodes"]:
             n["selected"] = False
             if "width" in n:
@@ -2617,15 +2637,15 @@
             raise NotImplementedError
 
     async def query_data_items(self, graph_id: str, node_id: str):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, DataStorageNodeBase)
         return node.get_items()
-    
+
     async def query_all_data_node_ids(self, graph_id: str):
         assert graph_id in self.flow_dict, f"can't find graph {graph_id}"
         gh = self.flow_dict[graph_id]
         res: List[Tuple[str, str]] = []
         for n in gh.nodes:
             if isinstance(n, DataStorageNode):
                 res.append((n.id, n.readable_id))
@@ -2657,47 +2677,53 @@
             return node.read_data(key)
 
     async def query_data_attrs(self, graph_id: str, node_id: str):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, DataStorageNodeBase), f"{type(node)}"
         return node.get_data_attrs()
-    
-    async def delete_datastorage_data(self, graph_id: str, node_id: str, key: Optional[str]):
+
+    async def delete_datastorage_data(self, graph_id: str, node_id: str,
+                                      key: Optional[str]):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, DataStorageNodeBase)
         res = node.remove_data(key)
         if isinstance(node, DataStorageNode):
             await self._user_ev_q.put(
                 (node.get_uid(), UserDataUpdateEvent(node.get_data_attrs())))
-        return res 
-    
-    async def rename_datastorage_data(self, graph_id: str, node_id: str, key: str, newname: str):
+        return res
+
+    async def rename_datastorage_data(self, graph_id: str, node_id: str,
+                                      key: str, newname: str):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, DataStorageNodeBase)
         res = node.rename_data(key, newname)
         if isinstance(node, DataStorageNode):
             await self._user_ev_q.put(
                 (node.get_uid(), UserDataUpdateEvent(node.get_data_attrs())))
-        return res 
+        return res
 
     async def get_ssh_node_data(self, graph_id: str, node_id: str):
         node_desp = self._get_node_desp(graph_id, node_id)
         node = node_desp.node
         assert isinstance(node, DirectSSHNode)
         url_parts = node.url.split(":")
         if len(url_parts) == 1:
             url_no_port = node.url
             port = 22
         else:
             url_no_port = url_parts[0]
             port = int(url_parts[1])
-        return SSHTarget(url_no_port, port, node.username, node.password, init_commands=node.init_commands)
+        return SSHTarget(url_no_port,
+                         port,
+                         node.username,
+                         node.password,
+                         init_commands=node.init_commands)
 
     @marker.mark_server_event(event_type=ServiceEventType.Exit)
     async def _on_exit(self):
         # send exit message to all remote workers
         for g in self.flow_dict.values():
             for n in g.nodes:
                 if isinstance(n, RemoteSSHNode):
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/serv/flowapp.py` & `tensorpc-0.11.0/tensorpc/flow/serv/flowapp.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 import inspect
 import io
 from pathlib import Path
 import pickle
 from runpy import run_path
 from typing import Any, Dict, List, Optional
 from tensorpc.core.defs import FileDesp, FileResource
-from tensorpc.flow.coretypes import ScheduleEvent, get_uid
+from tensorpc.flow.coretypes import ScheduleEvent, VscodeTensorpcMessage, get_uid
 from tensorpc.core.tree_id import UniqueTreeId
 
 from tensorpc.flow.flowapp import appctx
 from tensorpc.flow.flowapp.appcore import ALL_OBSERVED_FUNCTIONS, enter_app_conetxt
 from tensorpc.flow.flowapp.components.mui import FlexBox, flex_wrapper
 from tensorpc.flow.flowapp.core import AppEditorEvent, AppEditorFrontendEvent, AppEvent, AppEventType, InitLSPClientEvent, LayoutEvent, NotifyEvent, NotifyType, ScheduleNextForApp, UIEvent, UIExceptionEvent, UISaveStateEvent, UserMessage
 from tensorpc.flow.flowapp.app import App, EditableApp
@@ -38,15 +38,15 @@
 from tensorpc.flow.langserv import close_tmux_lang_server, get_tmux_lang_server_info_may_create
 from ..client import AppLocalMeta, MasterMeta
 from tensorpc import prim
 from tensorpc.flow.serv_names import serv_names
 from tensorpc.core.serviceunit import ServiceEventType
 import traceback
 import time
-import sys 
+import sys
 from urllib import parse
 
 
 class FlowApp:
     """this service must run inside devflow.
     if headless is enabled, all event sent to frontend will be ignored.
     if external_argv is enabled, it will be used as sys.argv and launched
@@ -64,18 +64,18 @@
         self.module_name = module_name
         self.config = config
         self.shutdown_ev = asyncio.Event()
         self.master_meta = MasterMeta()
         self.app_meta = AppLocalMeta()
         process_title = self.master_meta.process_title
         try:
-            import setproctitle
+            import setproctitle  # type: ignore
             setproctitle.setproctitle(process_title)
         except ImportError:
-            pass 
+            pass
         if not headless:
             assert self.master_meta.is_inside_devflow, "this service must run inside devflow"
             # assert self.master_meta.is_http_valid
         self._send_loop_task: Optional[asyncio.Task] = None
         self._need_to_send_env: Optional[AppEvent] = None
         self.shutdown_ev.clear()
         if not headless:
@@ -92,35 +92,37 @@
         else:
             obj = self.dynamic_app_cls.obj_type(**self.config)
         if isinstance(obj, App):
             self.app: App = obj
         elif isinstance(obj, FlexBox):
             # external root
             external_root = obj
-            self.app: App = EditableApp(external_root=external_root, reload_manager=reload_mgr)
+            self.app: App = EditableApp(external_root=external_root,
+                                        reload_manager=reload_mgr)
         else:
             # other object, must declare a tensorpc_flow_layout
             # external_root = flex_wrapper(obj)
-            self.app: App = EditableApp(external_wrapped_obj=obj, reload_manager=reload_mgr)
+            self.app: App = EditableApp(external_wrapped_obj=obj,
+                                        reload_manager=reload_mgr)
             self.app._app_force_use_layout_function()
         self.app._flow_app_comp_core.reload_mgr = reload_mgr
         self.app_su = ServiceUnit(module_name, config)
         self.app_su.init_service(obj)
         self.app._app_dynamic_cls = self.dynamic_app_cls
         self.app._app_service_unit = self.app_su
         self.app._flow_app_is_headless = headless
         self._send_loop_queue: "asyncio.Queue[AppEvent]" = self.app._queue
         # self.app._send_callback = self._send_http_event
         self._send_loop_task = asyncio.create_task(self._send_loop())
         self.lsp_port = self.master_meta.lsp_port
         if self.lsp_port is not None:
-            assert self.master_meta.lsp_fwd_port is not None 
+            assert self.master_meta.lsp_fwd_port is not None
             self.lsp_fwd_port = self.master_meta.lsp_fwd_port
         else:
-            self.lsp_fwd_port = None 
+            self.lsp_fwd_port = None
         self.external_argv = external_argv
         self._external_argv_task: Optional[asyncio.Future] = None
 
     @marker.mark_server_event(event_type=marker.ServiceEventType.Init)
     async def init(self):
         if self.app._force_special_layout_method:
             layout_created = False
@@ -128,37 +130,46 @@
             if special_methods.create_layout is not None:
                 await self.app._app_run_layout_function(
                     decorator_fn=special_methods.create_layout.get_binded_fn())
                 layout_created = True
             if not layout_created:
                 await self.app._app_run_layout_function()
         else:
-            self.app.root._attach(UniqueTreeId.from_parts(["root"]), self.app._flow_app_comp_core)
+            self.app.root._attach(UniqueTreeId.from_parts(["root"]),
+                                  self.app._flow_app_comp_core)
         # print(lay["layout"])
         self.app.app_initialize()
         await self.app.app_initialize_async()
         enable_lsp = self.lsp_port is not None and self.app._flowapp_enable_lsp
-        print(enable_lsp,  self.lsp_port)
+        print(enable_lsp, self.lsp_port)
         if enable_lsp:
             assert self.lsp_port is not None
-            get_tmux_lang_server_info_may_create("pyright", self.master_meta.node_id, self.lsp_port)
+            get_tmux_lang_server_info_may_create("pyright",
+                                                 self.master_meta.node_id,
+                                                 self.lsp_port)
         lay = self.app._get_app_layout()
         self.app._flowapp_is_inited = True
         await self._send_loop_queue.put(
             AppEvent("", {AppEventType.UpdateLayout: LayoutEvent(lay)}))
         # TODO should we just use grpc client to query init state here?
-        init_event: Dict[AppEventType, Any] = {AppEventType.Notify: NotifyEvent(NotifyType.AppStart)}
+        init_event: Dict[AppEventType, Any] = {
+            AppEventType.Notify: NotifyEvent(NotifyType.AppStart)
+        }
         if self.lsp_fwd_port is not None and enable_lsp:
-            init_event[AppEventType.InitLSPClient] = InitLSPClientEvent(self.lsp_fwd_port, self.app._flowapp_internal_lsp_config.get_dict())
-        await self._send_loop_queue.put(
-            AppEvent("", init_event))
+            init_event[AppEventType.InitLSPClient] = InitLSPClientEvent(
+                self.lsp_fwd_port,
+                self.app._flowapp_internal_lsp_config.get_dict())
+        await self._send_loop_queue.put(AppEvent("", init_event))
         if self.external_argv is not None:
             with enter_app_conetxt(self.app):
                 print("??????????????????????????", self.external_argv)
-                self._external_argv_task = asyncio.create_task(appctx.run_in_executor_with_exception_inspect(partial(self._run_app_script, argv=self.external_argv),))
+                self._external_argv_task = asyncio.create_task(
+                    appctx.run_in_executor_with_exception_inspect(
+                        partial(self._run_app_script,
+                                argv=self.external_argv), ))
 
     def _run_app_script(self, argv: List[str]):
         argv_bkp = sys.argv
         sys.argv = argv
         print("???", argv)
         try:
             run_path(argv[0], run_name="__main__")
@@ -205,77 +216,92 @@
             ev = ScheduleEvent(time.time_ns(), res, {})
             appev = ScheduleNextForApp(ev.to_dict())
             await self._send_loop_queue.put(
                 AppEvent(self._uid, {
                     AppEventType.ScheduleNext: appev,
                 }))
 
+    async def handle_vscode_event(self, data: dict):
+        """run event come from vscode, you need to install vscode-tensorpc-bridge extension first,
+        then enable it in machine which run this app.
+        """
+        ev = VscodeTensorpcMessage(
+            type=data["type"],
+            currentUri=data["currentUri"],
+            workspaceUri=data["workspaceUri"],
+            selections=data["selections"] if "selections" in data else None,
+        )
+        await self.app.run_vscode_event(ev)
+
     def get_layout(self, editor_only: bool = False):
         if editor_only:
             res = self.app._get_app_editor_state()
         else:
             res = self.app._get_app_layout()
         if self.app._flowapp_enable_lsp:
             res["lspPort"] = self.lsp_port
-        return res 
+        return res
 
-    async def get_file(self, file_key: str, chunk_size=2 ** 16):
+    async def get_file(self, file_key: str, chunk_size=2**16):
         if file_key in self.app._flowapp_file_resource_handlers:
             url = parse.urlparse(file_key)
             base = url.path
             file_key_qparams = parse.parse_qs(url.query)
             # we only use first value
             if len(file_key_qparams) > 0:
-                file_key_qparams = {k: v[0] for k, v in file_key_qparams.items()}
+                file_key_qparams = {
+                    k: v[0]
+                    for k, v in file_key_qparams.items()
+                }
             else:
                 file_key_qparams = {}
             try:
                 handler = self.app._flowapp_file_resource_handlers[base]
                 res = handler(**file_key_qparams)
                 if inspect.iscoroutine(res):
-                    res = await res 
+                    res = await res
                 assert isinstance(res, (str, bytes, FileResource))
                 if isinstance(res, (str, bytes)):
                     if isinstance(res, str):
                         res = res.encode()
                     bio = io.BytesIO(res)
                     chunk = bio.read(chunk_size)
                     yield FileDesp(base)
                     while chunk:
                         yield chunk
                         chunk = bio.read(chunk_size)
                 else:
-                    fname = res.name 
+                    fname = res.name
                     if res.chunk_size is not None:
                         assert res.chunk_size > 1024
                         chunk_size = res.chunk_size
                     if res.path is not None:
                         yield FileDesp(Path(res.path).name, res.content_type)
                         with open(res.path, "rb") as f:
                             chunk = f.read(chunk_size)
                             while chunk:
                                 yield chunk
                                 chunk = f.read(chunk_size)
                     elif res.content is not None:
-                        content = res.content 
+                        content = res.content
                         if isinstance(content, str):
                             content = content.encode()
                         bio = io.BytesIO(content)
                         chunk = bio.read(chunk_size)
                         yield FileDesp(fname, res.content_type)
                         while chunk:
                             yield chunk
                             chunk = bio.read(chunk_size)
                     else:
                         raise NotImplementedError
             except:
                 traceback.print_exc()
                 raise
         else:
-            raise NotImplementedError 
+            raise NotImplementedError
 
     async def _http_remote_call(self, key: str, *args, **kwargs):
         return await http_remote_call(prim.get_http_client_session(),
                                       self.master_meta.http_url, key, *args,
                                       **kwargs)
 
     async def _send_http_event(self, ev: AppEvent):
@@ -296,15 +322,15 @@
                                           ev.to_dict())
         else:
             return await robj.remote_call(serv_names.FLOW_PUT_APP_EVENT,
                                           ev.to_dict())
 
     async def _send_grpc_event_large(self, ev: AppEvent,
                                      robj: tensorpc.AsyncRemoteManager):
-        # import rich 
+        # import rich
         # rich.print(ev.to_dict())
         if self.master_meta.is_worker:
             return await robj.chunked_remote_call(
                 serv_names.FLOWWORKER_PUT_APP_EVENT, self.master_meta.graph_id,
                 ev.to_dict())
         else:
             return await robj.chunked_remote_call(
@@ -340,15 +366,16 @@
                     break
                 ev: AppEvent = send_task.result()
                 if ev.is_loopback:
                     for k, v in ev.type_to_event.items():
                         if k == AppEventType.UIEvent:
                             assert isinstance(v, UIEvent)
                             await self.app._handle_event_with_ctx(v)
-                    send_task = asyncio.create_task(self._send_loop_queue.get())
+                    send_task = asyncio.create_task(
+                        self._send_loop_queue.get())
                     wait_tasks: List[asyncio.Task] = [shut_task, send_task]
                     continue
                 ts = time.time()
                 # assign uid here.
                 ev.uid = self._uid
                 send_task = asyncio.create_task(self._send_loop_queue.get())
                 wait_tasks: List[asyncio.Task] = [shut_task, send_task]
@@ -401,15 +428,19 @@
         # we can't close language server here
         # because we must wait for frontend shutdown client.
         # close_tmux_lang_server(self.master_meta.node_id)
         try:
             grpc_url = self.master_meta.grpc_url
             uiev = UISaveStateEvent(self.app._get_simple_app_state())
             editorev = self.app.set_editor_value_event("")
-            ev = AppEvent(self._uid, {AppEventType.UISaveStateEvent: uiev, AppEventType.AppEditor: editorev})
+            ev = AppEvent(
+                self._uid, {
+                    AppEventType.UISaveStateEvent: uiev,
+                    AppEventType.AppEditor: editorev
+                })
             # TODO remove this dump
             # check user error, user can't store invalid
             # object that exists after reload module.
             pickle.dumps(ev)
             async with tensorpc.AsyncRemoteManager(grpc_url) as robj:
                 await self._send_grpc_event_large(ev, robj)
```

### Comparing `tensorpc-0.10.7/tensorpc/flow/serv/worker.py` & `tensorpc-0.11.0/tensorpc/flow/serv/worker.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/flow/serv_names.py` & `tensorpc-0.11.0/tensorpc/flow/serv_names.py`

 * *Files 2% similar despite different names*

```diff
@@ -168,30 +168,29 @@
 
     @property
     def FLOWWORKER_APP_GET_LAYOUT(self):
         from tensorpc.flow.serv.worker import FlowWorker
         return get_service_key_by_type(FlowWorker,
                                        FlowWorker.get_layout.__name__)
 
-
     @property
     def APP_RUN_SINGLE_EVENT(self):
         from tensorpc.flow.serv.flowapp import FlowApp
         return get_service_key_by_type(FlowApp,
                                        FlowApp.run_single_event.__name__)
 
     @property
     def APP_GET_LAYOUT(self):
         from tensorpc.flow.serv.flowapp import FlowApp
         return get_service_key_by_type(FlowApp, FlowApp.get_layout.__name__)
+
     @property
     def APP_GET_FILE(self):
         from tensorpc.flow.serv.flowapp import FlowApp
-        return get_service_key_by_type(FlowApp,
-                                       FlowApp.get_file.__name__)
+        return get_service_key_by_type(FlowApp, FlowApp.get_file.__name__)
 
     @property
     def FLOW_DATA_SAVE(self):
         from tensorpc.flow.serv.core import Flow
         return get_service_key_by_type(Flow,
                                        Flow.save_data_to_storage.__name__)
 
@@ -200,35 +199,34 @@
         from tensorpc.flow.serv.core import Flow
         return get_service_key_by_type(Flow,
                                        Flow.read_data_from_storage.__name__)
 
     @property
     def FLOW_DATA_LIST_ITEM_METAS(self):
         from tensorpc.flow.serv.core import Flow
-        return get_service_key_by_type(Flow,
-                                       Flow.query_data_attrs.__name__)
+        return get_service_key_by_type(Flow, Flow.query_data_attrs.__name__)
 
     @property
     def FLOW_DATA_QUERY_DATA_NODE_IDS(self):
         from tensorpc.flow.serv.core import Flow
         return get_service_key_by_type(Flow,
                                        Flow.query_all_data_node_ids.__name__)
-    
+
     @property
     def FLOW_DATA_DELETE_ITEM(self):
         from tensorpc.flow.serv.core import Flow
         return get_service_key_by_type(Flow,
                                        Flow.delete_datastorage_data.__name__)
-    
+
     @property
     def FLOW_DATA_RENAME_ITEM(self):
         from tensorpc.flow.serv.core import Flow
         return get_service_key_by_type(Flow,
                                        Flow.rename_datastorage_data.__name__)
 
     @property
     def FLOW_GET_SSH_NODE_DATA(self):
         from tensorpc.flow.serv.core import Flow
-        return get_service_key_by_type(Flow,
-                                       Flow.get_ssh_node_data.__name__)
+        return get_service_key_by_type(Flow, Flow.get_ssh_node_data.__name__)
+
 
 serv_names = _ServiceNames()
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/arraybuf_pb2.py` & `tensorpc-0.11.0/tensorpc/protos/arraybuf_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,28 +6,27 @@
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0e\x61rraybuf.proto\x12\x0ftensorpc.protos\"\xe4\x02\n\x05\x64type\x12-\n\x04type\x18\x01 \x01(\x0e\x32\x1f.tensorpc.protos.dtype.DataType\x12\x34\n\nbyte_order\x18\x02 \x01(\x0e\x32 .tensorpc.protos.dtype.ByteOrder\"@\n\tByteOrder\x12\x10\n\x0clittleEndian\x10\x00\x12\r\n\tbigEndian\x10\x01\x12\n\n\x06native\x10\x02\x12\x06\n\x02na\x10\x03\"\xb3\x01\n\x08\x44\x61taType\x12\x0b\n\x07\x66loat64\x10\x00\x12\x0b\n\x07\x66loat32\x10\x01\x12\x0b\n\x07\x66loat16\x10\x02\x12\n\n\x06uint64\x10\x03\x12\n\n\x06uint32\x10\x04\x12\n\n\x06uint16\x10\x05\x12\t\n\x05uint8\x10\x06\x12\t\n\x05int64\x10\x07\x12\t\n\x05int32\x10\x08\x12\t\n\x05int16\x10\t\x12\x08\n\x04int8\x10\n\x12\t\n\x05\x62ool_\x10\x0b\x12\x0f\n\x0b\x43ustomBytes\x10\x0c\x12\n\n\x06\x42\x61se64\x10\r\"M\n\x07ndarray\x12\r\n\x05shape\x18\x01 \x03(\x03\x12%\n\x05\x64type\x18\x02 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"C\n\tarrayjson\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\tb\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x0e\x61rraybuf.proto\x12\x0ftensorpc.protos\"\xe4\x02\n\x05\x64type\x12-\n\x04type\x18\x01 \x01(\x0e\x32\x1f.tensorpc.protos.dtype.DataType\x12\x34\n\nbyte_order\x18\x02 \x01(\x0e\x32 .tensorpc.protos.dtype.ByteOrder\"@\n\tByteOrder\x12\x10\n\x0clittleEndian\x10\x00\x12\r\n\tbigEndian\x10\x01\x12\n\n\x06native\x10\x02\x12\x06\n\x02na\x10\x03\"\xb3\x01\n\x08\x44\x61taType\x12\x0b\n\x07\x66loat64\x10\x00\x12\x0b\n\x07\x66loat32\x10\x01\x12\x0b\n\x07\x66loat16\x10\x02\x12\n\n\x06uint64\x10\x03\x12\n\n\x06uint32\x10\x04\x12\n\n\x06uint16\x10\x05\x12\t\n\x05uint8\x10\x06\x12\t\n\x05int64\x10\x07\x12\t\n\x05int32\x10\x08\x12\t\n\x05int16\x10\t\x12\x08\n\x04int8\x10\n\x12\t\n\x05\x62ool_\x10\x0b\x12\x0f\n\x0b\x43ustomBytes\x10\x0c\x12\n\n\x06\x42\x61se64\x10\r\"M\n\x07ndarray\x12\r\n\x05shape\x18\x01 \x03(\x03\x12%\n\x05\x64type\x18\x02 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"C\n\tarrayjson\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\tb\x06proto3'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'arraybuf_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
-  DESCRIPTOR._options = None
-  _DTYPE._serialized_start=36
-  _DTYPE._serialized_end=392
-  _DTYPE_BYTEORDER._serialized_start=146
-  _DTYPE_BYTEORDER._serialized_end=210
-  _DTYPE_DATATYPE._serialized_start=213
-  _DTYPE_DATATYPE._serialized_end=392
-  _NDARRAY._serialized_start=394
-  _NDARRAY._serialized_end=471
-  _ARRAYJSON._serialized_start=473
-  _ARRAYJSON._serialized_end=540
+    DESCRIPTOR._options = None
+    _DTYPE._serialized_start = 36
+    _DTYPE._serialized_end = 392
+    _DTYPE_BYTEORDER._serialized_start = 146
+    _DTYPE_BYTEORDER._serialized_end = 210
+    _DTYPE_DATATYPE._serialized_start = 213
+    _DTYPE_DATATYPE._serialized_end = 392
+    _NDARRAY._serialized_start = 394
+    _NDARRAY._serialized_end = 471
+    _ARRAYJSON._serialized_start = 473
+    _ARRAYJSON._serialized_end = 540
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/arraybuf_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos/arraybuf_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/protos/remote_object_pb2.py` & `tensorpc-0.11.0/tensorpc/protos/remote_object_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,22 +6,23 @@
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
 from . import arraybuf_pb2 as arraybuf__pb2
 from . import rpc_message_pb2 as rpc__message__pb2
 
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13remote_object.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\x1a\x11rpc_message.proto2\xc1\t\n\x0cRemoteObject\x12T\n\nRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x12[\n\x0fRemoteGenerator\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x30\x01\x12`\n\x0eRemoteJsonCall\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x12g\n\x13RemoteJsonGenerator\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x30\x01\x12^\n\x10RemoteStreamCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12Z\n\x0eServerShutdown\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12W\n\x0bHealthCheck\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12U\n\x0fQueryServerMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12V\n\x10QueryServiceMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12_\n\x11\x43hunkedRemoteCall\x12!.tensorpc.protos.RemoteCallStream\x1a!.tensorpc.protos.RemoteCallStream\"\x00(\x01\x30\x01\x12\x62\n\x16\x43lientStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x12`\n\x12\x42iStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12H\n\x08SayHello\x12\x1d.tensorpc.protos.HelloRequest\x1a\x1b.tensorpc.protos.HelloReply\"\x00\x62\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x13remote_object.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\x1a\x11rpc_message.proto2\xc1\t\n\x0cRemoteObject\x12T\n\nRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x12[\n\x0fRemoteGenerator\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x30\x01\x12`\n\x0eRemoteJsonCall\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x12g\n\x13RemoteJsonGenerator\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x30\x01\x12^\n\x10RemoteStreamCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12Z\n\x0eServerShutdown\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12W\n\x0bHealthCheck\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12U\n\x0fQueryServerMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12V\n\x10QueryServiceMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12_\n\x11\x43hunkedRemoteCall\x12!.tensorpc.protos.RemoteCallStream\x1a!.tensorpc.protos.RemoteCallStream\"\x00(\x01\x30\x01\x12\x62\n\x16\x43lientStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x12`\n\x12\x42iStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12H\n\x08SayHello\x12\x1d.tensorpc.protos.HelloRequest\x1a\x1b.tensorpc.protos.HelloReply\"\x00\x62\x06proto3'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'remote_object_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'remote_object_pb2',
+                                        globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
-  DESCRIPTOR._options = None
-  _REMOTEOBJECT._serialized_start=76
-  _REMOTEOBJECT._serialized_end=1293
+    DESCRIPTOR._options = None
+    _REMOTEOBJECT._serialized_start = 76
+    _REMOTEOBJECT._serialized_end = 1293
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/remote_object_pb2_grpc.py` & `tensorpc-0.11.0/tensorpc/protos/remote_object_pb2_grpc.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,78 +11,96 @@
     def __init__(self, channel):
         """Constructor.
 
         Args:
             channel: A grpc.Channel.
         """
         self.RemoteCall = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/RemoteCall',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/RemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
         self.RemoteGenerator = channel.unary_stream(
-                '/tensorpc.protos.RemoteObject/RemoteGenerator',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/RemoteGenerator',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
         self.RemoteJsonCall = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/RemoteJsonCall',
-                request_serializer=rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteJsonCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/RemoteJsonCall',
+            request_serializer=rpc__message__pb2.RemoteJsonCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteJsonCallReply.
+            FromString,
+        )
         self.RemoteJsonGenerator = channel.unary_stream(
-                '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
-                request_serializer=rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteJsonCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
+            request_serializer=rpc__message__pb2.RemoteJsonCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteJsonCallReply.
+            FromString,
+        )
         self.RemoteStreamCall = channel.stream_stream(
-                '/tensorpc.protos.RemoteObject/RemoteStreamCall',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/RemoteStreamCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
         self.ServerShutdown = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/ServerShutdown',
-                request_serializer=rpc__message__pb2.HealthCheckRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.HealthCheckReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/ServerShutdown',
+            request_serializer=rpc__message__pb2.HealthCheckRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HealthCheckReply.
+            FromString,
+        )
         self.HealthCheck = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/HealthCheck',
-                request_serializer=rpc__message__pb2.HealthCheckRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.HealthCheckReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/HealthCheck',
+            request_serializer=rpc__message__pb2.HealthCheckRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HealthCheckReply.
+            FromString,
+        )
         self.QueryServerMeta = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/QueryServerMeta',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.SimpleReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/QueryServerMeta',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.SimpleReply.FromString,
+        )
         self.QueryServiceMeta = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/QueryServiceMeta',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.SimpleReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/QueryServiceMeta',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.SimpleReply.FromString,
+        )
         self.ChunkedRemoteCall = channel.stream_stream(
-                '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
-                request_serializer=rpc__message__pb2.RemoteCallStream.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallStream.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallStream.
+            FromString,
+        )
         self.ClientStreamRemoteCall = channel.stream_unary(
-                '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
         self.BiStreamRemoteCall = channel.stream_stream(
-                '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
-                request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
         self.SayHello = channel.unary_unary(
-                '/tensorpc.protos.RemoteObject/SayHello',
-                request_serializer=rpc__message__pb2.HelloRequest.SerializeToString,
-                response_deserializer=rpc__message__pb2.HelloReply.FromString,
-                )
+            '/tensorpc.protos.RemoteObject/SayHello',
+            request_serializer=rpc__message__pb2.HelloRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HelloReply.FromString,
+        )
 
 
 class RemoteObjectServicer(object):
     """Missing associated documentation comment in .proto file."""
 
     def RemoteCall(self, request, context):
         """Missing associated documentation comment in .proto file."""
@@ -162,302 +180,356 @@
         context.set_code(grpc.StatusCode.UNIMPLEMENTED)
         context.set_details('Method not implemented!')
         raise NotImplementedError('Method not implemented!')
 
 
 def add_RemoteObjectServicer_to_server(servicer, server):
     rpc_method_handlers = {
-            'RemoteCall': grpc.unary_unary_rpc_method_handler(
-                    servicer.RemoteCall,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-            ),
-            'RemoteGenerator': grpc.unary_stream_rpc_method_handler(
-                    servicer.RemoteGenerator,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-            ),
-            'RemoteJsonCall': grpc.unary_unary_rpc_method_handler(
-                    servicer.RemoteJsonCall,
-                    request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteJsonCallReply.SerializeToString,
-            ),
-            'RemoteJsonGenerator': grpc.unary_stream_rpc_method_handler(
-                    servicer.RemoteJsonGenerator,
-                    request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteJsonCallReply.SerializeToString,
-            ),
-            'RemoteStreamCall': grpc.stream_stream_rpc_method_handler(
-                    servicer.RemoteStreamCall,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-            ),
-            'ServerShutdown': grpc.unary_unary_rpc_method_handler(
-                    servicer.ServerShutdown,
-                    request_deserializer=rpc__message__pb2.HealthCheckRequest.FromString,
-                    response_serializer=rpc__message__pb2.HealthCheckReply.SerializeToString,
-            ),
-            'HealthCheck': grpc.unary_unary_rpc_method_handler(
-                    servicer.HealthCheck,
-                    request_deserializer=rpc__message__pb2.HealthCheckRequest.FromString,
-                    response_serializer=rpc__message__pb2.HealthCheckReply.SerializeToString,
-            ),
-            'QueryServerMeta': grpc.unary_unary_rpc_method_handler(
-                    servicer.QueryServerMeta,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.SimpleReply.SerializeToString,
-            ),
-            'QueryServiceMeta': grpc.unary_unary_rpc_method_handler(
-                    servicer.QueryServiceMeta,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.SimpleReply.SerializeToString,
-            ),
-            'ChunkedRemoteCall': grpc.stream_stream_rpc_method_handler(
-                    servicer.ChunkedRemoteCall,
-                    request_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallStream.SerializeToString,
-            ),
-            'ClientStreamRemoteCall': grpc.stream_unary_rpc_method_handler(
-                    servicer.ClientStreamRemoteCall,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-            ),
-            'BiStreamRemoteCall': grpc.stream_stream_rpc_method_handler(
-                    servicer.BiStreamRemoteCall,
-                    request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-                    response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-            ),
-            'SayHello': grpc.unary_unary_rpc_method_handler(
-                    servicer.SayHello,
-                    request_deserializer=rpc__message__pb2.HelloRequest.FromString,
-                    response_serializer=rpc__message__pb2.HelloReply.SerializeToString,
-            ),
+        'RemoteCall':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.RemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'RemoteGenerator':
+        grpc.unary_stream_rpc_method_handler(
+            servicer.RemoteGenerator,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'RemoteJsonCall':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.RemoteJsonCall,
+            request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteJsonCallReply.
+            SerializeToString,
+        ),
+        'RemoteJsonGenerator':
+        grpc.unary_stream_rpc_method_handler(
+            servicer.RemoteJsonGenerator,
+            request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteJsonCallReply.
+            SerializeToString,
+        ),
+        'RemoteStreamCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.RemoteStreamCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'ServerShutdown':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.ServerShutdown,
+            request_deserializer=rpc__message__pb2.HealthCheckRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.HealthCheckReply.
+            SerializeToString,
+        ),
+        'HealthCheck':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.HealthCheck,
+            request_deserializer=rpc__message__pb2.HealthCheckRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.HealthCheckReply.
+            SerializeToString,
+        ),
+        'QueryServerMeta':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.QueryServerMeta,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.SimpleReply.
+            SerializeToString,
+        ),
+        'QueryServiceMeta':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.QueryServiceMeta,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.SimpleReply.
+            SerializeToString,
+        ),
+        'ChunkedRemoteCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.ChunkedRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
+            response_serializer=rpc__message__pb2.RemoteCallStream.
+            SerializeToString,
+        ),
+        'ClientStreamRemoteCall':
+        grpc.stream_unary_rpc_method_handler(
+            servicer.ClientStreamRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'BiStreamRemoteCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.BiStreamRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'SayHello':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.SayHello,
+            request_deserializer=rpc__message__pb2.HelloRequest.FromString,
+            response_serializer=rpc__message__pb2.HelloReply.SerializeToString,
+        ),
     }
     generic_handler = grpc.method_handlers_generic_handler(
-            'tensorpc.protos.RemoteObject', rpc_method_handlers)
-    server.add_generic_rpc_handlers((generic_handler,))
+        'tensorpc.protos.RemoteObject', rpc_method_handlers)
+    server.add_generic_rpc_handlers((generic_handler, ))
 
 
- # This class is part of an EXPERIMENTAL API.
+# This class is part of an EXPERIMENTAL API.
 class RemoteObject(object):
     """Missing associated documentation comment in .proto file."""
 
     @staticmethod
     def RemoteCall(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/RemoteCall',
+                   target,
+                   options=(),
+                   channel_credentials=None,
+                   call_credentials=None,
+                   insecure=False,
+                   compression=None,
+                   wait_for_ready=None,
+                   timeout=None,
+                   metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/RemoteCall',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def RemoteGenerator(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_stream(request, target, '/tensorpc.protos.RemoteObject/RemoteGenerator',
+                        target,
+                        options=(),
+                        channel_credentials=None,
+                        call_credentials=None,
+                        insecure=False,
+                        compression=None,
+                        wait_for_ready=None,
+                        timeout=None,
+                        metadata=None):
+        return grpc.experimental.unary_stream(
+            request, target, '/tensorpc.protos.RemoteObject/RemoteGenerator',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def RemoteJsonCall(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/RemoteJsonCall',
+                       target,
+                       options=(),
+                       channel_credentials=None,
+                       call_credentials=None,
+                       insecure=False,
+                       compression=None,
+                       wait_for_ready=None,
+                       timeout=None,
+                       metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/RemoteJsonCall',
             rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteJsonCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteJsonCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def RemoteJsonGenerator(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_stream(request, target, '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
+                            target,
+                            options=(),
+                            channel_credentials=None,
+                            call_credentials=None,
+                            insecure=False,
+                            compression=None,
+                            wait_for_ready=None,
+                            timeout=None,
+                            metadata=None):
+        return grpc.experimental.unary_stream(
+            request, target,
+            '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
             rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteJsonCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteJsonCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def RemoteStreamCall(request_iterator,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.stream_stream(request_iterator, target, '/tensorpc.protos.RemoteObject/RemoteStreamCall',
+                         target,
+                         options=(),
+                         channel_credentials=None,
+                         call_credentials=None,
+                         insecure=False,
+                         compression=None,
+                         wait_for_ready=None,
+                         timeout=None,
+                         metadata=None):
+        return grpc.experimental.stream_stream(
+            request_iterator, target,
+            '/tensorpc.protos.RemoteObject/RemoteStreamCall',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def ServerShutdown(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/ServerShutdown',
+                       target,
+                       options=(),
+                       channel_credentials=None,
+                       call_credentials=None,
+                       insecure=False,
+                       compression=None,
+                       wait_for_ready=None,
+                       timeout=None,
+                       metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/ServerShutdown',
             rpc__message__pb2.HealthCheckRequest.SerializeToString,
-            rpc__message__pb2.HealthCheckReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.HealthCheckReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def HealthCheck(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/HealthCheck',
+                    target,
+                    options=(),
+                    channel_credentials=None,
+                    call_credentials=None,
+                    insecure=False,
+                    compression=None,
+                    wait_for_ready=None,
+                    timeout=None,
+                    metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/HealthCheck',
             rpc__message__pb2.HealthCheckRequest.SerializeToString,
-            rpc__message__pb2.HealthCheckReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.HealthCheckReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def QueryServerMeta(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/QueryServerMeta',
+                        target,
+                        options=(),
+                        channel_credentials=None,
+                        call_credentials=None,
+                        insecure=False,
+                        compression=None,
+                        wait_for_ready=None,
+                        timeout=None,
+                        metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/QueryServerMeta',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.SimpleReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.SimpleReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def QueryServiceMeta(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/QueryServiceMeta',
+                         target,
+                         options=(),
+                         channel_credentials=None,
+                         call_credentials=None,
+                         insecure=False,
+                         compression=None,
+                         wait_for_ready=None,
+                         timeout=None,
+                         metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/QueryServiceMeta',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.SimpleReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.SimpleReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def ChunkedRemoteCall(request_iterator,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.stream_stream(request_iterator, target, '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
+                          target,
+                          options=(),
+                          channel_credentials=None,
+                          call_credentials=None,
+                          insecure=False,
+                          compression=None,
+                          wait_for_ready=None,
+                          timeout=None,
+                          metadata=None):
+        return grpc.experimental.stream_stream(
+            request_iterator, target,
+            '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
             rpc__message__pb2.RemoteCallStream.SerializeToString,
-            rpc__message__pb2.RemoteCallStream.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallStream.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def ClientStreamRemoteCall(request_iterator,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.stream_unary(request_iterator, target, '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
+                               target,
+                               options=(),
+                               channel_credentials=None,
+                               call_credentials=None,
+                               insecure=False,
+                               compression=None,
+                               wait_for_ready=None,
+                               timeout=None,
+                               metadata=None):
+        return grpc.experimental.stream_unary(
+            request_iterator, target,
+            '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def BiStreamRemoteCall(request_iterator,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.stream_stream(request_iterator, target, '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
+                           target,
+                           options=(),
+                           channel_credentials=None,
+                           call_credentials=None,
+                           insecure=False,
+                           compression=None,
+                           wait_for_ready=None,
+                           timeout=None,
+                           metadata=None):
+        return grpc.experimental.stream_stream(
+            request_iterator, target,
+            '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
             rpc__message__pb2.RemoteCallRequest.SerializeToString,
-            rpc__message__pb2.RemoteCallReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.RemoteCallReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
 
     @staticmethod
     def SayHello(request,
-            target,
-            options=(),
-            channel_credentials=None,
-            call_credentials=None,
-            insecure=False,
-            compression=None,
-            wait_for_ready=None,
-            timeout=None,
-            metadata=None):
-        return grpc.experimental.unary_unary(request, target, '/tensorpc.protos.RemoteObject/SayHello',
+                 target,
+                 options=(),
+                 channel_credentials=None,
+                 call_credentials=None,
+                 insecure=False,
+                 compression=None,
+                 wait_for_ready=None,
+                 timeout=None,
+                 metadata=None):
+        return grpc.experimental.unary_unary(
+            request, target, '/tensorpc.protos.RemoteObject/SayHello',
             rpc__message__pb2.HelloRequest.SerializeToString,
-            rpc__message__pb2.HelloReply.FromString,
-            options, channel_credentials,
-            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)
+            rpc__message__pb2.HelloReply.FromString, options,
+            channel_credentials, insecure, call_credentials, compression,
+            wait_for_ready, timeout, metadata)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/rpc_message_pb2.py` & `tensorpc-0.11.0/tensorpc/protos/rpc_message_pb2.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,41 +6,42 @@
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
 from . import arraybuf_pb2 as arraybuf__pb2
 
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x11rpc_message.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\".\n\x0bSimpleReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12\x11\n\texception\x18\x02 \x01(\t\"\x85\x01\n\x11RemoteCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x10\n\x08\x63\x61llback\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\"\x85\x01\n\x15RemoteJsonCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\r\n\x05\x66lags\x18\x02 \x01(\x03\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\t\x12\x13\n\x0bservice_key\x18\x04 \x01(\t\x12\x10\n\x08\x63\x61llback\x18\x05 \x01(\t\"o\n\x0fRemoteCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"o\n\x13RemoteJsonCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\t\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"\xd9\x01\n\x10RemoteCallStream\x12\x11\n\tnum_chunk\x18\x01 \x01(\x05\x12\x10\n\x08\x63hunk_id\x18\x02 \x01(\x05\x12\x10\n\x08num_args\x18\x03 \x01(\x05\x12\x0e\n\x06\x61rg_id\x18\x04 \x01(\x05\x12\r\n\x05\x66lags\x18\x05 \x01(\x03\x12%\n\x05\x64type\x18\x06 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x14\n\x0c\x63hunked_data\x18\x07 \x01(\x0c\x12\x10\n\x08\x66unc_key\x18\x08 \x01(\t\x12\r\n\x05shape\x18\t \x03(\x03\x12\x11\n\texception\x18\n \x01(\t\"%\n\x12HealthCheckRequest\x12\x0f\n\x07service\x18\x01 \x01(\t\" \n\x10HealthCheckReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1c\n\x0cHelloRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1a\n\nHelloReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t*\xa4\x01\n\x0c\x45ncodeMethod\x12\x0b\n\x07Unknown\x10\x00\x12\x08\n\x04Json\x10\x01\x12\n\n\x06Pickle\x10\x02\x12\x0f\n\x0bMessagePack\x10\x03\x12\r\n\tJsonArray\x10\x10\x12\x0f\n\x0bPickleArray\x10 \x12\x14\n\x10MessagePackArray\x10\x30\x12\x0f\n\x0bNoArrayMask\x10\x0f\x12\x0e\n\tArrayMask\x10\xf0\x01\x12\t\n\x04Mask\x10\xff\x01\x62\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x11rpc_message.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\".\n\x0bSimpleReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12\x11\n\texception\x18\x02 \x01(\t\"\x85\x01\n\x11RemoteCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x10\n\x08\x63\x61llback\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\"\x85\x01\n\x15RemoteJsonCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\r\n\x05\x66lags\x18\x02 \x01(\x03\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\t\x12\x13\n\x0bservice_key\x18\x04 \x01(\t\x12\x10\n\x08\x63\x61llback\x18\x05 \x01(\t\"o\n\x0fRemoteCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"o\n\x13RemoteJsonCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\t\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"\xd9\x01\n\x10RemoteCallStream\x12\x11\n\tnum_chunk\x18\x01 \x01(\x05\x12\x10\n\x08\x63hunk_id\x18\x02 \x01(\x05\x12\x10\n\x08num_args\x18\x03 \x01(\x05\x12\x0e\n\x06\x61rg_id\x18\x04 \x01(\x05\x12\r\n\x05\x66lags\x18\x05 \x01(\x03\x12%\n\x05\x64type\x18\x06 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x14\n\x0c\x63hunked_data\x18\x07 \x01(\x0c\x12\x10\n\x08\x66unc_key\x18\x08 \x01(\t\x12\r\n\x05shape\x18\t \x03(\x03\x12\x11\n\texception\x18\n \x01(\t\"%\n\x12HealthCheckRequest\x12\x0f\n\x07service\x18\x01 \x01(\t\" \n\x10HealthCheckReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1c\n\x0cHelloRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1a\n\nHelloReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t*\xa4\x01\n\x0c\x45ncodeMethod\x12\x0b\n\x07Unknown\x10\x00\x12\x08\n\x04Json\x10\x01\x12\n\n\x06Pickle\x10\x02\x12\x0f\n\x0bMessagePack\x10\x03\x12\r\n\tJsonArray\x10\x10\x12\x0f\n\x0bPickleArray\x10 \x12\x14\n\x10MessagePackArray\x10\x30\x12\x0f\n\x0bNoArrayMask\x10\x0f\x12\x0e\n\tArrayMask\x10\xf0\x01\x12\t\n\x04Mask\x10\xff\x01\x62\x06proto3'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'rpc_message_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'rpc_message_pb2',
+                                        globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
-  DESCRIPTOR._options = None
-  _ENCODEMETHOD._serialized_start=952
-  _ENCODEMETHOD._serialized_end=1116
-  _SIMPLEREPLY._serialized_start=54
-  _SIMPLEREPLY._serialized_end=100
-  _REMOTECALLREQUEST._serialized_start=103
-  _REMOTECALLREQUEST._serialized_end=236
-  _REMOTEJSONCALLREQUEST._serialized_start=239
-  _REMOTEJSONCALLREQUEST._serialized_end=372
-  _REMOTECALLREPLY._serialized_start=374
-  _REMOTECALLREPLY._serialized_end=485
-  _REMOTEJSONCALLREPLY._serialized_start=487
-  _REMOTEJSONCALLREPLY._serialized_end=598
-  _REMOTECALLSTREAM._serialized_start=601
-  _REMOTECALLSTREAM._serialized_end=818
-  _HEALTHCHECKREQUEST._serialized_start=820
-  _HEALTHCHECKREQUEST._serialized_end=857
-  _HEALTHCHECKREPLY._serialized_start=859
-  _HEALTHCHECKREPLY._serialized_end=891
-  _HELLOREQUEST._serialized_start=893
-  _HELLOREQUEST._serialized_end=921
-  _HELLOREPLY._serialized_start=923
-  _HELLOREPLY._serialized_end=949
+    DESCRIPTOR._options = None
+    _ENCODEMETHOD._serialized_start = 952
+    _ENCODEMETHOD._serialized_end = 1116
+    _SIMPLEREPLY._serialized_start = 54
+    _SIMPLEREPLY._serialized_end = 100
+    _REMOTECALLREQUEST._serialized_start = 103
+    _REMOTECALLREQUEST._serialized_end = 236
+    _REMOTEJSONCALLREQUEST._serialized_start = 239
+    _REMOTEJSONCALLREQUEST._serialized_end = 372
+    _REMOTECALLREPLY._serialized_start = 374
+    _REMOTECALLREPLY._serialized_end = 485
+    _REMOTEJSONCALLREPLY._serialized_start = 487
+    _REMOTEJSONCALLREPLY._serialized_end = 598
+    _REMOTECALLSTREAM._serialized_start = 601
+    _REMOTECALLSTREAM._serialized_end = 818
+    _HEALTHCHECKREQUEST._serialized_start = 820
+    _HEALTHCHECKREQUEST._serialized_end = 857
+    _HEALTHCHECKREPLY._serialized_start = 859
+    _HEALTHCHECKREPLY._serialized_end = 891
+    _HELLOREQUEST._serialized_start = 893
+    _HELLOREQUEST._serialized_end = 921
+    _HELLOREPLY._serialized_start = 923
+    _HELLOREPLY._serialized_end = 949
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/rpc_message_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos/rpc_message_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/protos/wsdef_pb2.py` & `tensorpc-0.11.0/tensorpc/protos/wsdef_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,20 +6,19 @@
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0bwsdef.proto\x12\x0f\x64istflow.protos\"y\n\x06Header\x12\x12\n\nservice_id\x18\x01 \x01(\x07\x12\x13\n\x0b\x63hunk_index\x18\x02 \x01(\x07\x12\x0e\n\x06rpc_id\x18\x03 \x01(\x06\x12\x0c\n\x04\x64\x61ta\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\x12\x13\n\x0b\x64ynamic_key\x18\x06 \x01(\tb\x06proto3')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x0bwsdef.proto\x12\x0f\x64istflow.protos\"y\n\x06Header\x12\x12\n\nservice_id\x18\x01 \x01(\x07\x12\x13\n\x0b\x63hunk_index\x18\x02 \x01(\x07\x12\x0e\n\x06rpc_id\x18\x03 \x01(\x06\x12\x0c\n\x04\x64\x61ta\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\x12\x13\n\x0b\x64ynamic_key\x18\x06 \x01(\tb\x06proto3'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'wsdef_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
-  DESCRIPTOR._options = None
-  _HEADER._serialized_start=32
-  _HEADER._serialized_end=153
+    DESCRIPTOR._options = None
+    _HEADER._serialized_start = 32
+    _HEADER._serialized_end = 153
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos/wsdef_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos/wsdef_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/arraybuf_pb2.py` & `tensorpc-0.11.0/tensorpc/protos_legacy/arraybuf_pb2.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,277 +6,353 @@
 from google.protobuf import message as _message
 from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
-
-
 DESCRIPTOR = _descriptor.FileDescriptor(
-  name='arraybuf.proto',
-  package='tensorpc.protos',
-  syntax='proto3',
-  serialized_options=None,
-  serialized_pb=b'\n\x0e\x61rraybuf.proto\x12\x0ftensorpc.protos\"\xe4\x02\n\x05\x64type\x12-\n\x04type\x18\x01 \x01(\x0e\x32\x1f.tensorpc.protos.dtype.DataType\x12\x34\n\nbyte_order\x18\x02 \x01(\x0e\x32 .tensorpc.protos.dtype.ByteOrder\"@\n\tByteOrder\x12\x10\n\x0clittleEndian\x10\x00\x12\r\n\tbigEndian\x10\x01\x12\n\n\x06native\x10\x02\x12\x06\n\x02na\x10\x03\"\xb3\x01\n\x08\x44\x61taType\x12\x0b\n\x07\x66loat64\x10\x00\x12\x0b\n\x07\x66loat32\x10\x01\x12\x0b\n\x07\x66loat16\x10\x02\x12\n\n\x06uint64\x10\x03\x12\n\n\x06uint32\x10\x04\x12\n\n\x06uint16\x10\x05\x12\t\n\x05uint8\x10\x06\x12\t\n\x05int64\x10\x07\x12\t\n\x05int32\x10\x08\x12\t\n\x05int16\x10\t\x12\x08\n\x04int8\x10\n\x12\t\n\x05\x62ool_\x10\x0b\x12\x0f\n\x0b\x43ustomBytes\x10\x0c\x12\n\n\x06\x42\x61se64\x10\r\"M\n\x07ndarray\x12\r\n\x05shape\x18\x01 \x03(\x03\x12%\n\x05\x64type\x18\x02 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"C\n\tarrayjson\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\tb\x06proto3'
+    name='arraybuf.proto',
+    package='tensorpc.protos',
+    syntax='proto3',
+    serialized_options=None,
+    serialized_pb=
+    b'\n\x0e\x61rraybuf.proto\x12\x0ftensorpc.protos\"\xe4\x02\n\x05\x64type\x12-\n\x04type\x18\x01 \x01(\x0e\x32\x1f.tensorpc.protos.dtype.DataType\x12\x34\n\nbyte_order\x18\x02 \x01(\x0e\x32 .tensorpc.protos.dtype.ByteOrder\"@\n\tByteOrder\x12\x10\n\x0clittleEndian\x10\x00\x12\r\n\tbigEndian\x10\x01\x12\n\n\x06native\x10\x02\x12\x06\n\x02na\x10\x03\"\xb3\x01\n\x08\x44\x61taType\x12\x0b\n\x07\x66loat64\x10\x00\x12\x0b\n\x07\x66loat32\x10\x01\x12\x0b\n\x07\x66loat16\x10\x02\x12\n\n\x06uint64\x10\x03\x12\n\n\x06uint32\x10\x04\x12\n\n\x06uint16\x10\x05\x12\t\n\x05uint8\x10\x06\x12\t\n\x05int64\x10\x07\x12\t\n\x05int32\x10\x08\x12\t\n\x05int16\x10\t\x12\x08\n\x04int8\x10\n\x12\t\n\x05\x62ool_\x10\x0b\x12\x0f\n\x0b\x43ustomBytes\x10\x0c\x12\n\n\x06\x42\x61se64\x10\r\"M\n\x07ndarray\x12\r\n\x05shape\x18\x01 \x03(\x03\x12%\n\x05\x64type\x18\x02 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"C\n\tarrayjson\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\tb\x06proto3'
 )
 
-
-
 _DTYPE_BYTEORDER = _descriptor.EnumDescriptor(
-  name='ByteOrder',
-  full_name='tensorpc.protos.dtype.ByteOrder',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='littleEndian', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='bigEndian', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='native', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='na', index=3, number=3,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=146,
-  serialized_end=210,
+    name='ByteOrder',
+    full_name='tensorpc.protos.dtype.ByteOrder',
+    filename=None,
+    file=DESCRIPTOR,
+    values=[
+        _descriptor.EnumValueDescriptor(name='littleEndian',
+                                        index=0,
+                                        number=0,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='bigEndian',
+                                        index=1,
+                                        number=1,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='native',
+                                        index=2,
+                                        number=2,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='na',
+                                        index=3,
+                                        number=3,
+                                        serialized_options=None,
+                                        type=None),
+    ],
+    containing_type=None,
+    serialized_options=None,
+    serialized_start=146,
+    serialized_end=210,
 )
 _sym_db.RegisterEnumDescriptor(_DTYPE_BYTEORDER)
 
 _DTYPE_DATATYPE = _descriptor.EnumDescriptor(
-  name='DataType',
-  full_name='tensorpc.protos.dtype.DataType',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='float64', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='float32', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='float16', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='uint64', index=3, number=3,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='uint32', index=4, number=4,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='uint16', index=5, number=5,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='uint8', index=6, number=6,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='int64', index=7, number=7,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='int32', index=8, number=8,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='int16', index=9, number=9,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='int8', index=10, number=10,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='bool_', index=11, number=11,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='CustomBytes', index=12, number=12,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Base64', index=13, number=13,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=213,
-  serialized_end=392,
+    name='DataType',
+    full_name='tensorpc.protos.dtype.DataType',
+    filename=None,
+    file=DESCRIPTOR,
+    values=[
+        _descriptor.EnumValueDescriptor(name='float64',
+                                        index=0,
+                                        number=0,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='float32',
+                                        index=1,
+                                        number=1,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='float16',
+                                        index=2,
+                                        number=2,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='uint64',
+                                        index=3,
+                                        number=3,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='uint32',
+                                        index=4,
+                                        number=4,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='uint16',
+                                        index=5,
+                                        number=5,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='uint8',
+                                        index=6,
+                                        number=6,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='int64',
+                                        index=7,
+                                        number=7,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='int32',
+                                        index=8,
+                                        number=8,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='int16',
+                                        index=9,
+                                        number=9,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='int8',
+                                        index=10,
+                                        number=10,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='bool_',
+                                        index=11,
+                                        number=11,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='CustomBytes',
+                                        index=12,
+                                        number=12,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='Base64',
+                                        index=13,
+                                        number=13,
+                                        serialized_options=None,
+                                        type=None),
+    ],
+    containing_type=None,
+    serialized_options=None,
+    serialized_start=213,
+    serialized_end=392,
 )
 _sym_db.RegisterEnumDescriptor(_DTYPE_DATATYPE)
 
-
 _DTYPE = _descriptor.Descriptor(
-  name='dtype',
-  full_name='tensorpc.protos.dtype',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='type', full_name='tensorpc.protos.dtype.type', index=0,
-      number=1, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='byte_order', full_name='tensorpc.protos.dtype.byte_order', index=1,
-      number=2, type=14, cpp_type=8, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-    _DTYPE_BYTEORDER,
-    _DTYPE_DATATYPE,
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=36,
-  serialized_end=392,
+    name='dtype',
+    full_name='tensorpc.protos.dtype',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(name='type',
+                                    full_name='tensorpc.protos.dtype.type',
+                                    index=0,
+                                    number=1,
+                                    type=14,
+                                    cpp_type=8,
+                                    label=1,
+                                    has_default_value=False,
+                                    default_value=0,
+                                    message_type=None,
+                                    enum_type=None,
+                                    containing_type=None,
+                                    is_extension=False,
+                                    extension_scope=None,
+                                    serialized_options=None,
+                                    file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='byte_order',
+            full_name='tensorpc.protos.dtype.byte_order',
+            index=1,
+            number=2,
+            type=14,
+            cpp_type=8,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[
+        _DTYPE_BYTEORDER,
+        _DTYPE_DATATYPE,
+    ],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=36,
+    serialized_end=392,
 )
 
-
 _NDARRAY = _descriptor.Descriptor(
-  name='ndarray',
-  full_name='tensorpc.protos.ndarray',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='shape', full_name='tensorpc.protos.ndarray.shape', index=0,
-      number=1, type=3, cpp_type=2, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='dtype', full_name='tensorpc.protos.ndarray.dtype', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.ndarray.data', index=2,
-      number=3, type=12, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"",
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=394,
-  serialized_end=471,
+    name='ndarray',
+    full_name='tensorpc.protos.ndarray',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(name='shape',
+                                    full_name='tensorpc.protos.ndarray.shape',
+                                    index=0,
+                                    number=1,
+                                    type=3,
+                                    cpp_type=2,
+                                    label=3,
+                                    has_default_value=False,
+                                    default_value=[],
+                                    message_type=None,
+                                    enum_type=None,
+                                    containing_type=None,
+                                    is_extension=False,
+                                    extension_scope=None,
+                                    serialized_options=None,
+                                    file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(name='dtype',
+                                    full_name='tensorpc.protos.ndarray.dtype',
+                                    index=1,
+                                    number=2,
+                                    type=11,
+                                    cpp_type=10,
+                                    label=1,
+                                    has_default_value=False,
+                                    default_value=None,
+                                    message_type=None,
+                                    enum_type=None,
+                                    containing_type=None,
+                                    is_extension=False,
+                                    extension_scope=None,
+                                    serialized_options=None,
+                                    file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(name='data',
+                                    full_name='tensorpc.protos.ndarray.data',
+                                    index=2,
+                                    number=3,
+                                    type=12,
+                                    cpp_type=9,
+                                    label=1,
+                                    has_default_value=False,
+                                    default_value=b"",
+                                    message_type=None,
+                                    enum_type=None,
+                                    containing_type=None,
+                                    is_extension=False,
+                                    extension_scope=None,
+                                    serialized_options=None,
+                                    file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=394,
+    serialized_end=471,
 )
 
-
 _ARRAYJSON = _descriptor.Descriptor(
-  name='arrayjson',
-  full_name='tensorpc.protos.arrayjson',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='arrays', full_name='tensorpc.protos.arrayjson.arrays', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.arrayjson.data', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=473,
-  serialized_end=540,
+    name='arrayjson',
+    full_name='tensorpc.protos.arrayjson',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='arrays',
+            full_name='tensorpc.protos.arrayjson.arrays',
+            index=0,
+            number=1,
+            type=11,
+            cpp_type=10,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(name='data',
+                                    full_name='tensorpc.protos.arrayjson.data',
+                                    index=1,
+                                    number=2,
+                                    type=9,
+                                    cpp_type=9,
+                                    label=1,
+                                    has_default_value=False,
+                                    default_value=b"".decode('utf-8'),
+                                    message_type=None,
+                                    enum_type=None,
+                                    containing_type=None,
+                                    is_extension=False,
+                                    extension_scope=None,
+                                    serialized_options=None,
+                                    file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=473,
+    serialized_end=540,
 )
 
 _DTYPE.fields_by_name['type'].enum_type = _DTYPE_DATATYPE
 _DTYPE.fields_by_name['byte_order'].enum_type = _DTYPE_BYTEORDER
 _DTYPE_BYTEORDER.containing_type = _DTYPE
 _DTYPE_DATATYPE.containing_type = _DTYPE
 _NDARRAY.fields_by_name['dtype'].message_type = _DTYPE
 _ARRAYJSON.fields_by_name['arrays'].message_type = _NDARRAY
 DESCRIPTOR.message_types_by_name['dtype'] = _DTYPE
 DESCRIPTOR.message_types_by_name['ndarray'] = _NDARRAY
 DESCRIPTOR.message_types_by_name['arrayjson'] = _ARRAYJSON
 _sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
-dtype = _reflection.GeneratedProtocolMessageType('dtype', (_message.Message,), {
-  'DESCRIPTOR' : _DTYPE,
-  '__module__' : 'arraybuf_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.dtype)
-  })
+dtype = _reflection.GeneratedProtocolMessageType(
+    'dtype',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _DTYPE,
+        '__module__': 'arraybuf_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.dtype)
+    })
 _sym_db.RegisterMessage(dtype)
 
-ndarray = _reflection.GeneratedProtocolMessageType('ndarray', (_message.Message,), {
-  'DESCRIPTOR' : _NDARRAY,
-  '__module__' : 'arraybuf_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.ndarray)
-  })
+ndarray = _reflection.GeneratedProtocolMessageType(
+    'ndarray',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _NDARRAY,
+        '__module__': 'arraybuf_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.ndarray)
+    })
 _sym_db.RegisterMessage(ndarray)
 
-arrayjson = _reflection.GeneratedProtocolMessageType('arrayjson', (_message.Message,), {
-  'DESCRIPTOR' : _ARRAYJSON,
-  '__module__' : 'arraybuf_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.arrayjson)
-  })
+arrayjson = _reflection.GeneratedProtocolMessageType(
+    'arrayjson',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _ARRAYJSON,
+        '__module__': 'arraybuf_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.arrayjson)
+    })
 _sym_db.RegisterMessage(arrayjson)
 
-
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/arraybuf_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos_legacy/arraybuf_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/remote_object_pb2.py` & `tensorpc-0.11.0/tensorpc/protos_legacy/remote_object_pb2.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,159 +6,156 @@
 from google.protobuf import message as _message
 from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
 from . import arraybuf_pb2 as arraybuf__pb2
 from . import rpc_message_pb2 as rpc__message__pb2
 
-
 DESCRIPTOR = _descriptor.FileDescriptor(
-  name='remote_object.proto',
-  package='tensorpc.protos',
-  syntax='proto3',
-  serialized_options=None,
-  serialized_pb=b'\n\x13remote_object.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\x1a\x11rpc_message.proto2\xc1\t\n\x0cRemoteObject\x12T\n\nRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x12[\n\x0fRemoteGenerator\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x30\x01\x12`\n\x0eRemoteJsonCall\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x12g\n\x13RemoteJsonGenerator\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x30\x01\x12^\n\x10RemoteStreamCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12Z\n\x0eServerShutdown\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12W\n\x0bHealthCheck\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12U\n\x0fQueryServerMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12V\n\x10QueryServiceMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12_\n\x11\x43hunkedRemoteCall\x12!.tensorpc.protos.RemoteCallStream\x1a!.tensorpc.protos.RemoteCallStream\"\x00(\x01\x30\x01\x12\x62\n\x16\x43lientStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x12`\n\x12\x42iStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12H\n\x08SayHello\x12\x1d.tensorpc.protos.HelloRequest\x1a\x1b.tensorpc.protos.HelloReply\"\x00\x62\x06proto3'
-  ,
-  dependencies=[arraybuf__pb2.DESCRIPTOR,rpc__message__pb2.DESCRIPTOR,])
-
-
+    name='remote_object.proto',
+    package='tensorpc.protos',
+    syntax='proto3',
+    serialized_options=None,
+    serialized_pb=
+    b'\n\x13remote_object.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\x1a\x11rpc_message.proto2\xc1\t\n\x0cRemoteObject\x12T\n\nRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x12[\n\x0fRemoteGenerator\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00\x30\x01\x12`\n\x0eRemoteJsonCall\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x12g\n\x13RemoteJsonGenerator\x12&.tensorpc.protos.RemoteJsonCallRequest\x1a$.tensorpc.protos.RemoteJsonCallReply\"\x00\x30\x01\x12^\n\x10RemoteStreamCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12Z\n\x0eServerShutdown\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12W\n\x0bHealthCheck\x12#.tensorpc.protos.HealthCheckRequest\x1a!.tensorpc.protos.HealthCheckReply\"\x00\x12U\n\x0fQueryServerMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12V\n\x10QueryServiceMeta\x12\".tensorpc.protos.RemoteCallRequest\x1a\x1c.tensorpc.protos.SimpleReply\"\x00\x12_\n\x11\x43hunkedRemoteCall\x12!.tensorpc.protos.RemoteCallStream\x1a!.tensorpc.protos.RemoteCallStream\"\x00(\x01\x30\x01\x12\x62\n\x16\x43lientStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x12`\n\x12\x42iStreamRemoteCall\x12\".tensorpc.protos.RemoteCallRequest\x1a .tensorpc.protos.RemoteCallReply\"\x00(\x01\x30\x01\x12H\n\x08SayHello\x12\x1d.tensorpc.protos.HelloRequest\x1a\x1b.tensorpc.protos.HelloReply\"\x00\x62\x06proto3',
+    dependencies=[
+        arraybuf__pb2.DESCRIPTOR,
+        rpc__message__pb2.DESCRIPTOR,
+    ])
 
 _sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
-
-
 _REMOTEOBJECT = _descriptor.ServiceDescriptor(
-  name='RemoteObject',
-  full_name='tensorpc.protos.RemoteObject',
-  file=DESCRIPTOR,
-  index=0,
-  serialized_options=None,
-  serialized_start=76,
-  serialized_end=1293,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='RemoteCall',
-    full_name='tensorpc.protos.RemoteObject.RemoteCall',
+    name='RemoteObject',
+    full_name='tensorpc.protos.RemoteObject',
+    file=DESCRIPTOR,
     index=0,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._REMOTECALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='RemoteGenerator',
-    full_name='tensorpc.protos.RemoteObject.RemoteGenerator',
-    index=1,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._REMOTECALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='RemoteJsonCall',
-    full_name='tensorpc.protos.RemoteObject.RemoteJsonCall',
-    index=2,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTEJSONCALLREQUEST,
-    output_type=rpc__message__pb2._REMOTEJSONCALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='RemoteJsonGenerator',
-    full_name='tensorpc.protos.RemoteObject.RemoteJsonGenerator',
-    index=3,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTEJSONCALLREQUEST,
-    output_type=rpc__message__pb2._REMOTEJSONCALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='RemoteStreamCall',
-    full_name='tensorpc.protos.RemoteObject.RemoteStreamCall',
-    index=4,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._REMOTECALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='ServerShutdown',
-    full_name='tensorpc.protos.RemoteObject.ServerShutdown',
-    index=5,
-    containing_service=None,
-    input_type=rpc__message__pb2._HEALTHCHECKREQUEST,
-    output_type=rpc__message__pb2._HEALTHCHECKREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='HealthCheck',
-    full_name='tensorpc.protos.RemoteObject.HealthCheck',
-    index=6,
-    containing_service=None,
-    input_type=rpc__message__pb2._HEALTHCHECKREQUEST,
-    output_type=rpc__message__pb2._HEALTHCHECKREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='QueryServerMeta',
-    full_name='tensorpc.protos.RemoteObject.QueryServerMeta',
-    index=7,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._SIMPLEREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='QueryServiceMeta',
-    full_name='tensorpc.protos.RemoteObject.QueryServiceMeta',
-    index=8,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._SIMPLEREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='ChunkedRemoteCall',
-    full_name='tensorpc.protos.RemoteObject.ChunkedRemoteCall',
-    index=9,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLSTREAM,
-    output_type=rpc__message__pb2._REMOTECALLSTREAM,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='ClientStreamRemoteCall',
-    full_name='tensorpc.protos.RemoteObject.ClientStreamRemoteCall',
-    index=10,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._REMOTECALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='BiStreamRemoteCall',
-    full_name='tensorpc.protos.RemoteObject.BiStreamRemoteCall',
-    index=11,
-    containing_service=None,
-    input_type=rpc__message__pb2._REMOTECALLREQUEST,
-    output_type=rpc__message__pb2._REMOTECALLREPLY,
-    serialized_options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='SayHello',
-    full_name='tensorpc.protos.RemoteObject.SayHello',
-    index=12,
-    containing_service=None,
-    input_type=rpc__message__pb2._HELLOREQUEST,
-    output_type=rpc__message__pb2._HELLOREPLY,
     serialized_options=None,
-  ),
-])
+    serialized_start=76,
+    serialized_end=1293,
+    methods=[
+        _descriptor.MethodDescriptor(
+            name='RemoteCall',
+            full_name='tensorpc.protos.RemoteObject.RemoteCall',
+            index=0,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._REMOTECALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='RemoteGenerator',
+            full_name='tensorpc.protos.RemoteObject.RemoteGenerator',
+            index=1,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._REMOTECALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='RemoteJsonCall',
+            full_name='tensorpc.protos.RemoteObject.RemoteJsonCall',
+            index=2,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTEJSONCALLREQUEST,
+            output_type=rpc__message__pb2._REMOTEJSONCALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='RemoteJsonGenerator',
+            full_name='tensorpc.protos.RemoteObject.RemoteJsonGenerator',
+            index=3,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTEJSONCALLREQUEST,
+            output_type=rpc__message__pb2._REMOTEJSONCALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='RemoteStreamCall',
+            full_name='tensorpc.protos.RemoteObject.RemoteStreamCall',
+            index=4,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._REMOTECALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='ServerShutdown',
+            full_name='tensorpc.protos.RemoteObject.ServerShutdown',
+            index=5,
+            containing_service=None,
+            input_type=rpc__message__pb2._HEALTHCHECKREQUEST,
+            output_type=rpc__message__pb2._HEALTHCHECKREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='HealthCheck',
+            full_name='tensorpc.protos.RemoteObject.HealthCheck',
+            index=6,
+            containing_service=None,
+            input_type=rpc__message__pb2._HEALTHCHECKREQUEST,
+            output_type=rpc__message__pb2._HEALTHCHECKREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='QueryServerMeta',
+            full_name='tensorpc.protos.RemoteObject.QueryServerMeta',
+            index=7,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._SIMPLEREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='QueryServiceMeta',
+            full_name='tensorpc.protos.RemoteObject.QueryServiceMeta',
+            index=8,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._SIMPLEREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='ChunkedRemoteCall',
+            full_name='tensorpc.protos.RemoteObject.ChunkedRemoteCall',
+            index=9,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLSTREAM,
+            output_type=rpc__message__pb2._REMOTECALLSTREAM,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='ClientStreamRemoteCall',
+            full_name='tensorpc.protos.RemoteObject.ClientStreamRemoteCall',
+            index=10,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._REMOTECALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='BiStreamRemoteCall',
+            full_name='tensorpc.protos.RemoteObject.BiStreamRemoteCall',
+            index=11,
+            containing_service=None,
+            input_type=rpc__message__pb2._REMOTECALLREQUEST,
+            output_type=rpc__message__pb2._REMOTECALLREPLY,
+            serialized_options=None,
+        ),
+        _descriptor.MethodDescriptor(
+            name='SayHello',
+            full_name='tensorpc.protos.RemoteObject.SayHello',
+            index=12,
+            containing_service=None,
+            input_type=rpc__message__pb2._HELLOREQUEST,
+            output_type=rpc__message__pb2._HELLOREPLY,
+            serialized_options=None,
+        ),
+    ])
 _sym_db.RegisterServiceDescriptor(_REMOTEOBJECT)
 
 DESCRIPTOR.services_by_name['RemoteObject'] = _REMOTEOBJECT
 
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/remote_object_pb2_grpc.py` & `tensorpc-0.11.0/tensorpc/protos_legacy/remote_object_pb2_grpc.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,250 +1,304 @@
 # Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
 import grpc
 
 from . import rpc_message_pb2 as rpc__message__pb2
 
 
 class RemoteObjectStub(object):
-  # missing associated documentation comment in .proto file
-  pass
+    # missing associated documentation comment in .proto file
+    pass
 
-  def __init__(self, channel):
-    """Constructor.
+    def __init__(self, channel):
+        """Constructor.
 
     Args:
       channel: A grpc.Channel.
     """
-    self.RemoteCall = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/RemoteCall',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-        )
-    self.RemoteGenerator = channel.unary_stream(
-        '/tensorpc.protos.RemoteObject/RemoteGenerator',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-        )
-    self.RemoteJsonCall = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/RemoteJsonCall',
-        request_serializer=rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteJsonCallReply.FromString,
-        )
-    self.RemoteJsonGenerator = channel.unary_stream(
-        '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
-        request_serializer=rpc__message__pb2.RemoteJsonCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteJsonCallReply.FromString,
-        )
-    self.RemoteStreamCall = channel.stream_stream(
-        '/tensorpc.protos.RemoteObject/RemoteStreamCall',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-        )
-    self.ServerShutdown = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/ServerShutdown',
-        request_serializer=rpc__message__pb2.HealthCheckRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.HealthCheckReply.FromString,
-        )
-    self.HealthCheck = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/HealthCheck',
-        request_serializer=rpc__message__pb2.HealthCheckRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.HealthCheckReply.FromString,
-        )
-    self.QueryServerMeta = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/QueryServerMeta',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.SimpleReply.FromString,
-        )
-    self.QueryServiceMeta = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/QueryServiceMeta',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.SimpleReply.FromString,
-        )
-    self.ChunkedRemoteCall = channel.stream_stream(
-        '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
-        request_serializer=rpc__message__pb2.RemoteCallStream.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
-        )
-    self.ClientStreamRemoteCall = channel.stream_unary(
-        '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-        )
-    self.BiStreamRemoteCall = channel.stream_stream(
-        '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
-        request_serializer=rpc__message__pb2.RemoteCallRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
-        )
-    self.SayHello = channel.unary_unary(
-        '/tensorpc.protos.RemoteObject/SayHello',
-        request_serializer=rpc__message__pb2.HelloRequest.SerializeToString,
-        response_deserializer=rpc__message__pb2.HelloReply.FromString,
+        self.RemoteCall = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/RemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
+        self.RemoteGenerator = channel.unary_stream(
+            '/tensorpc.protos.RemoteObject/RemoteGenerator',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
+        self.RemoteJsonCall = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/RemoteJsonCall',
+            request_serializer=rpc__message__pb2.RemoteJsonCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteJsonCallReply.
+            FromString,
+        )
+        self.RemoteJsonGenerator = channel.unary_stream(
+            '/tensorpc.protos.RemoteObject/RemoteJsonGenerator',
+            request_serializer=rpc__message__pb2.RemoteJsonCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteJsonCallReply.
+            FromString,
+        )
+        self.RemoteStreamCall = channel.stream_stream(
+            '/tensorpc.protos.RemoteObject/RemoteStreamCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
+        self.ServerShutdown = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/ServerShutdown',
+            request_serializer=rpc__message__pb2.HealthCheckRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HealthCheckReply.
+            FromString,
+        )
+        self.HealthCheck = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/HealthCheck',
+            request_serializer=rpc__message__pb2.HealthCheckRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HealthCheckReply.
+            FromString,
+        )
+        self.QueryServerMeta = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/QueryServerMeta',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.SimpleReply.FromString,
+        )
+        self.QueryServiceMeta = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/QueryServiceMeta',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.SimpleReply.FromString,
+        )
+        self.ChunkedRemoteCall = channel.stream_stream(
+            '/tensorpc.protos.RemoteObject/ChunkedRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallStream.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallStream.
+            FromString,
+        )
+        self.ClientStreamRemoteCall = channel.stream_unary(
+            '/tensorpc.protos.RemoteObject/ClientStreamRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
+        self.BiStreamRemoteCall = channel.stream_stream(
+            '/tensorpc.protos.RemoteObject/BiStreamRemoteCall',
+            request_serializer=rpc__message__pb2.RemoteCallRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.RemoteCallReply.FromString,
+        )
+        self.SayHello = channel.unary_unary(
+            '/tensorpc.protos.RemoteObject/SayHello',
+            request_serializer=rpc__message__pb2.HelloRequest.
+            SerializeToString,
+            response_deserializer=rpc__message__pb2.HelloReply.FromString,
         )
 
 
 class RemoteObjectServicer(object):
-  # missing associated documentation comment in .proto file
-  pass
-
-  def RemoteCall(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def RemoteGenerator(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def RemoteJsonCall(self, request, context):
     # missing associated documentation comment in .proto file
     pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
 
-  def RemoteJsonGenerator(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def RemoteStreamCall(self, request_iterator, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
+    def RemoteCall(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def RemoteGenerator(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def RemoteJsonCall(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def RemoteJsonGenerator(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def RemoteStreamCall(self, request_iterator, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def ServerShutdown(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def HealthCheck(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def QueryServerMeta(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def QueryServiceMeta(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def ChunkedRemoteCall(self, request_iterator, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
-  def ServerShutdown(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def HealthCheck(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def QueryServerMeta(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def QueryServiceMeta(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def ChunkedRemoteCall(self, request_iterator, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def ClientStreamRemoteCall(self, request_iterator, context):
-    """first RemoteCallRequest will save paramsters of generator call, following are generator data.
+    def ClientStreamRemoteCall(self, request_iterator, context):
+        """first RemoteCallRequest will save paramsters of generator call, following are generator data.
     """
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def BiStreamRemoteCall(self, request_iterator, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
-
-  def SayHello(self, request, context):
-    # missing associated documentation comment in .proto file
-    pass
-    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
-    context.set_details('Method not implemented!')
-    raise NotImplementedError('Method not implemented!')
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def BiStreamRemoteCall(self, request_iterator, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
+
+    def SayHello(self, request, context):
+        # missing associated documentation comment in .proto file
+        pass
+        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
+        context.set_details('Method not implemented!')
+        raise NotImplementedError('Method not implemented!')
 
 
 def add_RemoteObjectServicer_to_server(servicer, server):
-  rpc_method_handlers = {
-      'RemoteCall': grpc.unary_unary_rpc_method_handler(
-          servicer.RemoteCall,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-      ),
-      'RemoteGenerator': grpc.unary_stream_rpc_method_handler(
-          servicer.RemoteGenerator,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-      ),
-      'RemoteJsonCall': grpc.unary_unary_rpc_method_handler(
-          servicer.RemoteJsonCall,
-          request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteJsonCallReply.SerializeToString,
-      ),
-      'RemoteJsonGenerator': grpc.unary_stream_rpc_method_handler(
-          servicer.RemoteJsonGenerator,
-          request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteJsonCallReply.SerializeToString,
-      ),
-      'RemoteStreamCall': grpc.stream_stream_rpc_method_handler(
-          servicer.RemoteStreamCall,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-      ),
-      'ServerShutdown': grpc.unary_unary_rpc_method_handler(
-          servicer.ServerShutdown,
-          request_deserializer=rpc__message__pb2.HealthCheckRequest.FromString,
-          response_serializer=rpc__message__pb2.HealthCheckReply.SerializeToString,
-      ),
-      'HealthCheck': grpc.unary_unary_rpc_method_handler(
-          servicer.HealthCheck,
-          request_deserializer=rpc__message__pb2.HealthCheckRequest.FromString,
-          response_serializer=rpc__message__pb2.HealthCheckReply.SerializeToString,
-      ),
-      'QueryServerMeta': grpc.unary_unary_rpc_method_handler(
-          servicer.QueryServerMeta,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.SimpleReply.SerializeToString,
-      ),
-      'QueryServiceMeta': grpc.unary_unary_rpc_method_handler(
-          servicer.QueryServiceMeta,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.SimpleReply.SerializeToString,
-      ),
-      'ChunkedRemoteCall': grpc.stream_stream_rpc_method_handler(
-          servicer.ChunkedRemoteCall,
-          request_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallStream.SerializeToString,
-      ),
-      'ClientStreamRemoteCall': grpc.stream_unary_rpc_method_handler(
-          servicer.ClientStreamRemoteCall,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-      ),
-      'BiStreamRemoteCall': grpc.stream_stream_rpc_method_handler(
-          servicer.BiStreamRemoteCall,
-          request_deserializer=rpc__message__pb2.RemoteCallRequest.FromString,
-          response_serializer=rpc__message__pb2.RemoteCallReply.SerializeToString,
-      ),
-      'SayHello': grpc.unary_unary_rpc_method_handler(
-          servicer.SayHello,
-          request_deserializer=rpc__message__pb2.HelloRequest.FromString,
-          response_serializer=rpc__message__pb2.HelloReply.SerializeToString,
-      ),
-  }
-  generic_handler = grpc.method_handlers_generic_handler(
-      'tensorpc.protos.RemoteObject', rpc_method_handlers)
-  server.add_generic_rpc_handlers((generic_handler,))
+    rpc_method_handlers = {
+        'RemoteCall':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.RemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'RemoteGenerator':
+        grpc.unary_stream_rpc_method_handler(
+            servicer.RemoteGenerator,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'RemoteJsonCall':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.RemoteJsonCall,
+            request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteJsonCallReply.
+            SerializeToString,
+        ),
+        'RemoteJsonGenerator':
+        grpc.unary_stream_rpc_method_handler(
+            servicer.RemoteJsonGenerator,
+            request_deserializer=rpc__message__pb2.RemoteJsonCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteJsonCallReply.
+            SerializeToString,
+        ),
+        'RemoteStreamCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.RemoteStreamCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'ServerShutdown':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.ServerShutdown,
+            request_deserializer=rpc__message__pb2.HealthCheckRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.HealthCheckReply.
+            SerializeToString,
+        ),
+        'HealthCheck':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.HealthCheck,
+            request_deserializer=rpc__message__pb2.HealthCheckRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.HealthCheckReply.
+            SerializeToString,
+        ),
+        'QueryServerMeta':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.QueryServerMeta,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.SimpleReply.
+            SerializeToString,
+        ),
+        'QueryServiceMeta':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.QueryServiceMeta,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.SimpleReply.
+            SerializeToString,
+        ),
+        'ChunkedRemoteCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.ChunkedRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallStream.FromString,
+            response_serializer=rpc__message__pb2.RemoteCallStream.
+            SerializeToString,
+        ),
+        'ClientStreamRemoteCall':
+        grpc.stream_unary_rpc_method_handler(
+            servicer.ClientStreamRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'BiStreamRemoteCall':
+        grpc.stream_stream_rpc_method_handler(
+            servicer.BiStreamRemoteCall,
+            request_deserializer=rpc__message__pb2.RemoteCallRequest.
+            FromString,
+            response_serializer=rpc__message__pb2.RemoteCallReply.
+            SerializeToString,
+        ),
+        'SayHello':
+        grpc.unary_unary_rpc_method_handler(
+            servicer.SayHello,
+            request_deserializer=rpc__message__pb2.HelloRequest.FromString,
+            response_serializer=rpc__message__pb2.HelloReply.SerializeToString,
+        ),
+    }
+    generic_handler = grpc.method_handlers_generic_handler(
+        'tensorpc.protos.RemoteObject', rpc_method_handlers)
+    server.add_generic_rpc_handlers((generic_handler, ))
```

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/rpc_message_pb2.py` & `tensorpc-0.11.0/tensorpc/protos_legacy/rpc_message_pb2.py`

 * *Files 15% similar despite different names*

```diff
@@ -7,78 +7,88 @@
 from google.protobuf import message as _message
 from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
-
 from . import arraybuf_pb2 as arraybuf__pb2
 
-
 DESCRIPTOR = _descriptor.FileDescriptor(
-  name='rpc_message.proto',
-  package='tensorpc.protos',
-  syntax='proto3',
-  serialized_options=None,
-  serialized_pb=b'\n\x11rpc_message.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\".\n\x0bSimpleReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12\x11\n\texception\x18\x02 \x01(\t\"\x85\x01\n\x11RemoteCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x10\n\x08\x63\x61llback\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\"\x85\x01\n\x15RemoteJsonCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\r\n\x05\x66lags\x18\x02 \x01(\x03\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\t\x12\x13\n\x0bservice_key\x18\x04 \x01(\t\x12\x10\n\x08\x63\x61llback\x18\x05 \x01(\t\"o\n\x0fRemoteCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"o\n\x13RemoteJsonCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\t\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"\xd9\x01\n\x10RemoteCallStream\x12\x11\n\tnum_chunk\x18\x01 \x01(\x05\x12\x10\n\x08\x63hunk_id\x18\x02 \x01(\x05\x12\x10\n\x08num_args\x18\x03 \x01(\x05\x12\x0e\n\x06\x61rg_id\x18\x04 \x01(\x05\x12\r\n\x05\x66lags\x18\x05 \x01(\x03\x12%\n\x05\x64type\x18\x06 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x14\n\x0c\x63hunked_data\x18\x07 \x01(\x0c\x12\x10\n\x08\x66unc_key\x18\x08 \x01(\t\x12\r\n\x05shape\x18\t \x03(\x03\x12\x11\n\texception\x18\n \x01(\t\"%\n\x12HealthCheckRequest\x12\x0f\n\x07service\x18\x01 \x01(\t\" \n\x10HealthCheckReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1c\n\x0cHelloRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1a\n\nHelloReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t*\xa4\x01\n\x0c\x45ncodeMethod\x12\x0b\n\x07Unknown\x10\x00\x12\x08\n\x04Json\x10\x01\x12\n\n\x06Pickle\x10\x02\x12\x0f\n\x0bMessagePack\x10\x03\x12\r\n\tJsonArray\x10\x10\x12\x0f\n\x0bPickleArray\x10 \x12\x14\n\x10MessagePackArray\x10\x30\x12\x0f\n\x0bNoArrayMask\x10\x0f\x12\x0e\n\tArrayMask\x10\xf0\x01\x12\t\n\x04Mask\x10\xff\x01\x62\x06proto3'
-  ,
-  dependencies=[arraybuf__pb2.DESCRIPTOR,])
+    name='rpc_message.proto',
+    package='tensorpc.protos',
+    syntax='proto3',
+    serialized_options=None,
+    serialized_pb=
+    b'\n\x11rpc_message.proto\x12\x0ftensorpc.protos\x1a\x0e\x61rraybuf.proto\".\n\x0bSimpleReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\x12\x11\n\texception\x18\x02 \x01(\t\"\x85\x01\n\x11RemoteCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x10\n\x08\x63\x61llback\x18\x04 \x01(\t\x12\x13\n\x0bservice_key\x18\x05 \x01(\t\"\x85\x01\n\x15RemoteJsonCallRequest\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\r\n\x05\x66lags\x18\x02 \x01(\x03\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\t\x12\x13\n\x0bservice_key\x18\x04 \x01(\t\x12\x10\n\x08\x63\x61llback\x18\x05 \x01(\t\"o\n\x0fRemoteCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x10\n\x08\x62lock_id\x18\x02 \x01(\x03\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"o\n\x13RemoteJsonCallReply\x12(\n\x06\x61rrays\x18\x01 \x03(\x0b\x32\x18.tensorpc.protos.ndarray\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\t\x12\r\n\x05\x66lags\x18\x03 \x01(\x03\x12\x11\n\texception\x18\x04 \x01(\t\"\xd9\x01\n\x10RemoteCallStream\x12\x11\n\tnum_chunk\x18\x01 \x01(\x05\x12\x10\n\x08\x63hunk_id\x18\x02 \x01(\x05\x12\x10\n\x08num_args\x18\x03 \x01(\x05\x12\x0e\n\x06\x61rg_id\x18\x04 \x01(\x05\x12\r\n\x05\x66lags\x18\x05 \x01(\x03\x12%\n\x05\x64type\x18\x06 \x01(\x0b\x32\x16.tensorpc.protos.dtype\x12\x14\n\x0c\x63hunked_data\x18\x07 \x01(\x0c\x12\x10\n\x08\x66unc_key\x18\x08 \x01(\t\x12\r\n\x05shape\x18\t \x03(\x03\x12\x11\n\texception\x18\n \x01(\t\"%\n\x12HealthCheckRequest\x12\x0f\n\x07service\x18\x01 \x01(\t\" \n\x10HealthCheckReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1c\n\x0cHelloRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t\"\x1a\n\nHelloReply\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\t*\xa4\x01\n\x0c\x45ncodeMethod\x12\x0b\n\x07Unknown\x10\x00\x12\x08\n\x04Json\x10\x01\x12\n\n\x06Pickle\x10\x02\x12\x0f\n\x0bMessagePack\x10\x03\x12\r\n\tJsonArray\x10\x10\x12\x0f\n\x0bPickleArray\x10 \x12\x14\n\x10MessagePackArray\x10\x30\x12\x0f\n\x0bNoArrayMask\x10\x0f\x12\x0e\n\tArrayMask\x10\xf0\x01\x12\t\n\x04Mask\x10\xff\x01\x62\x06proto3',
+    dependencies=[
+        arraybuf__pb2.DESCRIPTOR,
+    ])
 
 _ENCODEMETHOD = _descriptor.EnumDescriptor(
-  name='EncodeMethod',
-  full_name='tensorpc.protos.EncodeMethod',
-  filename=None,
-  file=DESCRIPTOR,
-  values=[
-    _descriptor.EnumValueDescriptor(
-      name='Unknown', index=0, number=0,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Json', index=1, number=1,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Pickle', index=2, number=2,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='MessagePack', index=3, number=3,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='JsonArray', index=4, number=16,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='PickleArray', index=5, number=32,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='MessagePackArray', index=6, number=48,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='NoArrayMask', index=7, number=15,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='ArrayMask', index=8, number=240,
-      serialized_options=None,
-      type=None),
-    _descriptor.EnumValueDescriptor(
-      name='Mask', index=9, number=255,
-      serialized_options=None,
-      type=None),
-  ],
-  containing_type=None,
-  serialized_options=None,
-  serialized_start=952,
-  serialized_end=1116,
+    name='EncodeMethod',
+    full_name='tensorpc.protos.EncodeMethod',
+    filename=None,
+    file=DESCRIPTOR,
+    values=[
+        _descriptor.EnumValueDescriptor(name='Unknown',
+                                        index=0,
+                                        number=0,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='Json',
+                                        index=1,
+                                        number=1,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='Pickle',
+                                        index=2,
+                                        number=2,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='MessagePack',
+                                        index=3,
+                                        number=3,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='JsonArray',
+                                        index=4,
+                                        number=16,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='PickleArray',
+                                        index=5,
+                                        number=32,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='MessagePackArray',
+                                        index=6,
+                                        number=48,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='NoArrayMask',
+                                        index=7,
+                                        number=15,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='ArrayMask',
+                                        index=8,
+                                        number=240,
+                                        serialized_options=None,
+                                        type=None),
+        _descriptor.EnumValueDescriptor(name='Mask',
+                                        index=9,
+                                        number=255,
+                                        serialized_options=None,
+                                        type=None),
+    ],
+    containing_type=None,
+    serialized_options=None,
+    serialized_start=952,
+    serialized_end=1116,
 )
 _sym_db.RegisterEnumDescriptor(_ENCODEMETHOD)
 
 EncodeMethod = enum_type_wrapper.EnumTypeWrapper(_ENCODEMETHOD)
 Unknown = 0
 Json = 1
 Pickle = 2
@@ -86,576 +96,908 @@
 JsonArray = 16
 PickleArray = 32
 MessagePackArray = 48
 NoArrayMask = 15
 ArrayMask = 240
 Mask = 255
 
-
-
 _SIMPLEREPLY = _descriptor.Descriptor(
-  name='SimpleReply',
-  full_name='tensorpc.protos.SimpleReply',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.SimpleReply.data', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='exception', full_name='tensorpc.protos.SimpleReply.exception', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=54,
-  serialized_end=100,
+    name='SimpleReply',
+    full_name='tensorpc.protos.SimpleReply',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.SimpleReply.data',
+            index=0,
+            number=1,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='exception',
+            full_name='tensorpc.protos.SimpleReply.exception',
+            index=1,
+            number=2,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=54,
+    serialized_end=100,
 )
 
-
 _REMOTECALLREQUEST = _descriptor.Descriptor(
-  name='RemoteCallRequest',
-  full_name='tensorpc.protos.RemoteCallRequest',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='arrays', full_name='tensorpc.protos.RemoteCallRequest.arrays', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='block_id', full_name='tensorpc.protos.RemoteCallRequest.block_id', index=1,
-      number=2, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='flags', full_name='tensorpc.protos.RemoteCallRequest.flags', index=2,
-      number=3, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='callback', full_name='tensorpc.protos.RemoteCallRequest.callback', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='service_key', full_name='tensorpc.protos.RemoteCallRequest.service_key', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=103,
-  serialized_end=236,
+    name='RemoteCallRequest',
+    full_name='tensorpc.protos.RemoteCallRequest',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='arrays',
+            full_name='tensorpc.protos.RemoteCallRequest.arrays',
+            index=0,
+            number=1,
+            type=11,
+            cpp_type=10,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='block_id',
+            full_name='tensorpc.protos.RemoteCallRequest.block_id',
+            index=1,
+            number=2,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='flags',
+            full_name='tensorpc.protos.RemoteCallRequest.flags',
+            index=2,
+            number=3,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='callback',
+            full_name='tensorpc.protos.RemoteCallRequest.callback',
+            index=3,
+            number=4,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='service_key',
+            full_name='tensorpc.protos.RemoteCallRequest.service_key',
+            index=4,
+            number=5,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=103,
+    serialized_end=236,
 )
 
-
 _REMOTEJSONCALLREQUEST = _descriptor.Descriptor(
-  name='RemoteJsonCallRequest',
-  full_name='tensorpc.protos.RemoteJsonCallRequest',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='arrays', full_name='tensorpc.protos.RemoteJsonCallRequest.arrays', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='flags', full_name='tensorpc.protos.RemoteJsonCallRequest.flags', index=1,
-      number=2, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.RemoteJsonCallRequest.data', index=2,
-      number=3, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='service_key', full_name='tensorpc.protos.RemoteJsonCallRequest.service_key', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='callback', full_name='tensorpc.protos.RemoteJsonCallRequest.callback', index=4,
-      number=5, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=239,
-  serialized_end=372,
+    name='RemoteJsonCallRequest',
+    full_name='tensorpc.protos.RemoteJsonCallRequest',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='arrays',
+            full_name='tensorpc.protos.RemoteJsonCallRequest.arrays',
+            index=0,
+            number=1,
+            type=11,
+            cpp_type=10,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='flags',
+            full_name='tensorpc.protos.RemoteJsonCallRequest.flags',
+            index=1,
+            number=2,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.RemoteJsonCallRequest.data',
+            index=2,
+            number=3,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='service_key',
+            full_name='tensorpc.protos.RemoteJsonCallRequest.service_key',
+            index=3,
+            number=4,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='callback',
+            full_name='tensorpc.protos.RemoteJsonCallRequest.callback',
+            index=4,
+            number=5,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=239,
+    serialized_end=372,
 )
 
-
 _REMOTECALLREPLY = _descriptor.Descriptor(
-  name='RemoteCallReply',
-  full_name='tensorpc.protos.RemoteCallReply',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='arrays', full_name='tensorpc.protos.RemoteCallReply.arrays', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='block_id', full_name='tensorpc.protos.RemoteCallReply.block_id', index=1,
-      number=2, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='flags', full_name='tensorpc.protos.RemoteCallReply.flags', index=2,
-      number=3, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='exception', full_name='tensorpc.protos.RemoteCallReply.exception', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=374,
-  serialized_end=485,
+    name='RemoteCallReply',
+    full_name='tensorpc.protos.RemoteCallReply',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='arrays',
+            full_name='tensorpc.protos.RemoteCallReply.arrays',
+            index=0,
+            number=1,
+            type=11,
+            cpp_type=10,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='block_id',
+            full_name='tensorpc.protos.RemoteCallReply.block_id',
+            index=1,
+            number=2,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='flags',
+            full_name='tensorpc.protos.RemoteCallReply.flags',
+            index=2,
+            number=3,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='exception',
+            full_name='tensorpc.protos.RemoteCallReply.exception',
+            index=3,
+            number=4,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=374,
+    serialized_end=485,
 )
 
-
 _REMOTEJSONCALLREPLY = _descriptor.Descriptor(
-  name='RemoteJsonCallReply',
-  full_name='tensorpc.protos.RemoteJsonCallReply',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='arrays', full_name='tensorpc.protos.RemoteJsonCallReply.arrays', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.RemoteJsonCallReply.data', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='flags', full_name='tensorpc.protos.RemoteJsonCallReply.flags', index=2,
-      number=3, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='exception', full_name='tensorpc.protos.RemoteJsonCallReply.exception', index=3,
-      number=4, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=487,
-  serialized_end=598,
+    name='RemoteJsonCallReply',
+    full_name='tensorpc.protos.RemoteJsonCallReply',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='arrays',
+            full_name='tensorpc.protos.RemoteJsonCallReply.arrays',
+            index=0,
+            number=1,
+            type=11,
+            cpp_type=10,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.RemoteJsonCallReply.data',
+            index=1,
+            number=2,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='flags',
+            full_name='tensorpc.protos.RemoteJsonCallReply.flags',
+            index=2,
+            number=3,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='exception',
+            full_name='tensorpc.protos.RemoteJsonCallReply.exception',
+            index=3,
+            number=4,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=487,
+    serialized_end=598,
 )
 
-
 _REMOTECALLSTREAM = _descriptor.Descriptor(
-  name='RemoteCallStream',
-  full_name='tensorpc.protos.RemoteCallStream',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='num_chunk', full_name='tensorpc.protos.RemoteCallStream.num_chunk', index=0,
-      number=1, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chunk_id', full_name='tensorpc.protos.RemoteCallStream.chunk_id', index=1,
-      number=2, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='num_args', full_name='tensorpc.protos.RemoteCallStream.num_args', index=2,
-      number=3, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='arg_id', full_name='tensorpc.protos.RemoteCallStream.arg_id', index=3,
-      number=4, type=5, cpp_type=1, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='flags', full_name='tensorpc.protos.RemoteCallStream.flags', index=4,
-      number=5, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='dtype', full_name='tensorpc.protos.RemoteCallStream.dtype', index=5,
-      number=6, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='chunked_data', full_name='tensorpc.protos.RemoteCallStream.chunked_data', index=6,
-      number=7, type=12, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"",
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='func_key', full_name='tensorpc.protos.RemoteCallStream.func_key', index=7,
-      number=8, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='shape', full_name='tensorpc.protos.RemoteCallStream.shape', index=8,
-      number=9, type=3, cpp_type=2, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='exception', full_name='tensorpc.protos.RemoteCallStream.exception', index=9,
-      number=10, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=601,
-  serialized_end=818,
+    name='RemoteCallStream',
+    full_name='tensorpc.protos.RemoteCallStream',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='num_chunk',
+            full_name='tensorpc.protos.RemoteCallStream.num_chunk',
+            index=0,
+            number=1,
+            type=5,
+            cpp_type=1,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='chunk_id',
+            full_name='tensorpc.protos.RemoteCallStream.chunk_id',
+            index=1,
+            number=2,
+            type=5,
+            cpp_type=1,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='num_args',
+            full_name='tensorpc.protos.RemoteCallStream.num_args',
+            index=2,
+            number=3,
+            type=5,
+            cpp_type=1,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='arg_id',
+            full_name='tensorpc.protos.RemoteCallStream.arg_id',
+            index=3,
+            number=4,
+            type=5,
+            cpp_type=1,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='flags',
+            full_name='tensorpc.protos.RemoteCallStream.flags',
+            index=4,
+            number=5,
+            type=3,
+            cpp_type=2,
+            label=1,
+            has_default_value=False,
+            default_value=0,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='dtype',
+            full_name='tensorpc.protos.RemoteCallStream.dtype',
+            index=5,
+            number=6,
+            type=11,
+            cpp_type=10,
+            label=1,
+            has_default_value=False,
+            default_value=None,
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='chunked_data',
+            full_name='tensorpc.protos.RemoteCallStream.chunked_data',
+            index=6,
+            number=7,
+            type=12,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"",
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='func_key',
+            full_name='tensorpc.protos.RemoteCallStream.func_key',
+            index=7,
+            number=8,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='shape',
+            full_name='tensorpc.protos.RemoteCallStream.shape',
+            index=8,
+            number=9,
+            type=3,
+            cpp_type=2,
+            label=3,
+            has_default_value=False,
+            default_value=[],
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+        _descriptor.FieldDescriptor(
+            name='exception',
+            full_name='tensorpc.protos.RemoteCallStream.exception',
+            index=9,
+            number=10,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=601,
+    serialized_end=818,
 )
 
-
 _HEALTHCHECKREQUEST = _descriptor.Descriptor(
-  name='HealthCheckRequest',
-  full_name='tensorpc.protos.HealthCheckRequest',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='service', full_name='tensorpc.protos.HealthCheckRequest.service', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=820,
-  serialized_end=857,
+    name='HealthCheckRequest',
+    full_name='tensorpc.protos.HealthCheckRequest',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='service',
+            full_name='tensorpc.protos.HealthCheckRequest.service',
+            index=0,
+            number=1,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=820,
+    serialized_end=857,
 )
 
-
 _HEALTHCHECKREPLY = _descriptor.Descriptor(
-  name='HealthCheckReply',
-  full_name='tensorpc.protos.HealthCheckReply',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.HealthCheckReply.data', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=859,
-  serialized_end=891,
+    name='HealthCheckReply',
+    full_name='tensorpc.protos.HealthCheckReply',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.HealthCheckReply.data',
+            index=0,
+            number=1,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=859,
+    serialized_end=891,
 )
 
-
 _HELLOREQUEST = _descriptor.Descriptor(
-  name='HelloRequest',
-  full_name='tensorpc.protos.HelloRequest',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.HelloRequest.data', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=893,
-  serialized_end=921,
+    name='HelloRequest',
+    full_name='tensorpc.protos.HelloRequest',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.HelloRequest.data',
+            index=0,
+            number=1,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=893,
+    serialized_end=921,
 )
 
-
 _HELLOREPLY = _descriptor.Descriptor(
-  name='HelloReply',
-  full_name='tensorpc.protos.HelloReply',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='data', full_name='tensorpc.protos.HelloReply.data', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=b"".decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      serialized_options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  serialized_options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=923,
-  serialized_end=949,
-)
-
-_REMOTECALLREQUEST.fields_by_name['arrays'].message_type = arraybuf__pb2._NDARRAY
-_REMOTEJSONCALLREQUEST.fields_by_name['arrays'].message_type = arraybuf__pb2._NDARRAY
+    name='HelloReply',
+    full_name='tensorpc.protos.HelloReply',
+    filename=None,
+    file=DESCRIPTOR,
+    containing_type=None,
+    fields=[
+        _descriptor.FieldDescriptor(
+            name='data',
+            full_name='tensorpc.protos.HelloReply.data',
+            index=0,
+            number=1,
+            type=9,
+            cpp_type=9,
+            label=1,
+            has_default_value=False,
+            default_value=b"".decode('utf-8'),
+            message_type=None,
+            enum_type=None,
+            containing_type=None,
+            is_extension=False,
+            extension_scope=None,
+            serialized_options=None,
+            file=DESCRIPTOR),
+    ],
+    extensions=[],
+    nested_types=[],
+    enum_types=[],
+    serialized_options=None,
+    is_extendable=False,
+    syntax='proto3',
+    extension_ranges=[],
+    oneofs=[],
+    serialized_start=923,
+    serialized_end=949,
+)
+
+_REMOTECALLREQUEST.fields_by_name[
+    'arrays'].message_type = arraybuf__pb2._NDARRAY
+_REMOTEJSONCALLREQUEST.fields_by_name[
+    'arrays'].message_type = arraybuf__pb2._NDARRAY
 _REMOTECALLREPLY.fields_by_name['arrays'].message_type = arraybuf__pb2._NDARRAY
-_REMOTEJSONCALLREPLY.fields_by_name['arrays'].message_type = arraybuf__pb2._NDARRAY
+_REMOTEJSONCALLREPLY.fields_by_name[
+    'arrays'].message_type = arraybuf__pb2._NDARRAY
 _REMOTECALLSTREAM.fields_by_name['dtype'].message_type = arraybuf__pb2._DTYPE
 DESCRIPTOR.message_types_by_name['SimpleReply'] = _SIMPLEREPLY
 DESCRIPTOR.message_types_by_name['RemoteCallRequest'] = _REMOTECALLREQUEST
-DESCRIPTOR.message_types_by_name['RemoteJsonCallRequest'] = _REMOTEJSONCALLREQUEST
+DESCRIPTOR.message_types_by_name[
+    'RemoteJsonCallRequest'] = _REMOTEJSONCALLREQUEST
 DESCRIPTOR.message_types_by_name['RemoteCallReply'] = _REMOTECALLREPLY
 DESCRIPTOR.message_types_by_name['RemoteJsonCallReply'] = _REMOTEJSONCALLREPLY
 DESCRIPTOR.message_types_by_name['RemoteCallStream'] = _REMOTECALLSTREAM
 DESCRIPTOR.message_types_by_name['HealthCheckRequest'] = _HEALTHCHECKREQUEST
 DESCRIPTOR.message_types_by_name['HealthCheckReply'] = _HEALTHCHECKREPLY
 DESCRIPTOR.message_types_by_name['HelloRequest'] = _HELLOREQUEST
 DESCRIPTOR.message_types_by_name['HelloReply'] = _HELLOREPLY
 DESCRIPTOR.enum_types_by_name['EncodeMethod'] = _ENCODEMETHOD
 _sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
-SimpleReply = _reflection.GeneratedProtocolMessageType('SimpleReply', (_message.Message,), {
-  'DESCRIPTOR' : _SIMPLEREPLY,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.SimpleReply)
-  })
+SimpleReply = _reflection.GeneratedProtocolMessageType(
+    'SimpleReply',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _SIMPLEREPLY,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.SimpleReply)
+    })
 _sym_db.RegisterMessage(SimpleReply)
 
-RemoteCallRequest = _reflection.GeneratedProtocolMessageType('RemoteCallRequest', (_message.Message,), {
-  'DESCRIPTOR' : _REMOTECALLREQUEST,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallRequest)
-  })
+RemoteCallRequest = _reflection.GeneratedProtocolMessageType(
+    'RemoteCallRequest',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _REMOTECALLREQUEST,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallRequest)
+    })
 _sym_db.RegisterMessage(RemoteCallRequest)
 
-RemoteJsonCallRequest = _reflection.GeneratedProtocolMessageType('RemoteJsonCallRequest', (_message.Message,), {
-  'DESCRIPTOR' : _REMOTEJSONCALLREQUEST,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteJsonCallRequest)
-  })
+RemoteJsonCallRequest = _reflection.GeneratedProtocolMessageType(
+    'RemoteJsonCallRequest',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _REMOTEJSONCALLREQUEST,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteJsonCallRequest)
+    })
 _sym_db.RegisterMessage(RemoteJsonCallRequest)
 
-RemoteCallReply = _reflection.GeneratedProtocolMessageType('RemoteCallReply', (_message.Message,), {
-  'DESCRIPTOR' : _REMOTECALLREPLY,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallReply)
-  })
+RemoteCallReply = _reflection.GeneratedProtocolMessageType(
+    'RemoteCallReply',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _REMOTECALLREPLY,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallReply)
+    })
 _sym_db.RegisterMessage(RemoteCallReply)
 
-RemoteJsonCallReply = _reflection.GeneratedProtocolMessageType('RemoteJsonCallReply', (_message.Message,), {
-  'DESCRIPTOR' : _REMOTEJSONCALLREPLY,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteJsonCallReply)
-  })
+RemoteJsonCallReply = _reflection.GeneratedProtocolMessageType(
+    'RemoteJsonCallReply',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _REMOTEJSONCALLREPLY,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteJsonCallReply)
+    })
 _sym_db.RegisterMessage(RemoteJsonCallReply)
 
-RemoteCallStream = _reflection.GeneratedProtocolMessageType('RemoteCallStream', (_message.Message,), {
-  'DESCRIPTOR' : _REMOTECALLSTREAM,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallStream)
-  })
+RemoteCallStream = _reflection.GeneratedProtocolMessageType(
+    'RemoteCallStream',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _REMOTECALLSTREAM,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.RemoteCallStream)
+    })
 _sym_db.RegisterMessage(RemoteCallStream)
 
-HealthCheckRequest = _reflection.GeneratedProtocolMessageType('HealthCheckRequest', (_message.Message,), {
-  'DESCRIPTOR' : _HEALTHCHECKREQUEST,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.HealthCheckRequest)
-  })
+HealthCheckRequest = _reflection.GeneratedProtocolMessageType(
+    'HealthCheckRequest',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _HEALTHCHECKREQUEST,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.HealthCheckRequest)
+    })
 _sym_db.RegisterMessage(HealthCheckRequest)
 
-HealthCheckReply = _reflection.GeneratedProtocolMessageType('HealthCheckReply', (_message.Message,), {
-  'DESCRIPTOR' : _HEALTHCHECKREPLY,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.HealthCheckReply)
-  })
+HealthCheckReply = _reflection.GeneratedProtocolMessageType(
+    'HealthCheckReply',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _HEALTHCHECKREPLY,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.HealthCheckReply)
+    })
 _sym_db.RegisterMessage(HealthCheckReply)
 
-HelloRequest = _reflection.GeneratedProtocolMessageType('HelloRequest', (_message.Message,), {
-  'DESCRIPTOR' : _HELLOREQUEST,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.HelloRequest)
-  })
+HelloRequest = _reflection.GeneratedProtocolMessageType(
+    'HelloRequest',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _HELLOREQUEST,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.HelloRequest)
+    })
 _sym_db.RegisterMessage(HelloRequest)
 
-HelloReply = _reflection.GeneratedProtocolMessageType('HelloReply', (_message.Message,), {
-  'DESCRIPTOR' : _HELLOREPLY,
-  '__module__' : 'rpc_message_pb2'
-  # @@protoc_insertion_point(class_scope:tensorpc.protos.HelloReply)
-  })
+HelloReply = _reflection.GeneratedProtocolMessageType(
+    'HelloReply',
+    (_message.Message, ),
+    {
+        'DESCRIPTOR': _HELLOREPLY,
+        '__module__': 'rpc_message_pb2'
+        # @@protoc_insertion_point(class_scope:tensorpc.protos.HelloReply)
+    })
 _sym_db.RegisterMessage(HelloReply)
 
-
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/rpc_message_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos_legacy/rpc_message_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/protos_legacy/wsdef_pb2.pyi` & `tensorpc-0.11.0/tensorpc/protos_legacy/wsdef_pb2.pyi`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/serve/__init__.py` & `tensorpc-0.11.0/tensorpc/services/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/serve/__main__.py` & `tensorpc-0.11.0/tensorpc/serve/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/serve/flowapp_script/__main__.py` & `tensorpc-0.11.0/tensorpc/serve/flowapp_script/__main__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,29 @@
 import json
-import sys 
+import sys
 from tensorpc.serve.__main__ import serve_in_terminal
 import base64
-from pathlib import Path 
+from pathlib import Path
+
 
 def main():
     enc_b64 = sys.argv[1]
     serv_option_str = base64.b64decode(enc_b64).decode("utf-8")
     serv_option = json.loads(serv_option_str)
     assert "module" in serv_option, "you must provide single module and other serve args"
     may_be_path = sys.argv[2]
     assert Path(may_be_path).exists(), "you must provide a valid path"
     module = serv_option["module"]
     if "serv_config_b64" in serv_option:
         serv_config_b64 = serv_option["serv_config_b64"]
-        serv_config = json.loads(base64.b64decode(serv_config_b64).decode("utf-8"))
+        serv_config = json.loads(
+            base64.b64decode(serv_config_b64).decode("utf-8"))
         serv_config[module]["external_argv"] = sys.argv[2:]
-        serv_config_b64 = base64.b64encode(json.dumps(serv_config).encode("utf-8")).decode("utf-8")
+        serv_config_b64 = base64.b64encode(
+            json.dumps(serv_config).encode("utf-8")).decode("utf-8")
         serv_option["serv_config_b64"] = serv_config_b64
         serv_option.pop("module")
     serve_in_terminal(module, **serv_option)
 
+
 if __name__ == "__main__":
-    main()
+    main()
```

### Comparing `tensorpc-0.10.7/tensorpc/serve_sync/__main__.py` & `tensorpc-0.11.0/tensorpc/serve_sync/__main__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/services/__init__.py` & `tensorpc-0.11.0/tensorpc/services/flow/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/services/collection.py` & `tensorpc-0.11.0/tensorpc/services/collection.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,39 +13,45 @@
 # limitations under the License.
 
 from pathlib import Path
 import queue
 import threading
 from typing import Optional, Union, List, Dict
 import multiprocessing
-import sys 
+import sys
 from tensorpc import marker, prim, AsyncRemoteManager
-import traceback 
-import asyncio 
+import traceback
+import asyncio
 import numpy as np
-import time 
+import time
+
+from tensorpc.core.serviceunit import ServiceEventType
+
 
-from tensorpc.core.serviceunit import ServiceEventType 
 class Simple:
+
     def __init__(self) -> None:
         print("???????????")
+
     def echo(self, x):
         return x
-    
+
     def sleep(self, interval: float):
         time.sleep(interval)
-    
+
+
 class SpeedTestServer:
 
     def recv_data(self, x):
-        return 
-    
+        return
+
     def send_data(self, size_mb: int):
         return np.zeros([size_mb * 1024 * 1024], dtype=np.uint8)
 
+
 class FileOps:
 
     def print_in_server(self, content):
         print(content)
 
     def get_file(self, path, start_chunk=0, chunk_size=65536):
         """service that get a large file from server.
@@ -116,74 +122,86 @@
             with path.open("wb") as f:
                 async for chunk in gen_iter:
                     f.write(chunk)
         except Exception as e:
             path.unlink()
             raise e
 
+
 class ProcessObserver:
-    def __init__(self, q: Optional[Union[queue.Queue, multiprocessing.Queue]] = None) -> None:
+
+    def __init__(
+            self,
+            q: Optional[Union[queue.Queue,
+                              multiprocessing.Queue]] = None) -> None:
         self.q = q
-    
+
     @marker.mark_server_event(event_type=ServiceEventType.BeforeServerStart)
     def server_start(self):
         if self.q is not None:
             port = prim.get_server_grpc_port()
             self.q.put(port)
 
-
     def get_threads_current_status(self):
         this_tid = threading.get_ident()
         res = []
         threading_path = Path(threading.__file__)
         for threadId, frame in sys._current_frames().items():
             if threadId == this_tid:
                 continue
             if threading_path == Path(frame.f_code.co_filename):
                 continue
             res.append({
                 "thread_id": threadId,
                 "filename": frame.f_code.co_filename,
                 "lineno": frame.f_lineno,
             })
-        return res 
+        return res
+
 
 class ProcessObserveManager:
+
     def __init__(self) -> None:
         is_sync_server = prim.get_server_exposed_props().is_sync
         self._lock = asyncio.Lock()
         self.clients: Dict[str, AsyncRemoteManager] = {}
         self.is_sync_server = is_sync_server
 
     @marker.mark_server_event(event_type=marker.ServiceEventType.Init)
     async def init(self):
         if self.is_sync_server:
-            return 
+            return
         self._check_loop_task = asyncio.create_task(self._check_client_loop())
 
     async def register_client(self, url: str, identifier: str):
         if self.is_sync_server:
-            raise ValueError("register_client can only be called in async server.")
+            raise ValueError(
+                "register_client can only be called in async server.")
         async with self._lock:
             self.clients[identifier] = AsyncRemoteManager(url)
 
     async def register_local_client(self, port: int, identifier: str):
         return await self.register_client(f"localhost:{port}", identifier)
-    
+
     async def _check_client_loop(self):
         stdn_ev = prim.get_async_shutdown_event()
         shut_task = asyncio.create_task(stdn_ev.wait())
 
-        wait_tasks: List[asyncio.Task] = [shut_task, asyncio.create_task(asyncio.sleep(1))]
+        wait_tasks: List[asyncio.Task] = [
+            shut_task, asyncio.create_task(asyncio.sleep(1))
+        ]
 
         while True:
-            (done, pending) = await asyncio.wait(
-                wait_tasks, return_when=asyncio.FIRST_COMPLETED)
+            (done,
+             pending) = await asyncio.wait(wait_tasks,
+                                           return_when=asyncio.FIRST_COMPLETED)
             if shut_task in done:
                 break
-            wait_tasks: List[asyncio.Task] = [shut_task, asyncio.create_task(asyncio.sleep(1))]
+            wait_tasks: List[asyncio.Task] = [
+                shut_task, asyncio.create_task(asyncio.sleep(1))
+            ]
             for identifier, client in self.clients.items():
                 try:
                     res = await client.health_check(timeout=1)
                 except Exception as e:
                     async with self._lock:
                         self.clients.pop(identifier)
```

### Comparing `tensorpc-0.10.7/tensorpc/services/flow/__init__.py` & `tensorpc-0.11.0/tensorpc/services/flow/core.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/services/for_test.py` & `tensorpc-0.11.0/tensorpc/services/for_test.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/services/vis.py` & `tensorpc-0.11.0/tensorpc/services/vis.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/tools.py` & `tensorpc-0.11.0/tensorpc/tools.py`

 * *Files 1% similar despite different names*

```diff
@@ -97,23 +97,23 @@
             for i in range(len(lines)):
                 if import_as_pattern.fullmatch(lines[i]):
                     print(lines[i])
                     lines[i] = "from . " + lines[i]
             with path.open("w") as f:
                 f.writelines(lines)
 
+
 def main_legacy():
-    compile_proto(
-        Path(__file__).parent / "protos_legacy",
-        Path(__file__).parent.resolve() / "protos_legacy",
-        js_out=False,
-        with_pyi=False)
+    compile_proto(Path(__file__).parent / "protos_legacy",
+                  Path(__file__).parent.resolve() / "protos_legacy",
+                  js_out=False,
+                  with_pyi=False)
+
 
 def main():
-    compile_proto(
-        Path(__file__).parent / "protos",
-        Path(__file__).parent.resolve() / "protos",
-        js_out=False)
+    compile_proto(Path(__file__).parent / "protos",
+                  Path(__file__).parent.resolve() / "protos",
+                  js_out=False)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `tensorpc-0.10.7/tensorpc/utils/__init__.py` & `tensorpc-0.11.0/tensorpc/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/utils/df_logging.py` & `tensorpc-0.11.0/tensorpc/utils/df_logging.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/utils/gpuusage.py` & `tensorpc-0.11.0/tensorpc/utils/gpuusage.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from typing import Dict, List, Optional, Tuple
-import subprocess 
+import subprocess
 import dataclasses
-import io 
-import csv 
+import io
+import csv
+
 
 @dataclasses.dataclass
 class GPUMeasure:
     name: str
     gpuusage: int
     memusage: int
     temperature: int
@@ -14,14 +15,15 @@
     memtotal: int
 
     def to_string(self):
         msg = f"gpu={self.gpuusage}%,mem={self.memused}/{self.memtotal}MB,"
         msg += f"{self.temperature}\u2103,io={self.memusage}%"
         return msg
 
+
 def get_nvidia_gpu_measures() -> List[GPUMeasure]:
     gpumeasures: List[GPUMeasure] = []
 
     querys = [
         "gpu_name",
         "utilization.gpu",
         "utilization.memory",
@@ -45,8 +47,8 @@
             memtotal = int(query["memory.total"].split(" ")[0])
             temp = int(query["temperature.gpu"])
             gpumeasure = GPUMeasure(query["gpu_name"], gpuusage, memusage,
                                     temp, memused, memtotal)
             gpumeasures.append(gpumeasure)
     except:
         return []
-    return gpumeasures
+    return gpumeasures
```

### Comparing `tensorpc-0.10.7/tensorpc/utils/registry.py` & `tensorpc-0.11.0/tensorpc/utils/registry.py`

 * *Files 0% similar despite different names*

```diff
@@ -81,14 +81,15 @@
 
 class HashableSeqRegistryKeyOnly(Generic[T]):
 
     def __init__(self):
         self.global_dict: Dict[Hashable, List[T]] = {}
 
     def register(self, key: Optional[Hashable] = None):
+
         def wrapper(func: T) -> T:
             key_ = key
             if key is None:
                 key_ = func.__name__
             if key_ not in self.global_dict:
                 self.global_dict[key_] = []
             self.global_dict[key_].append(func)
```

### Comparing `tensorpc-0.10.7/tensorpc/utils/reload.py` & `tensorpc-0.11.0/tensorpc/utils/reload.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/utils/subproc.py` & `tensorpc-0.11.0/tensorpc/utils/subproc.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc/utils/uniquename.py` & `tensorpc-0.11.0/tensorpc/utils/uniquename.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Set
+from typing import Set, Optional
 
 
 def _make_unique_name(unique_set, name, max_count=10000) -> str:
     if name not in unique_set:
         unique_set.add(name)
         return name
     for i in range(max_count):
@@ -25,17 +25,19 @@
             unique_set.add(new_name)
             return new_name
     raise ValueError("max count reached")
 
 
 class UniqueNamePool:
 
-    def __init__(self, max_count=10000):
+    def __init__(self, max_count=10000, init_set: Optional[Set[str]] = None):
         self.max_count = max_count
         self.unique_set: Set[str] = set()
+        if init_set is not None:
+            self.unique_set.update(init_set)
 
     def __call__(self, name):
         return _make_unique_name(self.unique_set, name, self.max_count)
 
     def __contains__(self, key: str):
         return key in self.unique_set
```

### Comparing `tensorpc-0.10.7/tensorpc/utils/wait_tools.py` & `tensorpc-0.11.0/tensorpc/utils/wait_tools.py`

 * *Files identical despite different names*

### Comparing `tensorpc-0.10.7/tensorpc.egg-info/PKG-INFO` & `tensorpc-0.11.0/tensorpc.egg-info/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 Metadata-Version: 2.1
 Name: tensorpc
-Version: 0.10.7
+Version: 0.11.0
 Summary: Backend for devflow.
 Home-page: https://github.com/FindDefinition/tensorpc
 Author: Yan Yan
 Author-email: yanyan.sub@outlook.com
 License: MIT
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: Implementation :: CPython
 Classifier: Programming Language :: Python :: Implementation :: PyPy
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: protobuf>=4.21.6
-Requires-Dist: grpcio>=1.49.0
-Requires-Dist: grpcio-tools>=1.49.0
+Requires-Dist: protobuf>=3.18.0
+Requires-Dist: grpcio>=1.48.2
+Requires-Dist: grpcio-tools>=1.48.2
 Requires-Dist: fire
 Requires-Dist: pytest
 Requires-Dist: pytest-asyncio
 Requires-Dist: pyyaml
 Requires-Dist: numpy
 Requires-Dist: msgpack
 Requires-Dist: requests
```

### Comparing `tensorpc-0.10.7/tensorpc.egg-info/SOURCES.txt` & `tensorpc-0.11.0/tensorpc.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -158,14 +158,21 @@
 tensorpc/examples/tutorials/06-sample apps/6.1-Chat App.md
 tensorpc/examples/tutorials/06-sample apps/6.2-Deep Learning.md
 tensorpc/examples/tutorials/07-V Api/7.1-Basic.md
 tensorpc/examples/tutorials/07-V Api/7.2-Events.md
 tensorpc/examples/tutorials/07-V Api/7.3-Lines.md
 tensorpc/examples/tutorials/07-V Api/7.4-Image.md
 tensorpc/examples/tutorials/07-V Api/7.5-Points.md
+tensorpc/examples/tutorials/08-flow/8.1-overview.md
+tensorpc/examples/tutorials/08-flow/8.2-custom node.md
+tensorpc/examples/tutorials/08-flow/8.3-layout.md
+tensorpc/examples/tutorials/08-flow/8.4-changenode.md
+tensorpc/examples/tutorials/08-flow/8.5-drag and drop.md
+tensorpc/examples/tutorials/08-flow/8.6-handles.md
+tensorpc/examples/tutorials/08-flow/8.7-validation.md
 tensorpc/flow/__init__.py
 tensorpc/flow/client.py
 tensorpc/flow/constants.py
 tensorpc/flow/coretypes.py
 tensorpc/flow/jsonlike.py
 tensorpc/flow/marker.py
 tensorpc/flow/serv_names.py
@@ -183,14 +190,15 @@
 tensorpc/flow/flowapp/appctx/canvas.py
 tensorpc/flow/flowapp/appctx/core.py
 tensorpc/flow/flowapp/appctx/inspector.py
 tensorpc/flow/flowapp/components/__init__.py
 tensorpc/flow/flowapp/components/annocore.py
 tensorpc/flow/flowapp/components/common.py
 tensorpc/flow/flowapp/components/core.py
+tensorpc/flow/flowapp/components/flow.py
 tensorpc/flow/flowapp/components/leaflet.py
 tensorpc/flow/flowapp/components/mui.py
 tensorpc/flow/flowapp/components/plotly.py
 tensorpc/flow/flowapp/components/three.py
 tensorpc/flow/flowapp/components/threecore.py
 tensorpc/flow/flowapp/components/typemetas.py
 tensorpc/flow/flowapp/components/plus/__init__.py
```

### Comparing `tensorpc-0.10.7/test/test_tmux_scheduler.py` & `tensorpc-0.11.0/test/test_tmux_scheduler.py`

 * *Files identical despite different names*

