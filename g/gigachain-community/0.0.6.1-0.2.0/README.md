# Comparing `tmp/gigachain_community-0.0.6.1.tar.gz` & `tmp/gigachain_community-0.2.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "gigachain_community-0.0.6.1.tar", max compression
+gzip compressed data, was "gigachain_community-0.2.0.tar", max compression
```

## Comparing `gigachain_community-0.0.6.1.tar` & `gigachain_community-0.2.0.tar`

### file list

```diff
@@ -1,884 +1,1150 @@
--rw-r--r--   0        0        0     1201 2023-12-21 15:51:51.774421 gigachain_community-0.0.6.1/README.md
--rw-r--r--   0        0        0      307 2023-12-18 12:05:45.464203 gigachain_community-0.0.6.1/langchain_community/__init__.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.464419 gigachain_community-0.0.6.1/langchain_community/adapters/__init__.py
--rw-r--r--   0        0        0    12143 2023-12-21 15:51:51.775428 gigachain_community-0.0.6.1/langchain_community/adapters/openai.py
--rw-r--r--   0        0        0     3327 2023-12-18 12:05:45.466331 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/__init__.py
--rw-r--r--   0        0        0       25 2023-12-18 12:05:45.466917 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/ainetwork/__init__.py
--rw-r--r--   0        0        0     1781 2023-12-18 12:05:45.467514 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/ainetwork/toolkit.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.467736 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/amadeus/__init__.py
--rw-r--r--   0        0        0      961 2023-12-18 12:05:45.468139 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/amadeus/toolkit.py
--rw-r--r--   0        0        0     1035 2023-12-18 12:05:45.468506 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/azure_cognitive_services.py
--rw-r--r--   0        0        0      397 2023-12-18 12:05:45.468850 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/base.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.469039 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/clickup/__init__.py
--rw-r--r--   0        0        0     3594 2023-12-18 12:05:45.469520 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/clickup/toolkit.py
--rw-r--r--   0        0        0     1091 2023-12-18 12:05:45.470285 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/csv/__init__.py
--rw-r--r--   0        0        0      177 2023-12-18 12:05:45.470616 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/file_management/__init__.py
--rw-r--r--   0        0        0     3250 2023-12-18 12:05:45.471261 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/file_management/toolkit.py
--rw-r--r--   0        0        0       22 2023-12-18 12:05:45.471646 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/github/__init__.py
--rw-r--r--   0        0        0    10135 2023-12-21 15:51:51.776418 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/github/toolkit.py
--rw-r--r--   0        0        0       22 2023-12-18 12:05:45.472496 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gitlab/__init__.py
--rw-r--r--   0        0        0     2904 2023-12-18 12:05:45.472989 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gitlab/toolkit.py
--rw-r--r--   0        0        0       21 2023-12-18 12:05:45.473348 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gmail/__init__.py
--rw-r--r--   0        0        0     2045 2023-12-18 12:05:45.473719 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gmail/toolkit.py
--rw-r--r--   0        0        0       20 2023-12-18 12:05:45.474062 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/jira/__init__.py
--rw-r--r--   0        0        0     2227 2023-12-18 12:05:45.474505 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/jira/toolkit.py
--rw-r--r--   0        0        0       18 2023-12-18 12:05:45.474878 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/__init__.py
--rw-r--r--   0        0        0     1894 2023-12-18 12:05:45.475275 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/base.py
--rw-r--r--   0        0        0     3288 2023-12-18 12:05:45.475625 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/prompt.py
--rw-r--r--   0        0        0      595 2023-12-18 12:05:45.475954 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/toolkit.py
--rw-r--r--   0        0        0       23 2023-12-18 12:05:45.476309 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/multion/__init__.py
--rw-r--r--   0        0        0     1190 2023-12-18 12:05:45.476676 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/multion/toolkit.py
--rw-r--r--   0        0        0       19 2023-12-18 12:05:45.477080 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nasa/__init__.py
--rw-r--r--   0        0        0     1931 2023-12-18 12:05:45.477469 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nasa/toolkit.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.477737 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nla/__init__.py
--rw-r--r--   0        0        0     2038 2023-12-18 12:05:45.478181 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nla/tool.py
--rw-r--r--   0        0        0     4228 2023-12-18 12:05:45.478766 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nla/toolkit.py
--rw-r--r--   0        0        0       25 2023-12-18 12:05:45.479165 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/office365/__init__.py
--rw-r--r--   0        0        0     1812 2023-12-18 12:05:45.479543 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/office365/toolkit.py
--rw-r--r--   0        0        0       26 2023-12-18 12:05:45.479989 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/__init__.py
--rw-r--r--   0        0        0     2870 2023-12-18 12:05:45.480437 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/base.py
--rw-r--r--   0        0        0    12987 2023-12-18 12:05:45.481612 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/planner.py
--rw-r--r--   0        0        0    19382 2023-12-18 12:05:45.482208 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/planner_prompt.py
--rw-r--r--   0        0        0     3174 2023-12-18 12:05:45.482651 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/prompt.py
--rw-r--r--   0        0        0     2557 2023-12-18 12:05:45.483038 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/spec.py
--rw-r--r--   0        0        0     3317 2023-12-18 12:05:45.483317 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/toolkit.py
--rw-r--r--   0        0        0      174 2023-12-18 12:05:45.483588 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/playwright/__init__.py
--rw-r--r--   0        0        0     4274 2023-12-18 12:05:45.483947 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/playwright/toolkit.py
--rw-r--r--   0        0        0       22 2023-12-18 12:05:45.484213 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/__init__.py
--rw-r--r--   0        0        0     2507 2023-12-18 12:05:45.484545 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/base.py
--rw-r--r--   0        0        0     2649 2023-12-18 12:05:45.484863 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/chat_base.py
--rw-r--r--   0        0        0     5098 2023-12-18 12:05:45.485270 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/prompt.py
--rw-r--r--   0        0        0     3771 2023-12-18 12:05:45.485517 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/toolkit.py
--rw-r--r--   0        0        0       21 2023-12-18 12:05:45.485842 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/slack/__init__.py
--rw-r--r--   0        0        0     1114 2023-12-18 12:05:45.486216 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/slack/toolkit.py
--rw-r--r--   0        0        0       23 2023-12-18 12:05:45.486932 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/__init__.py
--rw-r--r--   0        0        0     2355 2023-12-18 12:05:45.487251 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/base.py
--rw-r--r--   0        0        0     2297 2023-12-18 12:05:45.487719 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/prompt.py
--rw-r--r--   0        0        0     1084 2023-12-18 12:05:45.488627 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/toolkit.py
--rw-r--r--   0        0        0       17 2023-12-18 12:05:45.489163 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/__init__.py
--rw-r--r--   0        0        0     3866 2023-12-18 12:05:45.489752 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/base.py
--rw-r--r--   0        0        0     2727 2023-12-18 12:05:45.490038 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/prompt.py
--rw-r--r--   0        0        0     2862 2023-12-18 12:05:45.490548 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/toolkit.py
--rw-r--r--   0        0        0       21 2023-12-18 12:05:45.490899 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/steam/__init__.py
--rw-r--r--   0        0        0     1465 2023-12-18 12:05:45.491412 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/steam/toolkit.py
--rw-r--r--   0        0        0     1095 2023-12-18 12:05:45.491874 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/xorbits/__init__.py
--rw-r--r--   0        0        0       22 2023-12-18 12:05:45.492214 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/zapier/__init__.py
--rw-r--r--   0        0        0     1933 2023-12-18 12:05:45.492667 gigachain_community-0.0.6.1/langchain_community/agent_toolkits/zapier/toolkit.py
--rw-r--r--   0        0        0    58113 2023-12-21 15:51:51.777487 gigachain_community-0.0.6.1/langchain_community/cache.py
--rw-r--r--   0        0        0     2671 2023-12-18 12:05:45.494039 gigachain_community-0.0.6.1/langchain_community/callbacks/__init__.py
--rw-r--r--   0        0        0    14408 2023-12-18 12:05:45.494709 gigachain_community-0.0.6.1/langchain_community/callbacks/aim_callback.py
--rw-r--r--   0        0        0    14820 2023-12-18 12:05:45.495433 gigachain_community-0.0.6.1/langchain_community/callbacks/argilla_callback.py
--rw-r--r--   0        0        0     7438 2023-12-18 12:05:45.496089 gigachain_community-0.0.6.1/langchain_community/callbacks/arize_callback.py
--rw-r--r--   0        0        0    11266 2023-12-18 12:05:45.496863 gigachain_community-0.0.6.1/langchain_community/callbacks/arthur_callback.py
--rw-r--r--   0        0        0    18897 2023-12-18 12:05:45.497277 gigachain_community-0.0.6.1/langchain_community/callbacks/clearml_callback.py
--rw-r--r--   0        0        0    23193 2023-12-18 12:05:45.497760 gigachain_community-0.0.6.1/langchain_community/callbacks/comet_ml_callback.py
--rw-r--r--   0        0        0     6340 2023-12-18 12:05:45.498346 gigachain_community-0.0.6.1/langchain_community/callbacks/confident_callback.py
--rw-r--r--   0        0        0     6498 2023-12-18 12:05:45.498833 gigachain_community-0.0.6.1/langchain_community/callbacks/context_callback.py
--rw-r--r--   0        0        0    13138 2023-12-18 12:05:45.499879 gigachain_community-0.0.6.1/langchain_community/callbacks/flyte_callback.py
--rw-r--r--   0        0        0     2587 2023-12-18 12:05:45.500223 gigachain_community-0.0.6.1/langchain_community/callbacks/human.py
--rw-r--r--   0        0        0     9224 2023-12-18 12:05:45.500727 gigachain_community-0.0.6.1/langchain_community/callbacks/infino_callback.py
--rw-r--r--   0        0        0    13880 2023-12-18 12:05:45.501822 gigachain_community-0.0.6.1/langchain_community/callbacks/labelstudio_callback.py
--rw-r--r--   0        0        0    20527 2023-12-18 12:05:45.502476 gigachain_community-0.0.6.1/langchain_community/callbacks/llmonitor_callback.py
--rw-r--r--   0        0        0     1891 2023-12-18 12:05:45.503349 gigachain_community-0.0.6.1/langchain_community/callbacks/manager.py
--rw-r--r--   0        0        0    24156 2023-12-18 12:05:45.503686 gigachain_community-0.0.6.1/langchain_community/callbacks/mlflow_callback.py
--rw-r--r--   0        0        0     7309 2023-12-18 12:05:45.504428 gigachain_community-0.0.6.1/langchain_community/callbacks/openai_info.py
--rw-r--r--   0        0        0     5535 2023-12-18 12:05:45.505111 gigachain_community-0.0.6.1/langchain_community/callbacks/promptlayer_callback.py
--rw-r--r--   0        0        0     8758 2023-12-18 12:05:45.506012 gigachain_community-0.0.6.1/langchain_community/callbacks/sagemaker_callback.py
--rw-r--r--   0        0        0     3211 2023-12-18 12:05:45.506509 gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/__init__.py
--rw-r--r--   0        0        0     5435 2023-12-18 12:05:45.507080 gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/mutable_expander.py
--rw-r--r--   0        0        0    15534 2023-12-18 12:05:45.507739 gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/streamlit_callback_handler.py
--rw-r--r--   0        0        0      498 2023-12-18 12:05:45.508143 gigachain_community-0.0.6.1/langchain_community/callbacks/tracers/__init__.py
--rw-r--r--   0        0        0     4713 2023-12-21 15:51:51.778605 gigachain_community-0.0.6.1/langchain_community/callbacks/tracers/comet.py
--rw-r--r--   0        0        0    19114 2023-12-18 12:05:45.509395 gigachain_community-0.0.6.1/langchain_community/callbacks/tracers/wandb.py
--rw-r--r--   0        0        0     4526 2023-12-18 12:05:45.510089 gigachain_community-0.0.6.1/langchain_community/callbacks/trubrics_callback.py
--rw-r--r--   0        0        0     8505 2023-12-18 12:05:45.510787 gigachain_community-0.0.6.1/langchain_community/callbacks/utils.py
--rw-r--r--   0        0        0    20601 2023-12-18 12:05:45.511652 gigachain_community-0.0.6.1/langchain_community/callbacks/wandb_callback.py
--rw-r--r--   0        0        0     8067 2023-12-18 12:05:45.512609 gigachain_community-0.0.6.1/langchain_community/callbacks/whylabs_callback.py
--rw-r--r--   0        0        0      452 2023-12-18 12:05:45.513350 gigachain_community-0.0.6.1/langchain_community/chat_loaders/__init__.py
--rw-r--r--   0        0        0      444 2023-12-18 12:05:45.513788 gigachain_community-0.0.6.1/langchain_community/chat_loaders/base.py
--rw-r--r--   0        0        0     2462 2023-12-18 12:05:45.514215 gigachain_community-0.0.6.1/langchain_community/chat_loaders/facebook_messenger.py
--rw-r--r--   0        0        0     4048 2023-12-18 12:05:45.514652 gigachain_community-0.0.6.1/langchain_community/chat_loaders/gmail.py
--rw-r--r--   0        0        0     6768 2023-12-21 15:51:51.779551 gigachain_community-0.0.6.1/langchain_community/chat_loaders/imessage.py
--rw-r--r--   0        0        0     5742 2023-12-18 12:05:45.515939 gigachain_community-0.0.6.1/langchain_community/chat_loaders/langsmith.py
--rw-r--r--   0        0        0     3112 2023-12-18 12:05:45.516606 gigachain_community-0.0.6.1/langchain_community/chat_loaders/slack.py
--rw-r--r--   0        0        0     5390 2023-12-18 12:05:45.517563 gigachain_community-0.0.6.1/langchain_community/chat_loaders/telegram.py
--rw-r--r--   0        0        0     3270 2023-12-18 12:05:45.517951 gigachain_community-0.0.6.1/langchain_community/chat_loaders/utils.py
--rw-r--r--   0        0        0     4280 2023-12-18 12:05:45.518507 gigachain_community-0.0.6.1/langchain_community/chat_loaders/whatsapp.py
--rw-r--r--   0        0        0     2570 2023-12-18 12:05:45.519149 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/__init__.py
--rw-r--r--   0        0        0     4127 2023-12-18 12:05:45.519995 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/astradb.py
--rw-r--r--   0        0        0     2392 2023-12-18 12:05:45.520478 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/cassandra.py
--rw-r--r--   0        0        0     6510 2023-12-18 12:05:45.521121 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/cosmos_db.py
--rw-r--r--   0        0        0     5810 2023-12-18 12:05:45.521680 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/dynamodb.py
--rw-r--r--   0        0        0     6836 2023-12-28 13:38:34.455277 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/elasticsearch.py
--rw-r--r--   0        0        0     1379 2023-12-18 12:05:45.522767 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/file.py
--rw-r--r--   0        0        0     3349 2023-12-18 12:05:45.523479 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/firestore.py
--rw-r--r--   0        0        0      633 2023-12-18 12:05:45.524050 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/in_memory.py
--rw-r--r--   0        0        0     7112 2023-12-18 12:05:45.524641 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/momento.py
--rw-r--r--   0        0        0     2734 2023-12-18 12:05:45.524978 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/mongodb.py
--rw-r--r--   0        0        0     4288 2023-12-18 12:05:45.525750 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/neo4j.py
--rw-r--r--   0        0        0     2659 2023-12-18 12:05:45.526184 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/postgres.py
--rw-r--r--   0        0        0     1971 2023-12-18 12:05:45.526547 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/redis.py
--rw-r--r--   0        0        0     9515 2023-12-18 12:05:45.526952 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/rocksetdb.py
--rw-r--r--   0        0        0    10331 2023-12-18 12:05:45.527836 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/singlestoredb.py
--rw-r--r--   0        0        0     4717 2023-12-18 12:05:45.528385 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/sql.py
--rw-r--r--   0        0        0     1176 2023-12-18 12:05:45.528853 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/streamlit.py
--rw-r--r--   0        0        0     2148 2023-12-18 12:05:45.529276 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/upstash_redis.py
--rw-r--r--   0        0        0     4652 2023-12-18 12:05:45.529848 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/xata.py
--rw-r--r--   0        0        0     6452 2023-12-18 12:05:45.530494 gigachain_community-0.0.6.1/langchain_community/chat_message_histories/zep.py
--rw-r--r--   0        0        0     3403 2023-12-28 13:38:34.456648 gigachain_community-0.0.6.1/langchain_community/chat_models/__init__.py
--rw-r--r--   0        0        0     7963 2023-12-18 12:05:45.531534 gigachain_community-0.0.6.1/langchain_community/chat_models/anthropic.py
--rw-r--r--   0        0        0     7945 2023-12-18 12:05:45.532200 gigachain_community-0.0.6.1/langchain_community/chat_models/anyscale.py
--rw-r--r--   0        0        0    11171 2023-12-28 13:38:34.458109 gigachain_community-0.0.6.1/langchain_community/chat_models/azure_openai.py
--rw-r--r--   0        0        0     6007 2023-12-18 12:05:45.533360 gigachain_community-0.0.6.1/langchain_community/chat_models/azureml_endpoint.py
--rw-r--r--   0        0        0    10083 2023-12-18 12:05:45.534129 gigachain_community-0.0.6.1/langchain_community/chat_models/baichuan.py
--rw-r--r--   0        0        0    12467 2023-12-21 15:51:51.781739 gigachain_community-0.0.6.1/langchain_community/chat_models/baidu_qianfan_endpoint.py
--rw-r--r--   0        0        0     4284 2023-12-18 12:05:45.535676 gigachain_community-0.0.6.1/langchain_community/chat_models/bedrock.py
--rw-r--r--   0        0        0     7571 2023-12-18 12:05:45.536327 gigachain_community-0.0.6.1/langchain_community/chat_models/cohere.py
--rw-r--r--   0        0        0     1250 2023-12-18 12:05:45.537272 gigachain_community-0.0.6.1/langchain_community/chat_models/databricks.py
--rw-r--r--   0        0        0     7874 2023-12-18 12:05:45.537817 gigachain_community-0.0.6.1/langchain_community/chat_models/ernie.py
--rw-r--r--   0        0        0     5586 2023-12-18 12:05:45.538348 gigachain_community-0.0.6.1/langchain_community/chat_models/everlyai.py
--rw-r--r--   0        0        0     3217 2023-12-18 12:05:45.538739 gigachain_community-0.0.6.1/langchain_community/chat_models/fake.py
--rw-r--r--   0        0        0    11742 2023-12-21 15:51:51.782873 gigachain_community-0.0.6.1/langchain_community/chat_models/fireworks.py
--rw-r--r--   0        0        0     6225 2023-12-21 15:51:51.785824 gigachain_community-0.0.6.1/langchain_community/chat_models/gigachat.py
--rw-r--r--   0        0        0    11526 2023-12-18 12:05:45.540634 gigachain_community-0.0.6.1/langchain_community/chat_models/google_palm.py
--rw-r--r--   0        0        0    12749 2023-12-28 13:38:34.459453 gigachain_community-0.0.6.1/langchain_community/chat_models/gpt_router.py
--rw-r--r--   0        0        0     5549 2023-12-28 13:38:34.459951 gigachain_community-0.0.6.1/langchain_community/chat_models/huggingface.py
--rw-r--r--   0        0        0     4202 2023-12-18 12:05:45.541184 gigachain_community-0.0.6.1/langchain_community/chat_models/human.py
--rw-r--r--   0        0        0    10549 2023-12-18 12:05:45.541892 gigachain_community-0.0.6.1/langchain_community/chat_models/hunyuan.py
--rw-r--r--   0        0        0     7654 2023-12-18 12:05:45.542512 gigachain_community-0.0.6.1/langchain_community/chat_models/javelin_ai_gateway.py
--rw-r--r--   0        0        0    15167 2023-12-18 12:05:45.542994 gigachain_community-0.0.6.1/langchain_community/chat_models/jinachat.py
--rw-r--r--   0        0        0    10588 2023-12-18 12:05:45.543497 gigachain_community-0.0.6.1/langchain_community/chat_models/konko.py
--rw-r--r--   0        0        0    15671 2023-12-18 12:05:45.544541 gigachain_community-0.0.6.1/langchain_community/chat_models/litellm.py
--rw-r--r--   0        0        0      967 2023-12-21 15:51:51.787479 gigachain_community-0.0.6.1/langchain_community/chat_models/meta.py
--rw-r--r--   0        0        0     3180 2023-12-18 12:05:45.545432 gigachain_community-0.0.6.1/langchain_community/chat_models/minimax.py
--rw-r--r--   0        0        0     7412 2023-12-18 12:05:45.546073 gigachain_community-0.0.6.1/langchain_community/chat_models/mlflow.py
--rw-r--r--   0        0        0     7241 2023-12-18 12:05:45.546694 gigachain_community-0.0.6.1/langchain_community/chat_models/mlflow_ai_gateway.py
--rw-r--r--   0        0        0    13165 2023-12-28 13:38:34.460887 gigachain_community-0.0.6.1/langchain_community/chat_models/ollama.py
--rw-r--r--   0        0        0    26580 2023-12-21 15:51:51.789695 gigachain_community-0.0.6.1/langchain_community/chat_models/openai.py
--rw-r--r--   0        0        0    11281 2023-12-18 12:05:45.550648 gigachain_community-0.0.6.1/langchain_community/chat_models/pai_eas_endpoint.py
--rw-r--r--   0        0        0     5261 2023-12-18 12:05:45.551381 gigachain_community-0.0.6.1/langchain_community/chat_models/promptlayer_openai.py
--rw-r--r--   0        0        0    14047 2023-12-21 15:51:51.793738 gigachain_community-0.0.6.1/langchain_community/chat_models/tongyi.py
--rw-r--r--   0        0        0    14385 2023-12-28 13:38:34.462031 gigachain_community-0.0.6.1/langchain_community/chat_models/vertexai.py
--rw-r--r--   0        0        0     5119 2023-12-21 15:51:51.797089 gigachain_community-0.0.6.1/langchain_community/chat_models/volcengine_maas.py
--rw-r--r--   0        0        0     9392 2023-12-21 15:51:51.798113 gigachain_community-0.0.6.1/langchain_community/chat_models/yandex.py
--rw-r--r--   0        0        0      549 2023-12-18 12:05:45.555730 gigachain_community-0.0.6.1/langchain_community/docstore/__init__.py
--rw-r--r--   0        0        0     1090 2023-12-18 12:05:45.556119 gigachain_community-0.0.6.1/langchain_community/docstore/arbitrary_fn.py
--rw-r--r--   0        0        0      833 2023-12-18 12:05:45.556954 gigachain_community-0.0.6.1/langchain_community/docstore/base.py
--rw-r--r--   0        0        0       70 2023-12-18 12:05:45.557366 gigachain_community-0.0.6.1/langchain_community/docstore/document.py
--rw-r--r--   0        0        0     1610 2023-12-18 12:05:45.557773 gigachain_community-0.0.6.1/langchain_community/docstore/in_memory.py
--rw-r--r--   0        0        0     1487 2023-12-18 12:05:45.558219 gigachain_community-0.0.6.1/langchain_community/docstore/wikipedia.py
--rw-r--r--   0        0        0    16095 2023-12-28 13:38:34.463521 gigachain_community-0.0.6.1/langchain_community/document_loaders/__init__.py
--rw-r--r--   0        0        0     2841 2023-12-18 12:05:45.559522 gigachain_community-0.0.6.1/langchain_community/document_loaders/acreom.py
--rw-r--r--   0        0        0    10240 2023-12-18 12:05:45.560037 gigachain_community-0.0.6.1/langchain_community/document_loaders/airbyte.py
--rw-r--r--   0        0        0      815 2023-12-18 12:05:45.560483 gigachain_community-0.0.6.1/langchain_community/document_loaders/airbyte_json.py
--rw-r--r--   0        0        0     1281 2023-12-18 12:05:45.560843 gigachain_community-0.0.6.1/langchain_community/document_loaders/airtable.py
--rw-r--r--   0        0        0     2759 2023-12-18 12:05:45.561222 gigachain_community-0.0.6.1/langchain_community/document_loaders/apify_dataset.py
--rw-r--r--   0        0        0     5272 2023-12-18 12:05:45.561756 gigachain_community-0.0.6.1/langchain_community/document_loaders/arcgis_loader.py
--rw-r--r--   0        0        0      879 2023-12-28 13:38:34.464672 gigachain_community-0.0.6.1/langchain_community/document_loaders/arxiv.py
--rw-r--r--   0        0        0     4218 2023-12-18 12:05:45.562628 gigachain_community-0.0.6.1/langchain_community/document_loaders/assemblyai.py
--rw-r--r--   0        0        0     8129 2023-12-18 12:05:45.563265 gigachain_community-0.0.6.1/langchain_community/document_loaders/async_html.py
--rw-r--r--   0        0        0      563 2023-12-18 12:05:45.563932 gigachain_community-0.0.6.1/langchain_community/document_loaders/azlyrics.py
--rw-r--r--   0        0        0     1545 2023-12-18 12:05:45.564679 gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_ai_data.py
--rw-r--r--   0        0        0     1582 2023-12-18 12:05:45.565135 gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_blob_storage_container.py
--rw-r--r--   0        0        0     1644 2023-12-18 12:05:45.565693 gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_blob_storage_file.py
--rw-r--r--   0        0        0     1857 2023-12-18 12:05:45.566128 gigachain_community-0.0.6.1/langchain_community/document_loaders/baiducloud_bos_directory.py
--rw-r--r--   0        0        0     1931 2023-12-18 12:05:45.566645 gigachain_community-0.0.6.1/langchain_community/document_loaders/baiducloud_bos_file.py
--rw-r--r--   0        0        0     3175 2023-12-18 12:05:45.567038 gigachain_community-0.0.6.1/langchain_community/document_loaders/base.py
--rw-r--r--   0        0        0     6992 2023-12-18 12:05:45.567647 gigachain_community-0.0.6.1/langchain_community/document_loaders/base_o365.py
--rw-r--r--   0        0        0     3922 2023-12-18 12:05:45.568154 gigachain_community-0.0.6.1/langchain_community/document_loaders/bibtex.py
--rw-r--r--   0        0        0     3673 2023-12-18 12:05:45.568593 gigachain_community-0.0.6.1/langchain_community/document_loaders/bigquery.py
--rw-r--r--   0        0        0     2736 2023-12-18 12:05:45.569329 gigachain_community-0.0.6.1/langchain_community/document_loaders/bilibili.py
--rw-r--r--   0        0        0    10302 2023-12-18 12:05:45.569842 gigachain_community-0.0.6.1/langchain_community/document_loaders/blackboard.py
--rw-r--r--   0        0        0      374 2023-12-18 12:05:45.570390 gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/__init__.py
--rw-r--r--   0        0        0     5340 2023-12-18 12:05:45.571009 gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/file_system.py
--rw-r--r--   0        0        0     6636 2023-12-18 12:05:45.571991 gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/schema.py
--rw-r--r--   0        0        0     1526 2023-12-18 12:05:45.572418 gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/youtube_audio.py
--rw-r--r--   0        0        0     5709 2023-12-18 12:05:45.573013 gigachain_community-0.0.6.1/langchain_community/document_loaders/blockchain.py
--rw-r--r--   0        0        0     1047 2023-12-18 12:05:45.573408 gigachain_community-0.0.6.1/langchain_community/document_loaders/brave_search.py
--rw-r--r--   0        0        0     2124 2023-12-18 12:05:45.573742 gigachain_community-0.0.6.1/langchain_community/document_loaders/browserless.py
--rw-r--r--   0        0        0     1986 2023-12-18 12:05:45.574097 gigachain_community-0.0.6.1/langchain_community/document_loaders/chatgpt.py
--rw-r--r--   0        0        0     2746 2023-12-18 12:05:45.574483 gigachain_community-0.0.6.1/langchain_community/document_loaders/chromium.py
--rw-r--r--   0        0        0      527 2023-12-18 12:05:45.575010 gigachain_community-0.0.6.1/langchain_community/document_loaders/college_confidential.py
--rw-r--r--   0        0        0     3269 2023-12-18 12:05:45.575773 gigachain_community-0.0.6.1/langchain_community/document_loaders/concurrent.py
--rw-r--r--   0        0        0    27612 2023-12-18 12:05:45.576326 gigachain_community-0.0.6.1/langchain_community/document_loaders/confluence.py
--rw-r--r--   0        0        0     1052 2023-12-18 12:05:45.576792 gigachain_community-0.0.6.1/langchain_community/document_loaders/conllu.py
--rw-r--r--   0        0        0     3649 2023-12-18 12:05:45.577211 gigachain_community-0.0.6.1/langchain_community/document_loaders/couchbase.py
--rw-r--r--   0        0        0     5926 2023-12-18 12:05:45.577752 gigachain_community-0.0.6.1/langchain_community/document_loaders/csv_loader.py
--rw-r--r--   0        0        0     6617 2023-12-18 12:05:45.578545 gigachain_community-0.0.6.1/langchain_community/document_loaders/cube_semantic.py
--rw-r--r--   0        0        0     4937 2023-12-18 12:05:45.579652 gigachain_community-0.0.6.1/langchain_community/document_loaders/datadog_logs.py
--rw-r--r--   0        0        0     1923 2023-12-18 12:05:45.580180 gigachain_community-0.0.6.1/langchain_community/document_loaders/dataframe.py
--rw-r--r--   0        0        0     2054 2023-12-18 12:05:45.580587 gigachain_community-0.0.6.1/langchain_community/document_loaders/diffbot.py
--rw-r--r--   0        0        0     5942 2023-12-28 13:38:34.465530 gigachain_community-0.0.6.1/langchain_community/document_loaders/directory.py
--rw-r--r--   0        0        0     1237 2023-12-18 12:05:45.581567 gigachain_community-0.0.6.1/langchain_community/document_loaders/discord.py
--rw-r--r--   0        0        0     3146 2023-12-28 13:38:34.466027 gigachain_community-0.0.6.1/langchain_community/document_loaders/doc_intelligence.py
--rw-r--r--   0        0        0    13450 2023-12-18 12:05:45.582298 gigachain_community-0.0.6.1/langchain_community/document_loaders/docugami.py
--rw-r--r--   0        0        0     1852 2023-12-21 15:51:51.799258 gigachain_community-0.0.6.1/langchain_community/document_loaders/docusaurus.py
--rw-r--r--   0        0        0     6229 2023-12-18 12:05:45.583083 gigachain_community-0.0.6.1/langchain_community/document_loaders/dropbox.py
--rw-r--r--   0        0        0     3150 2023-12-18 12:05:45.583436 gigachain_community-0.0.6.1/langchain_community/document_loaders/duckdb_loader.py
--rw-r--r--   0        0        0     3834 2023-12-18 12:05:45.583873 gigachain_community-0.0.6.1/langchain_community/document_loaders/email.py
--rw-r--r--   0        0        0     1496 2023-12-18 12:05:45.584255 gigachain_community-0.0.6.1/langchain_community/document_loaders/epub.py
--rw-r--r--   0        0        0     7798 2023-12-18 12:05:45.584871 gigachain_community-0.0.6.1/langchain_community/document_loaders/etherscan.py
--rw-r--r--   0        0        0     5757 2023-12-18 12:05:45.585493 gigachain_community-0.0.6.1/langchain_community/document_loaders/evernote.py
--rw-r--r--   0        0        0     1687 2023-12-18 12:05:45.585995 gigachain_community-0.0.6.1/langchain_community/document_loaders/excel.py
--rw-r--r--   0        0        0     1270 2023-12-18 12:05:45.586905 gigachain_community-0.0.6.1/langchain_community/document_loaders/facebook_chat.py
--rw-r--r--   0        0        0     2254 2023-12-18 12:05:45.587314 gigachain_community-0.0.6.1/langchain_community/document_loaders/fauna.py
--rw-r--r--   0        0        0     1543 2023-12-18 12:05:45.587742 gigachain_community-0.0.6.1/langchain_community/document_loaders/figma.py
--rw-r--r--   0        0        0     2118 2023-12-18 12:05:45.588204 gigachain_community-0.0.6.1/langchain_community/document_loaders/gcs_directory.py
--rw-r--r--   0        0        0     3138 2023-12-28 13:38:34.466902 gigachain_community-0.0.6.1/langchain_community/document_loaders/gcs_file.py
--rw-r--r--   0        0        0     6390 2023-12-18 12:05:45.589033 gigachain_community-0.0.6.1/langchain_community/document_loaders/generic.py
--rw-r--r--   0        0        0     2518 2023-12-18 12:05:45.589513 gigachain_community-0.0.6.1/langchain_community/document_loaders/geodataframe.py
--rw-r--r--   0        0        0     4116 2023-12-18 12:05:45.590181 gigachain_community-0.0.6.1/langchain_community/document_loaders/git.py
--rw-r--r--   0        0        0     3453 2023-12-18 12:05:45.590495 gigachain_community-0.0.6.1/langchain_community/document_loaders/gitbook.py
--rw-r--r--   0        0        0     6973 2023-12-18 12:05:45.590928 gigachain_community-0.0.6.1/langchain_community/document_loaders/github.py
--rw-r--r--   0        0        0     5096 2023-12-18 12:05:45.591407 gigachain_community-0.0.6.1/langchain_community/document_loaders/google_speech_to_text.py
--rw-r--r--   0        0        0    14225 2023-12-18 12:05:45.592229 gigachain_community-0.0.6.1/langchain_community/document_loaders/googledrive.py
--rw-r--r--   0        0        0      928 2023-12-18 12:05:45.592507 gigachain_community-0.0.6.1/langchain_community/document_loaders/gutenberg.py
--rw-r--r--   0        0        0     1557 2023-12-18 12:05:45.592815 gigachain_community-0.0.6.1/langchain_community/document_loaders/helpers.py
--rw-r--r--   0        0        0     2075 2023-12-18 12:05:45.593049 gigachain_community-0.0.6.1/langchain_community/document_loaders/hn.py
--rw-r--r--   0        0        0     1158 2023-12-18 12:05:45.593334 gigachain_community-0.0.6.1/langchain_community/document_loaders/html.py
--rw-r--r--   0        0        0     2045 2023-12-28 13:38:34.467659 gigachain_community-0.0.6.1/langchain_community/document_loaders/html_bs.py
--rw-r--r--   0        0        0     3208 2023-12-18 12:05:45.594186 gigachain_community-0.0.6.1/langchain_community/document_loaders/hugging_face_dataset.py
--rw-r--r--   0        0        0     7642 2023-12-18 12:05:45.594633 gigachain_community-0.0.6.1/langchain_community/document_loaders/ifixit.py
--rw-r--r--   0        0        0     1173 2023-12-18 12:05:45.595279 gigachain_community-0.0.6.1/langchain_community/document_loaders/image.py
--rw-r--r--   0        0        0     3594 2023-12-18 12:05:45.596852 gigachain_community-0.0.6.1/langchain_community/document_loaders/image_captions.py
--rw-r--r--   0        0        0      477 2023-12-18 12:05:45.597367 gigachain_community-0.0.6.1/langchain_community/document_loaders/imsdb.py
--rw-r--r--   0        0        0     1688 2023-12-18 12:05:45.597983 gigachain_community-0.0.6.1/langchain_community/document_loaders/iugu.py
--rw-r--r--   0        0        0     3705 2023-12-18 12:05:45.598689 gigachain_community-0.0.6.1/langchain_community/document_loaders/joplin.py
--rw-r--r--   0        0        0     5907 2023-12-18 12:05:45.599520 gigachain_community-0.0.6.1/langchain_community/document_loaders/json_loader.py
--rw-r--r--   0        0        0     6058 2023-12-21 15:51:51.800637 gigachain_community-0.0.6.1/langchain_community/document_loaders/lakefs.py
--rw-r--r--   0        0        0     2008 2023-12-18 12:05:45.600876 gigachain_community-0.0.6.1/langchain_community/document_loaders/larksuite.py
--rw-r--r--   0        0        0     1818 2023-12-28 13:38:34.468309 gigachain_community-0.0.6.1/langchain_community/document_loaders/markdown.py
--rw-r--r--   0        0        0     3112 2023-12-18 12:05:45.601726 gigachain_community-0.0.6.1/langchain_community/document_loaders/mastodon.py
--rw-r--r--   0        0        0     3282 2023-12-18 12:05:45.602072 gigachain_community-0.0.6.1/langchain_community/document_loaders/max_compute.py
--rw-r--r--   0        0        0     3455 2023-12-18 12:05:45.602561 gigachain_community-0.0.6.1/langchain_community/document_loaders/mediawikidump.py
--rw-r--r--   0        0        0      846 2023-12-18 12:05:45.602927 gigachain_community-0.0.6.1/langchain_community/document_loaders/merge.py
--rw-r--r--   0        0        0     2540 2023-12-28 13:38:34.469196 gigachain_community-0.0.6.1/langchain_community/document_loaders/mhtml.py
--rw-r--r--   0        0        0     3074 2023-12-18 12:05:45.603564 gigachain_community-0.0.6.1/langchain_community/document_loaders/modern_treasury.py
--rw-r--r--   0        0        0     2522 2023-12-18 12:05:45.604001 gigachain_community-0.0.6.1/langchain_community/document_loaders/mongodb.py
--rw-r--r--   0        0        0     4296 2023-12-18 12:05:45.604511 gigachain_community-0.0.6.1/langchain_community/document_loaders/news.py
--rw-r--r--   0        0        0     4301 2023-12-18 12:05:45.605118 gigachain_community-0.0.6.1/langchain_community/document_loaders/notebook.py
--rw-r--r--   0        0        0      814 2023-12-18 12:05:45.605522 gigachain_community-0.0.6.1/langchain_community/document_loaders/notion.py
--rw-r--r--   0        0        0     6718 2023-12-18 12:05:45.606331 gigachain_community-0.0.6.1/langchain_community/document_loaders/notiondb.py
--rw-r--r--   0        0        0     1088 2023-12-18 12:05:45.606811 gigachain_community-0.0.6.1/langchain_community/document_loaders/nuclia.py
--rw-r--r--   0        0        0     3593 2023-12-18 12:05:45.607288 gigachain_community-0.0.6.1/langchain_community/document_loaders/obs_directory.py
--rw-r--r--   0        0        0     4768 2023-12-18 12:05:45.607862 gigachain_community-0.0.6.1/langchain_community/document_loaders/obs_file.py
--rw-r--r--   0        0        0     6150 2023-12-18 12:05:45.608389 gigachain_community-0.0.6.1/langchain_community/document_loaders/obsidian.py
--rw-r--r--   0        0        0     1770 2023-12-18 12:05:45.608973 gigachain_community-0.0.6.1/langchain_community/document_loaders/odt.py
--rw-r--r--   0        0        0     3381 2023-12-18 12:05:45.609740 gigachain_community-0.0.6.1/langchain_community/document_loaders/onedrive.py
--rw-r--r--   0        0        0     1154 2023-12-18 12:05:45.610167 gigachain_community-0.0.6.1/langchain_community/document_loaders/onedrive_file.py
--rw-r--r--   0        0        0     8074 2023-12-18 12:05:45.610777 gigachain_community-0.0.6.1/langchain_community/document_loaders/onenote.py
--rw-r--r--   0        0        0     1331 2023-12-18 12:05:45.611177 gigachain_community-0.0.6.1/langchain_community/document_loaders/open_city_data.py
--rw-r--r--   0        0        0     1748 2023-12-18 12:05:45.611624 gigachain_community-0.0.6.1/langchain_community/document_loaders/org_mode.py
--rw-r--r--   0        0        0      947 2023-12-28 13:38:34.470016 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/__init__.py
--rw-r--r--   0        0        0    10716 2023-12-18 12:05:45.612771 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/audio.py
--rw-r--r--   0        0        0     4534 2023-12-28 13:38:34.470440 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/doc_intelligence.py
--rw-r--r--   0        0        0    15283 2023-12-18 12:05:45.613557 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/docai.py
--rw-r--r--   0        0        0     2502 2023-12-18 12:05:45.614020 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/generic.py
--rw-r--r--   0        0        0     5767 2023-12-18 12:05:45.614963 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/grobid.py
--rw-r--r--   0        0        0      109 2023-12-18 12:05:45.615405 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/html/__init__.py
--rw-r--r--   0        0        0     1609 2023-12-18 12:05:45.615795 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/html/bs4.py
--rw-r--r--   0        0        0      136 2023-12-18 12:05:45.616382 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/__init__.py
--rw-r--r--   0        0        0     3745 2023-12-18 12:05:45.616807 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/cobol.py
--rw-r--r--   0        0        0      495 2023-12-18 12:05:45.617260 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/code_segmenter.py
--rw-r--r--   0        0        0     2103 2023-12-18 12:05:45.617642 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/javascript.py
--rw-r--r--   0        0        0     5114 2023-12-18 12:05:45.618562 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/language_parser.py
--rw-r--r--   0        0        0     1679 2023-12-18 12:05:45.618918 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/python.py
--rw-r--r--   0        0        0     1664 2023-12-18 12:05:45.619356 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/msword.py
--rw-r--r--   0        0        0    21006 2023-12-28 13:38:34.472391 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/pdf.py
--rw-r--r--   0        0        0     1214 2023-12-18 12:05:45.621291 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/registry.py
--rw-r--r--   0        0        0      505 2023-12-18 12:05:45.621708 gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/txt.py
--rw-r--r--   0        0        0    26024 2023-12-18 12:05:45.622152 gigachain_community-0.0.6.1/langchain_community/document_loaders/pdf.py
--rw-r--r--   0        0        0     1161 2023-12-18 12:05:45.622618 gigachain_community-0.0.6.1/langchain_community/document_loaders/polars_dataframe.py
--rw-r--r--   0        0        0     2508 2023-12-18 12:05:45.623093 gigachain_community-0.0.6.1/langchain_community/document_loaders/powerpoint.py
--rw-r--r--   0        0        0     1371 2023-12-18 12:05:45.623484 gigachain_community-0.0.6.1/langchain_community/document_loaders/psychic.py
--rw-r--r--   0        0        0     1172 2023-12-18 12:05:45.623852 gigachain_community-0.0.6.1/langchain_community/document_loaders/pubmed.py
--rw-r--r--   0        0        0     3388 2023-12-18 12:05:45.624242 gigachain_community-0.0.6.1/langchain_community/document_loaders/pyspark_dataframe.py
--rw-r--r--   0        0        0      527 2023-12-18 12:05:45.624513 gigachain_community-0.0.6.1/langchain_community/document_loaders/python.py
--rw-r--r--   0        0        0     8426 2023-12-18 12:05:45.624979 gigachain_community-0.0.6.1/langchain_community/document_loaders/quip.py
--rw-r--r--   0        0        0     6928 2023-12-18 12:05:45.625504 gigachain_community-0.0.6.1/langchain_community/document_loaders/readthedocs.py
--rw-r--r--   0        0        0    11591 2023-12-18 12:05:45.626220 gigachain_community-0.0.6.1/langchain_community/document_loaders/recursive_url_loader.py
--rw-r--r--   0        0        0     4584 2023-12-18 12:05:45.626883 gigachain_community-0.0.6.1/langchain_community/document_loaders/reddit.py
--rw-r--r--   0        0        0      705 2023-12-18 12:05:45.627281 gigachain_community-0.0.6.1/langchain_community/document_loaders/roam.py
--rw-r--r--   0        0        0     4604 2023-12-18 12:05:45.627882 gigachain_community-0.0.6.1/langchain_community/document_loaders/rocksetdb.py
--rw-r--r--   0        0        0     4936 2023-12-28 13:38:34.473933 gigachain_community-0.0.6.1/langchain_community/document_loaders/rspace.py
--rw-r--r--   0        0        0     4895 2023-12-18 12:05:45.629065 gigachain_community-0.0.6.1/langchain_community/document_loaders/rss.py
--rw-r--r--   0        0        0     1832 2023-12-18 12:05:45.629524 gigachain_community-0.0.6.1/langchain_community/document_loaders/rst.py
--rw-r--r--   0        0        0     2061 2023-12-18 12:05:45.629875 gigachain_community-0.0.6.1/langchain_community/document_loaders/rtf.py
--rw-r--r--   0        0        0     5759 2023-12-18 12:05:45.630757 gigachain_community-0.0.6.1/langchain_community/document_loaders/s3_directory.py
--rw-r--r--   0        0        0     5412 2023-12-18 12:05:45.631317 gigachain_community-0.0.6.1/langchain_community/document_loaders/s3_file.py
--rw-r--r--   0        0        0     2318 2023-12-18 12:05:45.631739 gigachain_community-0.0.6.1/langchain_community/document_loaders/sharepoint.py
--rw-r--r--   0        0        0     8530 2023-12-18 12:05:45.632550 gigachain_community-0.0.6.1/langchain_community/document_loaders/sitemap.py
--rw-r--r--   0        0        0     4131 2023-12-18 12:05:45.633088 gigachain_community-0.0.6.1/langchain_community/document_loaders/slack_directory.py
--rw-r--r--   0        0        0     4799 2023-12-18 12:05:45.633700 gigachain_community-0.0.6.1/langchain_community/document_loaders/snowflake_loader.py
--rw-r--r--   0        0        0     2004 2023-12-18 12:05:45.634087 gigachain_community-0.0.6.1/langchain_community/document_loaders/spreedly.py
--rw-r--r--   0        0        0      851 2023-12-18 12:05:45.634481 gigachain_community-0.0.6.1/langchain_community/document_loaders/srt.py
--rw-r--r--   0        0        0     1811 2023-12-18 12:05:45.634845 gigachain_community-0.0.6.1/langchain_community/document_loaders/stripe.py
--rw-r--r--   0        0        0     9008 2023-12-18 12:05:45.635464 gigachain_community-0.0.6.1/langchain_community/document_loaders/telegram.py
--rw-r--r--   0        0        0     1783 2023-12-18 12:05:45.635902 gigachain_community-0.0.6.1/langchain_community/document_loaders/tencent_cos_directory.py
--rw-r--r--   0        0        0     1700 2023-12-28 13:38:34.474832 gigachain_community-0.0.6.1/langchain_community/document_loaders/tencent_cos_file.py
--rw-r--r--   0        0        0     3024 2023-12-18 12:05:45.636604 gigachain_community-0.0.6.1/langchain_community/document_loaders/tensorflow_datasets.py
--rw-r--r--   0        0        0     2010 2023-12-18 12:05:45.636912 gigachain_community-0.0.6.1/langchain_community/document_loaders/text.py
--rw-r--r--   0        0        0      950 2023-12-18 12:05:45.637263 gigachain_community-0.0.6.1/langchain_community/document_loaders/tomarkdown.py
--rw-r--r--   0        0        0     1572 2023-12-18 12:05:45.637651 gigachain_community-0.0.6.1/langchain_community/document_loaders/toml.py
--rw-r--r--   0        0        0     6561 2023-12-18 12:05:45.638520 gigachain_community-0.0.6.1/langchain_community/document_loaders/trello.py
--rw-r--r--   0        0        0     1293 2023-12-18 12:05:45.639113 gigachain_community-0.0.6.1/langchain_community/document_loaders/tsv.py
--rw-r--r--   0        0        0     3438 2023-12-18 12:05:45.639710 gigachain_community-0.0.6.1/langchain_community/document_loaders/twitter.py
--rw-r--r--   0        0        0    13793 2023-12-18 12:05:45.640484 gigachain_community-0.0.6.1/langchain_community/document_loaders/unstructured.py
--rw-r--r--   0        0        0     6019 2023-12-18 12:05:45.641072 gigachain_community-0.0.6.1/langchain_community/document_loaders/url.py
--rw-r--r--   0        0        0     7590 2023-12-18 12:05:45.641572 gigachain_community-0.0.6.1/langchain_community/document_loaders/url_playwright.py
--rw-r--r--   0        0        0     6640 2023-12-18 12:05:45.642173 gigachain_community-0.0.6.1/langchain_community/document_loaders/url_selenium.py
--rw-r--r--   0        0        0     1682 2023-12-18 12:05:45.642509 gigachain_community-0.0.6.1/langchain_community/document_loaders/weather.py
--rw-r--r--   0        0        0    10164 2023-12-18 12:05:45.643373 gigachain_community-0.0.6.1/langchain_community/document_loaders/web_base.py
--rw-r--r--   0        0        0     1770 2023-12-18 12:05:45.643807 gigachain_community-0.0.6.1/langchain_community/document_loaders/whatsapp_chat.py
--rw-r--r--   0        0        0     2142 2023-12-18 12:05:45.644447 gigachain_community-0.0.6.1/langchain_community/document_loaders/wikipedia.py
--rw-r--r--   0        0        0     4583 2023-12-18 12:05:45.645033 gigachain_community-0.0.6.1/langchain_community/document_loaders/word_document.py
--rw-r--r--   0        0        0     1489 2023-12-18 12:05:45.645378 gigachain_community-0.0.6.1/langchain_community/document_loaders/xml.py
--rw-r--r--   0        0        0     1119 2023-12-18 12:05:45.645826 gigachain_community-0.0.6.1/langchain_community/document_loaders/xorbits.py
--rw-r--r--   0        0        0    14798 2023-12-18 12:05:45.647523 gigachain_community-0.0.6.1/langchain_community/document_loaders/youtube.py
--rw-r--r--   0        0        0     1849 2023-12-18 12:05:45.647949 gigachain_community-0.0.6.1/langchain_community/document_transformers/__init__.py
--rw-r--r--   0        0        0     5214 2023-12-21 15:51:51.802594 gigachain_community-0.0.6.1/langchain_community/document_transformers/beautiful_soup_transformer.py
--rw-r--r--   0        0        0     3469 2023-12-18 12:05:45.648810 gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_extract.py
--rw-r--r--   0        0        0     2193 2023-12-18 12:05:45.649227 gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_qa.py
--rw-r--r--   0        0        0     2337 2023-12-18 12:05:45.649692 gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_translate.py
--rw-r--r--   0        0        0     7882 2023-12-18 12:05:45.650106 gigachain_community-0.0.6.1/langchain_community/document_transformers/embeddings_redundant_filter.py
--rw-r--r--   0        0        0     4133 2023-12-18 12:05:45.650606 gigachain_community-0.0.6.1/langchain_community/document_transformers/google_translate.py
--rw-r--r--   0        0        0     1834 2023-12-18 12:05:45.650961 gigachain_community-0.0.6.1/langchain_community/document_transformers/html2text.py
--rw-r--r--   0        0        0     1410 2023-12-18 12:05:45.653916 gigachain_community-0.0.6.1/langchain_community/document_transformers/long_context_reorder.py
--rw-r--r--   0        0        0     1475 2023-12-18 12:05:45.654343 gigachain_community-0.0.6.1/langchain_community/document_transformers/nuclia_text_transform.py
--rw-r--r--   0        0        0     6243 2023-12-18 12:05:45.655195 gigachain_community-0.0.6.1/langchain_community/document_transformers/openai_functions.py
--rw-r--r--   0        0        0     6033 2023-12-18 12:05:45.656355 gigachain_community-0.0.6.1/langchain_community/document_transformers/xsl/html_chunks_with_headers.xslt
--rw-r--r--   0        0        0     6534 2023-12-28 13:38:34.477982 gigachain_community-0.0.6.1/langchain_community/embeddings/__init__.py
--rw-r--r--   0        0        0     9614 2023-12-18 12:05:45.657933 gigachain_community-0.0.6.1/langchain_community/embeddings/aleph_alpha.py
--rw-r--r--   0        0        0     1865 2023-12-18 12:05:45.658580 gigachain_community-0.0.6.1/langchain_community/embeddings/awa.py
--rw-r--r--   0        0        0     7063 2023-12-21 15:51:51.803470 gigachain_community-0.0.6.1/langchain_community/embeddings/azure_openai.py
--rw-r--r--   0        0        0     4827 2023-12-21 15:51:51.804348 gigachain_community-0.0.6.1/langchain_community/embeddings/baidu_qianfan_endpoint.py
--rw-r--r--   0        0        0     6723 2023-12-18 12:05:45.660498 gigachain_community-0.0.6.1/langchain_community/embeddings/bedrock.py
--rw-r--r--   0        0        0     2725 2023-12-18 12:05:45.660858 gigachain_community-0.0.6.1/langchain_community/embeddings/bookend.py
--rw-r--r--   0        0        0     5454 2023-12-18 12:05:45.661406 gigachain_community-0.0.6.1/langchain_community/embeddings/clarifai.py
--rw-r--r--   0        0        0     2869 2023-12-18 12:05:45.661963 gigachain_community-0.0.6.1/langchain_community/embeddings/cloudflare_workersai.py
--rw-r--r--   0        0        0     4660 2023-12-18 12:05:45.662561 gigachain_community-0.0.6.1/langchain_community/embeddings/cohere.py
--rw-r--r--   0        0        0     4956 2023-12-18 12:05:45.663235 gigachain_community-0.0.6.1/langchain_community/embeddings/dashscope.py
--rw-r--r--   0        0        0     1310 2023-12-18 12:05:45.663728 gigachain_community-0.0.6.1/langchain_community/embeddings/databricks.py
--rw-r--r--   0        0        0     4426 2023-12-18 12:05:45.664376 gigachain_community-0.0.6.1/langchain_community/embeddings/deepinfra.py
--rw-r--r--   0        0        0     3564 2023-12-21 15:51:51.805303 gigachain_community-0.0.6.1/langchain_community/embeddings/edenai.py
--rw-r--r--   0        0        0     8401 2023-12-18 12:05:45.665414 gigachain_community-0.0.6.1/langchain_community/embeddings/elasticsearch.py
--rw-r--r--   0        0        0     5418 2023-12-28 13:38:34.479313 gigachain_community-0.0.6.1/langchain_community/embeddings/embaas.py
--rw-r--r--   0        0        0     4859 2023-12-18 12:05:45.667020 gigachain_community-0.0.6.1/langchain_community/embeddings/ernie.py
--rw-r--r--   0        0        0     1512 2023-12-18 12:05:45.667503 gigachain_community-0.0.6.1/langchain_community/embeddings/fake.py
--rw-r--r--   0        0        0     3452 2023-12-18 12:05:45.667862 gigachain_community-0.0.6.1/langchain_community/embeddings/fastembed.py
--rw-r--r--   0        0        0     4932 2024-01-09 11:56:52.846703 gigachain_community-0.0.6.1/langchain_community/embeddings/gigachat.py
--rw-r--r--   0        0        0     3272 2023-12-18 12:05:45.668242 gigachain_community-0.0.6.1/langchain_community/embeddings/google_palm.py
--rw-r--r--   0        0        0     1664 2023-12-18 12:05:45.669050 gigachain_community-0.0.6.1/langchain_community/embeddings/gpt4all.py
--rw-r--r--   0        0        0     5274 2023-12-21 15:51:51.806243 gigachain_community-0.0.6.1/langchain_community/embeddings/gradient_ai.py
--rw-r--r--   0        0        0    11898 2023-12-18 12:05:45.670340 gigachain_community-0.0.6.1/langchain_community/embeddings/huggingface.py
--rw-r--r--   0        0        0     3789 2023-12-18 12:05:45.670872 gigachain_community-0.0.6.1/langchain_community/embeddings/huggingface_hub.py
--rw-r--r--   0        0        0    10260 2023-12-18 12:05:45.671375 gigachain_community-0.0.6.1/langchain_community/embeddings/infinity.py
--rw-r--r--   0        0        0     3673 2023-12-18 12:05:45.671824 gigachain_community-0.0.6.1/langchain_community/embeddings/javelin_ai_gateway.py
--rw-r--r--   0        0        0     2607 2023-12-28 13:38:34.482924 gigachain_community-0.0.6.1/langchain_community/embeddings/jina.py
--rw-r--r--   0        0        0     2873 2023-12-18 12:05:45.672526 gigachain_community-0.0.6.1/langchain_community/embeddings/johnsnowlabs.py
--rw-r--r--   0        0        0     4163 2023-12-18 12:05:45.673477 gigachain_community-0.0.6.1/langchain_community/embeddings/llamacpp.py
--rw-r--r--   0        0        0     2096 2023-12-18 12:05:45.673924 gigachain_community-0.0.6.1/langchain_community/embeddings/llm_rails.py
--rw-r--r--   0        0        0    12261 2023-12-27 10:44:25.850461 gigachain_community-0.0.6.1/langchain_community/embeddings/localai.py
--rw-r--r--   0        0        0     4798 2023-12-28 13:38:34.484154 gigachain_community-0.0.6.1/langchain_community/embeddings/minimax.py
--rw-r--r--   0        0        0     2417 2023-12-18 12:05:45.675851 gigachain_community-0.0.6.1/langchain_community/embeddings/mlflow.py
--rw-r--r--   0        0        0     2479 2023-12-18 12:05:45.676322 gigachain_community-0.0.6.1/langchain_community/embeddings/mlflow_gateway.py
--rw-r--r--   0        0        0     2384 2023-12-18 12:05:45.678107 gigachain_community-0.0.6.1/langchain_community/embeddings/modelscope_hub.py
--rw-r--r--   0        0        0     5115 2023-12-18 12:05:45.678704 gigachain_community-0.0.6.1/langchain_community/embeddings/mosaicml.py
--rw-r--r--   0        0        0     2203 2023-12-18 12:05:45.679072 gigachain_community-0.0.6.1/langchain_community/embeddings/nlpcloud.py
--rw-r--r--   0        0        0     3420 2023-12-18 12:05:45.679432 gigachain_community-0.0.6.1/langchain_community/embeddings/octoai_embeddings.py
--rw-r--r--   0        0        0     7691 2023-12-18 12:05:45.680736 gigachain_community-0.0.6.1/langchain_community/embeddings/ollama.py
--rw-r--r--   0        0        0    29207 2023-12-18 12:05:45.681405 gigachain_community-0.0.6.1/langchain_community/embeddings/openai.py
--rw-r--r--   0        0        0     7591 2023-12-18 12:05:45.682567 gigachain_community-0.0.6.1/langchain_community/embeddings/sagemaker_endpoint.py
--rw-r--r--   0        0        0     3807 2023-12-18 12:05:45.683325 gigachain_community-0.0.6.1/langchain_community/embeddings/self_hosted.py
--rw-r--r--   0        0        0     6589 2023-12-18 12:05:45.684011 gigachain_community-0.0.6.1/langchain_community/embeddings/self_hosted_hugging_face.py
--rw-r--r--   0        0        0      189 2023-12-18 12:05:45.684427 gigachain_community-0.0.6.1/langchain_community/embeddings/sentence_transformer.py
--rw-r--r--   0        0        0     3743 2023-12-18 12:05:45.684825 gigachain_community-0.0.6.1/langchain_community/embeddings/spacy_embeddings.py
--rw-r--r--   0        0        0     2413 2023-12-18 12:05:45.685136 gigachain_community-0.0.6.1/langchain_community/embeddings/tensorflow_hub.py
--rw-r--r--   0        0        0    13929 2023-12-28 13:38:34.485210 gigachain_community-0.0.6.1/langchain_community/embeddings/vertexai.py
--rw-r--r--   0        0        0     6255 2023-12-18 12:05:45.687621 gigachain_community-0.0.6.1/langchain_community/embeddings/voyageai.py
--rw-r--r--   0        0        0     3303 2023-12-18 12:05:45.688334 gigachain_community-0.0.6.1/langchain_community/embeddings/xinference.py
--rw-r--r--   0        0        0     5934 2023-12-21 15:51:51.807967 gigachain_community-0.0.6.1/langchain_community/embeddings/yandex.py
--rw-r--r--   0        0        0      930 2023-12-18 12:05:45.688723 gigachain_community-0.0.6.1/langchain_community/graphs/__init__.py
--rw-r--r--   0        0        0     6961 2023-12-18 12:05:45.689581 gigachain_community-0.0.6.1/langchain_community/graphs/arangodb_graph.py
--rw-r--r--   0        0        0     5294 2023-12-18 12:05:45.690457 gigachain_community-0.0.6.1/langchain_community/graphs/falkordb_graph.py
--rw-r--r--   0        0        0     1584 2023-12-18 12:05:45.690984 gigachain_community-0.0.6.1/langchain_community/graphs/graph_document.py
--rw-r--r--   0        0        0     1008 2023-12-18 12:05:45.691396 gigachain_community-0.0.6.1/langchain_community/graphs/graph_store.py
--rw-r--r--   0        0        0     2510 2023-12-18 12:05:45.691835 gigachain_community-0.0.6.1/langchain_community/graphs/hugegraph.py
--rw-r--r--   0        0        0     4227 2023-12-18 12:05:45.692460 gigachain_community-0.0.6.1/langchain_community/graphs/kuzu_graph.py
--rw-r--r--   0        0        0     1644 2023-12-18 12:05:45.692879 gigachain_community-0.0.6.1/langchain_community/graphs/memgraph_graph.py
--rw-r--r--   0        0        0     8112 2023-12-18 12:05:45.693526 gigachain_community-0.0.6.1/langchain_community/graphs/nebula_graph.py
--rw-r--r--   0        0        0     8388 2023-12-18 12:05:45.693954 gigachain_community-0.0.6.1/langchain_community/graphs/neo4j_graph.py
--rw-r--r--   0        0        0     9398 2023-12-18 12:05:45.694545 gigachain_community-0.0.6.1/langchain_community/graphs/neptune_graph.py
--rw-r--r--   0        0        0     6545 2023-12-18 12:05:45.695142 gigachain_community-0.0.6.1/langchain_community/graphs/networkx_graph.py
--rw-r--r--   0        0        0    10118 2023-12-18 12:05:45.695554 gigachain_community-0.0.6.1/langchain_community/graphs/rdf_graph.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.695817 gigachain_community-0.0.6.1/langchain_community/indexes/__init__.py
--rw-r--r--   0        0        0    20960 2023-12-18 12:05:45.696278 gigachain_community-0.0.6.1/langchain_community/indexes/_sql_record_manager.py
--rw-r--r--   0        0        0     5221 2023-12-18 12:05:45.696935 gigachain_community-0.0.6.1/langchain_community/indexes/base.py
--rw-r--r--   0        0        0    23006 2023-12-28 13:38:34.486291 gigachain_community-0.0.6.1/langchain_community/llms/__init__.py
--rw-r--r--   0        0        0     5292 2023-12-18 12:05:45.698661 gigachain_community-0.0.6.1/langchain_community/llms/ai21.py
--rw-r--r--   0        0        0    11493 2023-12-18 12:05:45.699215 gigachain_community-0.0.6.1/langchain_community/llms/aleph_alpha.py
--rw-r--r--   0        0        0     3061 2023-12-18 12:05:45.699586 gigachain_community-0.0.6.1/langchain_community/llms/amazon_api_gateway.py
--rw-r--r--   0        0        0    12356 2023-12-18 12:05:45.700279 gigachain_community-0.0.6.1/langchain_community/llms/anthropic.py
--rw-r--r--   0        0        0    10630 2023-12-18 12:05:45.700779 gigachain_community-0.0.6.1/langchain_community/llms/anyscale.py
--rw-r--r--   0        0        0     9619 2023-12-21 15:51:51.809621 gigachain_community-0.0.6.1/langchain_community/llms/aphrodite.py
--rw-r--r--   0        0        0     4356 2023-12-18 12:05:45.701385 gigachain_community-0.0.6.1/langchain_community/llms/arcee.py
--rw-r--r--   0        0        0     6007 2023-12-18 12:05:45.701968 gigachain_community-0.0.6.1/langchain_community/llms/aviary.py
--rw-r--r--   0        0        0    10251 2023-12-18 12:05:45.702392 gigachain_community-0.0.6.1/langchain_community/llms/azureml_endpoint.py
--rw-r--r--   0        0        0     7293 2023-12-21 15:51:51.810589 gigachain_community-0.0.6.1/langchain_community/llms/baidu_qianfan_endpoint.py
--rw-r--r--   0        0        0     4725 2023-12-18 12:05:45.703913 gigachain_community-0.0.6.1/langchain_community/llms/bananadev.py
--rw-r--r--   0        0        0     3187 2023-12-28 13:38:34.487169 gigachain_community-0.0.6.1/langchain_community/llms/baseten.py
--rw-r--r--   0        0        0     9099 2023-12-18 12:05:45.705119 gigachain_community-0.0.6.1/langchain_community/llms/beam.py
--rw-r--r--   0        0        0    15469 2023-12-28 13:38:34.488257 gigachain_community-0.0.6.1/langchain_community/llms/bedrock.py
--rw-r--r--   0        0        0     6232 2023-12-18 12:05:45.707024 gigachain_community-0.0.6.1/langchain_community/llms/bittensor.py
--rw-r--r--   0        0        0     4044 2023-12-18 12:05:45.707481 gigachain_community-0.0.6.1/langchain_community/llms/cerebriumai.py
--rw-r--r--   0        0        0     3969 2023-12-18 12:05:45.707879 gigachain_community-0.0.6.1/langchain_community/llms/chatglm.py
--rw-r--r--   0        0        0     7262 2023-12-18 12:05:45.708779 gigachain_community-0.0.6.1/langchain_community/llms/clarifai.py
--rw-r--r--   0        0        0     4277 2023-12-18 12:05:45.709389 gigachain_community-0.0.6.1/langchain_community/llms/cloudflare_workersai.py
--rw-r--r--   0        0        0     7970 2023-12-18 12:05:45.710013 gigachain_community-0.0.6.1/langchain_community/llms/cohere.py
--rw-r--r--   0        0        0     4227 2023-12-18 12:05:45.711016 gigachain_community-0.0.6.1/langchain_community/llms/ctransformers.py
--rw-r--r--   0        0        0     4135 2023-12-18 12:05:45.712038 gigachain_community-0.0.6.1/langchain_community/llms/ctranslate2.py
--rw-r--r--   0        0        0    17597 2023-12-28 13:38:34.489750 gigachain_community-0.0.6.1/langchain_community/llms/databricks.py
--rw-r--r--   0        0        0     7006 2023-12-18 12:05:45.713379 gigachain_community-0.0.6.1/langchain_community/llms/deepinfra.py
--rw-r--r--   0        0        0     8615 2023-12-28 13:38:34.491881 gigachain_community-0.0.6.1/langchain_community/llms/deepsparse.py
--rw-r--r--   0        0        0     9463 2023-12-21 15:51:51.811774 gigachain_community-0.0.6.1/langchain_community/llms/edenai.py
--rw-r--r--   0        0        0     2444 2023-12-18 12:05:45.714846 gigachain_community-0.0.6.1/langchain_community/llms/fake.py
--rw-r--r--   0        0        0    11773 2023-12-21 15:51:51.813007 gigachain_community-0.0.6.1/langchain_community/llms/fireworks.py
--rw-r--r--   0        0        0     3762 2023-12-18 12:05:45.715821 gigachain_community-0.0.6.1/langchain_community/llms/forefrontai.py
--rw-r--r--   0        0        0    10505 2023-12-28 13:38:34.495098 gigachain_community-0.0.6.1/langchain_community/llms/gigachat.py
--rw-r--r--   0        0        0     8854 2023-12-21 15:51:51.816031 gigachain_community-0.0.6.1/langchain_community/llms/google_palm.py
--rw-r--r--   0        0        0     5301 2023-12-18 12:05:45.718164 gigachain_community-0.0.6.1/langchain_community/llms/gooseai.py
--rw-r--r--   0        0        0     6525 2023-12-18 12:05:45.718855 gigachain_community-0.0.6.1/langchain_community/llms/gpt4all.py
--rw-r--r--   0        0        0    14199 2023-12-18 12:05:45.719653 gigachain_community-0.0.6.1/langchain_community/llms/gradient_ai.py
--rw-r--r--   0        0        0      664 2023-12-18 12:05:45.720222 gigachain_community-0.0.6.1/langchain_community/llms/grammars/json.gbnf
--rw-r--r--   0        0        0      167 2023-12-18 12:05:45.720572 gigachain_community-0.0.6.1/langchain_community/llms/grammars/list.gbnf
--rw-r--r--   0        0        0     5508 2023-12-18 12:05:45.721443 gigachain_community-0.0.6.1/langchain_community/llms/huggingface_endpoint.py
--rw-r--r--   0        0        0     4658 2023-12-18 12:05:45.721873 gigachain_community-0.0.6.1/langchain_community/llms/huggingface_hub.py
--rw-r--r--   0        0        0     9215 2023-12-18 12:05:45.722687 gigachain_community-0.0.6.1/langchain_community/llms/huggingface_pipeline.py
--rw-r--r--   0        0        0    11403 2023-12-18 12:05:45.723462 gigachain_community-0.0.6.1/langchain_community/llms/huggingface_text_gen_inference.py
--rw-r--r--   0        0        0     2582 2023-12-18 12:05:45.723857 gigachain_community-0.0.6.1/langchain_community/llms/human.py
--rw-r--r--   0        0        0     4723 2023-12-18 12:05:45.724545 gigachain_community-0.0.6.1/langchain_community/llms/javelin_ai_gateway.py
--rw-r--r--   0        0        0     5087 2023-12-18 12:05:45.724912 gigachain_community-0.0.6.1/langchain_community/llms/koboldai.py
--rw-r--r--   0        0        0    12537 2023-12-18 12:05:45.725808 gigachain_community-0.0.6.1/langchain_community/llms/llamacpp.py
--rw-r--r--   0        0        0     1328 2023-12-18 12:05:45.726192 gigachain_community-0.0.6.1/langchain_community/llms/loading.py
--rw-r--r--   0        0        0     1965 2023-12-18 12:05:45.726579 gigachain_community-0.0.6.1/langchain_community/llms/manifest.py
--rw-r--r--   0        0        0     5470 2023-12-18 12:05:45.727040 gigachain_community-0.0.6.1/langchain_community/llms/minimax.py
--rw-r--r--   0        0        0     3845 2023-12-18 12:05:45.727455 gigachain_community-0.0.6.1/langchain_community/llms/mlflow.py
--rw-r--r--   0        0        0     3279 2023-12-18 12:05:45.728237 gigachain_community-0.0.6.1/langchain_community/llms/mlflow_ai_gateway.py
--rw-r--r--   0        0        0     3288 2023-12-18 12:05:45.730018 gigachain_community-0.0.6.1/langchain_community/llms/modal.py
--rw-r--r--   0        0        0     6150 2023-12-18 12:05:45.730902 gigachain_community-0.0.6.1/langchain_community/llms/mosaicml.py
--rw-r--r--   0        0        0     5049 2023-12-18 12:05:45.731352 gigachain_community-0.0.6.1/langchain_community/llms/nlpcloud.py
--rw-r--r--   0        0        0    12162 2023-12-28 13:38:34.495799 gigachain_community-0.0.6.1/langchain_community/llms/oci_data_science_model_deployment_endpoint.py
--rw-r--r--   0        0        0     5207 2023-12-18 12:05:45.731708 gigachain_community-0.0.6.1/langchain_community/llms/octoai_endpoint.py
--rw-r--r--   0        0        0    16092 2023-12-28 13:38:34.496966 gigachain_community-0.0.6.1/langchain_community/llms/ollama.py
--rw-r--r--   0        0        0     3980 2023-12-18 12:05:45.732462 gigachain_community-0.0.6.1/langchain_community/llms/opaqueprompts.py
--rw-r--r--   0        0        0    47378 2023-12-21 15:51:51.818449 gigachain_community-0.0.6.1/langchain_community/llms/openai.py
--rw-r--r--   0        0        0    10743 2023-12-18 12:05:45.733474 gigachain_community-0.0.6.1/langchain_community/llms/openllm.py
--rw-r--r--   0        0        0      902 2023-12-18 12:05:45.733910 gigachain_community-0.0.6.1/langchain_community/llms/openlm.py
--rw-r--r--   0        0        0     8059 2023-12-18 12:05:45.734279 gigachain_community-0.0.6.1/langchain_community/llms/pai_eas_endpoint.py
--rw-r--r--   0        0        0     5402 2023-12-28 13:38:34.498208 gigachain_community-0.0.6.1/langchain_community/llms/petals.py
--rw-r--r--   0        0        0     4182 2023-12-28 13:38:34.499044 gigachain_community-0.0.6.1/langchain_community/llms/pipelineai.py
--rw-r--r--   0        0        0     1594 2023-12-28 13:38:34.499702 gigachain_community-0.0.6.1/langchain_community/llms/predibase.py
--rw-r--r--   0        0        0     4410 2023-12-18 12:05:45.735769 gigachain_community-0.0.6.1/langchain_community/llms/predictionguard.py
--rw-r--r--   0        0        0     8809 2023-12-21 15:51:51.819566 gigachain_community-0.0.6.1/langchain_community/llms/promptlayer_openai.py
--rw-r--r--   0        0        0     8024 2023-12-18 12:05:45.736765 gigachain_community-0.0.6.1/langchain_community/llms/replicate.py
--rw-r--r--   0        0        0     7387 2023-12-18 12:05:45.737203 gigachain_community-0.0.6.1/langchain_community/llms/rwkv.py
--rw-r--r--   0        0        0    13193 2023-12-18 12:05:45.737817 gigachain_community-0.0.6.1/langchain_community/llms/sagemaker_endpoint.py
--rw-r--r--   0        0        0     7743 2023-12-18 12:05:45.738142 gigachain_community-0.0.6.1/langchain_community/llms/self_hosted.py
--rw-r--r--   0        0        0     7743 2023-12-18 12:05:45.738949 gigachain_community-0.0.6.1/langchain_community/llms/self_hosted_hugging_face.py
--rw-r--r--   0        0        0     4720 2023-12-28 13:38:34.500543 gigachain_community-0.0.6.1/langchain_community/llms/stochasticai.py
--rw-r--r--   0        0        0     7552 2023-12-18 12:05:45.739825 gigachain_community-0.0.6.1/langchain_community/llms/symblai_nebula.py
--rw-r--r--   0        0        0    14081 2023-12-21 15:51:51.820638 gigachain_community-0.0.6.1/langchain_community/llms/textgen.py
--rw-r--r--   0        0        0     5243 2023-12-18 12:05:45.740891 gigachain_community-0.0.6.1/langchain_community/llms/titan_takeoff.py
--rw-r--r--   0        0        0     7349 2023-12-21 15:51:51.821440 gigachain_community-0.0.6.1/langchain_community/llms/titan_takeoff_pro.py
--rw-r--r--   0        0        0     7526 2023-12-21 15:51:51.822232 gigachain_community-0.0.6.1/langchain_community/llms/together.py
--rw-r--r--   0        0        0     9593 2023-12-21 15:51:51.823228 gigachain_community-0.0.6.1/langchain_community/llms/tongyi.py
--rw-r--r--   0        0        0      258 2023-12-18 12:05:45.742413 gigachain_community-0.0.6.1/langchain_community/llms/utils.py
--rw-r--r--   0        0        0    18948 2023-12-21 15:51:51.824271 gigachain_community-0.0.6.1/langchain_community/llms/vertexai.py
--rw-r--r--   0        0        0     5592 2023-12-18 12:05:45.743737 gigachain_community-0.0.6.1/langchain_community/llms/vllm.py
--rw-r--r--   0        0        0     6591 2023-12-28 13:38:34.501403 gigachain_community-0.0.6.1/langchain_community/llms/volcengine_maas.py
--rw-r--r--   0        0        0    13164 2023-12-18 12:05:45.744770 gigachain_community-0.0.6.1/langchain_community/llms/watsonxllm.py
--rw-r--r--   0        0        0     4970 2023-12-18 12:05:45.745301 gigachain_community-0.0.6.1/langchain_community/llms/writer.py
--rw-r--r--   0        0        0     6325 2023-12-21 15:51:51.825366 gigachain_community-0.0.6.1/langchain_community/llms/xinference.py
--rw-r--r--   0        0        0    10797 2023-12-21 15:51:51.826293 gigachain_community-0.0.6.1/langchain_community/llms/yandex.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.746445 gigachain_community-0.0.6.1/langchain_community/py.typed
--rw-r--r--   0        0        0     4245 2023-12-21 15:51:51.827178 gigachain_community-0.0.6.1/langchain_community/retrievers/__init__.py
--rw-r--r--   0        0        0     4328 2023-12-18 12:05:45.747540 gigachain_community-0.0.6.1/langchain_community/retrievers/arcee.py
--rw-r--r--   0        0        0      773 2023-12-18 12:05:45.747949 gigachain_community-0.0.6.1/langchain_community/retrievers/arxiv.py
--rw-r--r--   0        0        0     4279 2023-12-18 12:05:45.748397 gigachain_community-0.0.6.1/langchain_community/retrievers/azure_cognitive_search.py
--rw-r--r--   0        0        0     4691 2023-12-21 15:51:51.828732 gigachain_community-0.0.6.1/langchain_community/retrievers/bedrock.py
--rw-r--r--   0        0        0     3648 2023-12-18 12:05:45.749291 gigachain_community-0.0.6.1/langchain_community/retrievers/bm25.py
--rw-r--r--   0        0        0     2684 2023-12-18 12:05:45.749632 gigachain_community-0.0.6.1/langchain_community/retrievers/chaindesk.py
--rw-r--r--   0        0        0     3024 2023-12-18 12:05:45.749873 gigachain_community-0.0.6.1/langchain_community/retrievers/chatgpt_plugin_retriever.py
--rw-r--r--   0        0        0     2816 2023-12-18 12:05:45.750128 gigachain_community-0.0.6.1/langchain_community/retrievers/cohere_rag_retriever.py
--rw-r--r--   0        0        0     2338 2023-12-18 12:05:45.750555 gigachain_community-0.0.6.1/langchain_community/retrievers/databerry.py
--rw-r--r--   0        0        0     6778 2023-12-18 12:05:45.751130 gigachain_community-0.0.6.1/langchain_community/retrievers/docarray.py
--rw-r--r--   0        0        0     4639 2023-12-18 12:05:45.751921 gigachain_community-0.0.6.1/langchain_community/retrievers/elastic_search_bm25.py
--rw-r--r--   0        0        0     2000 2023-12-18 12:05:45.752650 gigachain_community-0.0.6.1/langchain_community/retrievers/embedchain.py
--rw-r--r--   0        0        0     4583 2023-12-18 12:05:45.753268 gigachain_community-0.0.6.1/langchain_community/retrievers/google_cloud_documentai_warehouse.py
--rw-r--r--   0        0        0    17552 2023-12-28 13:38:34.502238 gigachain_community-0.0.6.1/langchain_community/retrievers/google_vertex_ai_search.py
--rw-r--r--   0        0        0     1985 2023-12-18 12:05:45.754528 gigachain_community-0.0.6.1/langchain_community/retrievers/kay.py
--rw-r--r--   0        0        0    13811 2023-12-21 15:51:51.830140 gigachain_community-0.0.6.1/langchain_community/retrievers/kendra.py
--rw-r--r--   0        0        0     2570 2023-12-18 12:05:45.756151 gigachain_community-0.0.6.1/langchain_community/retrievers/knn.py
--rw-r--r--   0        0        0     3162 2023-12-18 12:05:45.756542 gigachain_community-0.0.6.1/langchain_community/retrievers/llama_index.py
--rw-r--r--   0        0        0     1486 2023-12-18 12:05:45.756990 gigachain_community-0.0.6.1/langchain_community/retrievers/metal.py
--rw-r--r--   0        0        0     2435 2023-12-18 12:05:45.757360 gigachain_community-0.0.6.1/langchain_community/retrievers/milvus.py
--rw-r--r--   0        0        0      644 2023-12-18 12:05:45.757713 gigachain_community-0.0.6.1/langchain_community/retrievers/outline.py
--rw-r--r--   0        0        0     5733 2023-12-18 12:05:45.758713 gigachain_community-0.0.6.1/langchain_community/retrievers/pinecone_hybrid_search.py
--rw-r--r--   0        0        0      643 2023-12-18 12:05:45.759084 gigachain_community-0.0.6.1/langchain_community/retrievers/pubmed.py
--rw-r--r--   0        0        0      104 2023-12-18 12:05:45.759462 gigachain_community-0.0.6.1/langchain_community/retrievers/pupmed.py
--rw-r--r--   0        0        0     7468 2023-12-21 15:51:51.831006 gigachain_community-0.0.6.1/langchain_community/retrievers/qdrant_sparse_vector_retriever.py
--rw-r--r--   0        0        0     1935 2023-12-18 12:05:45.759849 gigachain_community-0.0.6.1/langchain_community/retrievers/remote_retriever.py
--rw-r--r--   0        0        0     4131 2023-12-18 12:05:45.760244 gigachain_community-0.0.6.1/langchain_community/retrievers/svm.py
--rw-r--r--   0        0        0     2855 2023-12-18 12:05:45.760593 gigachain_community-0.0.6.1/langchain_community/retrievers/tavily_search_api.py
--rw-r--r--   0        0        0     4079 2023-12-18 12:05:45.760932 gigachain_community-0.0.6.1/langchain_community/retrievers/tfidf.py
--rw-r--r--   0        0        0     4555 2023-12-18 12:05:45.761391 gigachain_community-0.0.6.1/langchain_community/retrievers/vespa_retriever.py
--rw-r--r--   0        0        0     6195 2023-12-18 12:05:45.762141 gigachain_community-0.0.6.1/langchain_community/retrievers/weaviate_hybrid_search.py
--rw-r--r--   0        0        0      656 2023-12-18 12:05:45.762889 gigachain_community-0.0.6.1/langchain_community/retrievers/wikipedia.py
--rw-r--r--   0        0        0     2356 2023-12-18 12:05:45.763365 gigachain_community-0.0.6.1/langchain_community/retrievers/you.py
--rw-r--r--   0        0        0     5904 2023-12-18 12:05:45.763882 gigachain_community-0.0.6.1/langchain_community/retrievers/zep.py
--rw-r--r--   0        0        0     2735 2023-12-18 12:05:45.764315 gigachain_community-0.0.6.1/langchain_community/retrievers/zilliz.py
--rw-r--r--   0        0        0      501 2023-12-18 12:05:45.764791 gigachain_community-0.0.6.1/langchain_community/storage/__init__.py
--rw-r--r--   0        0        0      179 2023-12-18 12:05:45.765177 gigachain_community-0.0.6.1/langchain_community/storage/exceptions.py
--rw-r--r--   0        0        0     4788 2023-12-18 12:05:45.765839 gigachain_community-0.0.6.1/langchain_community/storage/redis.py
--rw-r--r--   0        0        0     5764 2023-12-18 12:05:45.766664 gigachain_community-0.0.6.1/langchain_community/storage/upstash_redis.py
--rw-r--r--   0        0        0    33514 2023-12-18 12:05:45.767310 gigachain_community-0.0.6.1/langchain_community/tools/__init__.py
--rw-r--r--   0        0        0     3185 2023-12-18 12:05:45.767799 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/app.py
--rw-r--r--   0        0        0     2109 2023-12-18 12:05:45.768158 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/base.py
--rw-r--r--   0        0        0     4140 2023-12-18 12:05:45.768950 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/owner.py
--rw-r--r--   0        0        0     2746 2023-12-18 12:05:45.769433 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/rule.py
--rw-r--r--   0        0        0     1074 2023-12-18 12:05:45.769853 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/transfer.py
--rw-r--r--   0        0        0     2314 2023-12-18 12:05:45.770226 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/utils.py
--rw-r--r--   0        0        0     2624 2023-12-18 12:05:45.770601 gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/value.py
--rw-r--r--   0        0        0      257 2023-12-18 12:05:45.770978 gigachain_community-0.0.6.1/langchain_community/tools/amadeus/__init__.py
--rw-r--r--   0        0        0      435 2023-12-18 12:05:45.771420 gigachain_community-0.0.6.1/langchain_community/tools/amadeus/base.py
--rw-r--r--   0        0        0     1845 2023-12-18 12:05:45.771860 gigachain_community-0.0.6.1/langchain_community/tools/amadeus/closest_airport.py
--rw-r--r--   0        0        0     5611 2023-12-18 12:05:45.772295 gigachain_community-0.0.6.1/langchain_community/tools/amadeus/flight_search.py
--rw-r--r--   0        0        0     1276 2023-12-18 12:05:45.773346 gigachain_community-0.0.6.1/langchain_community/tools/amadeus/utils.py
--rw-r--r--   0        0        0       25 2023-12-18 12:05:45.773797 gigachain_community-0.0.6.1/langchain_community/tools/arxiv/__init__.py
--rw-r--r--   0        0        0     1228 2023-12-21 15:51:51.832262 gigachain_community-0.0.6.1/langchain_community/tools/arxiv/tool.py
--rw-r--r--   0        0        0      802 2023-12-18 12:05:45.774697 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/__init__.py
--rw-r--r--   0        0        0     5375 2023-12-18 12:05:45.775110 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/form_recognizer.py
--rw-r--r--   0        0        0     5304 2023-12-18 12:05:45.775653 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/image_analysis.py
--rw-r--r--   0        0        0     4336 2023-12-18 12:05:45.776161 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/speech2text.py
--rw-r--r--   0        0        0     3675 2023-12-18 12:05:45.776545 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/text2speech.py
--rw-r--r--   0        0        0     3542 2023-12-21 15:51:51.833253 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/text_analytics_health.py
--rw-r--r--   0        0        0      776 2023-12-18 12:05:45.777398 gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/utils.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.777639 gigachain_community-0.0.6.1/langchain_community/tools/bearly/__init__.py
--rw-r--r--   0        0        0     5468 2023-12-18 12:05:45.777984 gigachain_community-0.0.6.1/langchain_community/tools/bearly/tool.py
--rw-r--r--   0        0        0      170 2023-12-18 12:05:45.778478 gigachain_community-0.0.6.1/langchain_community/tools/bing_search/__init__.py
--rw-r--r--   0        0        0     1463 2023-12-18 12:05:45.779631 gigachain_community-0.0.6.1/langchain_community/tools/bing_search/tool.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.780012 gigachain_community-0.0.6.1/langchain_community/tools/brave_search/__init__.py
--rw-r--r--   0        0        0     1354 2023-12-18 12:05:45.780571 gigachain_community-0.0.6.1/langchain_community/tools/brave_search/tool.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.780739 gigachain_community-0.0.6.1/langchain_community/tools/clickup/__init__.py
--rw-r--r--   0        0        0     8298 2023-12-18 12:05:45.781291 gigachain_community-0.0.6.1/langchain_community/tools/clickup/prompt.py
--rw-r--r--   0        0        0     1230 2023-12-18 12:05:45.781701 gigachain_community-0.0.6.1/langchain_community/tools/clickup/tool.py
--rw-r--r--   0        0        0     1336 2023-12-18 12:05:45.782109 gigachain_community-0.0.6.1/langchain_community/tools/convert_to_openai.py
--rw-r--r--   0        0        0      268 2023-12-18 12:05:45.782537 gigachain_community-0.0.6.1/langchain_community/tools/dataforseo_api_search/__init__.py
--rw-r--r--   0        0        0     2214 2023-12-18 12:05:45.783121 gigachain_community-0.0.6.1/langchain_community/tools/dataforseo_api_search/tool.py
--rw-r--r--   0        0        0      147 2023-12-18 12:05:45.783495 gigachain_community-0.0.6.1/langchain_community/tools/ddg_search/__init__.py
--rw-r--r--   0        0        0     2641 2023-12-21 15:51:51.835050 gigachain_community-0.0.6.1/langchain_community/tools/ddg_search/tool.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.783987 gigachain_community-0.0.6.1/langchain_community/tools/e2b_data_analysis/__init__.py
--rw-r--r--   0        0        0     8015 2023-12-28 13:38:34.503446 gigachain_community-0.0.6.1/langchain_community/tools/e2b_data_analysis/tool.py
--rw-r--r--   0        0        0    20701 2023-12-21 15:51:51.836025 gigachain_community-0.0.6.1/langchain_community/tools/e2b_data_analysis/unparse.py
--rw-r--r--   0        0        0     1024 2023-12-18 12:05:45.785160 gigachain_community-0.0.6.1/langchain_community/tools/edenai/__init__.py
--rw-r--r--   0        0        0     3465 2023-12-18 12:05:45.785432 gigachain_community-0.0.6.1/langchain_community/tools/edenai/audio_speech_to_text.py
--rw-r--r--   0        0        0     3885 2023-12-18 12:05:45.785916 gigachain_community-0.0.6.1/langchain_community/tools/edenai/audio_text_to_speech.py
--rw-r--r--   0        0        0     5386 2023-12-18 12:05:45.786625 gigachain_community-0.0.6.1/langchain_community/tools/edenai/edenai_base_tool.py
--rw-r--r--   0        0        0     2249 2023-12-18 12:05:45.787257 gigachain_community-0.0.6.1/langchain_community/tools/edenai/image_explicitcontent.py
--rw-r--r--   0        0        0     2476 2023-12-18 12:05:45.787647 gigachain_community-0.0.6.1/langchain_community/tools/edenai/image_objectdetection.py
--rw-r--r--   0        0        0     1966 2023-12-18 12:05:45.788039 gigachain_community-0.0.6.1/langchain_community/tools/edenai/ocr_identityparser.py
--rw-r--r--   0        0        0     2187 2023-12-18 12:05:45.788452 gigachain_community-0.0.6.1/langchain_community/tools/edenai/ocr_invoiceparser.py
--rw-r--r--   0        0        0     2407 2023-12-18 12:05:45.789172 gigachain_community-0.0.6.1/langchain_community/tools/edenai/text_moderation.py
--rw-r--r--   0        0        0      164 2023-12-18 12:05:45.789668 gigachain_community-0.0.6.1/langchain_community/tools/eleven_labs/__init__.py
--rw-r--r--   0        0        0      203 2023-12-18 12:05:45.790052 gigachain_community-0.0.6.1/langchain_community/tools/eleven_labs/models.py
--rw-r--r--   0        0        0     2709 2023-12-18 12:05:45.790521 gigachain_community-0.0.6.1/langchain_community/tools/eleven_labs/text2speech.py
--rw-r--r--   0        0        0      723 2023-12-18 12:05:45.791133 gigachain_community-0.0.6.1/langchain_community/tools/file_management/__init__.py
--rw-r--r--   0        0        0     1749 2023-12-18 12:05:45.791626 gigachain_community-0.0.6.1/langchain_community/tools/file_management/copy.py
--rw-r--r--   0        0        0     1345 2023-12-18 12:05:45.791998 gigachain_community-0.0.6.1/langchain_community/tools/file_management/delete.py
--rw-r--r--   0        0        0     1965 2023-12-18 12:05:45.792403 gigachain_community-0.0.6.1/langchain_community/tools/file_management/file_search.py
--rw-r--r--   0        0        0     1432 2023-12-18 12:05:45.792791 gigachain_community-0.0.6.1/langchain_community/tools/file_management/list_dir.py
--rw-r--r--   0        0        0     1889 2023-12-18 12:05:45.793186 gigachain_community-0.0.6.1/langchain_community/tools/file_management/move.py
--rw-r--r--   0        0        0     1340 2023-12-18 12:05:45.793625 gigachain_community-0.0.6.1/langchain_community/tools/file_management/read.py
--rw-r--r--   0        0        0     1726 2023-12-18 12:05:45.794428 gigachain_community-0.0.6.1/langchain_community/tools/file_management/utils.py
--rw-r--r--   0        0        0     1614 2023-12-18 12:05:45.794830 gigachain_community-0.0.6.1/langchain_community/tools/file_management/write.py
--rw-r--r--   0        0        0       20 2023-12-18 12:05:45.795246 gigachain_community-0.0.6.1/langchain_community/tools/github/__init__.py
--rw-r--r--   0        0        0     6220 2023-12-18 12:05:45.795757 gigachain_community-0.0.6.1/langchain_community/tools/github/prompt.py
--rw-r--r--   0        0        0     1194 2023-12-18 12:05:45.796153 gigachain_community-0.0.6.1/langchain_community/tools/github/tool.py
--rw-r--r--   0        0        0       20 2023-12-18 12:05:45.796530 gigachain_community-0.0.6.1/langchain_community/tools/gitlab/__init__.py
--rw-r--r--   0        0        0     3438 2023-12-18 12:05:45.797359 gigachain_community-0.0.6.1/langchain_community/tools/gitlab/prompt.py
--rw-r--r--   0        0        0      972 2023-12-18 12:05:45.797816 gigachain_community-0.0.6.1/langchain_community/tools/gitlab/tool.py
--rw-r--r--   0        0        0      601 2023-12-18 12:05:45.798228 gigachain_community-0.0.6.1/langchain_community/tools/gmail/__init__.py
--rw-r--r--   0        0        0     1004 2023-12-18 12:05:45.798672 gigachain_community-0.0.6.1/langchain_community/tools/gmail/base.py
--rw-r--r--   0        0        0     2564 2023-12-18 12:05:45.799060 gigachain_community-0.0.6.1/langchain_community/tools/gmail/create_draft.py
--rw-r--r--   0        0        0     2202 2023-12-18 12:05:45.799653 gigachain_community-0.0.6.1/langchain_community/tools/gmail/get_message.py
--rw-r--r--   0        0        0     1560 2023-12-18 12:05:45.800071 gigachain_community-0.0.6.1/langchain_community/tools/gmail/get_thread.py
--rw-r--r--   0        0        0     4874 2023-12-18 12:05:45.801270 gigachain_community-0.0.6.1/langchain_community/tools/gmail/search.py
--rw-r--r--   0        0        0     2939 2023-12-28 13:38:34.504472 gigachain_community-0.0.6.1/langchain_community/tools/gmail/send_message.py
--rw-r--r--   0        0        0     4528 2023-12-18 12:05:45.802173 gigachain_community-0.0.6.1/langchain_community/tools/gmail/utils.py
--rw-r--r--   0        0        0      136 2023-12-18 12:05:45.802569 gigachain_community-0.0.6.1/langchain_community/tools/golden_query/__init__.py
--rw-r--r--   0        0        0     1108 2023-12-18 12:05:45.802914 gigachain_community-0.0.6.1/langchain_community/tools/golden_query/tool.py
--rw-r--r--   0        0        0      171 2023-12-18 12:05:45.803350 gigachain_community-0.0.6.1/langchain_community/tools/google_cloud/__init__.py
--rw-r--r--   0        0        0     3173 2023-12-18 12:05:45.803756 gigachain_community-0.0.6.1/langchain_community/tools/google_cloud/texttospeech.py
--rw-r--r--   0        0        0      152 2023-12-18 12:05:45.805823 gigachain_community-0.0.6.1/langchain_community/tools/google_finance/__init__.py
--rw-r--r--   0        0        0      854 2023-12-18 12:05:45.806289 gigachain_community-0.0.6.1/langchain_community/tools/google_finance/tool.py
--rw-r--r--   0        0        0      140 2023-12-18 12:05:45.806731 gigachain_community-0.0.6.1/langchain_community/tools/google_jobs/__init__.py
--rw-r--r--   0        0        0      826 2023-12-18 12:05:45.807119 gigachain_community-0.0.6.1/langchain_community/tools/google_jobs/tool.py
--rw-r--r--   0        0        0      140 2023-12-18 12:05:45.807515 gigachain_community-0.0.6.1/langchain_community/tools/google_lens/__init__.py
--rw-r--r--   0        0        0      822 2023-12-18 12:05:45.807846 gigachain_community-0.0.6.1/langchain_community/tools/google_lens/tool.py
--rw-r--r--   0        0        0      140 2023-12-18 12:05:45.808212 gigachain_community-0.0.6.1/langchain_community/tools/google_places/__init__.py
--rw-r--r--   0        0        0     1141 2023-12-18 12:05:45.808602 gigachain_community-0.0.6.1/langchain_community/tools/google_places/tool.py
--rw-r--r--   0        0        0      152 2023-12-18 12:05:45.809035 gigachain_community-0.0.6.1/langchain_community/tools/google_scholar/__init__.py
--rw-r--r--   0        0        0      847 2023-12-18 12:05:45.809424 gigachain_community-0.0.6.1/langchain_community/tools/google_scholar/tool.py
--rw-r--r--   0        0        0      195 2023-12-18 12:05:45.810892 gigachain_community-0.0.6.1/langchain_community/tools/google_search/__init__.py
--rw-r--r--   0        0        0     1489 2023-12-18 12:05:45.811333 gigachain_community-0.0.6.1/langchain_community/tools/google_search/tool.py
--rw-r--r--   0        0        0      243 2023-12-18 12:05:45.811765 gigachain_community-0.0.6.1/langchain_community/tools/google_serper/__init__.py
--rw-r--r--   0        0        0     2113 2023-12-18 12:05:45.812243 gigachain_community-0.0.6.1/langchain_community/tools/google_serper/tool.py
--rw-r--r--   0        0        0      148 2023-12-18 12:05:45.812859 gigachain_community-0.0.6.1/langchain_community/tools/google_trends/__init__.py
--rw-r--r--   0        0        0      844 2023-12-18 12:05:45.813598 gigachain_community-0.0.6.1/langchain_community/tools/google_trends/tool.py
--rw-r--r--   0        0        0       47 2023-12-18 12:05:45.813986 gigachain_community-0.0.6.1/langchain_community/tools/graphql/__init__.py
--rw-r--r--   0        0        0     1204 2023-12-18 12:05:45.814359 gigachain_community-0.0.6.1/langchain_community/tools/graphql/tool.py
--rw-r--r--   0        0        0      132 2023-12-18 12:05:45.814785 gigachain_community-0.0.6.1/langchain_community/tools/human/__init__.py
--rw-r--r--   0        0        0      983 2023-12-18 12:05:45.815173 gigachain_community-0.0.6.1/langchain_community/tools/human/tool.py
--rw-r--r--   0        0        0     2286 2023-12-18 12:05:45.815512 gigachain_community-0.0.6.1/langchain_community/tools/ifttt.py
--rw-r--r--   0        0        0       43 2023-12-18 12:05:45.815966 gigachain_community-0.0.6.1/langchain_community/tools/interaction/__init__.py
--rw-r--r--   0        0        0      464 2023-12-18 12:05:45.816375 gigachain_community-0.0.6.1/langchain_community/tools/interaction/tool.py
--rw-r--r--   0        0        0       17 2023-12-18 12:05:45.817112 gigachain_community-0.0.6.1/langchain_community/tools/jira/__init__.py
--rw-r--r--   0        0        0     3170 2023-12-18 12:05:45.818054 gigachain_community-0.0.6.1/langchain_community/tools/jira/prompt.py
--rw-r--r--   0        0        0     1342 2023-12-18 12:05:45.818546 gigachain_community-0.0.6.1/langchain_community/tools/jira/tool.py
--rw-r--r--   0        0        0       46 2023-12-18 12:05:45.819119 gigachain_community-0.0.6.1/langchain_community/tools/json/__init__.py
--rw-r--r--   0        0        0     4139 2023-12-18 12:05:45.819580 gigachain_community-0.0.6.1/langchain_community/tools/json/tool.py
--rw-r--r--   0        0        0      134 2023-12-18 12:05:45.820014 gigachain_community-0.0.6.1/langchain_community/tools/memorize/__init__.py
--rw-r--r--   0        0        0     1828 2023-12-21 15:51:51.837175 gigachain_community-0.0.6.1/langchain_community/tools/memorize/tool.py
--rw-r--r--   0        0        0       35 2023-12-18 12:05:45.821474 gigachain_community-0.0.6.1/langchain_community/tools/merriam_webster/__init__.py
--rw-r--r--   0        0        0      853 2023-12-18 12:05:45.822073 gigachain_community-0.0.6.1/langchain_community/tools/merriam_webster/tool.py
--rw-r--r--   0        0        0      154 2023-12-18 12:05:45.822754 gigachain_community-0.0.6.1/langchain_community/tools/metaphor_search/__init__.py
--rw-r--r--   0        0        0     2690 2023-12-18 12:05:45.823297 gigachain_community-0.0.6.1/langchain_community/tools/metaphor_search/tool.py
--rw-r--r--   0        0        0      359 2023-12-18 12:05:45.823831 gigachain_community-0.0.6.1/langchain_community/tools/multion/__init__.py
--rw-r--r--   0        0        0     2052 2023-12-18 12:05:45.824250 gigachain_community-0.0.6.1/langchain_community/tools/multion/close_session.py
--rw-r--r--   0        0        0     2585 2023-12-18 12:05:45.824782 gigachain_community-0.0.6.1/langchain_community/tools/multion/create_session.py
--rw-r--r--   0        0        0     2822 2023-12-18 12:05:45.825198 gigachain_community-0.0.6.1/langchain_community/tools/multion/update_session.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.825384 gigachain_community-0.0.6.1/langchain_community/tools/nasa/__init__.py
--rw-r--r--   0        0        0     5197 2023-12-18 12:05:45.825968 gigachain_community-0.0.6.1/langchain_community/tools/nasa/prompt.py
--rw-r--r--   0        0        0      831 2023-12-18 12:05:45.826349 gigachain_community-0.0.6.1/langchain_community/tools/nasa/tool.py
--rw-r--r--   0        0        0      111 2023-12-18 12:05:45.826705 gigachain_community-0.0.6.1/langchain_community/tools/nuclia/__init__.py
--rw-r--r--   0        0        0     7915 2023-12-18 12:05:45.827114 gigachain_community-0.0.6.1/langchain_community/tools/nuclia/tool.py
--rw-r--r--   0        0        0      654 2023-12-18 12:05:45.827517 gigachain_community-0.0.6.1/langchain_community/tools/office365/__init__.py
--rw-r--r--   0        0        0      508 2023-12-18 12:05:45.827887 gigachain_community-0.0.6.1/langchain_community/tools/office365/base.py
--rw-r--r--   0        0        0     1858 2023-12-18 12:05:45.828512 gigachain_community-0.0.6.1/langchain_community/tools/office365/create_draft_message.py
--rw-r--r--   0        0        0     4833 2023-12-18 12:05:45.828888 gigachain_community-0.0.6.1/langchain_community/tools/office365/events_search.py
--rw-r--r--   0        0        0     4246 2023-12-18 12:05:45.829447 gigachain_community-0.0.6.1/langchain_community/tools/office365/messages_search.py
--rw-r--r--   0        0        0     2898 2023-12-18 12:05:45.829858 gigachain_community-0.0.6.1/langchain_community/tools/office365/send_event.py
--rw-r--r--   0        0        0     1789 2023-12-18 12:05:45.830410 gigachain_community-0.0.6.1/langchain_community/tools/office365/send_message.py
--rw-r--r--   0        0        0     2212 2023-12-18 12:05:45.831194 gigachain_community-0.0.6.1/langchain_community/tools/office365/utils.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.831412 gigachain_community-0.0.6.1/langchain_community/tools/openapi/__init__.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.831880 gigachain_community-0.0.6.1/langchain_community/tools/openapi/utils/__init__.py
--rw-r--r--   0        0        0    21288 2023-12-18 12:05:45.832707 gigachain_community-0.0.6.1/langchain_community/tools/openapi/utils/api_models.py
--rw-r--r--   0        0        0      191 2023-12-18 12:05:45.833201 gigachain_community-0.0.6.1/langchain_community/tools/openapi/utils/openapi_utils.py
--rw-r--r--   0        0        0      162 2023-12-18 12:05:45.833667 gigachain_community-0.0.6.1/langchain_community/tools/openweathermap/__init__.py
--rw-r--r--   0        0        0      966 2023-12-18 12:05:45.834076 gigachain_community-0.0.6.1/langchain_community/tools/openweathermap/tool.py
--rw-r--r--   0        0        0      763 2023-12-18 12:05:45.834481 gigachain_community-0.0.6.1/langchain_community/tools/playwright/__init__.py
--rw-r--r--   0        0        0     2135 2023-12-18 12:05:45.834869 gigachain_community-0.0.6.1/langchain_community/tools/playwright/base.py
--rw-r--r--   0        0        0     3083 2023-12-18 12:05:45.835282 gigachain_community-0.0.6.1/langchain_community/tools/playwright/click.py
--rw-r--r--   0        0        0     1340 2023-12-18 12:05:45.835906 gigachain_community-0.0.6.1/langchain_community/tools/playwright/current_page.py
--rw-r--r--   0        0        0     3051 2023-12-18 12:05:45.836449 gigachain_community-0.0.6.1/langchain_community/tools/playwright/extract_hyperlinks.py
--rw-r--r--   0        0        0     2383 2023-12-18 12:05:45.836893 gigachain_community-0.0.6.1/langchain_community/tools/playwright/extract_text.py
--rw-r--r--   0        0        0     3743 2023-12-18 12:05:45.837329 gigachain_community-0.0.6.1/langchain_community/tools/playwright/get_elements.py
--rw-r--r--   0        0        0     2878 2023-12-18 12:05:45.837738 gigachain_community-0.0.6.1/langchain_community/tools/playwright/navigate.py
--rw-r--r--   0        0        0     1926 2023-12-18 12:05:45.838120 gigachain_community-0.0.6.1/langchain_community/tools/playwright/navigate_back.py
--rw-r--r--   0        0        0     3049 2023-12-18 12:05:45.838549 gigachain_community-0.0.6.1/langchain_community/tools/playwright/utils.py
--rw-r--r--   0        0        0     2902 2023-12-18 12:05:45.838925 gigachain_community-0.0.6.1/langchain_community/tools/plugin.py
--rw-r--r--   0        0        0       52 2023-12-18 12:05:45.839326 gigachain_community-0.0.6.1/langchain_community/tools/powerbi/__init__.py
--rw-r--r--   0        0        0     7339 2023-12-18 12:05:45.839823 gigachain_community-0.0.6.1/langchain_community/tools/powerbi/prompt.py
--rw-r--r--   0        0        0    11075 2023-12-18 12:05:45.840707 gigachain_community-0.0.6.1/langchain_community/tools/powerbi/tool.py
--rw-r--r--   0        0        0       26 2023-12-18 12:05:45.841143 gigachain_community-0.0.6.1/langchain_community/tools/pubmed/__init__.py
--rw-r--r--   0        0        0      944 2023-12-18 12:05:45.841564 gigachain_community-0.0.6.1/langchain_community/tools/pubmed/tool.py
--rw-r--r--   0        0        0     1965 2023-12-18 12:05:45.842001 gigachain_community-0.0.6.1/langchain_community/tools/reddit_search/tool.py
--rw-r--r--   0        0        0     1586 2023-12-18 12:05:45.842412 gigachain_community-0.0.6.1/langchain_community/tools/render.py
--rw-r--r--   0        0        0       52 2023-12-18 12:05:45.842780 gigachain_community-0.0.6.1/langchain_community/tools/requests/__init__.py
--rw-r--r--   0        0        0     6323 2023-12-18 12:05:45.843362 gigachain_community-0.0.6.1/langchain_community/tools/requests/tool.py
--rw-r--r--   0        0        0       31 2023-12-18 12:05:45.843730 gigachain_community-0.0.6.1/langchain_community/tools/scenexplain/__init__.py
--rw-r--r--   0        0        0     1100 2023-12-18 12:05:45.844164 gigachain_community-0.0.6.1/langchain_community/tools/scenexplain/tool.py
--rw-r--r--   0        0        0      214 2023-12-18 12:05:45.844592 gigachain_community-0.0.6.1/langchain_community/tools/searchapi/__init__.py
--rw-r--r--   0        0        0     2114 2023-12-18 12:05:45.844972 gigachain_community-0.0.6.1/langchain_community/tools/searchapi/tool.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.845178 gigachain_community-0.0.6.1/langchain_community/tools/searx_search/__init__.py
--rw-r--r--   0        0        0     2261 2023-12-18 12:05:45.845824 gigachain_community-0.0.6.1/langchain_community/tools/searx_search/tool.py
--rw-r--r--   0        0        0      103 2023-12-18 12:05:45.846255 gigachain_community-0.0.6.1/langchain_community/tools/shell/__init__.py
--rw-r--r--   0        0        0     2663 2023-12-18 12:05:45.846662 gigachain_community-0.0.6.1/langchain_community/tools/shell/tool.py
--rw-r--r--   0        0        0      502 2023-12-18 12:05:45.847147 gigachain_community-0.0.6.1/langchain_community/tools/slack/__init__.py
--rw-r--r--   0        0        0      460 2023-12-18 12:05:45.847474 gigachain_community-0.0.6.1/langchain_community/tools/slack/base.py
--rw-r--r--   0        0        0     1118 2023-12-21 15:51:51.838169 gigachain_community-0.0.6.1/langchain_community/tools/slack/get_channel.py
--rw-r--r--   0        0        0     1402 2023-12-21 15:51:51.839105 gigachain_community-0.0.6.1/langchain_community/tools/slack/get_message.py
--rw-r--r--   0        0        0     2071 2023-12-18 12:05:45.848876 gigachain_community-0.0.6.1/langchain_community/tools/slack/schedule_message.py
--rw-r--r--   0        0        0     1222 2023-12-18 12:05:45.849246 gigachain_community-0.0.6.1/langchain_community/tools/slack/send_message.py
--rw-r--r--   0        0        0     1135 2023-12-18 12:05:45.849856 gigachain_community-0.0.6.1/langchain_community/tools/slack/utils.py
--rw-r--r--   0        0        0       18 2023-12-18 12:05:45.850305 gigachain_community-0.0.6.1/langchain_community/tools/sleep/__init__.py
--rw-r--r--   0        0        0     1229 2023-12-18 12:05:45.850716 gigachain_community-0.0.6.1/langchain_community/tools/sleep/tool.py
--rw-r--r--   0        0        0       44 2023-12-18 12:05:45.851086 gigachain_community-0.0.6.1/langchain_community/tools/spark_sql/__init__.py
--rw-r--r--   0        0        0      550 2023-12-18 12:05:45.851476 gigachain_community-0.0.6.1/langchain_community/tools/spark_sql/prompt.py
--rw-r--r--   0        0        0     4376 2023-12-18 12:05:45.851871 gigachain_community-0.0.6.1/langchain_community/tools/spark_sql/tool.py
--rw-r--r--   0        0        0       49 2023-12-18 12:05:45.852158 gigachain_community-0.0.6.1/langchain_community/tools/sql_database/__init__.py
--rw-r--r--   0        0        0      597 2023-12-18 12:05:45.852450 gigachain_community-0.0.6.1/langchain_community/tools/sql_database/prompt.py
--rw-r--r--   0        0        0     4487 2023-12-18 12:05:45.852922 gigachain_community-0.0.6.1/langchain_community/tools/sql_database/tool.py
--rw-r--r--   0        0        0       33 2023-12-18 12:05:45.853383 gigachain_community-0.0.6.1/langchain_community/tools/stackexchange/__init__.py
--rw-r--r--   0        0        0      868 2023-12-18 12:05:45.853792 gigachain_community-0.0.6.1/langchain_community/tools/stackexchange/tool.py
--rw-r--r--   0        0        0       24 2023-12-18 12:05:45.854296 gigachain_community-0.0.6.1/langchain_community/tools/steam/__init__.py
--rw-r--r--   0        0        0     1657 2023-12-18 12:05:45.854700 gigachain_community-0.0.6.1/langchain_community/tools/steam/prompt.py
--rw-r--r--   0        0        0      842 2023-12-18 12:05:45.855152 gigachain_community-0.0.6.1/langchain_community/tools/steam/tool.py
--rw-r--r--   0        0        0      186 2023-12-18 12:05:45.855742 gigachain_community-0.0.6.1/langchain_community/tools/steamship_image_generation/__init__.py
--rw-r--r--   0        0        0     3377 2023-12-18 12:05:45.856114 gigachain_community-0.0.6.1/langchain_community/tools/steamship_image_generation/tool.py
--rw-r--r--   0        0        0     1395 2023-12-18 12:05:45.856475 gigachain_community-0.0.6.1/langchain_community/tools/steamship_image_generation/utils.py
--rw-r--r--   0        0        0      189 2023-12-18 12:05:45.856940 gigachain_community-0.0.6.1/langchain_community/tools/tavily_search/__init__.py
--rw-r--r--   0        0        0     3375 2023-12-28 13:38:34.505773 gigachain_community-0.0.6.1/langchain_community/tools/tavily_search/tool.py
--rw-r--r--   0        0        0       51 2023-12-18 12:05:45.858009 gigachain_community-0.0.6.1/langchain_community/tools/vectorstore/__init__.py
--rw-r--r--   0        0        0     3302 2023-12-18 12:05:45.858410 gigachain_community-0.0.6.1/langchain_community/tools/vectorstore/tool.py
--rw-r--r--   0        0        0       29 2023-12-18 12:05:45.858782 gigachain_community-0.0.6.1/langchain_community/tools/wikipedia/__init__.py
--rw-r--r--   0        0        0      867 2023-12-18 12:05:45.859163 gigachain_community-0.0.6.1/langchain_community/tools/wikipedia/tool.py
--rw-r--r--   0        0        0      156 2023-12-18 12:05:45.859606 gigachain_community-0.0.6.1/langchain_community/tools/wolfram_alpha/__init__.py
--rw-r--r--   0        0        0      887 2023-12-18 12:05:45.859959 gigachain_community-0.0.6.1/langchain_community/tools/wolfram_alpha/tool.py
--rw-r--r--   0        0        0     2473 2023-12-18 12:05:45.860331 gigachain_community-0.0.6.1/langchain_community/tools/yahoo_finance_news.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.860527 gigachain_community-0.0.6.1/langchain_community/tools/youtube/__init__.py
--rw-r--r--   0        0        0     1729 2023-12-18 12:05:45.861292 gigachain_community-0.0.6.1/langchain_community/tools/youtube/search.py
--rw-r--r--   0        0        0      193 2023-12-18 12:05:45.861830 gigachain_community-0.0.6.1/langchain_community/tools/zapier/__init__.py
--rw-r--r--   0        0        0     1182 2023-12-18 12:05:45.862240 gigachain_community-0.0.6.1/langchain_community/tools/zapier/prompt.py
--rw-r--r--   0        0        0     7730 2023-12-18 12:05:45.863337 gigachain_community-0.0.6.1/langchain_community/tools/zapier/tool.py
--rw-r--r--   0        0        0    10998 2023-12-18 12:05:45.864315 gigachain_community-0.0.6.1/langchain_community/utilities/__init__.py
--rw-r--r--   0        0        0     2192 2023-12-18 12:05:45.864880 gigachain_community-0.0.6.1/langchain_community/utilities/alpha_vantage.py
--rw-r--r--   0        0        0      844 2023-12-18 12:05:45.865288 gigachain_community-0.0.6.1/langchain_community/utilities/anthropic.py
--rw-r--r--   0        0        0     8458 2023-12-18 12:05:45.865753 gigachain_community-0.0.6.1/langchain_community/utilities/apify.py
--rw-r--r--   0        0        0     8710 2023-12-18 12:05:45.866259 gigachain_community-0.0.6.1/langchain_community/utilities/arcee.py
--rw-r--r--   0        0        0     9262 2023-12-21 15:51:51.841117 gigachain_community-0.0.6.1/langchain_community/utilities/arxiv.py
--rw-r--r--   0        0        0     2437 2023-12-18 12:05:45.867481 gigachain_community-0.0.6.1/langchain_community/utilities/awslambda.py
--rw-r--r--   0        0        0     2499 2023-12-18 12:05:45.867932 gigachain_community-0.0.6.1/langchain_community/utilities/bibtex.py
--rw-r--r--   0        0        0     3545 2023-12-18 12:05:45.868301 gigachain_community-0.0.6.1/langchain_community/utilities/bing_search.py
--rw-r--r--   0        0        0     2356 2023-12-18 12:05:45.868774 gigachain_community-0.0.6.1/langchain_community/utilities/brave_search.py
--rw-r--r--   0        0        0    19771 2023-12-18 12:05:45.869364 gigachain_community-0.0.6.1/langchain_community/utilities/clickup.py
--rw-r--r--   0        0        0     6377 2023-12-18 12:05:45.870021 gigachain_community-0.0.6.1/langchain_community/utilities/dalle_image_generator.py
--rw-r--r--   0        0        0     7854 2023-12-18 12:05:45.870814 gigachain_community-0.0.6.1/langchain_community/utilities/dataforseo_api_search.py
--rw-r--r--   0        0        0     4310 2023-12-18 12:05:45.871460 gigachain_community-0.0.6.1/langchain_community/utilities/duckduckgo_search.py
--rw-r--r--   0        0        0    31986 2023-12-21 15:51:51.842273 gigachain_community-0.0.6.1/langchain_community/utilities/github.py
--rw-r--r--   0        0        0    11959 2023-12-18 12:05:45.872934 gigachain_community-0.0.6.1/langchain_community/utilities/gitlab.py
--rw-r--r--   0        0        0     1858 2023-12-18 12:05:45.873304 gigachain_community-0.0.6.1/langchain_community/utilities/golden_query.py
--rw-r--r--   0        0        0     3400 2023-12-21 15:51:51.843661 gigachain_community-0.0.6.1/langchain_community/utilities/google_finance.py
--rw-r--r--   0        0        0     2804 2023-12-21 15:51:51.844769 gigachain_community-0.0.6.1/langchain_community/utilities/google_jobs.py
--rw-r--r--   0        0        0     2859 2023-12-18 12:05:45.874583 gigachain_community-0.0.6.1/langchain_community/utilities/google_lens.py
--rw-r--r--   0        0        0     4107 2023-12-18 12:05:45.874988 gigachain_community-0.0.6.1/langchain_community/utilities/google_places_api.py
--rw-r--r--   0        0        0     5171 2023-12-18 12:05:45.875442 gigachain_community-0.0.6.1/langchain_community/utilities/google_scholar.py
--rw-r--r--   0        0        0     5048 2023-12-18 12:05:45.876258 gigachain_community-0.0.6.1/langchain_community/utilities/google_search.py
--rw-r--r--   0        0        0     6503 2023-12-18 12:05:45.876775 gigachain_community-0.0.6.1/langchain_community/utilities/google_serper.py
--rw-r--r--   0        0        0     4033 2023-12-18 12:05:45.877158 gigachain_community-0.0.6.1/langchain_community/utilities/google_trends.py
--rw-r--r--   0        0        0     1903 2023-12-18 12:05:45.877780 gigachain_community-0.0.6.1/langchain_community/utilities/graphql.py
--rw-r--r--   0        0        0     6195 2023-12-18 12:05:45.878463 gigachain_community-0.0.6.1/langchain_community/utilities/jira.py
--rw-r--r--   0        0        0     2647 2023-12-18 12:05:45.879155 gigachain_community-0.0.6.1/langchain_community/utilities/max_compute.py
--rw-r--r--   0        0        0     3748 2023-12-18 12:05:45.879594 gigachain_community-0.0.6.1/langchain_community/utilities/merriam_webster.py
--rw-r--r--   0        0        0     6809 2023-12-18 12:05:45.880096 gigachain_community-0.0.6.1/langchain_community/utilities/metaphor_search.py
--rw-r--r--   0        0        0     1820 2023-12-21 15:51:51.846047 gigachain_community-0.0.6.1/langchain_community/utilities/nasa.py
--rw-r--r--   0        0        0     3287 2023-12-18 12:05:45.881008 gigachain_community-0.0.6.1/langchain_community/utilities/opaqueprompts.py
--rw-r--r--   0        0        0    11017 2023-12-18 12:05:45.881524 gigachain_community-0.0.6.1/langchain_community/utilities/openapi.py
--rw-r--r--   0        0        0     2462 2023-12-18 12:05:45.882200 gigachain_community-0.0.6.1/langchain_community/utilities/openweathermap.py
--rw-r--r--   0        0        0     3351 2023-12-18 12:05:45.882568 gigachain_community-0.0.6.1/langchain_community/utilities/outline.py
--rw-r--r--   0        0        0     2355 2023-12-18 12:05:45.883116 gigachain_community-0.0.6.1/langchain_community/utilities/portkey.py
--rw-r--r--   0        0        0    11246 2023-12-18 12:05:45.883973 gigachain_community-0.0.6.1/langchain_community/utilities/powerbi.py
--rw-r--r--   0        0        0     6936 2023-12-18 12:05:45.884535 gigachain_community-0.0.6.1/langchain_community/utilities/pubmed.py
--rw-r--r--   0        0        0     2159 2023-12-18 12:05:45.885022 gigachain_community-0.0.6.1/langchain_community/utilities/python.py
--rw-r--r--   0        0        0     4474 2023-12-18 12:05:45.885341 gigachain_community-0.0.6.1/langchain_community/utilities/reddit_search.py
--rw-r--r--   0        0        0     8221 2023-12-18 12:05:45.885856 gigachain_community-0.0.6.1/langchain_community/utilities/redis.py
--rw-r--r--   0        0        0     6992 2023-12-18 12:05:45.886179 gigachain_community-0.0.6.1/langchain_community/utilities/requests.py
--rw-r--r--   0        0        0     2220 2023-12-18 12:05:45.886517 gigachain_community-0.0.6.1/langchain_community/utilities/scenexplain.py
--rw-r--r--   0        0        0     5226 2023-12-18 12:05:45.887002 gigachain_community-0.0.6.1/langchain_community/utilities/searchapi.py
--rw-r--r--   0        0        0    16625 2023-12-18 12:05:45.887512 gigachain_community-0.0.6.1/langchain_community/utilities/searx_search.py
--rw-r--r--   0        0        0     8490 2023-12-18 12:05:45.889068 gigachain_community-0.0.6.1/langchain_community/utilities/serpapi.py
--rw-r--r--   0        0        0     7519 2023-12-18 12:05:45.889509 gigachain_community-0.0.6.1/langchain_community/utilities/spark_sql.py
--rw-r--r--   0        0        0    19639 2023-12-21 15:51:51.847139 gigachain_community-0.0.6.1/langchain_community/utilities/sql_database.py
--rw-r--r--   0        0        0     2639 2023-12-18 12:05:45.890765 gigachain_community-0.0.6.1/langchain_community/utilities/stackexchange.py
--rw-r--r--   0        0        0     5857 2023-12-18 12:05:45.891172 gigachain_community-0.0.6.1/langchain_community/utilities/steam.py
--rw-r--r--   0        0        0     6834 2023-12-18 12:05:45.891644 gigachain_community-0.0.6.1/langchain_community/utilities/tavily_search.py
--rw-r--r--   0        0        0     4006 2023-12-18 12:05:45.892063 gigachain_community-0.0.6.1/langchain_community/utilities/tensorflow_datasets.py
--rw-r--r--   0        0        0     3441 2023-12-18 12:05:45.892530 gigachain_community-0.0.6.1/langchain_community/utilities/twilio.py
--rw-r--r--   0        0        0     4072 2023-12-21 15:51:51.848192 gigachain_community-0.0.6.1/langchain_community/utilities/vertexai.py
--rw-r--r--   0        0        0     4045 2023-12-18 12:05:45.893264 gigachain_community-0.0.6.1/langchain_community/utilities/wikipedia.py
--rw-r--r--   0        0        0     2011 2023-12-18 12:05:45.893697 gigachain_community-0.0.6.1/langchain_community/utilities/wolfram_alpha.py
--rw-r--r--   0        0        0    11666 2023-12-18 12:05:45.894686 gigachain_community-0.0.6.1/langchain_community/utilities/zapier.py
--rw-r--r--   0        0        0        0 2023-12-18 12:05:45.894897 gigachain_community-0.0.6.1/langchain_community/utils/__init__.py
--rw-r--r--   0        0        0     2713 2023-12-18 12:05:45.895328 gigachain_community-0.0.6.1/langchain_community/utils/math.py
--rw-r--r--   0        0        0      264 2023-12-21 15:51:51.849060 gigachain_community-0.0.6.1/langchain_community/utils/openai.py
--rw-r--r--   0        0        0     1518 2023-12-18 12:05:45.896319 gigachain_community-0.0.6.1/langchain_community/utils/openai_functions.py
--rw-r--r--   0        0        0    15965 2023-12-21 15:51:51.850380 gigachain_community-0.0.6.1/langchain_community/vectorstores/__init__.py
--rw-r--r--   0        0        0    19780 2023-12-18 12:05:45.897986 gigachain_community-0.0.6.1/langchain_community/vectorstores/alibabacloud_opensearch.py
--rw-r--r--   0        0        0    15738 2023-12-18 12:05:45.898774 gigachain_community-0.0.6.1/langchain_community/vectorstores/analyticdb.py
--rw-r--r--   0        0        0    16632 2023-12-18 12:05:45.899285 gigachain_community-0.0.6.1/langchain_community/vectorstores/annoy.py
--rw-r--r--   0        0        0    28449 2023-12-18 12:05:45.900342 gigachain_community-0.0.6.1/langchain_community/vectorstores/astradb.py
--rw-r--r--   0        0        0    12136 2023-12-18 12:05:45.900865 gigachain_community-0.0.6.1/langchain_community/vectorstores/atlas.py
--rw-r--r--   0        0        0    21155 2023-12-18 12:05:45.918722 gigachain_community-0.0.6.1/langchain_community/vectorstores/awadb.py
--rw-r--r--   0        0        0    14522 2023-12-18 12:05:45.920951 gigachain_community-0.0.6.1/langchain_community/vectorstores/azure_cosmos_db.py
--rw-r--r--   0        0        0    27218 2023-12-18 12:05:45.921571 gigachain_community-0.0.6.1/langchain_community/vectorstores/azuresearch.py
--rw-r--r--   0        0        0    15034 2023-12-18 12:05:45.922399 gigachain_community-0.0.6.1/langchain_community/vectorstores/bageldb.py
--rw-r--r--   0        0        0    16548 2023-12-18 12:05:45.923166 gigachain_community-0.0.6.1/langchain_community/vectorstores/baiducloud_vector_search.py
--rw-r--r--   0        0        0    14809 2023-12-18 12:05:45.924380 gigachain_community-0.0.6.1/langchain_community/vectorstores/cassandra.py
--rw-r--r--   0        0        0    30367 2023-12-18 12:05:45.924915 gigachain_community-0.0.6.1/langchain_community/vectorstores/chroma.py
--rw-r--r--   0        0        0    10953 2023-12-21 15:51:51.851489 gigachain_community-0.0.6.1/langchain_community/vectorstores/clarifai.py
--rw-r--r--   0        0        0    17735 2023-12-18 12:05:45.926412 gigachain_community-0.0.6.1/langchain_community/vectorstores/clickhouse.py
--rw-r--r--   0        0        0    12713 2023-12-21 15:51:51.853117 gigachain_community-0.0.6.1/langchain_community/vectorstores/dashvector.py
--rw-r--r--   0        0        0    18360 2023-12-18 12:05:45.928909 gigachain_community-0.0.6.1/langchain_community/vectorstores/databricks_vector_search.py
--rw-r--r--   0        0        0    40265 2023-12-18 12:05:45.929819 gigachain_community-0.0.6.1/langchain_community/vectorstores/deeplake.py
--rw-r--r--   0        0        0    12973 2023-12-18 12:05:45.930835 gigachain_community-0.0.6.1/langchain_community/vectorstores/dingo.py
--rw-r--r--   0        0        0      236 2023-12-18 12:05:45.931359 gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/__init__.py
--rw-r--r--   0        0        0     6945 2023-12-18 12:05:45.931915 gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/base.py
--rw-r--r--   0        0        0     4044 2023-12-18 12:05:45.932338 gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/hnsw.py
--rw-r--r--   0        0        0     2418 2023-12-18 12:05:45.932697 gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/in_memory.py
--rw-r--r--   0        0        0    28714 2023-12-18 12:05:45.933055 gigachain_community-0.0.6.1/langchain_community/vectorstores/elastic_vector_search.py
--rw-r--r--   0        0        0    46536 2023-12-18 12:05:45.933608 gigachain_community-0.0.6.1/langchain_community/vectorstores/elasticsearch.py
--rw-r--r--   0        0        0    14183 2023-12-18 12:05:45.934415 gigachain_community-0.0.6.1/langchain_community/vectorstores/epsilla.py
--rw-r--r--   0        0        0    42989 2023-12-18 12:05:45.934999 gigachain_community-0.0.6.1/langchain_community/vectorstores/faiss.py
--rw-r--r--   0        0        0    26837 2023-12-18 12:05:45.935987 gigachain_community-0.0.6.1/langchain_community/vectorstores/hippo.py
--rw-r--r--   0        0        0    13642 2023-12-18 12:05:45.936884 gigachain_community-0.0.6.1/langchain_community/vectorstores/hologres.py
--rw-r--r--   0        0        0    14253 2023-12-28 13:38:34.506816 gigachain_community-0.0.6.1/langchain_community/vectorstores/jaguar.py
--rw-r--r--   0        0        0     4173 2023-12-18 12:05:45.938754 gigachain_community-0.0.6.1/langchain_community/vectorstores/lancedb.py
--rw-r--r--   0        0        0     7745 2023-12-18 12:05:45.939222 gigachain_community-0.0.6.1/langchain_community/vectorstores/llm_rails.py
--rw-r--r--   0        0        0    17047 2023-12-18 12:05:45.940104 gigachain_community-0.0.6.1/langchain_community/vectorstores/marqo.py
--rw-r--r--   0        0        0    21430 2023-12-21 15:51:51.855202 gigachain_community-0.0.6.1/langchain_community/vectorstores/matching_engine.py
--rw-r--r--   0        0        0    10400 2023-12-18 12:05:45.941286 gigachain_community-0.0.6.1/langchain_community/vectorstores/meilisearch.py
--rw-r--r--   0        0        0    31749 2023-12-18 12:05:45.941785 gigachain_community-0.0.6.1/langchain_community/vectorstores/milvus.py
--rw-r--r--   0        0        0    19047 2023-12-28 13:38:34.508044 gigachain_community-0.0.6.1/langchain_community/vectorstores/momento_vector_index.py
--rw-r--r--   0        0        0    13869 2023-12-21 15:51:51.856691 gigachain_community-0.0.6.1/langchain_community/vectorstores/mongodb_atlas.py
--rw-r--r--   0        0        0    22546 2023-12-18 12:05:45.943853 gigachain_community-0.0.6.1/langchain_community/vectorstores/myscale.py
--rw-r--r--   0        0        0    35000 2023-12-21 15:51:51.857828 gigachain_community-0.0.6.1/langchain_community/vectorstores/neo4j_vector.py
--rw-r--r--   0        0        0     5403 2023-12-18 12:05:45.944847 gigachain_community-0.0.6.1/langchain_community/vectorstores/nucliadb.py
--rw-r--r--   0        0        0    31850 2023-12-18 12:05:45.946289 gigachain_community-0.0.6.1/langchain_community/vectorstores/opensearch_vector_search.py
--rw-r--r--   0        0        0    17895 2023-12-18 12:05:45.948220 gigachain_community-0.0.6.1/langchain_community/vectorstores/pgembedding.py
--rw-r--r--   0        0        0     8891 2023-12-21 15:51:51.859006 gigachain_community-0.0.6.1/langchain_community/vectorstores/pgvecto_rs.py
--rw-r--r--   0        0        0    34006 2023-12-21 15:51:51.860255 gigachain_community-0.0.6.1/langchain_community/vectorstores/pgvector.py
--rw-r--r--   0        0        0    17517 2023-12-18 12:05:45.950012 gigachain_community-0.0.6.1/langchain_community/vectorstores/pinecone.py
--rw-r--r--   0        0        0    89408 2023-12-18 12:05:45.951827 gigachain_community-0.0.6.1/langchain_community/vectorstores/qdrant.py
--rw-r--r--   0        0        0      265 2023-12-18 12:05:45.952450 gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/__init__.py
--rw-r--r--   0        0        0    54448 2023-12-18 12:05:45.954415 gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/base.py
--rw-r--r--   0        0        0      420 2023-12-18 12:05:45.954942 gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/constants.py
--rw-r--r--   0        0        0    16227 2023-12-18 12:05:45.956102 gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/filters.py
--rw-r--r--   0        0        0    10419 2023-12-18 12:05:45.957786 gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/schema.py
--rw-r--r--   0        0        0    12192 2023-12-18 12:05:45.958270 gigachain_community-0.0.6.1/langchain_community/vectorstores/rocksetdb.py
--rw-r--r--   0        0        0    19610 2023-12-18 12:05:45.958595 gigachain_community-0.0.6.1/langchain_community/vectorstores/scann.py
--rw-r--r--   0        0        0     9707 2023-12-28 13:38:34.509507 gigachain_community-0.0.6.1/langchain_community/vectorstores/semadb.py
--rw-r--r--   0        0        0    17628 2023-12-18 12:05:45.959791 gigachain_community-0.0.6.1/langchain_community/vectorstores/singlestoredb.py
--rw-r--r--   0        0        0    12375 2023-12-21 15:51:51.861914 gigachain_community-0.0.6.1/langchain_community/vectorstores/sklearn.py
--rw-r--r--   0        0        0     7316 2023-12-18 12:05:45.963006 gigachain_community-0.0.6.1/langchain_community/vectorstores/sqlitevss.py
--rw-r--r--   0        0        0    17150 2023-12-18 12:05:45.968600 gigachain_community-0.0.6.1/langchain_community/vectorstores/starrocks.py
--rw-r--r--   0        0        0    15175 2023-12-18 12:05:45.970976 gigachain_community-0.0.6.1/langchain_community/vectorstores/supabase.py
--rw-r--r--   0        0        0    14706 2023-12-28 13:38:34.510538 gigachain_community-0.0.6.1/langchain_community/vectorstores/surrealdb.py
--rw-r--r--   0        0        0     9557 2023-12-18 12:05:45.975992 gigachain_community-0.0.6.1/langchain_community/vectorstores/tair.py
--rw-r--r--   0        0        0    13927 2023-12-21 15:51:51.864636 gigachain_community-0.0.6.1/langchain_community/vectorstores/tencentvectordb.py
--rw-r--r--   0        0        0     4927 2023-12-18 12:05:45.980251 gigachain_community-0.0.6.1/langchain_community/vectorstores/tigris.py
--rw-r--r--   0        0        0    28826 2023-12-21 15:51:51.865840 gigachain_community-0.0.6.1/langchain_community/vectorstores/tiledb.py
--rw-r--r--   0        0        0    29831 2023-12-21 15:51:51.867775 gigachain_community-0.0.6.1/langchain_community/vectorstores/timescalevector.py
--rw-r--r--   0        0        0     9712 2023-12-18 12:05:45.986913 gigachain_community-0.0.6.1/langchain_community/vectorstores/typesense.py
--rw-r--r--   0        0        0     5888 2023-12-18 12:05:45.988954 gigachain_community-0.0.6.1/langchain_community/vectorstores/usearch.py
--rw-r--r--   0        0        0     2474 2023-12-18 12:05:45.989508 gigachain_community-0.0.6.1/langchain_community/vectorstores/utils.py
--rw-r--r--   0        0        0    12908 2023-12-18 12:05:45.990381 gigachain_community-0.0.6.1/langchain_community/vectorstores/vald.py
--rw-r--r--   0        0        0    19844 2023-12-18 12:05:45.990761 gigachain_community-0.0.6.1/langchain_community/vectorstores/vearch.py
--rw-r--r--   0        0        0    21275 2023-12-28 13:38:34.511972 gigachain_community-0.0.6.1/langchain_community/vectorstores/vectara.py
--rw-r--r--   0        0        0     9785 2023-12-18 12:05:45.998180 gigachain_community-0.0.6.1/langchain_community/vectorstores/vespa.py
--rw-r--r--   0        0        0    19226 2023-12-18 12:05:46.002110 gigachain_community-0.0.6.1/langchain_community/vectorstores/weaviate.py
--rw-r--r--   0        0        0     9032 2023-12-18 12:05:46.007223 gigachain_community-0.0.6.1/langchain_community/vectorstores/xata.py
--rw-r--r--   0        0        0    10761 2023-12-18 12:05:46.013574 gigachain_community-0.0.6.1/langchain_community/vectorstores/yellowbrick.py
--rw-r--r--   0        0        0    23192 2023-12-18 12:05:46.014090 gigachain_community-0.0.6.1/langchain_community/vectorstores/zep.py
--rw-r--r--   0        0        0     7575 2023-12-18 12:05:46.016389 gigachain_community-0.0.6.1/langchain_community/vectorstores/zilliz.py
--rw-r--r--   0        0        0     9700 2024-01-09 14:50:12.847829 gigachain_community-0.0.6.1/pyproject.toml
--rw-r--r--   0        0        0     7263 1970-01-01 00:00:00.000000 gigachain_community-0.0.6.1/PKG-INFO
+-rw-r--r--   0        0        0     1352 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/README.md
+-rw-r--r--   0        0        0       97 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/__gigachain_community.py
+-rw-r--r--   0        0        0      569 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/__init__.py
+-rw-r--r--   0        0        0      336 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/adapters/__init__.py
+-rw-r--r--   0        0        0    12347 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/adapters/openai.py
+-rw-r--r--   0        0        0     6498 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/__init__.py
+-rw-r--r--   0        0        0       25 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/ainetwork/__init__.py
+-rw-r--r--   0        0        0     1762 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/ainetwork/toolkit.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/amadeus/__init__.py
+-rw-r--r--   0        0        0     1084 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/amadeus/toolkit.py
+-rw-r--r--   0        0        0     1057 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/azure_ai_services.py
+-rw-r--r--   0        0        0     1114 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/azure_cognitive_services.py
+-rw-r--r--   0        0        0      100 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/base.py
+-rw-r--r--   0        0        0       32 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/cassandra_database/__init__.py
+-rw-r--r--   0        0        0     1027 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/cassandra_database/toolkit.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/clickup/__init__.py
+-rw-r--r--   0        0        0     3602 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/clickup/toolkit.py
+-rw-r--r--   0        0        0       26 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/cogniswitch/__init__.py
+-rw-r--r--   0        0        0     1322 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/cogniswitch/toolkit.py
+-rw-r--r--   0        0        0      116 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/connery/__init__.py
+-rw-r--r--   0        0        0     1393 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/connery/toolkit.py
+-rw-r--r--   0        0        0     1091 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/csv/__init__.py
+-rw-r--r--   0        0        0      177 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/file_management/__init__.py
+-rw-r--r--   0        0        0     3196 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/file_management/toolkit.py
+-rw-r--r--   0        0        0       22 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/github/__init__.py
+-rw-r--r--   0        0        0    10143 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/github/toolkit.py
+-rw-r--r--   0        0        0       22 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/gitlab/__init__.py
+-rw-r--r--   0        0        0     2913 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/gitlab/toolkit.py
+-rw-r--r--   0        0        0       21 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/gmail/__init__.py
+-rw-r--r--   0        0        0     2026 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/gmail/toolkit.py
+-rw-r--r--   0        0        0       20 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/jira/__init__.py
+-rw-r--r--   0        0        0     2235 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/jira/toolkit.py
+-rw-r--r--   0        0        0       18 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/json/__init__.py
+-rw-r--r--   0        0        0     1895 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/json/base.py
+-rw-r--r--   0        0        0     3288 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/json/prompt.py
+-rw-r--r--   0        0        0      577 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/json/toolkit.py
+-rw-r--r--   0        0        0    28946 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/load_tools.py
+-rw-r--r--   0        0        0       23 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/multion/__init__.py
+-rw-r--r--   0        0        0     1173 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/multion/toolkit.py
+-rw-r--r--   0        0        0       19 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/nasa/__init__.py
+-rw-r--r--   0        0        0     1939 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/nasa/toolkit.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/nla/__init__.py
+-rw-r--r--   0        0        0     2006 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/nla/tool.py
+-rw-r--r--   0        0        0     4177 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/nla/toolkit.py
+-rw-r--r--   0        0        0       25 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/office365/__init__.py
+-rw-r--r--   0        0        0     1793 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/office365/toolkit.py
+-rw-r--r--   0        0        0       26 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/__init__.py
+-rw-r--r--   0        0        0     2871 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/base.py
+-rw-r--r--   0        0        0    13974 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/planner.py
+-rw-r--r--   0        0        0    19382 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/planner_prompt.py
+-rw-r--r--   0        0        0     3174 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/prompt.py
+-rw-r--r--   0        0        0     2557 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/spec.py
+-rw-r--r--   0        0        0     4239 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/toolkit.py
+-rw-r--r--   0        0        0      175 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/playwright/__init__.py
+-rw-r--r--   0        0        0     4224 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/playwright/toolkit.py
+-rw-r--r--   0        0        0       22 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/polygon/__init__.py
+-rw-r--r--   0        0        0     1107 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/polygon/toolkit.py
+-rw-r--r--   0        0        0       22 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/__init__.py
+-rw-r--r--   0        0        0     2492 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/base.py
+-rw-r--r--   0        0        0     2650 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/chat_base.py
+-rw-r--r--   0        0        0     5097 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/prompt.py
+-rw-r--r--   0        0        0     3779 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/toolkit.py
+-rw-r--r--   0        0        0       21 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/slack/__init__.py
+-rw-r--r--   0        0        0     1095 2024-05-24 09:20:54.634771 gigachain_community-0.2.0/langchain_community/agent_toolkits/slack/toolkit.py
+-rw-r--r--   0        0        0       23 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/__init__.py
+-rw-r--r--   0        0        0     2356 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/base.py
+-rw-r--r--   0        0        0     2297 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/prompt.py
+-rw-r--r--   0        0        0     1066 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/toolkit.py
+-rw-r--r--   0        0        0       17 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/__init__.py
+-rw-r--r--   0        0        0     9399 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/base.py
+-rw-r--r--   0        0        0     2727 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/prompt.py
+-rw-r--r--   0        0        0     2984 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/toolkit.py
+-rw-r--r--   0        0        0       21 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/steam/__init__.py
+-rw-r--r--   0        0        0     1474 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/steam/toolkit.py
+-rw-r--r--   0        0        0     1095 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/xorbits/__init__.py
+-rw-r--r--   0        0        0       22 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/zapier/__init__.py
+-rw-r--r--   0        0        0     1967 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/agent_toolkits/zapier/toolkit.py
+-rw-r--r--   0        0        0    91226 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/cache.py
+-rw-r--r--   0        0        0     5636 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/__init__.py
+-rw-r--r--   0        0        0    14597 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/aim_callback.py
+-rw-r--r--   0        0        0    14738 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/argilla_callback.py
+-rw-r--r--   0        0        0     7480 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/arize_callback.py
+-rw-r--r--   0        0        0    11243 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/arthur_callback.py
+-rw-r--r--   0        0        0     3906 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/bedrock_anthropic_callback.py
+-rw-r--r--   0        0        0    18634 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/clearml_callback.py
+-rw-r--r--   0        0        0    22975 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/comet_ml_callback.py
+-rw-r--r--   0        0        0     6382 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/confident_callback.py
+-rw-r--r--   0        0        0     6496 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/context_callback.py
+-rw-r--r--   0        0        0    11430 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/fiddler_callback.py
+-rw-r--r--   0        0        0    12769 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/flyte_callback.py
+-rw-r--r--   0        0        0     2587 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/human.py
+-rw-r--r--   0        0        0     8764 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/infino_callback.py
+-rw-r--r--   0        0        0    13879 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/labelstudio_callback.py
+-rw-r--r--   0        0        0    20555 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/llmonitor_callback.py
+-rw-r--r--   0        0        0     3201 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/manager.py
+-rw-r--r--   0        0        0    27406 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/mlflow_callback.py
+-rw-r--r--   0        0        0     8576 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/openai_info.py
+-rw-r--r--   0        0        0     5536 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/promptlayer_callback.py
+-rw-r--r--   0        0        0     8787 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/sagemaker_callback.py
+-rw-r--r--   0        0        0     3183 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/streamlit/__init__.py
+-rw-r--r--   0        0        0     5395 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/streamlit/mutable_expander.py
+-rw-r--r--   0        0        0    15562 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/streamlit/streamlit_callback_handler.py
+-rw-r--r--   0        0        0      498 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/tracers/__init__.py
+-rw-r--r--   0        0        0     4547 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/tracers/comet.py
+-rw-r--r--   0        0        0    19021 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/tracers/wandb.py
+-rw-r--r--   0        0        0     4526 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/trubrics_callback.py
+-rw-r--r--   0        0        0    14446 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/uptrain_callback.py
+-rw-r--r--   0        0        0     7879 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/utils.py
+-rw-r--r--   0        0        0    20385 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/wandb_callback.py
+-rw-r--r--   0        0        0     7881 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/callbacks/whylabs_callback.py
+-rw-r--r--   0        0        0      618 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/__init__.py
+-rw-r--r--   0        0        0      465 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/ernie_functions/__init__.py
+-rw-r--r--   0        0        0    23323 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/ernie_functions/base.py
+-rw-r--r--   0        0        0       49 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/__init__.py
+-rw-r--r--   0        0        0     8407 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/arangodb.py
+-rw-r--r--   0        0        0     3699 2024-05-24 09:20:54.638771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/base.py
+-rw-r--r--   0        0        0    10589 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/cypher.py
+-rw-r--r--   0        0        0     9625 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/cypher_utils.py
+-rw-r--r--   0        0        0     5295 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/falkordb.py
+-rw-r--r--   0        0        0     7818 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/gremlin.py
+-rw-r--r--   0        0        0     3727 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/hugegraph.py
+-rw-r--r--   0        0        0     5516 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/kuzu.py
+-rw-r--r--   0        0        0     3715 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/nebulagraph.py
+-rw-r--r--   0        0        0     6908 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/neptune_cypher.py
+-rw-r--r--   0        0        0     6818 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/neptune_sparql.py
+-rw-r--r--   0        0        0     7197 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/ontotext_graphdb.py
+-rw-r--r--   0        0        0    25955 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/prompts.py
+-rw-r--r--   0        0        0     5848 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/graph_qa/sparql.py
+-rw-r--r--   0        0        0     3251 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/llm_requests.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/openapi/__init__.py
+-rw-r--r--   0        0        0     8786 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/openapi/chain.py
+-rw-r--r--   0        0        0     1791 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/openapi/prompts.py
+-rw-r--r--   0        0        0     1974 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/openapi/requests_chain.py
+-rw-r--r--   0        0        0     1846 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/openapi/response_chain.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/pebblo_retrieval/__init__.py
+-rw-r--r--   0        0        0     7715 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/pebblo_retrieval/base.py
+-rw-r--r--   0        0        0    10334 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/pebblo_retrieval/enforcement_filters.py
+-rw-r--r--   0        0        0     1707 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chains/pebblo_retrieval/models.py
+-rw-r--r--   0        0        0     2748 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/__init__.py
+-rw-r--r--   0        0        0       85 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/base.py
+-rw-r--r--   0        0        0     2694 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/facebook_messenger.py
+-rw-r--r--   0        0        0     4213 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/gmail.py
+-rw-r--r--   0        0        0     7925 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/imessage.py
+-rw-r--r--   0        0        0     5736 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/langsmith.py
+-rw-r--r--   0        0        0     3127 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/slack.py
+-rw-r--r--   0        0        0     5379 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/telegram.py
+-rw-r--r--   0        0        0     3297 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/utils.py
+-rw-r--r--   0        0        0     4295 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_loaders/whatsapp.py
+-rw-r--r--   0        0        0     5677 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/__init__.py
+-rw-r--r--   0        0        0     5875 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/astradb.py
+-rw-r--r--   0        0        0     3017 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/cassandra.py
+-rw-r--r--   0        0        0     6511 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/cosmos_db.py
+-rw-r--r--   0        0        0     7250 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/dynamodb.py
+-rw-r--r--   0        0        0     7177 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/elasticsearch.py
+-rw-r--r--   0        0        0     1402 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/file.py
+-rw-r--r--   0        0        0     3350 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/firestore.py
+-rw-r--r--   0        0        0      130 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/in_memory.py
+-rw-r--r--   0        0        0     7112 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/momento.py
+-rw-r--r--   0        0        0     3107 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/mongodb.py
+-rw-r--r--   0        0        0     5129 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/neo4j.py
+-rw-r--r--   0        0        0     3345 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/postgres.py
+-rw-r--r--   0        0        0     2192 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/redis.py
+-rw-r--r--   0        0        0     9529 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/rocksetdb.py
+-rw-r--r--   0        0        0    10331 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/singlestoredb.py
+-rw-r--r--   0        0        0     4722 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/sql.py
+-rw-r--r--   0        0        0     1444 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/streamlit.py
+-rw-r--r--   0        0        0     5255 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/tidb.py
+-rw-r--r--   0        0        0     2148 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/upstash_redis.py
+-rw-r--r--   0        0        0     4639 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/xata.py
+-rw-r--r--   0        0        0     8910 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_message_histories/zep.py
+-rw-r--r--   0        0        0     9183 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/__init__.py
+-rw-r--r--   0        0        0     8210 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/anthropic.py
+-rw-r--r--   0        0        0     8251 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/anyscale.py
+-rw-r--r--   0        0        0    11342 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/azure_openai.py
+-rw-r--r--   0        0        0    15437 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/azureml_endpoint.py
+-rw-r--r--   0        0        0     9987 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/baichuan.py
+-rw-r--r--   0        0        0    23064 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/baidu_qianfan_endpoint.py
+-rw-r--r--   0        0        0    10959 2024-05-24 09:20:54.642771 gigachain_community-0.2.0/langchain_community/chat_models/bedrock.py
+-rw-r--r--   0        0        0     8226 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/cohere.py
+-rw-r--r--   0        0        0     8513 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/coze.py
+-rw-r--r--   0        0        0     5396 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/dappier.py
+-rw-r--r--   0        0        0     1532 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/databricks.py
+-rw-r--r--   0        0        0    16723 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/deepinfra.py
+-rw-r--r--   0        0        0    13134 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/edenai.py
+-rw-r--r--   0        0        0     8039 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/ernie.py
+-rw-r--r--   0        0        0     5588 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/everlyai.py
+-rw-r--r--   0        0        0     3218 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/fake.py
+-rw-r--r--   0        0        0    12018 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/fireworks.py
+-rw-r--r--   0        0        0     7114 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/friendli.py
+-rw-r--r--   0        0        0    22738 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/gigachat.py
+-rw-r--r--   0        0        0     4191 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/gigachat_tools.py
+-rw-r--r--   0        0        0    11609 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/google_palm.py
+-rw-r--r--   0        0        0    13080 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/gpt_router.py
+-rw-r--r--   0        0        0     7909 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/huggingface.py
+-rw-r--r--   0        0        0     3741 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/human.py
+-rw-r--r--   0        0        0    10653 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/hunyuan.py
+-rw-r--r--   0        0        0     7628 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/javelin_ai_gateway.py
+-rw-r--r--   0        0        0    15317 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/jinachat.py
+-rw-r--r--   0        0        0    20267 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/kinetica.py
+-rw-r--r--   0        0        0    10027 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/konko.py
+-rw-r--r--   0        0        0    15833 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/litellm.py
+-rw-r--r--   0        0        0     8078 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/litellm_router.py
+-rw-r--r--   0        0        0     8616 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/llama_edge.py
+-rw-r--r--   0        0        0    13486 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/maritalk.py
+-rw-r--r--   0        0        0      967 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/meta.py
+-rw-r--r--   0        0        0     3285 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/minimax.py
+-rw-r--r--   0        0        0     9874 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/mlflow.py
+-rw-r--r--   0        0        0     6711 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/mlflow_ai_gateway.py
+-rw-r--r--   0        0        0     6088 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/mlx.py
+-rw-r--r--   0        0        0     1928 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/moonshot.py
+-rw-r--r--   0        0        0     3309 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/octoai.py
+-rw-r--r--   0        0        0    13444 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/ollama.py
+-rw-r--r--   0        0        0    26842 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/openai.py
+-rw-r--r--   0        0        0    10504 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/pai_eas_endpoint.py
+-rw-r--r--   0        0        0    10717 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/perplexity.py
+-rw-r--r--   0        0        0    14623 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/premai.py
+-rw-r--r--   0        0        0     5257 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/promptlayer_openai.py
+-rw-r--r--   0        0        0     2322 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/solar.py
+-rw-r--r--   0        0        0    16900 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/sparkllm.py
+-rw-r--r--   0        0        0    21549 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/tongyi.py
+-rw-r--r--   0        0        0    14584 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/vertexai.py
+-rw-r--r--   0        0        0     5296 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/volcengine_maas.py
+-rw-r--r--   0        0        0    10741 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/yandex.py
+-rw-r--r--   0        0        0    17253 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/yuan2.py
+-rw-r--r--   0        0        0    16931 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/chat_models/zhipuai.py
+-rw-r--r--   0        0        0     1469 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/cross_encoders/__init__.py
+-rw-r--r--   0        0        0      117 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/cross_encoders/base.py
+-rw-r--r--   0        0        0      525 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/cross_encoders/fake.py
+-rw-r--r--   0        0        0     1946 2024-05-24 09:20:54.646771 gigachain_community-0.2.0/langchain_community/cross_encoders/huggingface.py
+-rw-r--r--   0        0        0     5335 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/cross_encoders/sagemaker_endpoint.py
+-rw-r--r--   0        0        0     1176 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/__init__.py
+-rw-r--r--   0        0        0     1080 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/arbitrary_fn.py
+-rw-r--r--   0        0        0      834 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/base.py
+-rw-r--r--   0        0        0       70 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/document.py
+-rw-r--r--   0        0        0     1611 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/in_memory.py
+-rw-r--r--   0        0        0     1471 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/docstore/wikipedia.py
+-rw-r--r--   0        0        0     1215 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_compressors/__init__.py
+-rw-r--r--   0        0        0     2444 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_compressors/flashrank_rerank.py
+-rw-r--r--   0        0        0     4342 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_compressors/jina_rerank.py
+-rw-r--r--   0        0        0     6695 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_compressors/llmlingua_filter.py
+-rw-r--r--   0        0        0     5704 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_compressors/openvino_rerank.py
+-rw-r--r--   0        0        0    35823 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/__init__.py
+-rw-r--r--   0        0        0     2803 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/acreom.py
+-rw-r--r--   0        0        0    10157 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/airbyte.py
+-rw-r--r--   0        0        0      865 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/airbyte_json.py
+-rw-r--r--   0        0        0     1157 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/airtable.py
+-rw-r--r--   0        0        0     2759 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/apify_dataset.py
+-rw-r--r--   0        0        0     5129 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/arcgis_loader.py
+-rw-r--r--   0        0        0      907 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/arxiv.py
+-rw-r--r--   0        0        0     8134 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/assemblyai.py
+-rw-r--r--   0        0        0     4642 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/astradb.py
+-rw-r--r--   0        0        0     8910 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/async_html.py
+-rw-r--r--   0        0        0     5886 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/athena.py
+-rw-r--r--   0        0        0      563 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/azlyrics.py
+-rw-r--r--   0        0        0     1432 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/azure_ai_data.py
+-rw-r--r--   0        0        0     1566 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/azure_blob_storage_container.py
+-rw-r--r--   0        0        0     1644 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/azure_blob_storage_file.py
+-rw-r--r--   0        0        0     1774 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/baiducloud_bos_directory.py
+-rw-r--r--   0        0        0     1848 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/baiducloud_bos_file.py
+-rw-r--r--   0        0        0      126 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/base.py
+-rw-r--r--   0        0        0     7249 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/base_o365.py
+-rw-r--r--   0        0        0     3540 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/bibtex.py
+-rw-r--r--   0        0        0     3852 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/bigquery.py
+-rw-r--r--   0        0        0     4401 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/bilibili.py
+-rw-r--r--   0        0        0    10288 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blackboard.py
+-rw-r--r--   0        0        0      948 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/__init__.py
+-rw-r--r--   0        0        0     5390 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/file_system.py
+-rw-r--r--   0        0        0      145 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/schema.py
+-rw-r--r--   0        0        0     1525 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/youtube_audio.py
+-rw-r--r--   0        0        0     5709 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/blockchain.py
+-rw-r--r--   0        0        0     1047 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/brave_search.py
+-rw-r--r--   0        0        0     1540 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/browserbase.py
+-rw-r--r--   0        0        0     2007 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/browserless.py
+-rw-r--r--   0        0        0     5142 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/cassandra.py
+-rw-r--r--   0        0        0     1986 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/chatgpt.py
+-rw-r--r--   0        0        0     3031 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/chm.py
+-rw-r--r--   0        0        0     3410 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/chromium.py
+-rw-r--r--   0        0        0      527 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/college_confidential.py
+-rw-r--r--   0        0        0     3416 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/concurrent.py
+-rw-r--r--   0        0        0    29324 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/confluence.py
+-rw-r--r--   0        0        0     1102 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/conllu.py
+-rw-r--r--   0        0        0     3515 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/couchbase.py
+-rw-r--r--   0        0        0     5835 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/csv_loader.py
+-rw-r--r--   0        0        0     6589 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/cube_semantic.py
+-rw-r--r--   0        0        0     4937 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/datadog_logs.py
+-rw-r--r--   0        0        0     2176 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/dataframe.py
+-rw-r--r--   0        0        0     2054 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/diffbot.py
+-rw-r--r--   0        0        0     8446 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/directory.py
+-rw-r--r--   0        0        0     1237 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/discord.py
+-rw-r--r--   0        0        0     3859 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/doc_intelligence.py
+-rw-r--r--   0        0        0    13621 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/docugami.py
+-rw-r--r--   0        0        0     1853 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/docusaurus.py
+-rw-r--r--   0        0        0     6271 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/dropbox.py
+-rw-r--r--   0        0        0     3150 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/duckdb_loader.py
+-rw-r--r--   0        0        0     3835 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/email.py
+-rw-r--r--   0        0        0     1496 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/epub.py
+-rw-r--r--   0        0        0     7753 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/etherscan.py
+-rw-r--r--   0        0        0     5931 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/evernote.py
+-rw-r--r--   0        0        0     1754 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/excel.py
+-rw-r--r--   0        0        0     1270 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/facebook_chat.py
+-rw-r--r--   0        0        0     2171 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/fauna.py
+-rw-r--r--   0        0        0     1543 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/figma.py
+-rw-r--r--   0        0        0     2453 2024-05-24 09:20:54.650771 gigachain_community-0.2.0/langchain_community/document_loaders/firecrawl.py
+-rw-r--r--   0        0        0     3041 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/gcs_directory.py
+-rw-r--r--   0        0        0     3316 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/gcs_file.py
+-rw-r--r--   0        0        0     6374 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/generic.py
+-rw-r--r--   0        0        0     2400 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/geodataframe.py
+-rw-r--r--   0        0        0     4018 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/git.py
+-rw-r--r--   0        0        0     3451 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/gitbook.py
+-rw-r--r--   0        0        0     8676 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/github.py
+-rw-r--r--   0        0        0     4459 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/glue_catalog.py
+-rw-r--r--   0        0        0     5279 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/google_speech_to_text.py
+-rw-r--r--   0        0        0    14435 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/googledrive.py
+-rw-r--r--   0        0        0      928 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/gutenberg.py
+-rw-r--r--   0        0        0     1640 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/helpers.py
+-rw-r--r--   0        0        0     2075 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/hn.py
+-rw-r--r--   0        0        0     1158 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/html.py
+-rw-r--r--   0        0        0     2098 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/html_bs.py
+-rw-r--r--   0        0        0     3095 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/hugging_face_dataset.py
+-rw-r--r--   0        0        0     3628 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/hugging_face_model.py
+-rw-r--r--   0        0        0     7642 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/ifixit.py
+-rw-r--r--   0        0        0     1173 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/image.py
+-rw-r--r--   0        0        0     3707 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/image_captions.py
+-rw-r--r--   0        0        0      477 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/imsdb.py
+-rw-r--r--   0        0        0     1688 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/iugu.py
+-rw-r--r--   0        0        0     3628 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/joplin.py
+-rw-r--r--   0        0        0     7366 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/json_loader.py
+-rw-r--r--   0        0        0     3919 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/kinetica_loader.py
+-rw-r--r--   0        0        0     6058 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/lakefs.py
+-rw-r--r--   0        0        0     2959 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/larksuite.py
+-rw-r--r--   0        0        0     4881 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/llmsherpa.py
+-rw-r--r--   0        0        0     1818 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/markdown.py
+-rw-r--r--   0        0        0     3079 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/mastodon.py
+-rw-r--r--   0        0        0     3199 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/max_compute.py
+-rw-r--r--   0        0        0     3859 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/mediawikidump.py
+-rw-r--r--   0        0        0      999 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/merge.py
+-rw-r--r--   0        0        0     2658 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/mhtml.py
+-rw-r--r--   0        0        0     8923 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/mintbase.py
+-rw-r--r--   0        0        0     3074 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/modern_treasury.py
+-rw-r--r--   0        0        0     3182 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/mongodb.py
+-rw-r--r--   0        0        0     4284 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/news.py
+-rw-r--r--   0        0        0     4297 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/notebook.py
+-rw-r--r--   0        0        0      834 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/notion.py
+-rw-r--r--   0        0        0     7715 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/notiondb.py
+-rw-r--r--   0        0        0     1102 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/nuclia.py
+-rw-r--r--   0        0        0     3593 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/obs_directory.py
+-rw-r--r--   0        0        0     4768 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/obs_file.py
+-rw-r--r--   0        0        0     6132 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/obsidian.py
+-rw-r--r--   0        0        0     1840 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/odt.py
+-rw-r--r--   0        0        0     3271 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/onedrive.py
+-rw-r--r--   0        0        0     1154 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/onedrive_file.py
+-rw-r--r--   0        0        0     7833 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/onenote.py
+-rw-r--r--   0        0        0     1219 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/open_city_data.py
+-rw-r--r--   0        0        0     4221 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/oracleadb_loader.py
+-rw-r--r--   0        0        0    15573 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/oracleai.py
+-rw-r--r--   0        0        0     1818 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/org_mode.py
+-rw-r--r--   0        0        0     1283 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/__init__.py
+-rw-r--r--   0        0        0    16911 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/audio.py
+-rw-r--r--   0        0        0     4050 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/doc_intelligence.py
+-rw-r--r--   0        0        0    15458 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/docai.py
+-rw-r--r--   0        0        0     2503 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/generic.py
+-rw-r--r--   0        0        0     5903 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/grobid.py
+-rw-r--r--   0        0        0      109 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/html/__init__.py
+-rw-r--r--   0        0        0     1608 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/html/bs4.py
+-rw-r--r--   0        0        0      136 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/__init__.py
+-rw-r--r--   0        0        0      877 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/c.py
+-rw-r--r--   0        0        0     3745 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/cobol.py
+-rw-r--r--   0        0        0      495 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/code_segmenter.py
+-rw-r--r--   0        0        0      893 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/cpp.py
+-rw-r--r--   0        0        0      893 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/csharp.py
+-rw-r--r--   0        0        0      693 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/go.py
+-rw-r--r--   0        0        0      736 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/java.py
+-rw-r--r--   0        0        0     2185 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/javascript.py
+-rw-r--r--   0        0        0      707 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/kotlin.py
+-rw-r--r--   0        0        0     7538 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/language_parser.py
+-rw-r--r--   0        0        0      790 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/lua.py
+-rw-r--r--   0        0        0      666 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/perl.py
+-rw-r--r--   0        0        0      850 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/php.py
+-rw-r--r--   0        0        0     1731 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/python.py
+-rw-r--r--   0        0        0      697 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/ruby.py
+-rw-r--r--   0        0        0      774 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/rust.py
+-rw-r--r--   0        0        0      772 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/scala.py
+-rw-r--r--   0        0        0     3473 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/tree_sitter_segmenter.py
+-rw-r--r--   0        0        0      795 2024-05-24 09:20:54.654771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/typescript.py
+-rw-r--r--   0        0        0     1812 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/msword.py
+-rw-r--r--   0        0        0    22512 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/pdf.py
+-rw-r--r--   0        0        0     1215 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/registry.py
+-rw-r--r--   0        0        0      564 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/txt.py
+-rw-r--r--   0        0        0     7902 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/parsers/vsdx.py
+-rw-r--r--   0        0        0    27775 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/pdf.py
+-rw-r--r--   0        0        0    20404 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/pebblo.py
+-rw-r--r--   0        0        0     1161 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/polars_dataframe.py
+-rw-r--r--   0        0        0     2508 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/powerpoint.py
+-rw-r--r--   0        0        0     1315 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/psychic.py
+-rw-r--r--   0        0        0     1118 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/pubmed.py
+-rw-r--r--   0        0        0     3388 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/pyspark_dataframe.py
+-rw-r--r--   0        0        0      590 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/python.py
+-rw-r--r--   0        0        0     8426 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/quip.py
+-rw-r--r--   0        0        0     6821 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/readthedocs.py
+-rw-r--r--   0        0        0    14800 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/recursive_url_loader.py
+-rw-r--r--   0        0        0     4584 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/reddit.py
+-rw-r--r--   0        0        0      725 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/roam.py
+-rw-r--r--   0        0        0     4527 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/rocksetdb.py
+-rw-r--r--   0        0        0     4859 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/rspace.py
+-rw-r--r--   0        0        0     4882 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/rss.py
+-rw-r--r--   0        0        0     1903 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/rst.py
+-rw-r--r--   0        0        0     2132 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/rtf.py
+-rw-r--r--   0        0        0     5871 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/s3_directory.py
+-rw-r--r--   0        0        0     5956 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/s3_file.py
+-rw-r--r--   0        0        0     3003 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/sharepoint.py
+-rw-r--r--   0        0        0     8524 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/sitemap.py
+-rw-r--r--   0        0        0     4027 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/slack_directory.py
+-rw-r--r--   0        0        0     4733 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/snowflake_loader.py
+-rw-r--r--   0        0        0     3412 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/spider.py
+-rw-r--r--   0        0        0     2004 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/spreedly.py
+-rw-r--r--   0        0        0     5581 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/sql_database.py
+-rw-r--r--   0        0        0      901 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/srt.py
+-rw-r--r--   0        0        0     1811 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/stripe.py
+-rw-r--r--   0        0        0     2965 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/surrealdb.py
+-rw-r--r--   0        0        0     9079 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/telegram.py
+-rw-r--r--   0        0        0     1700 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tencent_cos_directory.py
+-rw-r--r--   0        0        0     1617 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tencent_cos_file.py
+-rw-r--r--   0        0        0     2993 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tensorflow_datasets.py
+-rw-r--r--   0        0        0     2070 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/text.py
+-rw-r--r--   0        0        0     2610 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tidb.py
+-rw-r--r--   0        0        0      842 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tomarkdown.py
+-rw-r--r--   0        0        0     1458 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/toml.py
+-rw-r--r--   0        0        0     6584 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/trello.py
+-rw-r--r--   0        0        0     1363 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/tsv.py
+-rw-r--r--   0        0        0     3438 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/twitter.py
+-rw-r--r--   0        0        0    14329 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/unstructured.py
+-rw-r--r--   0        0        0     6020 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/url.py
+-rw-r--r--   0        0        0     8527 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/url_playwright.py
+-rw-r--r--   0        0        0     6640 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/url_selenium.py
+-rw-r--r--   0        0        0     1946 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/vsdx.py
+-rw-r--r--   0        0        0     1554 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/weather.py
+-rw-r--r--   0        0        0    10116 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/web_base.py
+-rw-r--r--   0        0        0     1750 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/whatsapp_chat.py
+-rw-r--r--   0        0        0     2227 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/wikipedia.py
+-rw-r--r--   0        0        0     4634 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/word_document.py
+-rw-r--r--   0        0        0     1595 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/xml.py
+-rw-r--r--   0        0        0     1119 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/xorbits.py
+-rw-r--r--   0        0        0    15501 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/youtube.py
+-rw-r--r--   0        0        0     2958 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_loaders/yuque.py
+-rw-r--r--   0        0        0     3903 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/__init__.py
+-rw-r--r--   0        0        0     6821 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/beautiful_soup_transformer.py
+-rw-r--r--   0        0        0     4240 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_extract.py
+-rw-r--r--   0        0        0     2155 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_qa.py
+-rw-r--r--   0        0        0     2331 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_translate.py
+-rw-r--r--   0        0        0     7883 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/embeddings_redundant_filter.py
+-rw-r--r--   0        0        0     4309 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/google_translate.py
+-rw-r--r--   0        0        0     1834 2024-05-24 09:20:54.658771 gigachain_community-0.2.0/langchain_community/document_transformers/html2text.py
+-rw-r--r--   0        0        0     1438 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/document_transformers/long_context_reorder.py
+-rw-r--r--   0        0        0     3154 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/document_transformers/markdownify.py
+-rw-r--r--   0        0        0     1500 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/document_transformers/nuclia_text_transform.py
+-rw-r--r--   0        0        0     6244 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/document_transformers/openai_functions.py
+-rw-r--r--   0        0        0     6033 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/document_transformers/xsl/html_chunks_with_headers.xslt
+-rw-r--r--   0        0        0    15650 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/__init__.py
+-rw-r--r--   0        0        0     9614 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/aleph_alpha.py
+-rw-r--r--   0        0        0     2610 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/anyscale.py
+-rw-r--r--   0        0        0     1865 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/awa.py
+-rw-r--r--   0        0        0     7239 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/azure_openai.py
+-rw-r--r--   0        0        0     4583 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/baichuan.py
+-rw-r--r--   0        0        0     5201 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/baidu_qianfan_endpoint.py
+-rw-r--r--   0        0        0     7274 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/bedrock.py
+-rw-r--r--   0        0        0     2725 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/bookend.py
+-rw-r--r--   0        0        0     4677 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/clarifai.py
+-rw-r--r--   0        0        0     2869 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/cloudflare_workersai.py
+-rw-r--r--   0        0        0     5514 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/cohere.py
+-rw-r--r--   0        0        0     5216 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/dashscope.py
+-rw-r--r--   0        0        0     1271 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/databricks.py
+-rw-r--r--   0        0        0     5002 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/deepinfra.py
+-rw-r--r--   0        0        0     3699 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/edenai.py
+-rw-r--r--   0        0        0     8546 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/elasticsearch.py
+-rw-r--r--   0        0        0     5528 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/embaas.py
+-rw-r--r--   0        0        0     4997 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/ernie.py
+-rw-r--r--   0        0        0     1512 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/fake.py
+-rw-r--r--   0        0        0     3770 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/fastembed.py
+-rw-r--r--   0        0        0     6547 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/gigachat.py
+-rw-r--r--   0        0        0     3272 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/google_palm.py
+-rw-r--r--   0        0        0     2223 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/gpt4all.py
+-rw-r--r--   0        0        0     5306 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/gradient_ai.py
+-rw-r--r--   0        0        0    13675 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/huggingface.py
+-rw-r--r--   0        0        0     5078 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/huggingface_hub.py
+-rw-r--r--   0        0        0    10290 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/infinity.py
+-rw-r--r--   0        0        0     5196 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/infinity_local.py
+-rw-r--r--   0        0        0     8168 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/itrex.py
+-rw-r--r--   0        0        0     3669 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/javelin_ai_gateway.py
+-rw-r--r--   0        0        0     2607 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/jina.py
+-rw-r--r--   0        0        0     2873 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/johnsnowlabs.py
+-rw-r--r--   0        0        0     3064 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/laser.py
+-rw-r--r--   0        0        0     4157 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/llamacpp.py
+-rw-r--r--   0        0        0     4009 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/llamafile.py
+-rw-r--r--   0        0        0     2320 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/llm_rails.py
+-rw-r--r--   0        0        0    12261 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/localai.py
+-rw-r--r--   0        0        0     4826 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/minimax.py
+-rw-r--r--   0        0        0     3047 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/mlflow.py
+-rw-r--r--   0        0        0     2447 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/mlflow_gateway.py
+-rw-r--r--   0        0        0     2384 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/modelscope_hub.py
+-rw-r--r--   0        0        0     5115 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/mosaicml.py
+-rw-r--r--   0        0        0     5753 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/nemo.py
+-rw-r--r--   0        0        0     2203 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/nlpcloud.py
+-rw-r--r--   0        0        0     7015 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/oci_generative_ai.py
+-rw-r--r--   0        0        0     3799 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/octoai_embeddings.py
+-rw-r--r--   0        0        0     7953 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/ollama.py
+-rw-r--r--   0        0        0    29239 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/openai.py
+-rw-r--r--   0        0        0    12346 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/openvino.py
+-rw-r--r--   0        0        0     7663 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/optimum_intel.py
+-rw-r--r--   0        0        0     5208 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/oracleai.py
+-rw-r--r--   0        0        0     4481 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/premai.py
+-rw-r--r--   0        0        0     7596 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/sagemaker_endpoint.py
+-rw-r--r--   0        0        0     5251 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/sambanova.py
+-rw-r--r--   0        0        0     3807 2024-05-24 09:20:54.662771 gigachain_community-0.2.0/langchain_community/embeddings/self_hosted.py
+-rw-r--r--   0        0        0     6583 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/self_hosted_hugging_face.py
+-rw-r--r--   0        0        0      190 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/sentence_transformer.py
+-rw-r--r--   0        0        0     4282 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/solar.py
+-rw-r--r--   0        0        0     3955 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/spacy_embeddings.py
+-rw-r--r--   0        0        0     6967 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/sparkllm.py
+-rw-r--r--   0        0        0     2413 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/tensorflow_hub.py
+-rw-r--r--   0        0        0     2364 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/text2vec.py
+-rw-r--r--   0        0        0     7685 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/titan_takeoff.py
+-rw-r--r--   0        0        0    14681 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/vertexai.py
+-rw-r--r--   0        0        0     4198 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/volcengine.py
+-rw-r--r--   0        0        0     7586 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/voyageai.py
+-rw-r--r--   0        0        0     3303 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/xinference.py
+-rw-r--r--   0        0        0     8006 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/embeddings/yandex.py
+-rw-r--r--   0        0        0      609 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/example_selectors/__init__.py
+-rw-r--r--   0        0        0     3843 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/example_selectors/ngram_overlap.py
+-rw-r--r--   0        0        0     3024 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/__init__.py
+-rw-r--r--   0        0        0    26675 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/age_graph.py
+-rw-r--r--   0        0        0     6961 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/arangodb_graph.py
+-rw-r--r--   0        0        0     6951 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/falkordb_graph.py
+-rw-r--r--   0        0        0     1584 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/graph_document.py
+-rw-r--r--   0        0        0      993 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/graph_store.py
+-rw-r--r--   0        0        0     8189 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/gremlin_graph.py
+-rw-r--r--   0        0        0     2511 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/hugegraph.py
+-rw-r--r--   0        0        0     3970 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/index_creator.py
+-rw-r--r--   0        0        0     3918 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/kuzu_graph.py
+-rw-r--r--   0        0        0     2575 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/memgraph_graph.py
+-rw-r--r--   0        0        0     8113 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/nebula_graph.py
+-rw-r--r--   0        0        0    29853 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/neo4j_graph.py
+-rw-r--r--   0        0        0    14357 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/neptune_graph.py
+-rw-r--r--   0        0        0    10315 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/neptune_rdf_graph.py
+-rw-r--r--   0        0        0     7897 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/networkx_graph.py
+-rw-r--r--   0        0        0     7646 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/ontotext_graphdb_graph.py
+-rw-r--r--   0        0        0    10577 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/rdf_graph.py
+-rw-r--r--   0        0        0     3543 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/graphs/tigergraph_graph.py
+-rw-r--r--   0        0        0      488 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/indexes/__init__.py
+-rw-r--r--   0        0        0     8199 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/indexes/_document_manager.py
+-rw-r--r--   0        0        0    21090 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/indexes/_sql_record_manager.py
+-rw-r--r--   0        0        0     5191 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/indexes/base.py
+-rw-r--r--   0        0        0    27750 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/__init__.py
+-rw-r--r--   0        0        0     5292 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/ai21.py
+-rw-r--r--   0        0        0    11540 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/aleph_alpha.py
+-rw-r--r--   0        0        0     3061 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/amazon_api_gateway.py
+-rw-r--r--   0        0        0    12760 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/anthropic.py
+-rw-r--r--   0        0        0    11931 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/anyscale.py
+-rw-r--r--   0        0        0     9619 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/aphrodite.py
+-rw-r--r--   0        0        0     4356 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/arcee.py
+-rw-r--r--   0        0        0     6007 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/aviary.py
+-rw-r--r--   0        0        0    20617 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/azureml_endpoint.py
+-rw-r--r--   0        0        0     3051 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/baichuan.py
+-rw-r--r--   0        0        0     7729 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/baidu_qianfan_endpoint.py
+-rw-r--r--   0        0        0     4830 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/bananadev.py
+-rw-r--r--   0        0        0     3187 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/baseten.py
+-rw-r--r--   0        0        0     9113 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/beam.py
+-rw-r--r--   0        0        0    31549 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/bedrock.py
+-rw-r--r--   0        0        0     5515 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/bigdl_llm.py
+-rw-r--r--   0        0        0     6232 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/bittensor.py
+-rw-r--r--   0        0        0     4044 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/cerebriumai.py
+-rw-r--r--   0        0        0     3950 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/chatglm.py
+-rw-r--r--   0        0        0     4884 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/chatglm3.py
+-rw-r--r--   0        0        0     6578 2024-05-24 09:20:54.666771 gigachain_community-0.2.0/langchain_community/llms/clarifai.py
+-rw-r--r--   0        0        0     4239 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/cloudflare_workersai.py
+-rw-r--r--   0        0        0     8709 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/cohere.py
+-rw-r--r--   0        0        0     4241 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/ctransformers.py
+-rw-r--r--   0        0        0     4135 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/ctranslate2.py
+-rw-r--r--   0        0        0    20535 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/databricks.py
+-rw-r--r--   0        0        0     7006 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/deepinfra.py
+-rw-r--r--   0        0        0     8657 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/deepsparse.py
+-rw-r--r--   0        0        0     9449 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/edenai.py
+-rw-r--r--   0        0        0     6459 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/exllamav2.py
+-rw-r--r--   0        0        0     2444 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/fake.py
+-rw-r--r--   0        0        0    11938 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/fireworks.py
+-rw-r--r--   0        0        0     3762 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/forefrontai.py
+-rw-r--r--   0        0        0    14570 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/friendli.py
+-rw-r--r--   0        0        0    12322 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/gigachat.py
+-rw-r--r--   0        0        0     8860 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/google_palm.py
+-rw-r--r--   0        0        0     5301 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/gooseai.py
+-rw-r--r--   0        0        0     6605 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/gpt4all.py
+-rw-r--r--   0        0        0    14199 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/gradient_ai.py
+-rw-r--r--   0        0        0      664 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/grammars/json.gbnf
+-rw-r--r--   0        0        0      167 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/grammars/list.gbnf
+-rw-r--r--   0        0        0    14634 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/huggingface_endpoint.py
+-rw-r--r--   0        0        0     5382 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/huggingface_hub.py
+-rw-r--r--   0        0        0    11127 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/huggingface_pipeline.py
+-rw-r--r--   0        0        0    11653 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/huggingface_text_gen_inference.py
+-rw-r--r--   0        0        0     2575 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/human.py
+-rw-r--r--   0        0        0     9521 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/ipex_llm.py
+-rw-r--r--   0        0        0     4723 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/javelin_ai_gateway.py
+-rw-r--r--   0        0        0     5094 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/koboldai.py
+-rw-r--r--   0        0        0     6555 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/konko.py
+-rw-r--r--   0        0        0     3477 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/layerup_security.py
+-rw-r--r--   0        0        0    12476 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/llamacpp.py
+-rw-r--r--   0        0        0    10422 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/llamafile.py
+-rw-r--r--   0        0        0     1746 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/loading.py
+-rw-r--r--   0        0        0     1965 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/manifest.py
+-rw-r--r--   0        0        0     5472 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/minimax.py
+-rw-r--r--   0        0        0     3431 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/mlflow.py
+-rw-r--r--   0        0        0     3240 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/mlflow_ai_gateway.py
+-rw-r--r--   0        0        0     6254 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/mlx_pipeline.py
+-rw-r--r--   0        0        0     3288 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/modal.py
+-rw-r--r--   0        0        0     4619 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/moonshot.py
+-rw-r--r--   0        0        0     6157 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/mosaicml.py
+-rw-r--r--   0        0        0     5053 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/nlpcloud.py
+-rw-r--r--   0        0        0    12182 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/oci_data_science_model_deployment_endpoint.py
+-rw-r--r--   0        0        0     9398 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/oci_generative_ai.py
+-rw-r--r--   0        0        0     3909 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/octoai_endpoint.py
+-rw-r--r--   0        0        0    17623 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/ollama.py
+-rw-r--r--   0        0        0     4110 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/opaqueprompts.py
+-rw-r--r--   0        0        0    47742 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/openai.py
+-rw-r--r--   0        0        0    11062 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/openllm.py
+-rw-r--r--   0        0        0      902 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/openlm.py
+-rw-r--r--   0        0        0     8059 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/pai_eas_endpoint.py
+-rw-r--r--   0        0        0     5402 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/petals.py
+-rw-r--r--   0        0        0     4210 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/pipelineai.py
+-rw-r--r--   0        0        0     8403 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/predibase.py
+-rw-r--r--   0        0        0     4417 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/predictionguard.py
+-rw-r--r--   0        0        0     8806 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/promptlayer_openai.py
+-rw-r--r--   0        0        0     8392 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/replicate.py
+-rw-r--r--   0        0        0     7402 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/rwkv.py
+-rw-r--r--   0        0        0    13166 2024-05-24 09:20:54.670771 gigachain_community-0.2.0/langchain_community/llms/sagemaker_endpoint.py
+-rw-r--r--   0        0        0    30454 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/sambanova.py
+-rw-r--r--   0        0        0     8477 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/self_hosted.py
+-rw-r--r--   0        0        0     7744 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/self_hosted_hugging_face.py
+-rw-r--r--   0        0        0     4106 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/solar.py
+-rw-r--r--   0        0        0    12601 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/sparkllm.py
+-rw-r--r--   0        0        0     4776 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/stochasticai.py
+-rw-r--r--   0        0        0     7552 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/symblai_nebula.py
+-rw-r--r--   0        0        0    14379 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/textgen.py
+-rw-r--r--   0        0        0     9307 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/titan_takeoff.py
+-rw-r--r--   0        0        0     7681 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/together.py
+-rw-r--r--   0        0        0    13156 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/tongyi.py
+-rw-r--r--   0        0        0      259 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/utils.py
+-rw-r--r--   0        0        0    19330 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/vertexai.py
+-rw-r--r--   0        0        0     5592 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/vllm.py
+-rw-r--r--   0        0        0     6591 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/volcengine_maas.py
+-rw-r--r--   0        0        0    15085 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/watsonxllm.py
+-rw-r--r--   0        0        0     8924 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/weight_only_quantization.py
+-rw-r--r--   0        0        0     4970 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/writer.py
+-rw-r--r--   0        0        0     6358 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/xinference.py
+-rw-r--r--   0        0        0    13008 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/yandex.py
+-rw-r--r--   0        0        0     5968 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/llms/yuan2.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/memory/__init__.py
+-rw-r--r--   0        0        0     5632 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/memory/kg.py
+-rw-r--r--   0        0        0     3600 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/memory/motorhead_memory.py
+-rw-r--r--   0        0        0     5623 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/memory/zep_memory.py
+-rw-r--r--   0        0        0      292 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/output_parsers/__init__.py
+-rw-r--r--   0        0        0     6701 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/output_parsers/ernie_functions.py
+-rw-r--r--   0        0        0     3283 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/output_parsers/rail_parser.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/py.typed
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/__init__.py
+-rw-r--r--   0        0        0     2188 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/astradb.py
+-rw-r--r--   0        0        0     1468 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/chroma.py
+-rw-r--r--   0        0        0     1912 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/dashvector.py
+-rw-r--r--   0        0        0     3144 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/databricks_vector_search.py
+-rw-r--r--   0        0        0     2625 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/deeplake.py
+-rw-r--r--   0        0        0     1343 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/dingo.py
+-rw-r--r--   0        0        0     3267 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/elasticsearch.py
+-rw-r--r--   0        0        0     3346 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/milvus.py
+-rw-r--r--   0        0        0     2297 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/mongodb_atlas.py
+-rw-r--r--   0        0        0     3630 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/myscale.py
+-rw-r--r--   0        0        0     3265 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/opensearch.py
+-rw-r--r--   0        0        0     1523 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/pgvector.py
+-rw-r--r--   0        0        0     1704 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/pinecone.py
+-rw-r--r--   0        0        0     3162 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/qdrant.py
+-rw-r--r--   0        0        0     3370 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/redis.py
+-rw-r--r--   0        0        0     2968 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/supabase.py
+-rw-r--r--   0        0        0     3703 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/tencentvectordb.py
+-rw-r--r--   0        0        0     2627 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/timescalevector.py
+-rw-r--r--   0        0        0     2158 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/vectara.py
+-rw-r--r--   0        0        0     2613 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/query_constructors/weaviate.py
+-rw-r--r--   0        0        0     9161 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/__init__.py
+-rw-r--r--   0        0        0     4297 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/arcee.py
+-rw-r--r--   0        0        0      773 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/arxiv.py
+-rw-r--r--   0        0        0     5098 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/azure_ai_search.py
+-rw-r--r--   0        0        0     4766 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/bedrock.py
+-rw-r--r--   0        0        0     3713 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/bm25.py
+-rw-r--r--   0        0        0     1550 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/breebs.py
+-rw-r--r--   0        0        0     2684 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/chaindesk.py
+-rw-r--r--   0        0        0     3024 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/chatgpt_plugin_retriever.py
+-rw-r--r--   0        0        0     3088 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/cohere_rag_retriever.py
+-rw-r--r--   0        0        0     2338 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/databerry.py
+-rw-r--r--   0        0        0     6778 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/docarray.py
+-rw-r--r--   0        0        0     2789 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/dria_index.py
+-rw-r--r--   0        0        0     4640 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/elastic_search_bm25.py
+-rw-r--r--   0        0        0     2087 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/embedchain.py
+-rw-r--r--   0        0        0     4763 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/google_cloud_documentai_warehouse.py
+-rw-r--r--   0        0        0    18798 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/google_vertex_ai_search.py
+-rw-r--r--   0        0        0     1985 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/kay.py
+-rw-r--r--   0        0        0    15685 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/kendra.py
+-rw-r--r--   0        0        0     3322 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/knn.py
+-rw-r--r--   0        0        0     3166 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/llama_index.py
+-rw-r--r--   0        0        0     1486 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/metal.py
+-rw-r--r--   0        0        0     2524 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/milvus.py
+-rw-r--r--   0        0        0      644 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/outline.py
+-rw-r--r--   0        0        0     5733 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/pinecone_hybrid_search.py
+-rw-r--r--   0        0        0      643 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/pubmed.py
+-rw-r--r--   0        0        0      104 2024-05-24 09:20:54.674771 gigachain_community-0.2.0/langchain_community/retrievers/pupmed.py
+-rw-r--r--   0        0        0     7539 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/qdrant_sparse_vector_retriever.py
+-rw-r--r--   0        0        0      670 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/rememberizer.py
+-rw-r--r--   0        0        0     1935 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/remote_retriever.py
+-rw-r--r--   0        0        0     4130 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/svm.py
+-rw-r--r--   0        0        0     2855 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/tavily_search_api.py
+-rw-r--r--   0        0        0     5569 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/tfidf.py
+-rw-r--r--   0        0        0     9340 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/thirdai_neuraldb.py
+-rw-r--r--   0        0        0     4555 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/vespa_retriever.py
+-rw-r--r--   0        0        0     6195 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/weaviate_hybrid_search.py
+-rw-r--r--   0        0        0     9340 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/web_research.py
+-rw-r--r--   0        0        0      656 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/wikipedia.py
+-rw-r--r--   0        0        0     1124 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/you.py
+-rw-r--r--   0        0        0     5904 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/zep.py
+-rw-r--r--   0        0        0     2719 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/retrievers/zilliz.py
+-rw-r--r--   0        0        0     1643 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/__init__.py
+-rw-r--r--   0        0        0     8691 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/astradb.py
+-rw-r--r--   0        0        0       89 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/exceptions.py
+-rw-r--r--   0        0        0     4375 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/mongodb.py
+-rw-r--r--   0        0        0     4802 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/redis.py
+-rw-r--r--   0        0        0     5762 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/storage/upstash_redis.py
+-rw-r--r--   0        0        0    23154 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/__init__.py
+-rw-r--r--   0        0        0     3185 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/app.py
+-rw-r--r--   0        0        0     2109 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/base.py
+-rw-r--r--   0        0        0     4140 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/owner.py
+-rw-r--r--   0        0        0     2746 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/rule.py
+-rw-r--r--   0        0        0     1074 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/transfer.py
+-rw-r--r--   0        0        0     2315 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/utils.py
+-rw-r--r--   0        0        0     2624 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/ainetwork/value.py
+-rw-r--r--   0        0        0      257 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/amadeus/__init__.py
+-rw-r--r--   0        0        0      436 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/amadeus/base.py
+-rw-r--r--   0        0        0     2338 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/amadeus/closest_airport.py
+-rw-r--r--   0        0        0     5771 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/amadeus/flight_search.py
+-rw-r--r--   0        0        0     1277 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/amadeus/utils.py
+-rw-r--r--   0        0        0       25 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/arxiv/__init__.py
+-rw-r--r--   0        0        0     1254 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/arxiv/tool.py
+-rw-r--r--   0        0        0      188 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/audio/__init__.py
+-rw-r--r--   0        0        0     3750 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/audio/huggingface_text_to_speech_inference.py
+-rw-r--r--   0        0        0      858 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/__init__.py
+-rw-r--r--   0        0        0     5486 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/document_intelligence.py
+-rw-r--r--   0        0        0     5735 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/image_analysis.py
+-rw-r--r--   0        0        0     4430 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/speech_to_text.py
+-rw-r--r--   0        0        0     3601 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/text_analytics_for_health.py
+-rw-r--r--   0        0        0     3811 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/text_to_speech.py
+-rw-r--r--   0        0        0      776 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/utils.py
+-rw-r--r--   0        0        0      802 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/__init__.py
+-rw-r--r--   0        0        0     5375 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/form_recognizer.py
+-rw-r--r--   0        0        0     5304 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/image_analysis.py
+-rw-r--r--   0        0        0     4336 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/speech2text.py
+-rw-r--r--   0        0        0     3675 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/text2speech.py
+-rw-r--r--   0        0        0     3542 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/text_analytics_health.py
+-rw-r--r--   0        0        0      776 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/utils.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/bearly/__init__.py
+-rw-r--r--   0        0        0     5562 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/bearly/tool.py
+-rw-r--r--   0        0        0      170 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/bing_search/__init__.py
+-rw-r--r--   0        0        0     1463 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/bing_search/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/brave_search/__init__.py
+-rw-r--r--   0        0        0     1354 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/brave_search/tool.py
+-rw-r--r--   0        0        0       21 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/cassandra_database/__init__.py
+-rw-r--r--   0        0        0     1221 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/cassandra_database/prompt.py
+-rw-r--r--   0        0        0     4925 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/cassandra_database/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/clickup/__init__.py
+-rw-r--r--   0        0        0     8298 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/clickup/prompt.py
+-rw-r--r--   0        0        0     1231 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/clickup/tool.py
+-rw-r--r--   0        0        0       20 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/cogniswitch/__init__.py
+-rw-r--r--   0        0        0    13896 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/cogniswitch/tool.py
+-rw-r--r--   0        0        0      188 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/connery/__init__.py
+-rw-r--r--   0        0        0      647 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/connery/models.py
+-rw-r--r--   0        0        0     5741 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/connery/service.py
+-rw-r--r--   0        0        0     5552 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/connery/tool.py
+-rw-r--r--   0        0        0      198 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/convert_to_openai.py
+-rw-r--r--   0        0        0      268 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/dataforseo_api_search/__init__.py
+-rw-r--r--   0        0        0     2214 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/dataforseo_api_search/tool.py
+-rw-r--r--   0        0        0      147 2024-05-24 09:20:54.678771 gigachain_community-0.2.0/langchain_community/tools/dataherald/__init__.py
+-rw-r--r--   0        0        0     1063 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/dataherald/tool.py
+-rw-r--r--   0        0        0      147 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/ddg_search/__init__.py
+-rw-r--r--   0        0        0     2963 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/ddg_search/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/e2b_data_analysis/__init__.py
+-rw-r--r--   0        0        0     8015 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/e2b_data_analysis/tool.py
+-rw-r--r--   0        0        0    20668 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/e2b_data_analysis/unparse.py
+-rw-r--r--   0        0        0     1025 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/__init__.py
+-rw-r--r--   0        0        0     3465 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/audio_speech_to_text.py
+-rw-r--r--   0        0        0     3885 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/audio_text_to_speech.py
+-rw-r--r--   0        0        0     5385 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/edenai_base_tool.py
+-rw-r--r--   0        0        0     2248 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/image_explicitcontent.py
+-rw-r--r--   0        0        0     2476 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/image_objectdetection.py
+-rw-r--r--   0        0        0     1966 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/ocr_identityparser.py
+-rw-r--r--   0        0        0     2187 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/ocr_invoiceparser.py
+-rw-r--r--   0        0        0     2407 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/edenai/text_moderation.py
+-rw-r--r--   0        0        0      164 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/eleven_labs/__init__.py
+-rw-r--r--   0        0        0      203 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/eleven_labs/models.py
+-rw-r--r--   0        0        0     2709 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/eleven_labs/text2speech.py
+-rw-r--r--   0        0        0      723 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/__init__.py
+-rw-r--r--   0        0        0     1749 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/copy.py
+-rw-r--r--   0        0        0     1345 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/delete.py
+-rw-r--r--   0        0        0     1965 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/file_search.py
+-rw-r--r--   0        0        0     1432 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/list_dir.py
+-rw-r--r--   0        0        0     1889 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/move.py
+-rw-r--r--   0        0        0     1340 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/read.py
+-rw-r--r--   0        0        0     1726 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/utils.py
+-rw-r--r--   0        0        0     1614 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/file_management/write.py
+-rw-r--r--   0        0        0       18 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/github/__init__.py
+-rw-r--r--   0        0        0     6220 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/github/prompt.py
+-rw-r--r--   0        0        0     1221 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/github/tool.py
+-rw-r--r--   0        0        0       18 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gitlab/__init__.py
+-rw-r--r--   0        0        0     3438 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gitlab/prompt.py
+-rw-r--r--   0        0        0      999 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gitlab/tool.py
+-rw-r--r--   0        0        0      601 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/__init__.py
+-rw-r--r--   0        0        0     1031 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/base.py
+-rw-r--r--   0        0        0     2564 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/create_draft.py
+-rw-r--r--   0        0        0     2258 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/get_message.py
+-rw-r--r--   0        0        0     1560 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/get_thread.py
+-rw-r--r--   0        0        0     5195 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/search.py
+-rw-r--r--   0        0        0     2940 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/send_message.py
+-rw-r--r--   0        0        0     4109 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/gmail/utils.py
+-rw-r--r--   0        0        0      135 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/golden_query/__init__.py
+-rw-r--r--   0        0        0     1108 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/golden_query/tool.py
+-rw-r--r--   0        0        0      171 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_cloud/__init__.py
+-rw-r--r--   0        0        0     3354 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_cloud/texttospeech.py
+-rw-r--r--   0        0        0      152 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_finance/__init__.py
+-rw-r--r--   0        0        0      854 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_finance/tool.py
+-rw-r--r--   0        0        0      140 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_jobs/__init__.py
+-rw-r--r--   0        0        0      826 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_jobs/tool.py
+-rw-r--r--   0        0        0      140 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_lens/__init__.py
+-rw-r--r--   0        0        0      822 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_lens/tool.py
+-rw-r--r--   0        0        0      140 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_places/__init__.py
+-rw-r--r--   0        0        0     1348 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_places/tool.py
+-rw-r--r--   0        0        0      152 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_scholar/__init__.py
+-rw-r--r--   0        0        0      847 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_scholar/tool.py
+-rw-r--r--   0        0        0      195 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_search/__init__.py
+-rw-r--r--   0        0        0     1798 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_search/tool.py
+-rw-r--r--   0        0        0      243 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_serper/__init__.py
+-rw-r--r--   0        0        0     2113 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_serper/tool.py
+-rw-r--r--   0        0        0      148 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_trends/__init__.py
+-rw-r--r--   0        0        0      844 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/google_trends/tool.py
+-rw-r--r--   0        0        0       47 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/graphql/__init__.py
+-rw-r--r--   0        0        0     1204 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/graphql/tool.py
+-rw-r--r--   0        0        0      132 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/human/__init__.py
+-rw-r--r--   0        0        0     1011 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/human/tool.py
+-rw-r--r--   0        0        0     2287 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/ifttt.py
+-rw-r--r--   0        0        0       43 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/interaction/__init__.py
+-rw-r--r--   0        0        0      463 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/interaction/tool.py
+-rw-r--r--   0        0        0       17 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/jira/__init__.py
+-rw-r--r--   0        0        0     3170 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/jira/prompt.py
+-rw-r--r--   0        0        0     1369 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/jira/tool.py
+-rw-r--r--   0        0        0       46 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/json/__init__.py
+-rw-r--r--   0        0        0     4140 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/json/tool.py
+-rw-r--r--   0        0        0      134 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/memorize/__init__.py
+-rw-r--r--   0        0        0     1828 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/memorize/tool.py
+-rw-r--r--   0        0        0       35 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/merriam_webster/__init__.py
+-rw-r--r--   0        0        0      854 2024-05-24 09:20:54.682771 gigachain_community-0.2.0/langchain_community/tools/merriam_webster/tool.py
+-rw-r--r--   0        0        0      154 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/metaphor_search/__init__.py
+-rw-r--r--   0        0        0     2851 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/metaphor_search/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/mojeek_search/__init__.py
+-rw-r--r--   0        0        0     1307 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/mojeek_search/tool.py
+-rw-r--r--   0        0        0      360 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/multion/__init__.py
+-rw-r--r--   0        0        0     1765 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/multion/close_session.py
+-rw-r--r--   0        0        0     2199 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/multion/create_session.py
+-rw-r--r--   0        0        0     2415 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/multion/update_session.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/nasa/__init__.py
+-rw-r--r--   0        0        0     5197 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/nasa/prompt.py
+-rw-r--r--   0        0        0      830 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/nasa/tool.py
+-rw-r--r--   0        0        0      111 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/nuclia/__init__.py
+-rw-r--r--   0        0        0     7941 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/nuclia/tool.py
+-rw-r--r--   0        0        0      654 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/__init__.py
+-rw-r--r--   0        0        0      509 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/base.py
+-rw-r--r--   0        0        0     1858 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/create_draft_message.py
+-rw-r--r--   0        0        0     4821 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/events_search.py
+-rw-r--r--   0        0        0     4235 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/messages_search.py
+-rw-r--r--   0        0        0     2898 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/send_event.py
+-rw-r--r--   0        0        0     1777 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/send_message.py
+-rw-r--r--   0        0        0     2228 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/office365/utils.py
+-rw-r--r--   0        0        0      219 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openai_dalle_image_generation/__init__.py
+-rw-r--r--   0        0        0      953 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openai_dalle_image_generation/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openapi/__init__.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openapi/utils/__init__.py
+-rw-r--r--   0        0        0    21315 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openapi/utils/api_models.py
+-rw-r--r--   0        0        0      192 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openapi/utils/openapi_utils.py
+-rw-r--r--   0        0        0      161 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openweathermap/__init__.py
+-rw-r--r--   0        0        0      994 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/openweathermap/tool.py
+-rw-r--r--   0        0        0      142 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/passio_nutrition_ai/__init__.py
+-rw-r--r--   0        0        0     1143 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/passio_nutrition_ai/tool.py
+-rw-r--r--   0        0        0      763 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/__init__.py
+-rw-r--r--   0        0        0     1950 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/base.py
+-rw-r--r--   0        0        0     3083 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/click.py
+-rw-r--r--   0        0        0     1340 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/current_page.py
+-rw-r--r--   0        0        0     3051 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/extract_hyperlinks.py
+-rw-r--r--   0        0        0     2383 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/extract_text.py
+-rw-r--r--   0        0        0     3743 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/get_elements.py
+-rw-r--r--   0        0        0     2878 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/navigate.py
+-rw-r--r--   0        0        0     1926 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/navigate_back.py
+-rw-r--r--   0        0        0     3050 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/playwright/utils.py
+-rw-r--r--   0        0        0     2902 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/plugin.py
+-rw-r--r--   0        0        0      439 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/polygon/__init__.py
+-rw-r--r--   0        0        0     2558 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/polygon/aggregates.py
+-rw-r--r--   0        0        0     1197 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/polygon/financials.py
+-rw-r--r--   0        0        0     1070 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/polygon/last_quote.py
+-rw-r--r--   0        0        0     1076 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/polygon/ticker_news.py
+-rw-r--r--   0        0        0       52 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/powerbi/__init__.py
+-rw-r--r--   0        0        0     7339 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/powerbi/prompt.py
+-rw-r--r--   0        0        0    11076 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/powerbi/tool.py
+-rw-r--r--   0        0        0       26 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/pubmed/__init__.py
+-rw-r--r--   0        0        0      971 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/pubmed/tool.py
+-rw-r--r--   0        0        0     1991 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/reddit_search/tool.py
+-rw-r--r--   0        0        0     2615 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/render.py
+-rw-r--r--   0        0        0       52 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/requests/__init__.py
+-rw-r--r--   0        0        0     7446 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/requests/tool.py
+-rw-r--r--   0        0        0       31 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/scenexplain/__init__.py
+-rw-r--r--   0        0        0     1127 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/scenexplain/tool.py
+-rw-r--r--   0        0        0      214 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/searchapi/__init__.py
+-rw-r--r--   0        0        0     2114 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/searchapi/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/searx_search/__init__.py
+-rw-r--r--   0        0        0     2262 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/searx_search/tool.py
+-rw-r--r--   0        0        0       36 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/semanticscholar/__init__.py
+-rw-r--r--   0        0        0     1216 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/semanticscholar/tool.py
+-rw-r--r--   0        0        0      103 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/shell/__init__.py
+-rw-r--r--   0        0        0     3143 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/shell/tool.py
+-rw-r--r--   0        0        0      502 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/__init__.py
+-rw-r--r--   0        0        0      461 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/base.py
+-rw-r--r--   0        0        0     1193 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/get_channel.py
+-rw-r--r--   0        0        0     1422 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/get_message.py
+-rw-r--r--   0        0        0     2071 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/schedule_message.py
+-rw-r--r--   0        0        0     1222 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/send_message.py
+-rw-r--r--   0        0        0     1136 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/slack/utils.py
+-rw-r--r--   0        0        0       18 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/sleep/__init__.py
+-rw-r--r--   0        0        0     1230 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/sleep/tool.py
+-rw-r--r--   0        0        0       44 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/spark_sql/__init__.py
+-rw-r--r--   0        0        0      550 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/spark_sql/prompt.py
+-rw-r--r--   0        0        0     4403 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/spark_sql/tool.py
+-rw-r--r--   0        0        0       49 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/sql_database/__init__.py
+-rw-r--r--   0        0        0      597 2024-05-24 09:20:54.686771 gigachain_community-0.2.0/langchain_community/tools/sql_database/prompt.py
+-rw-r--r--   0        0        0     5353 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/sql_database/tool.py
+-rw-r--r--   0        0        0       33 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/stackexchange/__init__.py
+-rw-r--r--   0        0        0      869 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/stackexchange/tool.py
+-rw-r--r--   0        0        0       24 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steam/__init__.py
+-rw-r--r--   0        0        0     1657 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steam/prompt.py
+-rw-r--r--   0        0        0      842 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steam/tool.py
+-rw-r--r--   0        0        0      186 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steamship_image_generation/__init__.py
+-rw-r--r--   0        0        0     3378 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steamship_image_generation/tool.py
+-rw-r--r--   0        0        0     1396 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/steamship_image_generation/utils.py
+-rw-r--r--   0        0        0      189 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/tavily_search/__init__.py
+-rw-r--r--   0        0        0     3427 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/tavily_search/tool.py
+-rw-r--r--   0        0        0       51 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/vectorstore/__init__.py
+-rw-r--r--   0        0        0     4717 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/vectorstore/tool.py
+-rw-r--r--   0        0        0       28 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wikidata/__init__.py
+-rw-r--r--   0        0        0      926 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wikidata/tool.py
+-rw-r--r--   0        0        0       29 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wikipedia/__init__.py
+-rw-r--r--   0        0        0      867 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wikipedia/tool.py
+-rw-r--r--   0        0        0      155 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wolfram_alpha/__init__.py
+-rw-r--r--   0        0        0      887 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/wolfram_alpha/tool.py
+-rw-r--r--   0        0        0     2753 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/yahoo_finance_news.py
+-rw-r--r--   0        0        0      125 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/you/__init__.py
+-rw-r--r--   0        0        0     1355 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/you/tool.py
+-rw-r--r--   0        0        0        0 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/youtube/__init__.py
+-rw-r--r--   0        0        0     1729 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/youtube/search.py
+-rw-r--r--   0        0        0      193 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/zapier/__init__.py
+-rw-r--r--   0        0        0     1182 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/zapier/prompt.py
+-rw-r--r--   0        0        0     7865 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/tools/zapier/tool.py
+-rw-r--r--   0        0        0    11062 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/__init__.py
+-rw-r--r--   0        0        0     5919 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/alpha_vantage.py
+-rw-r--r--   0        0        0      844 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/anthropic.py
+-rw-r--r--   0        0        0     8458 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/apify.py
+-rw-r--r--   0        0        0     8710 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/arcee.py
+-rw-r--r--   0        0        0    10022 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/arxiv.py
+-rw-r--r--   0        0        0     6104 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/astradb.py
+-rw-r--r--   0        0        0     2438 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/awslambda.py
+-rw-r--r--   0        0        0     2500 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/bibtex.py
+-rw-r--r--   0        0        0     3294 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/bing_search.py
+-rw-r--r--   0        0        0     2382 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/brave_search.py
+-rw-r--r--   0        0        0     1164 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/cassandra.py
+-rw-r--r--   0        0        0    24430 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/cassandra_database.py
+-rw-r--r--   0        0        0    19876 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/clickup.py
+-rw-r--r--   0        0        0     6378 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/dalle_image_generator.py
+-rw-r--r--   0        0        0     7854 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/dataforseo_api_search.py
+-rw-r--r--   0        0        0     2068 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/dataherald.py
+-rw-r--r--   0        0        0     3351 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/dria_index.py
+-rw-r--r--   0        0        0     4311 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/duckduckgo_search.py
+-rw-r--r--   0        0        0    32246 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/github.py
+-rw-r--r--   0        0        0    11976 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/gitlab.py
+-rw-r--r--   0        0        0     1859 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/golden_query.py
+-rw-r--r--   0        0        0     3401 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_finance.py
+-rw-r--r--   0        0        0     2805 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_jobs.py
+-rw-r--r--   0        0        0     3017 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_lens.py
+-rw-r--r--   0        0        0     4293 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_places_api.py
+-rw-r--r--   0        0        0     5172 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_scholar.py
+-rw-r--r--   0        0        0     5684 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_search.py
+-rw-r--r--   0        0        0     6504 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_serper.py
+-rw-r--r--   0        0        0     4165 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/google_trends.py
+-rw-r--r--   0        0        0     2089 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/graphql.py
+-rw-r--r--   0        0        0     5892 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/infobip.py
+-rw-r--r--   0        0        0     6196 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/jira.py
+-rw-r--r--   0        0        0     2647 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/max_compute.py
+-rw-r--r--   0        0        0     3749 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/merriam_webster.py
+-rw-r--r--   0        0        0     6810 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/metaphor_search.py
+-rw-r--r--   0        0        0     1319 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/mojeek_search.py
+-rw-r--r--   0        0        0     1821 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/nasa.py
+-rw-r--r--   0        0        0    21608 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/nvidia_riva.py
+-rw-r--r--   0        0        0     3287 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/opaqueprompts.py
+-rw-r--r--   0        0        0    11018 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/openapi.py
+-rw-r--r--   0        0        0     2463 2024-05-24 09:20:54.690771 gigachain_community-0.2.0/langchain_community/utilities/openweathermap.py
+-rw-r--r--   0        0        0     6224 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/oracleai.py
+-rw-r--r--   0        0        0     3352 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/outline.py
+-rw-r--r--   0        0        0     5613 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/passio_nutrition_ai.py
+-rw-r--r--   0        0        0     8856 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/pebblo.py
+-rw-r--r--   0        0        0     4310 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/polygon.py
+-rw-r--r--   0        0        0     2355 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/portkey.py
+-rw-r--r--   0        0        0    11246 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/powerbi.py
+-rw-r--r--   0        0        0     6950 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/pubmed.py
+-rw-r--r--   0        0        0     2159 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/python.py
+-rw-r--r--   0        0        0     4474 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/reddit_search.py
+-rw-r--r--   0        0        0     8251 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/redis.py
+-rw-r--r--   0        0        0     1651 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/rememberizer.py
+-rw-r--r--   0        0        0     8794 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/requests.py
+-rw-r--r--   0        0        0     2221 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/scenexplain.py
+-rw-r--r--   0        0        0     5226 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/searchapi.py
+-rw-r--r--   0        0        0    16653 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/searx_search.py
+-rw-r--r--   0        0        0     2790 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/semanticscholar.py
+-rw-r--r--   0        0        0     8705 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/serpapi.py
+-rw-r--r--   0        0        0     7520 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/spark_sql.py
+-rw-r--r--   0        0        0    22815 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/sql_database.py
+-rw-r--r--   0        0        0     2639 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/stackexchange.py
+-rw-r--r--   0        0        0     5857 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/steam.py
+-rw-r--r--   0        0        0     6890 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/tavily_search.py
+-rw-r--r--   0        0        0     4006 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/tensorflow_datasets.py
+-rw-r--r--   0        0        0     3442 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/twilio.py
+-rw-r--r--   0        0        0     4088 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/vertexai.py
+-rw-r--r--   0        0        0     5346 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/wikidata.py
+-rw-r--r--   0        0        0     4271 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/wikipedia.py
+-rw-r--r--   0        0        0     2012 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/wolfram_alpha.py
+-rw-r--r--   0        0        0     7952 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/you.py
+-rw-r--r--   0        0        0    11667 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utilities/zapier.py
+-rw-r--r--   0        0        0       45 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/__init__.py
+-rw-r--r--   0        0        0     1509 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/ernie_functions.py
+-rw-r--r--   0        0        0      305 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/gigachat_functions.py
+-rw-r--r--   0        0        0      775 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/google.py
+-rw-r--r--   0        0        0     2725 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/math.py
+-rw-r--r--   0        0        0      264 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/openai.py
+-rw-r--r--   0        0        0      377 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/utils/openai_functions.py
+-rw-r--r--   0        0        0    16943 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/__init__.py
+-rw-r--r--   0        0        0    19806 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/alibabacloud_opensearch.py
+-rw-r--r--   0        0        0    15752 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/analyticdb.py
+-rw-r--r--   0        0        0    17735 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/annoy.py
+-rw-r--r--   0        0        0    17067 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/apache_doris.py
+-rw-r--r--   0        0        0    46743 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/astradb.py
+-rw-r--r--   0        0        0    12136 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/atlas.py
+-rw-r--r--   0        0        0    21155 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/awadb.py
+-rw-r--r--   0        0        0    20882 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/azure_cosmos_db.py
+-rw-r--r--   0        0        0    30059 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/azuresearch.py
+-rw-r--r--   0        0        0    15245 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/bagel.py
+-rw-r--r--   0        0        0       78 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/bageldb.py
+-rw-r--r--   0        0        0    16548 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/baiducloud_vector_search.py
+-rw-r--r--   0        0        0    15272 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/baiduvectordb.py
+-rw-r--r--   0        0        0    34540 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/bigquery_vector_search.py
+-rw-r--r--   0        0        0    38804 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/cassandra.py
+-rw-r--r--   0        0        0    31090 2024-05-24 09:20:54.694771 gigachain_community-0.2.0/langchain_community/vectorstores/chroma.py
+-rw-r--r--   0        0        0    12095 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/clarifai.py
+-rw-r--r--   0        0        0    22074 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/clickhouse.py
+-rw-r--r--   0        0        0    22766 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/couchbase.py
+-rw-r--r--   0        0        0    13946 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/dashvector.py
+-rw-r--r--   0        0        0    23115 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/databricks_vector_search.py
+-rw-r--r--   0        0        0    43000 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/deeplake.py
+-rw-r--r--   0        0        0    13208 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/dingo.py
+-rw-r--r--   0        0        0      236 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/docarray/__init__.py
+-rw-r--r--   0        0        0     7021 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/docarray/base.py
+-rw-r--r--   0        0        0     4042 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/docarray/hnsw.py
+-rw-r--r--   0        0        0     2419 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/docarray/in_memory.py
+-rw-r--r--   0        0        0    11947 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/documentdb.py
+-rw-r--r--   0        0        0     9991 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/duckdb.py
+-rw-r--r--   0        0        0    20611 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/ecloud_vector_search.py
+-rw-r--r--   0        0        0    29007 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/elastic_vector_search.py
+-rw-r--r--   0        0        0    48577 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/elasticsearch.py
+-rw-r--r--   0        0        0    14184 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/epsilla.py
+-rw-r--r--   0        0        0    45888 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/faiss.py
+-rw-r--r--   0        0        0    25862 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/hanavector.py
+-rw-r--r--   0        0        0    26837 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/hippo.py
+-rw-r--r--   0        0        0    13642 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/hologres.py
+-rw-r--r--   0        0        0    21348 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/infinispanvs.py
+-rw-r--r--   0        0        0     6299 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/inmemory.py
+-rw-r--r--   0        0        0    14581 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/jaguar.py
+-rw-r--r--   0        0        0     9087 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/kdbai.py
+-rw-r--r--   0        0        0    34973 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/kinetica.py
+-rw-r--r--   0        0        0    10720 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/lancedb.py
+-rw-r--r--   0        0        0    38505 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/lantern.py
+-rw-r--r--   0        0        0     7746 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/llm_rails.py
+-rw-r--r--   0        0        0    17075 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/marqo.py
+-rw-r--r--   0        0        0    21617 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/matching_engine.py
+-rw-r--r--   0        0        0    12086 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/meilisearch.py
+-rw-r--r--   0        0        0    41955 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/milvus.py
+-rw-r--r--   0        0        0    19047 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/momento_vector_index.py
+-rw-r--r--   0        0        0    13700 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/mongodb_atlas.py
+-rw-r--r--   0        0        0    22612 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/myscale.py
+-rw-r--r--   0        0        0    53077 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/neo4j_vector.py
+-rw-r--r--   0        0        0     5404 2024-05-24 09:20:54.698771 gigachain_community-0.2.0/langchain_community/vectorstores/nucliadb.py
+-rw-r--r--   0        0        0    51650 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/opensearch_vector_search.py
+-rw-r--r--   0        0        0    32081 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/oraclevs.py
+-rw-r--r--   0        0        0     7708 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/pathway.py
+-rw-r--r--   0        0        0    17949 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/pgembedding.py
+-rw-r--r--   0        0        0     7838 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/pgvecto_rs.py
+-rw-r--r--   0        0        0    51716 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/pgvector.py
+-rw-r--r--   0        0        0    17678 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/pinecone.py
+-rw-r--r--   0        0        0    94285 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/qdrant.py
+-rw-r--r--   0        0        0      265 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/redis/__init__.py
+-rw-r--r--   0        0        0    56021 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/redis/base.py
+-rw-r--r--   0        0        0      420 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/redis/constants.py
+-rw-r--r--   0        0        0    16219 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/redis/filters.py
+-rw-r--r--   0        0        0    10418 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/redis/schema.py
+-rw-r--r--   0        0        0    18388 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/relyt.py
+-rw-r--r--   0        0        0    15235 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/rocksetdb.py
+-rw-r--r--   0        0        0    20719 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/scann.py
+-rw-r--r--   0        0        0     9721 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/semadb.py
+-rw-r--r--   0        0        0    45305 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/singlestoredb.py
+-rw-r--r--   0        0        0    12371 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/sklearn.py
+-rw-r--r--   0        0        0     7302 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/sqlitevss.py
+-rw-r--r--   0        0        0    17220 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/starrocks.py
+-rw-r--r--   0        0        0    15689 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/supabase.py
+-rw-r--r--   0        0        0    15428 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/surrealdb.py
+-rw-r--r--   0        0        0     9559 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/tair.py
+-rw-r--r--   0        0        0    20525 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/tencentvectordb.py
+-rw-r--r--   0        0        0    17534 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/thirdai_neuraldb.py
+-rw-r--r--   0        0        0    13630 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/tidb_vector.py
+-rw-r--r--   0        0        0     4927 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/tigris.py
+-rw-r--r--   0        0        0    29797 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/tiledb.py
+-rw-r--r--   0        0        0    29832 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/timescalevector.py
+-rw-r--r--   0        0        0     9713 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/typesense.py
+-rw-r--r--   0        0        0    33147 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/upstash.py
+-rw-r--r--   0        0        0     5737 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/usearch.py
+-rw-r--r--   0        0        0     2474 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/utils.py
+-rw-r--r--   0        0        0    12899 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/vald.py
+-rw-r--r--   0        0        0    53231 2024-05-24 09:20:54.702771 gigachain_community-0.2.0/langchain_community/vectorstores/vdms.py
+-rw-r--r--   0        0        0    19845 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/vearch.py
+-rw-r--r--   0        0        0    21908 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/vectara.py
+-rw-r--r--   0        0        0     9785 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/vespa.py
+-rw-r--r--   0        0        0    15510 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/vikingdb.py
+-rw-r--r--   0        0        0     8145 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/vlite.py
+-rw-r--r--   0        0        0    19253 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/weaviate.py
+-rw-r--r--   0        0        0     9018 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/xata.py
+-rw-r--r--   0        0        0    34543 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/yellowbrick.py
+-rw-r--r--   0        0        0    23192 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/zep.py
+-rw-r--r--   0        0        0     8255 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/langchain_community/vectorstores/zilliz.py
+-rw-r--r--   0        0        0    11247 2024-05-24 09:20:54.706771 gigachain_community-0.2.0/pyproject.toml
+-rw-r--r--   0        0        0     8918 1970-01-01 00:00:00.000000 gigachain_community-0.2.0/PKG-INFO
```

### Comparing `gigachain_community-0.0.6.1/README.md` & `gigachain_community-0.2.0/README.md`

 * *Files 21% similar despite different names*

```diff
@@ -2,24 +2,24 @@
 
 [![Downloads](https://static.pepy.tech/badge/langchain_community/month)](https://pepy.tech/project/langchain_community)
 [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
 
 ## Quick Install
 
 ```bash
-pip install langchain-community
+pip install gigachain-community
 ```
 
 ## What is it?
 
 LangChain Community contains third-party integrations that implement the base interfaces defined in LangChain Core, making them ready-to-use in any LangChain application.
 
 For full documentation see the [API reference](https://api.python.langchain.com/en/stable/community_api_reference.html).
 
-![LangChain Stack](../../docs/static/img/langchain_stack.png)
+![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](../../docs/static/img/langchain_stack.png "LangChain Framework Overview")
 
 ##  Releases & Versioning
 
 `langchain-community` is currently on version `0.0.x`
 
 All changes will be accompanied by a patch version increase.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/adapters/openai.py` & `gigachain_community-0.2.0/langchain_community/adapters/openai.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,33 +36,41 @@
     i = start
     async for x in iterable:
         yield i, x
         i += 1
 
 
 class IndexableBaseModel(BaseModel):
-    """Allows a BaseModel to return its fields by string variable indexing"""
+    """Allows a BaseModel to return its fields by string variable indexing."""
 
     def __getitem__(self, item: str) -> Any:
         return getattr(self, item)
 
 
 class Choice(IndexableBaseModel):
+    """Choice."""
+
     message: dict
 
 
 class ChatCompletions(IndexableBaseModel):
+    """Chat completions."""
+
     choices: List[Choice]
 
 
 class ChoiceChunk(IndexableBaseModel):
+    """Choice chunk."""
+
     delta: dict
 
 
 class ChatCompletionChunk(IndexableBaseModel):
+    """Chat completion chunk."""
+
     choices: List[ChoiceChunk]
 
 
 def convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
     """Convert a dictionary to a LangChain message.
 
     Args:
@@ -83,26 +91,26 @@
             additional_kwargs["function_call"] = dict(function_call)
         if tool_calls := _dict.get("tool_calls"):
             additional_kwargs["tool_calls"] = tool_calls
         return AIMessage(content=content, additional_kwargs=additional_kwargs)
     elif role == "system":
         return SystemMessage(content=_dict.get("content", ""))
     elif role == "function":
-        return FunctionMessage(content=_dict.get("content", ""), name=_dict.get("name"))
+        return FunctionMessage(content=_dict.get("content", ""), name=_dict.get("name"))  # type: ignore[arg-type]
     elif role == "tool":
         additional_kwargs = {}
         if "name" in _dict:
             additional_kwargs["name"] = _dict["name"]
         return ToolMessage(
             content=_dict.get("content", ""),
-            tool_call_id=_dict.get("tool_call_id"),
+            tool_call_id=_dict.get("tool_call_id"),  # type: ignore[arg-type]
             additional_kwargs=additional_kwargs,
         )
     else:
-        return ChatMessage(content=_dict.get("content", ""), role=role)
+        return ChatMessage(content=_dict.get("content", ""), role=role)  # type: ignore[arg-type]
 
 
 def convert_message_to_dict(message: BaseMessage) -> dict:
     """Convert a LangChain message to a dictionary.
 
     Args:
         message: The LangChain message.
@@ -297,15 +305,15 @@
         [convert_message_to_dict(s) for s in session["messages"]]
         for session in sessions
         if _has_assistant_message(session)
     ]
 
 
 class Completions:
-    """Completion."""
+    """Completions."""
 
     @overload
     @staticmethod
     def create(
         messages: Sequence[Dict[str, Any]],
         *,
         provider: str = "ChatOpenAI",
@@ -395,12 +403,14 @@
                     choices=[ChoiceChunk(delta=_convert_message_chunk(c, i))]
                 )
                 async for i, c in aenumerate(model_config.astream(converted_messages))
             )
 
 
 class Chat:
+    """Chat."""
+
     def __init__(self) -> None:
         self.completions = Completions()
 
 
 chat = Chat()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/ainetwork/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/ainetwork/toolkit.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Literal, Optional
 
 from langchain_core.pydantic_v1 import root_validator
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.ainetwork.app import AINAppOps
 from langchain_community.tools.ainetwork.owner import AINOwnerOps
 from langchain_community.tools.ainetwork.rule import AINRuleOps
 from langchain_community.tools.ainetwork.transfer import AINTransfer
 from langchain_community.tools.ainetwork.utils import authenticate
 from langchain_community.tools.ainetwork.value import AINValueOps
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/amadeus/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/slack/toolkit.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List
 
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
-from langchain_community.tools.amadeus.closest_airport import AmadeusClosestAirport
-from langchain_community.tools.amadeus.flight_search import AmadeusFlightSearch
-from langchain_community.tools.amadeus.utils import authenticate
+from langchain_community.tools.slack.get_channel import SlackGetChannel
+from langchain_community.tools.slack.get_message import SlackGetMessage
+from langchain_community.tools.slack.schedule_message import SlackScheduleMessage
+from langchain_community.tools.slack.send_message import SlackSendMessage
+from langchain_community.tools.slack.utils import login
 
 if TYPE_CHECKING:
-    from amadeus import Client
+    from slack_sdk import WebClient
 
 
-class AmadeusToolkit(BaseToolkit):
-    """Toolkit for interacting with Amadeus which offers APIs for travel."""
+class SlackToolkit(BaseToolkit):
+    """Toolkit for interacting with Slack."""
 
-    client: Client = Field(default_factory=authenticate)
+    client: WebClient = Field(default_factory=login)
 
     class Config:
         """Pydantic config."""
 
         arbitrary_types_allowed = True
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return [
-            AmadeusClosestAirport(),
-            AmadeusFlightSearch(),
+            SlackGetChannel(),
+            SlackGetMessage(),
+            SlackScheduleMessage(),
+            SlackSendMessage(),
         ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/clickup/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/clickup/toolkit.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Dict, List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.clickup.prompt import (
     CLICKUP_FOLDER_CREATE_PROMPT,
     CLICKUP_GET_ALL_TEAMS_PROMPT,
     CLICKUP_GET_FOLDERS_PROMPT,
     CLICKUP_GET_LIST_PROMPT,
     CLICKUP_GET_SPACES_PROMPT,
@@ -97,12 +98,12 @@
                 name=action["name"],
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=clickup_api_wrapper,
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/csv/__init__.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/csv/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/file_management/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/file_management/toolkit.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,34 @@
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Dict, List, Optional, Type
 
 from langchain_core.pydantic_v1 import root_validator
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.file_management.copy import CopyFileTool
 from langchain_community.tools.file_management.delete import DeleteFileTool
 from langchain_community.tools.file_management.file_search import FileSearchTool
 from langchain_community.tools.file_management.list_dir import ListDirectoryTool
 from langchain_community.tools.file_management.move import MoveFileTool
 from langchain_community.tools.file_management.read import ReadFileTool
 from langchain_community.tools.file_management.write import WriteFileTool
 
-_FILE_TOOLS = {
-    # "Type[Runnable[Any, Any]]" has no attribute "__fields__"  [attr-defined]
-    tool_cls.__fields__["name"].default: tool_cls  # type: ignore[attr-defined]
-    for tool_cls in [
-        CopyFileTool,
-        DeleteFileTool,
-        FileSearchTool,
-        MoveFileTool,
-        ReadFileTool,
-        WriteFileTool,
-        ListDirectoryTool,
-    ]
+_FILE_TOOLS: List[Type[BaseTool]] = [
+    CopyFileTool,
+    DeleteFileTool,
+    FileSearchTool,
+    MoveFileTool,
+    ReadFileTool,
+    WriteFileTool,
+    ListDirectoryTool,
+]
+_FILE_TOOLS_MAP: Dict[str, Type[BaseTool]] = {
+    tool_cls.__fields__["name"].default: tool_cls for tool_cls in _FILE_TOOLS
 }
 
 
 class FileManagementToolkit(BaseToolkit):
     """Toolkit for interacting with local files.
 
     *Security Notice*: This toolkit provides methods to interact with local files.
@@ -57,25 +56,25 @@
     selected_tools: Optional[List[str]] = None
     """If provided, only provide the selected tools. Defaults to all."""
 
     @root_validator
     def validate_tools(cls, values: dict) -> dict:
         selected_tools = values.get("selected_tools") or []
         for tool_name in selected_tools:
-            if tool_name not in _FILE_TOOLS:
+            if tool_name not in _FILE_TOOLS_MAP:
                 raise ValueError(
                     f"File Tool of name {tool_name} not supported."
-                    f" Permitted tools: {list(_FILE_TOOLS)}"
+                    f" Permitted tools: {list(_FILE_TOOLS_MAP)}"
                 )
         return values
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
-        allowed_tools = self.selected_tools or _FILE_TOOLS.keys()
+        allowed_tools = self.selected_tools or _FILE_TOOLS_MAP
         tools: List[BaseTool] = []
         for tool in allowed_tools:
-            tool_cls = _FILE_TOOLS[tool]
-            tools.append(tool_cls(root_dir=self.root_dir))  # type: ignore
+            tool_cls = _FILE_TOOLS_MAP[tool]
+            tools.append(tool_cls(root_dir=self.root_dir))  # type: ignore[call-arg]
         return tools
 
 
 __all__ = ["FileManagementToolkit"]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/github/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/github/toolkit.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """GitHub Toolkit."""
+
 from typing import Dict, List
 
 from langchain_core.pydantic_v1 import BaseModel, Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.github.prompt import (
     COMMENT_ON_ISSUE_PROMPT,
     CREATE_BRANCH_PROMPT,
     CREATE_FILE_PROMPT,
     CREATE_PULL_REQUEST_PROMPT,
     CREATE_REVIEW_REQUEST_PROMPT,
@@ -304,12 +305,12 @@
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=github_api_wrapper,
                 args_schema=action.get("args_schema", None),
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gitlab/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/gitlab/toolkit.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """GitHub Toolkit."""
+
 from typing import Dict, List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.gitlab.prompt import (
     COMMENT_ON_ISSUE_PROMPT,
     CREATE_FILE_PROMPT,
     CREATE_PULL_REQUEST_PROMPT,
     DELETE_FILE_PROMPT,
     GET_ISSUE_PROMPT,
@@ -83,12 +85,12 @@
                 name=action["name"],
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=gitlab_api_wrapper,
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/gmail/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/gmail/toolkit.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List
 
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.gmail.create_draft import GmailCreateDraft
 from langchain_community.tools.gmail.get_message import GmailGetMessage
 from langchain_community.tools.gmail.get_thread import GmailGetThread
 from langchain_community.tools.gmail.search import GmailSearch
 from langchain_community.tools.gmail.send_message import GmailSendMessage
 from langchain_community.tools.gmail.utils import build_resource_service
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/jira/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/jira/toolkit.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Dict, List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.jira.prompt import (
     JIRA_CATCH_ALL_PROMPT,
     JIRA_CONFLUENCE_PAGE_CREATE_PROMPT,
     JIRA_GET_ALL_PROJECTS_PROMPT,
     JIRA_ISSUE_CREATE_PROMPT,
     JIRA_JQL_PROMPT,
@@ -59,12 +60,12 @@
                 name=action["name"],
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=jira_api_wrapper,
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/base.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/json/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Json agent."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models import BaseLanguageModel
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/json/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/json/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/json/toolkit.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 from typing import List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.json.tool import (
     JsonGetValueTool,
     JsonListKeysTool,
     JsonSpec,
 )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/multion/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/multion/toolkit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 """MultiOn agent."""
+
 from __future__ import annotations
 
 from typing import List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.multion.close_session import MultionCloseSession
 from langchain_community.tools.multion.create_session import MultionCreateSession
 from langchain_community.tools.multion.update_session import MultionUpdateSession
 
 
 class MultionToolkit(BaseToolkit):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nasa/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/nasa/toolkit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Dict, List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.nasa.prompt import (
     NASA_CAPTIONS_PROMPT,
     NASA_MANIFEST_PROMPT,
     NASA_METADATA_PROMPT,
     NASA_SEARCH_PROMPT,
 )
@@ -46,12 +47,12 @@
                 name=action["name"],
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=nasa_api_wrapper,
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nla/tool.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/nla/tool.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,21 @@
 """Tool for interacting with a single API with natural language definition."""
 
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, Optional
+from typing import Any, Optional
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.tools import Tool
 
+from langchain_community.chains.openapi.chain import OpenAPIEndpointChain
 from langchain_community.tools.openapi.utils.api_models import APIOperation
 from langchain_community.tools.openapi.utils.openapi_utils import OpenAPISpec
 from langchain_community.utilities.requests import Requests
 
-if TYPE_CHECKING:
-    from langchain.chains.api.openapi.chain import OpenAPIEndpointChain
-
 
 class NLATool(Tool):
     """Natural Language API Tool."""
 
     @classmethod
     def from_open_api_endpoint_chain(
         cls, chain: OpenAPIEndpointChain, api_title: str
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/nla/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/nla/toolkit.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 from __future__ import annotations
 
 from typing import Any, List, Optional, Sequence
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.pydantic_v1 import Field
-from langchain_core.tools import BaseTool
+from langchain_core.tools import BaseTool, BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.agent_toolkits.nla.tool import NLATool
 from langchain_community.tools.openapi.utils.openapi_utils import OpenAPISpec
 from langchain_community.tools.plugin import AIPlugin
 from langchain_community.utilities.requests import Requests
 
 
 class NLAToolkit(BaseToolkit):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/office365/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/office365/toolkit.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List
 
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.office365.create_draft_message import (
     O365CreateDraftMessage,
 )
 from langchain_community.tools.office365.events_search import O365SearchEvents
 from langchain_community.tools.office365.messages_search import O365SearchEmails
 from langchain_community.tools.office365.send_event import O365SendEvent
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/base.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """OpenAPI spec agent."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models import BaseLanguageModel
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/planner.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/planner.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """Agent that interacts with OpenAPI APIs via a hierarchical planning approach."""
+
 import json
 import re
 from functools import partial
-from typing import Any, Callable, Dict, List, Optional
+from typing import Any, Callable, Dict, List, Optional, cast
 
 import yaml
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.prompts import BasePromptTemplate, PromptTemplate
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool, Tool
@@ -64,30 +65,32 @@
 class RequestsGetToolWithParsing(BaseRequestsTool, BaseTool):
     """Requests GET tool with LLM-instructed extraction of truncated responses."""
 
     name: str = "requests_get"
     """Tool name."""
     description = REQUESTS_GET_TOOL_DESCRIPTION
     """Tool description."""
-    response_length: Optional[int] = MAX_RESPONSE_LENGTH
+    response_length: int = MAX_RESPONSE_LENGTH
     """Maximum length of the response to be returned."""
     llm_chain: Any = Field(
         default_factory=_get_default_llm_chain_factory(PARSING_GET_PROMPT)
     )
     """LLMChain used to extract the response."""
 
     def _run(self, text: str) -> str:
         from langchain.output_parsers.json import parse_json_markdown
 
         try:
             data = parse_json_markdown(text)
         except json.JSONDecodeError as e:
             raise e
         data_params = data.get("params")
-        response = self.requests_wrapper.get(data["url"], params=data_params)
+        response: str = cast(
+            str, self.requests_wrapper.get(data["url"], params=data_params)
+        )
         response = response[: self.response_length]
         return self.llm_chain.predict(
             response=response, instructions=data["output_instructions"]
         ).strip()
 
     async def _arun(self, text: str) -> str:
         raise NotImplementedError()
@@ -96,29 +99,29 @@
 class RequestsPostToolWithParsing(BaseRequestsTool, BaseTool):
     """Requests POST tool with LLM-instructed extraction of truncated responses."""
 
     name: str = "requests_post"
     """Tool name."""
     description = REQUESTS_POST_TOOL_DESCRIPTION
     """Tool description."""
-    response_length: Optional[int] = MAX_RESPONSE_LENGTH
+    response_length: int = MAX_RESPONSE_LENGTH
     """Maximum length of the response to be returned."""
     llm_chain: Any = Field(
         default_factory=_get_default_llm_chain_factory(PARSING_POST_PROMPT)
     )
     """LLMChain used to extract the response."""
 
     def _run(self, text: str) -> str:
         from langchain.output_parsers.json import parse_json_markdown
 
         try:
             data = parse_json_markdown(text)
         except json.JSONDecodeError as e:
             raise e
-        response = self.requests_wrapper.post(data["url"], data["data"])
+        response: str = cast(str, self.requests_wrapper.post(data["url"], data["data"]))
         response = response[: self.response_length]
         return self.llm_chain.predict(
             response=response, instructions=data["output_instructions"]
         ).strip()
 
     async def _arun(self, text: str) -> str:
         raise NotImplementedError()
@@ -127,29 +130,31 @@
 class RequestsPatchToolWithParsing(BaseRequestsTool, BaseTool):
     """Requests PATCH tool with LLM-instructed extraction of truncated responses."""
 
     name: str = "requests_patch"
     """Tool name."""
     description = REQUESTS_PATCH_TOOL_DESCRIPTION
     """Tool description."""
-    response_length: Optional[int] = MAX_RESPONSE_LENGTH
+    response_length: int = MAX_RESPONSE_LENGTH
     """Maximum length of the response to be returned."""
     llm_chain: Any = Field(
         default_factory=_get_default_llm_chain_factory(PARSING_PATCH_PROMPT)
     )
     """LLMChain used to extract the response."""
 
     def _run(self, text: str) -> str:
         from langchain.output_parsers.json import parse_json_markdown
 
         try:
             data = parse_json_markdown(text)
         except json.JSONDecodeError as e:
             raise e
-        response = self.requests_wrapper.patch(data["url"], data["data"])
+        response: str = cast(
+            str, self.requests_wrapper.patch(data["url"], data["data"])
+        )
         response = response[: self.response_length]
         return self.llm_chain.predict(
             response=response, instructions=data["output_instructions"]
         ).strip()
 
     async def _arun(self, text: str) -> str:
         raise NotImplementedError()
@@ -158,40 +163,40 @@
 class RequestsPutToolWithParsing(BaseRequestsTool, BaseTool):
     """Requests PUT tool with LLM-instructed extraction of truncated responses."""
 
     name: str = "requests_put"
     """Tool name."""
     description = REQUESTS_PUT_TOOL_DESCRIPTION
     """Tool description."""
-    response_length: Optional[int] = MAX_RESPONSE_LENGTH
+    response_length: int = MAX_RESPONSE_LENGTH
     """Maximum length of the response to be returned."""
     llm_chain: Any = Field(
         default_factory=_get_default_llm_chain_factory(PARSING_PUT_PROMPT)
     )
     """LLMChain used to extract the response."""
 
     def _run(self, text: str) -> str:
         from langchain.output_parsers.json import parse_json_markdown
 
         try:
             data = parse_json_markdown(text)
         except json.JSONDecodeError as e:
             raise e
-        response = self.requests_wrapper.put(data["url"], data["data"])
+        response: str = cast(str, self.requests_wrapper.put(data["url"], data["data"]))
         response = response[: self.response_length]
         return self.llm_chain.predict(
             response=response, instructions=data["output_instructions"]
         ).strip()
 
     async def _arun(self, text: str) -> str:
         raise NotImplementedError()
 
 
 class RequestsDeleteToolWithParsing(BaseRequestsTool, BaseTool):
-    """A tool that sends a DELETE request and parses the response."""
+    """Tool that sends a DELETE request and parses the response."""
 
     name: str = "requests_delete"
     """The name of the tool."""
     description = REQUESTS_DELETE_TOOL_DESCRIPTION
     """The description of the tool."""
 
     response_length: Optional[int] = MAX_RESPONSE_LENGTH
@@ -204,15 +209,15 @@
     def _run(self, text: str) -> str:
         from langchain.output_parsers.json import parse_json_markdown
 
         try:
             data = parse_json_markdown(text)
         except json.JSONDecodeError as e:
             raise e
-        response = self.requests_wrapper.delete(data["url"])
+        response: str = cast(str, self.requests_wrapper.delete(data["url"]))
         response = response[: self.response_length]
         return self.llm_chain.predict(
             response=response, instructions=data["output_instructions"]
         ).strip()
 
     async def _arun(self, text: str) -> str:
         raise NotImplementedError()
@@ -244,27 +249,32 @@
 
 
 def _create_api_controller_agent(
     api_url: str,
     api_docs: str,
     requests_wrapper: RequestsWrapper,
     llm: BaseLanguageModel,
+    allow_dangerous_requests: bool,
 ) -> Any:
     from langchain.agents.agent import AgentExecutor
     from langchain.agents.mrkl.base import ZeroShotAgent
     from langchain.chains.llm import LLMChain
 
     get_llm_chain = LLMChain(llm=llm, prompt=PARSING_GET_PROMPT)
     post_llm_chain = LLMChain(llm=llm, prompt=PARSING_POST_PROMPT)
     tools: List[BaseTool] = [
-        RequestsGetToolWithParsing(
-            requests_wrapper=requests_wrapper, llm_chain=get_llm_chain
+        RequestsGetToolWithParsing(  # type: ignore[call-arg]
+            requests_wrapper=requests_wrapper,
+            llm_chain=get_llm_chain,
+            allow_dangerous_requests=allow_dangerous_requests,
         ),
-        RequestsPostToolWithParsing(
-            requests_wrapper=requests_wrapper, llm_chain=post_llm_chain
+        RequestsPostToolWithParsing(  # type: ignore[call-arg]
+            requests_wrapper=requests_wrapper,
+            llm_chain=post_llm_chain,
+            allow_dangerous_requests=allow_dangerous_requests,
         ),
     ]
     prompt = PromptTemplate(
         template=API_CONTROLLER_PROMPT,
         input_variables=["input", "agent_scratchpad"],
         partial_variables={
             "api_url": api_url,
@@ -282,14 +292,15 @@
     return AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)
 
 
 def _create_api_controller_tool(
     api_spec: ReducedOpenAPISpec,
     requests_wrapper: RequestsWrapper,
     llm: BaseLanguageModel,
+    allow_dangerous_requests: bool,
 ) -> Tool:
     """Expose controller as a tool.
 
     The tool is invoked with a plan from the planner, and dynamically
     creates a controller agent with relevant documentation only to
     constrain the context.
     """
@@ -310,15 +321,17 @@
                 regex_name = re.compile(re.sub("\{.*?\}", ".*", name))
                 if regex_name.match(endpoint_name):
                     found_match = True
                     docs_str += f"== Docs for {endpoint_name} == \n{yaml.dump(docs)}\n"
             if not found_match:
                 raise ValueError(f"{endpoint_name} endpoint does not exist.")
 
-        agent = _create_api_controller_agent(base_url, docs_str, requests_wrapper, llm)
+        agent = _create_api_controller_agent(
+            base_url, docs_str, requests_wrapper, llm, allow_dangerous_requests
+        )
         return agent.run(plan_str)
 
     return Tool(
         name=API_CONTROLLER_TOOL_NAME,
         func=_create_and_run_api_controller_agent,
         description=API_CONTROLLER_TOOL_DESCRIPTION,
     )
@@ -328,31 +341,42 @@
     api_spec: ReducedOpenAPISpec,
     requests_wrapper: RequestsWrapper,
     llm: BaseLanguageModel,
     shared_memory: Optional[Any] = None,
     callback_manager: Optional[BaseCallbackManager] = None,
     verbose: bool = True,
     agent_executor_kwargs: Optional[Dict[str, Any]] = None,
+    allow_dangerous_requests: bool = False,
     **kwargs: Any,
 ) -> Any:
-    """Instantiate OpenAI API planner and controller for a given spec.
+    """Construct an OpenAI API planner and controller for a given spec.
 
     Inject credentials via requests_wrapper.
 
     We use a top-level "orchestrator" agent to invoke the planner and controller,
     rather than a top-level planner
     that invokes a controller with its plan. This is to keep the planner simple.
+
+    You need to set allow_dangerous_requests to True to use Agent with BaseRequestsTool.
+    Requests can be dangerous and can lead to security vulnerabilities.
+    For example, users can ask a server to make a request to an internal
+    server. It's recommended to use requests through a proxy server
+    and avoid accepting inputs from untrusted sources without proper sandboxing.
+    Please see: https://python.langchain.com/docs/security
+    for further security information.
     """
     from langchain.agents.agent import AgentExecutor
     from langchain.agents.mrkl.base import ZeroShotAgent
     from langchain.chains.llm import LLMChain
 
     tools = [
         _create_api_planner_tool(api_spec, llm),
-        _create_api_controller_tool(api_spec, requests_wrapper, llm),
+        _create_api_controller_tool(
+            api_spec, requests_wrapper, llm, allow_dangerous_requests
+        ),
     ]
     prompt = PromptTemplate(
         template=API_ORCHESTRATOR_PROMPT,
         input_variables=["input", "agent_scratchpad"],
         partial_variables={
             "tool_names": ", ".join([tool.name for tool in tools]),
             "tool_descriptions": "\n".join(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/planner_prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/planner_prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/spec.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/spec.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/openapi/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/openapi/toolkit.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """Requests toolkit."""
+
 from __future__ import annotations
 
 from typing import Any, List
 
 from langchain_core.language_models import BaseLanguageModel
-from langchain_core.tools import Tool
+from langchain_core.tools import BaseToolkit, Tool
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.agent_toolkits.json.base import create_json_agent
 from langchain_community.agent_toolkits.json.toolkit import JsonToolkit
 from langchain_community.agent_toolkits.openapi.prompt import DESCRIPTION
 from langchain_community.tools import BaseTool
 from langchain_community.tools.json.tool import JsonSpec
 from langchain_community.tools.requests.tool import (
     RequestsDeleteTool,
@@ -37,23 +37,40 @@
         Control access to who can submit issue requests using this toolkit and
         what network access it has.
 
         See https://python.langchain.com/docs/security for more information.
     """
 
     requests_wrapper: TextRequestsWrapper
+    allow_dangerous_requests: bool = False
+    """Allow dangerous requests. See documentation for details."""
 
     def get_tools(self) -> List[BaseTool]:
         """Return a list of tools."""
         return [
-            RequestsGetTool(requests_wrapper=self.requests_wrapper),
-            RequestsPostTool(requests_wrapper=self.requests_wrapper),
-            RequestsPatchTool(requests_wrapper=self.requests_wrapper),
-            RequestsPutTool(requests_wrapper=self.requests_wrapper),
-            RequestsDeleteTool(requests_wrapper=self.requests_wrapper),
+            RequestsGetTool(
+                requests_wrapper=self.requests_wrapper,
+                allow_dangerous_requests=self.allow_dangerous_requests,
+            ),
+            RequestsPostTool(
+                requests_wrapper=self.requests_wrapper,
+                allow_dangerous_requests=self.allow_dangerous_requests,
+            ),
+            RequestsPatchTool(
+                requests_wrapper=self.requests_wrapper,
+                allow_dangerous_requests=self.allow_dangerous_requests,
+            ),
+            RequestsPutTool(
+                requests_wrapper=self.requests_wrapper,
+                allow_dangerous_requests=self.allow_dangerous_requests,
+            ),
+            RequestsDeleteTool(
+                requests_wrapper=self.requests_wrapper,
+                allow_dangerous_requests=self.allow_dangerous_requests,
+            ),
         ]
 
 
 class OpenAPIToolkit(BaseToolkit):
     """Toolkit for interacting with an OpenAPI API.
 
     *Security Note*: This toolkit contains tools that can read and modify
@@ -62,29 +79,39 @@
 
         For example, this toolkit can be used to delete data exposed via
         an OpenAPI compliant API.
     """
 
     json_agent: Any
     requests_wrapper: TextRequestsWrapper
+    allow_dangerous_requests: bool = False
+    """Allow dangerous requests. See documentation for details."""
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         json_agent_tool = Tool(
             name="json_explorer",
             func=self.json_agent.run,
             description=DESCRIPTION,
         )
-        request_toolkit = RequestsToolkit(requests_wrapper=self.requests_wrapper)
+        request_toolkit = RequestsToolkit(
+            requests_wrapper=self.requests_wrapper,
+            allow_dangerous_requests=self.allow_dangerous_requests,
+        )
         return [*request_toolkit.get_tools(), json_agent_tool]
 
     @classmethod
     def from_llm(
         cls,
         llm: BaseLanguageModel,
         json_spec: JsonSpec,
         requests_wrapper: TextRequestsWrapper,
+        allow_dangerous_requests: bool = False,
         **kwargs: Any,
     ) -> OpenAPIToolkit:
         """Create json agent from llm, then initialize."""
         json_agent = create_json_agent(llm, JsonToolkit(spec=json_spec), **kwargs)
-        return cls(json_agent=json_agent, requests_wrapper=requests_wrapper)
+        return cls(
+            json_agent=json_agent,
+            requests_wrapper=requests_wrapper,
+            allow_dangerous_requests=allow_dangerous_requests,
+        )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/playwright/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/playwright/toolkit.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """Playwright web browser toolkit."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Optional, Type, cast
 
 from langchain_core.pydantic_v1 import Extra, root_validator
-from langchain_core.tools import BaseTool
+from langchain_core.tools import BaseTool, BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools.playwright.base import (
     BaseBrowserTool,
     lazy_import_playwright_browsers,
 )
 from langchain_community.tools.playwright.click import ClickTool
 from langchain_community.tools.playwright.current_page import CurrentWebPageTool
 from langchain_community.tools.playwright.extract_hyperlinks import (
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/base.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Power BI agent."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models import BaseLanguageModel
 
@@ -54,15 +55,15 @@
             prompt=ZeroShotAgent.create_prompt(
                 tools,
                 prefix=prefix.format(top_k=top_k).format(tables=tables),
                 suffix=suffix,
                 input_variables=input_variables,
                 **prompt_params,
             ),
-            callback_manager=callback_manager,  # type: ignore
+            callback_manager=callback_manager,
             verbose=verbose,
         ),
         allowed_tools=[tool.name for tool in tools],
         **kwargs,
     )
     return AgentExecutor.from_agent_and_tools(
         agent=agent,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/chat_base.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/chat_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Power BI agent."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models.chat_models import BaseChatModel
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/prompt.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 # flake8: noqa
 """Prompts for PowerBI agent."""
 
-
 POWERBI_PREFIX = """ ,          PowerBI.
 
      ,             PowerBI,  -  Microsoft.            ,       .        ,  ",      ."   .
 
   ,      ,       ,     ,   ,    ,          ,  ,      , , 1  1000000.       ,    ,      {top_k} .
 """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/powerbi/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/powerbi/toolkit.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 """Toolkit for interacting with a Power BI dataset."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Optional, Union
 
 from langchain_core.callbacks import BaseCallbackManager
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.prompts import PromptTemplate
 from langchain_core.prompts.chat import (
     ChatPromptTemplate,
     HumanMessagePromptTemplate,
     SystemMessagePromptTemplate,
 )
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.powerbi.prompt import (
     QUESTION_TO_QUERY_BASE,
     SINGLE_QUESTION_TO_QUERY,
     USER_INPUT,
 )
 from langchain_community.tools.powerbi.tool import (
@@ -62,15 +63,15 @@
         """Get the tools in the toolkit."""
         return [
             QueryPowerBITool(
                 llm_chain=self._get_chain(),
                 powerbi=self.powerbi,
                 examples=self.examples,
                 max_iterations=self.max_iterations,
-                output_token_limit=self.output_token_limit,
+                output_token_limit=self.output_token_limit,  # type: ignore[arg-type]
                 tiktoken_model_name=self.tiktoken_model_name,
             ),
             InfoPowerBITool(powerbi=self.powerbi),
             ListPowerBITool(powerbi=self.powerbi),
         ]
 
     def _get_chain(self) -> LLMChain:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/slack/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/amadeus/toolkit.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,36 +1,34 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, List
+from typing import TYPE_CHECKING, List, Optional
 
+from langchain_core.language_models import BaseLanguageModel
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
-from langchain_community.tools.slack.get_channel import SlackGetChannel
-from langchain_community.tools.slack.get_message import SlackGetMessage
-from langchain_community.tools.slack.schedule_message import SlackScheduleMessage
-from langchain_community.tools.slack.send_message import SlackSendMessage
-from langchain_community.tools.slack.utils import login
+from langchain_community.tools.amadeus.closest_airport import AmadeusClosestAirport
+from langchain_community.tools.amadeus.flight_search import AmadeusFlightSearch
+from langchain_community.tools.amadeus.utils import authenticate
 
 if TYPE_CHECKING:
-    from slack_sdk import WebClient
+    from amadeus import Client
 
 
-class SlackToolkit(BaseToolkit):
-    """Toolkit for interacting with Slack."""
+class AmadeusToolkit(BaseToolkit):
+    """Toolkit for interacting with Amadeus which offers APIs for travel."""
 
-    client: WebClient = Field(default_factory=login)
+    client: Client = Field(default_factory=authenticate)
+    llm: Optional[BaseLanguageModel] = Field(default=None)
 
     class Config:
         """Pydantic config."""
 
         arbitrary_types_allowed = True
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return [
-            SlackGetChannel(),
-            SlackGetMessage(),
-            SlackScheduleMessage(),
-            SlackSendMessage(),
+            AmadeusClosestAirport(llm=self.llm),
+            AmadeusFlightSearch(),
         ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/base.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Spark SQL agent."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.callbacks import BaseCallbackManager, Callbacks
 from langchain_core.language_models import BaseLanguageModel
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/spark_sql/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/spark_sql/toolkit.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """Toolkit for interacting with Spark SQL."""
+
 from typing import List
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.spark_sql.tool import (
     InfoSparkSQLTool,
     ListSparkSQLTool,
     QueryCheckerTool,
     QuerySparkSQLTool,
 )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/base.py` & `gigachain_community-0.2.0/langchain_community/chat_models/human.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,108 +1,111 @@
-"""SQL agent."""
-from __future__ import annotations
+"""ChatModel wrapper which returns user input as the response.."""
 
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence
+from io import StringIO
+from typing import Any, Callable, Dict, List, Mapping, Optional
 
-from langchain_core.callbacks import BaseCallbackManager
-from langchain_core.language_models import BaseLanguageModel
-from langchain_core.messages import AIMessage, SystemMessage
-from langchain_core.prompts.chat import (
-    ChatPromptTemplate,
-    HumanMessagePromptTemplate,
-    MessagesPlaceholder,
+import yaml
+from langchain_core.callbacks import (
+    CallbackManagerForLLMRun,
 )
-
-from langchain_community.agent_toolkits.sql.prompt import (
-    SQL_FUNCTIONS_SUFFIX,
-    SQL_PREFIX,
-    SQL_SUFFIX,
+from langchain_core.language_models.chat_models import BaseChatModel
+from langchain_core.messages import (
+    BaseMessage,
+    HumanMessage,
+    _message_from_dict,
+    messages_to_dict,
 )
-from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
-from langchain_community.tools import BaseTool
+from langchain_core.outputs import ChatGeneration, ChatResult
+from langchain_core.pydantic_v1 import Field
 
-if TYPE_CHECKING:
-    from langchain.agents.agent import AgentExecutor
-    from langchain.agents.agent_types import AgentType
-
-
-def create_sql_agent(
-    llm: BaseLanguageModel,
-    toolkit: SQLDatabaseToolkit,
-    agent_type: Optional[AgentType] = None,
-    callback_manager: Optional[BaseCallbackManager] = None,
-    prefix: str = SQL_PREFIX,
-    suffix: Optional[str] = None,
-    format_instructions: Optional[str] = None,
-    input_variables: Optional[List[str]] = None,
-    top_k: int = 10,
-    max_iterations: Optional[int] = 15,
-    max_execution_time: Optional[float] = None,
-    early_stopping_method: str = "force",
-    verbose: bool = False,
-    agent_executor_kwargs: Optional[Dict[str, Any]] = None,
-    extra_tools: Sequence[BaseTool] = (),
-    **kwargs: Any,
-) -> AgentExecutor:
-    """Construct an SQL agent from an LLM and tools."""
-    from langchain.agents.agent import AgentExecutor, BaseSingleActionAgent
-    from langchain.agents.agent_types import AgentType
-    from langchain.agents.mrkl.base import ZeroShotAgent
-    from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
-    from langchain.chains.llm import LLMChain
-
-    agent_type = agent_type or AgentType.ZERO_SHOT_REACT_DESCRIPTION
-    tools = toolkit.get_tools() + list(extra_tools)
-    prefix = prefix.format(dialect=toolkit.dialect, top_k=top_k)
-    agent: BaseSingleActionAgent
-
-    if agent_type == AgentType.ZERO_SHOT_REACT_DESCRIPTION:
-        prompt_params = (
-            {"format_instructions": format_instructions}
-            if format_instructions is not None
-            else {}
-        )
-        prompt = ZeroShotAgent.create_prompt(
-            tools,
-            prefix=prefix,
-            suffix=suffix or SQL_SUFFIX,
-            input_variables=input_variables,
-            **prompt_params,
-        )
-        llm_chain = LLMChain(
-            llm=llm,
-            prompt=prompt,
-            callback_manager=callback_manager,
-        )
-        tool_names = [tool.name for tool in tools]
-        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)
+from langchain_community.llms.utils import enforce_stop_tokens
 
-    elif agent_type == AgentType.OPENAI_FUNCTIONS:
-        messages = [
-            SystemMessage(content=prefix),
-            HumanMessagePromptTemplate.from_template("{input}"),
-            AIMessage(content=suffix or SQL_FUNCTIONS_SUFFIX),
-            MessagesPlaceholder(variable_name="agent_scratchpad"),
-        ]
-        input_variables = ["input", "agent_scratchpad"]
-        _prompt = ChatPromptTemplate(input_variables=input_variables, messages=messages)
-
-        agent = OpenAIFunctionsAgent(
-            llm=llm,
-            prompt=_prompt,
-            tools=tools,
-            callback_manager=callback_manager,
-            **kwargs,
-        )
-    else:
-        raise ValueError(f"Agent type {agent_type} not supported at the moment.")
 
-    return AgentExecutor.from_agent_and_tools(
-        agent=agent,
-        tools=tools,
-        callback_manager=callback_manager,
-        verbose=verbose,
-        max_iterations=max_iterations,
-        max_execution_time=max_execution_time,
-        early_stopping_method=early_stopping_method,
-        **(agent_executor_kwargs or {}),
-    )
+def _display_messages(messages: List[BaseMessage]) -> None:
+    dict_messages = messages_to_dict(messages)
+    for message in dict_messages:
+        yaml_string = yaml.dump(
+            message,
+            default_flow_style=False,
+            sort_keys=False,
+            allow_unicode=True,
+            width=10000,
+            line_break=None,
+        )
+        print("\n", "======= start of message =======", "\n\n")  # noqa: T201
+        print(yaml_string)  # noqa: T201
+        print("======= end of message =======", "\n\n")  # noqa: T201
+
+
+def _collect_yaml_input(
+    messages: List[BaseMessage], stop: Optional[List[str]] = None
+) -> BaseMessage:
+    """Collects and returns user input as a single string."""
+    lines = []
+    while True:
+        line = input()
+        if not line.strip():
+            break
+        if stop and any(seq in line for seq in stop):
+            break
+        lines.append(line)
+    yaml_string = "\n".join(lines)
+
+    # Try to parse the input string as YAML
+    try:
+        message = _message_from_dict(yaml.safe_load(StringIO(yaml_string)))
+        if message is None:
+            return HumanMessage(content="")
+        if stop:
+            if isinstance(message.content, str):
+                message.content = enforce_stop_tokens(message.content, stop)
+            else:
+                raise ValueError("Cannot use when output is not a string.")
+        return message
+    except yaml.YAMLError:
+        raise ValueError("Invalid YAML string entered.")
+    except ValueError:
+        raise ValueError("Invalid message entered.")
+
+
+class HumanInputChatModel(BaseChatModel):
+    """ChatModel which returns user input as the response."""
+
+    input_func: Callable = Field(default_factory=lambda: _collect_yaml_input)
+    message_func: Callable = Field(default_factory=lambda: _display_messages)
+    separator: str = "\n"
+    input_kwargs: Mapping[str, Any] = {}
+    message_kwargs: Mapping[str, Any] = {}
+
+    @property
+    def _identifying_params(self) -> Dict[str, Any]:
+        return {
+            "input_func": self.input_func.__name__,
+            "message_func": self.message_func.__name__,
+        }
+
+    @property
+    def _llm_type(self) -> str:
+        """Returns the type of LLM."""
+        return "human-input-chat-model"
+
+    def _generate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> ChatResult:
+        """
+        Displays the messages to the user and returns their input as a response.
+
+        Args:
+            messages (List[BaseMessage]): The messages to be displayed to the user.
+            stop (Optional[List[str]]): A list of stop strings.
+            run_manager (Optional[CallbackManagerForLLMRun]): Currently not used.
+
+        Returns:
+            ChatResult: The user's input as a response.
+        """
+        self.message_func(messages, **self.message_kwargs)
+        user_input = self.input_func(messages, stop=stop, **self.input_kwargs)
+        return ChatResult(generations=[ChatGeneration(message=user_input)])
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/prompt.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/sql/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/sql/toolkit.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """Toolkit for interacting with an SQL database."""
+
 from typing import List
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.pydantic_v1 import Field
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.sql_database.tool import (
     InfoSQLDatabaseTool,
     ListSQLDatabaseTool,
     QuerySQLCheckerTool,
     QuerySQLDataBaseTool,
 )
@@ -65,7 +66,11 @@
         )
         return [
             query_sql_database_tool,
             info_sql_database_tool,
             list_sql_database_tool,
             query_sql_checker_tool,
         ]
+
+    def get_context(self) -> dict:
+        """Return db context that you may want in agent prompt."""
+        return self.db.get_context()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/steam/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/steam/toolkit.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """Steam Toolkit."""
+
 from typing import List
 
-from langchain_community.agent_toolkits.base import BaseToolkit
+from langchain_core.tools import BaseToolkit
+
 from langchain_community.tools import BaseTool
 from langchain_community.tools.steam.prompt import (
     STEAM_GET_GAMES_DETAILS,
     STEAM_GET_RECOMMENDED_GAMES,
 )
 from langchain_community.tools.steam.tool import SteamWebAPIQueryRun
 from langchain_community.utilities.steam import SteamWebAPIWrapper
@@ -37,12 +39,12 @@
                 name=action["name"],
                 description=action["description"],
                 mode=action["mode"],
                 api_wrapper=steam_api_wrapper,
             )
             for action in operations
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         return self.tools
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/xorbits/__init__.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/xorbits/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/agent_toolkits/zapier/toolkit.py` & `gigachain_community-0.2.0/langchain_community/agent_toolkits/zapier/toolkit.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """[DEPRECATED] Zapier Toolkit."""
+
 from typing import List
 
 from langchain_core._api import warn_deprecated
+from langchain_core.tools import BaseToolkit
 
-from langchain_community.agent_toolkits.base import BaseToolkit
 from langchain_community.tools import BaseTool
 from langchain_community.tools.zapier.tool import ZapierNLARunAction
 from langchain_community.utilities.zapier import ZapierNLAWrapper
 
 
 class ZapierToolkit(BaseToolkit):
     """Zapier Toolkit."""
@@ -25,15 +26,15 @@
                 action_id=action["id"],
                 zapier_description=action["description"],
                 params_schema=action["params"],
                 api_wrapper=zapier_nla_wrapper,
             )
             for action in actions
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     @classmethod
     async def async_from_zapier_nla_wrapper(
         cls, zapier_nla_wrapper: ZapierNLAWrapper
     ) -> "ZapierToolkit":
         """Create a toolkit from a ZapierNLAWrapper."""
         actions = await zapier_nla_wrapper.alist()
@@ -42,15 +43,15 @@
                 action_id=action["id"],
                 zapier_description=action["description"],
                 params_schema=action["params"],
                 api_wrapper=zapier_nla_wrapper,
             )
             for action in actions
         ]
-        return cls(tools=tools)
+        return cls(tools=tools)  # type: ignore[arg-type]
 
     def get_tools(self) -> List[BaseTool]:
         """Get the tools in the toolkit."""
         warn_deprecated(
             since="0.0.319",
             message=(
                 "This tool will be deprecated on 2023-11-17. See "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/cache.py` & `gigachain_community-0.2.0/langchain_community/cache.py`

 * *Files 27% similar despite different names*

```diff
@@ -15,61 +15,85 @@
 
 **Class hierarchy:**
 
 .. code-block::
 
     BaseCache --> <name>Cache  # Examples: InMemoryCache, RedisCache, GPTCache
 """
+
 from __future__ import annotations
 
 import hashlib
 import inspect
 import json
 import logging
 import uuid
 import warnings
+from abc import ABC
 from datetime import timedelta
-from functools import lru_cache
+from enum import Enum
+from functools import lru_cache, wraps
 from typing import (
     TYPE_CHECKING,
     Any,
+    Awaitable,
     Callable,
     Dict,
+    Generator,
     List,
     Optional,
+    Sequence,
     Tuple,
     Type,
     Union,
     cast,
 )
 
-from sqlalchemy import Column, Integer, String, create_engine, select
+from sqlalchemy import Column, Integer, String, create_engine, delete, select
 from sqlalchemy.engine import Row
 from sqlalchemy.engine.base import Engine
 from sqlalchemy.orm import Session
 
+from langchain_community.utilities.cassandra import SetupMode as CassandraSetupMode
+from langchain_community.vectorstores.azure_cosmos_db import (
+    CosmosDBSimilarityType,
+    CosmosDBVectorSearchType,
+)
+
 try:
     from sqlalchemy.orm import declarative_base
 except ImportError:
     from sqlalchemy.ext.declarative import declarative_base
 
+from langchain_core._api.deprecation import deprecated, warn_deprecated
 from langchain_core.caches import RETURN_VAL_TYPE, BaseCache
 from langchain_core.embeddings import Embeddings
-from langchain_core.language_models.llms import LLM, get_prompts
+from langchain_core.language_models.llms import LLM, aget_prompts, get_prompts
 from langchain_core.load.dump import dumps
 from langchain_core.load.load import loads
 from langchain_core.outputs import ChatGeneration, Generation
 from langchain_core.utils import get_from_env
 
+from langchain_community.utilities.astradb import (
+    SetupMode as AstraSetupMode,
+)
+from langchain_community.utilities.astradb import (
+    _AstraDBCollectionEnvironment,
+)
+from langchain_community.vectorstores import AzureCosmosDBVectorSearch
+from langchain_community.vectorstores import (
+    OpenSearchVectorSearch as OpenSearchVectorStore,
+)
 from langchain_community.vectorstores.redis import Redis as RedisVectorstore
 
 logger = logging.getLogger(__file__)
 
 if TYPE_CHECKING:
     import momento
+    from astrapy.db import AstraDB, AsyncAstraDB
     from cassandra.cluster import Session as CassandraSession
 
 
 def _hash(_input: str) -> str:
     """Use a deterministic hashing approach."""
     return hashlib.md5(_input.encode()).hexdigest()
 
@@ -185,14 +209,28 @@
         """Update cache based on prompt and llm_string."""
         self._cache[(prompt, llm_string)] = return_val
 
     def clear(self, **kwargs: Any) -> None:
         """Clear cache."""
         self._cache = {}
 
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        """Look up based on prompt and llm_string."""
+        return self.lookup(prompt, llm_string)
+
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        """Update cache based on prompt and llm_string."""
+        self.update(prompt, llm_string, return_val)
+
+    async def aclear(self, **kwargs: Any) -> None:
+        """Clear cache."""
+        self.clear()
+
 
 Base = declarative_base()
 
 
 class FullLLMCache(Base):  # type: ignore
     """SQLite table for full LLM Cache (all generations)."""
 
@@ -284,15 +322,15 @@
                 If provided, it sets the time duration for how long cached
                 items will remain valid. If not provided, cached items will not
                 have an automatic expiration.
         """
         try:
             from upstash_redis import Redis
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import upstash_redis python package. "
                 "Please install it with `pip install upstash_redis`."
             )
         if not isinstance(redis_, Redis):
             raise ValueError("Please pass in Upstash Redis object.")
         self.redis = redis_
         self.ttl = ttl
@@ -345,101 +383,220 @@
         if asynchronous:
             asynchronous = "ASYNC"
         else:
             asynchronous = "SYNC"
         self.redis.flushdb(flush_type=asynchronous)
 
 
-class RedisCache(BaseCache):
-    """Cache that uses Redis as a backend."""
+class _RedisCacheBase(BaseCache, ABC):
+    @staticmethod
+    def _key(prompt: str, llm_string: str) -> str:
+        """Compute key from prompt and llm_string"""
+        return _hash(prompt + llm_string)
+
+    @staticmethod
+    def _ensure_generation_type(return_val: RETURN_VAL_TYPE) -> None:
+        for gen in return_val:
+            if not isinstance(gen, Generation):
+                raise ValueError(
+                    "RedisCache only supports caching of normal LLM generations, "
+                    f"got {type(gen)}"
+                )
+
+    @staticmethod
+    def _get_generations(
+        results: dict[str | bytes, str | bytes],
+    ) -> Optional[List[Generation]]:
+        generations = []
+        if results:
+            for _, text in results.items():
+                try:
+                    generations.append(loads(cast(str, text)))
+                except Exception:
+                    logger.warning(
+                        "Retrieving a cache value that could not be deserialized "
+                        "properly. This is likely due to the cache being in an "
+                        "older format. Please recreate your cache to avoid this "
+                        "error."
+                    )
+                    # In a previous life we stored the raw text directly
+                    # in the table, so assume it's in that format.
+                    generations.append(Generation(text=text))  # type: ignore[arg-type]
+        return generations if generations else None
+
+    @staticmethod
+    def _configure_pipeline_for_update(
+        key: str, pipe: Any, return_val: RETURN_VAL_TYPE, ttl: Optional[int] = None
+    ) -> None:
+        pipe.hset(
+            key,
+            mapping={
+                str(idx): dumps(generation) for idx, generation in enumerate(return_val)
+            },
+        )
+        if ttl is not None:
+            pipe.expire(key, ttl)
+
+
+class RedisCache(_RedisCacheBase):
+    """
+    Cache that uses Redis as a backend. Allows to use a sync `redis.Redis` client.
+    """
 
     def __init__(self, redis_: Any, *, ttl: Optional[int] = None):
         """
         Initialize an instance of RedisCache.
 
         This method initializes an object with Redis caching capabilities.
         It takes a `redis_` parameter, which should be an instance of a Redis
-        client class, allowing the object to interact with a Redis
-        server for caching purposes.
+        client class (`redis.Redis`), allowing the object
+        to interact with a Redis server for caching purposes.
 
         Parameters:
             redis_ (Any): An instance of a Redis client class
-                (e.g., redis.Redis) used for caching.
+                (`redis.Redis`) to be used for caching.
                 This allows the object to communicate with a
                 Redis server for caching operations.
             ttl (int, optional): Time-to-live (TTL) for cached items in seconds.
                 If provided, it sets the time duration for how long cached
                 items will remain valid. If not provided, cached items will not
                 have an automatic expiration.
         """
         try:
             from redis import Redis
         except ImportError:
-            raise ValueError(
-                "Could not import redis python package. "
+            raise ImportError(
+                "Could not import `redis` python package. "
                 "Please install it with `pip install redis`."
             )
         if not isinstance(redis_, Redis):
-            raise ValueError("Please pass in Redis object.")
+            raise ValueError("Please pass a valid `redis.Redis` client.")
         self.redis = redis_
         self.ttl = ttl
 
-    def _key(self, prompt: str, llm_string: str) -> str:
-        """Compute key from prompt and llm_string"""
-        return _hash(prompt + llm_string)
-
     def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
         """Look up based on prompt and llm_string."""
-        generations = []
         # Read from a Redis HASH
-        results = self.redis.hgetall(self._key(prompt, llm_string))
-        if results:
-            for _, text in results.items():
-                try:
-                    generations.append(loads(text))
-                except Exception:
-                    logger.warning(
-                        "Retrieving a cache value that could not be deserialized "
-                        "properly. This is likely due to the cache being in an "
-                        "older format. Please recreate your cache to avoid this "
-                        "error."
-                    )
-                    # In a previous life we stored the raw text directly
-                    # in the table, so assume it's in that format.
-                    generations.append(Generation(text=text))
-        return generations if generations else None
+        try:
+            results = self.redis.hgetall(self._key(prompt, llm_string))
+            return self._get_generations(results)  # type: ignore[arg-type]
+        except Exception as e:
+            logger.error(f"Redis lookup failed: {e}")
+            return None
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
         """Update cache based on prompt and llm_string."""
-        for gen in return_val:
-            if not isinstance(gen, Generation):
-                raise ValueError(
-                    "RedisCache only supports caching of normal LLM generations, "
-                    f"got {type(gen)}"
-                )
-        # Write to a Redis HASH
+        self._ensure_generation_type(return_val)
         key = self._key(prompt, llm_string)
+        try:
+            with self.redis.pipeline() as pipe:
+                self._configure_pipeline_for_update(key, pipe, return_val, self.ttl)
+                pipe.execute()
+        except Exception as e:
+            logger.error(f"Redis update failed: {e}")
 
-        with self.redis.pipeline() as pipe:
-            pipe.hset(
-                key,
-                mapping={
-                    str(idx): dumps(generation)
-                    for idx, generation in enumerate(return_val)
-                },
+    def clear(self, **kwargs: Any) -> None:
+        """Clear cache. If `asynchronous` is True, flush asynchronously."""
+        try:
+            asynchronous = kwargs.get("asynchronous", False)
+            self.redis.flushdb(asynchronous=asynchronous, **kwargs)
+        except Exception as e:
+            logger.error(f"Redis clear failed: {e}")
+
+
+class AsyncRedisCache(_RedisCacheBase):
+    """
+    Cache that uses Redis as a backend. Allows to use an
+    async `redis.asyncio.Redis` client.
+    """
+
+    def __init__(self, redis_: Any, *, ttl: Optional[int] = None):
+        """
+        Initialize an instance of AsyncRedisCache.
+
+        This method initializes an object with Redis caching capabilities.
+        It takes a `redis_` parameter, which should be an instance of a Redis
+        client class (`redis.asyncio.Redis`), allowing the object
+        to interact with a Redis server for caching purposes.
+
+        Parameters:
+            redis_ (Any): An instance of a Redis client class
+                (`redis.asyncio.Redis`) to be used for caching.
+                This allows the object to communicate with a
+                Redis server for caching operations.
+            ttl (int, optional): Time-to-live (TTL) for cached items in seconds.
+                If provided, it sets the time duration for how long cached
+                items will remain valid. If not provided, cached items will not
+                have an automatic expiration.
+        """
+        try:
+            from redis.asyncio import Redis
+        except ImportError:
+            raise ImportError(
+                "Could not import `redis.asyncio` python package. "
+                "Please install it with `pip install redis`."
             )
-            if self.ttl is not None:
-                pipe.expire(key, self.ttl)
+        if not isinstance(redis_, Redis):
+            raise ValueError("Please pass a valid `redis.asyncio.Redis` client.")
+        self.redis = redis_
+        self.ttl = ttl
 
-            pipe.execute()
+    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        """Look up based on prompt and llm_string."""
+        raise NotImplementedError(
+            "This async Redis cache does not implement `lookup()` method. "
+            "Consider using the async `alookup()` version."
+        )
+
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        """Look up based on prompt and llm_string. Async version."""
+        try:
+            results = await self.redis.hgetall(self._key(prompt, llm_string))
+            return self._get_generations(results)  # type: ignore[arg-type]
+        except Exception as e:
+            logger.error(f"Redis async lookup failed: {e}")
+            return None
+
+    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
+        """Update cache based on prompt and llm_string."""
+        raise NotImplementedError(
+            "This async Redis cache does not implement `update()` method. "
+            "Consider using the async `aupdate()` version."
+        )
+
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        """Update cache based on prompt and llm_string. Async version."""
+        self._ensure_generation_type(return_val)
+        key = self._key(prompt, llm_string)
+        try:
+            async with self.redis.pipeline() as pipe:
+                self._configure_pipeline_for_update(key, pipe, return_val, self.ttl)
+                await pipe.execute()  # type: ignore[attr-defined]
+        except Exception as e:
+            logger.error(f"Redis async update failed: {e}")
 
     def clear(self, **kwargs: Any) -> None:
         """Clear cache. If `asynchronous` is True, flush asynchronously."""
-        asynchronous = kwargs.get("asynchronous", False)
-        self.redis.flushdb(asynchronous=asynchronous, **kwargs)
+        raise NotImplementedError(
+            "This async Redis cache does not implement `clear()` method. "
+            "Consider using the async `aclear()` version."
+        )
+
+    async def aclear(self, **kwargs: Any) -> None:
+        """
+        Clear cache. If `asynchronous` is True, flush asynchronously.
+        Async version.
+        """
+        try:
+            asynchronous = kwargs.get("asynchronous", False)
+            await self.redis.flushdb(asynchronous=asynchronous, **kwargs)
+        except Exception as e:
+            logger.error(f"Redis async clear failed: {e}")
 
 
 class RedisSemanticCache(BaseCache):
     """Cache that uses Redis as a vector-store backend."""
 
     # TODO - implement a TTL policy in Redis
 
@@ -657,19 +814,15 @@
         and then retrieve the data from the cache based on the `prompt`.
         """
         from gptcache.adapter.api import get
 
         _gptcache = self._get_gptcache(llm_string)
 
         res = get(prompt, cache_obj=_gptcache)
-        if res:
-            return [
-                Generation(**generation_dict) for generation_dict in json.loads(res)
-            ]
-        return None
+        return _loads_generations(res) if res is not None else None
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
         """Update cache.
         First, retrieve the corresponding cache object using the `llm_string` parameter,
         and then store the `prompt` and `return_val` in the cache object.
         """
         for gen in return_val:
@@ -677,15 +830,15 @@
                 raise ValueError(
                     "GPTCache only supports caching of normal LLM generations, "
                     f"got {type(gen)}"
                 )
         from gptcache.adapter.api import put
 
         _gptcache = self._get_gptcache(llm_string)
-        handled_data = json.dumps([generation.dict() for generation in return_val])
+        handled_data = _dumps_generations(return_val)
         put(prompt, handled_data, cache_obj=_gptcache)
         return None
 
     def clear(self, **kwargs: Any) -> None:
         """Clear cache."""
         from gptcache import Cache
 
@@ -888,92 +1041,144 @@
 CASSANDRA_CACHE_DEFAULT_TTL_SECONDS = None
 
 
 class CassandraCache(BaseCache):
     """
     Cache that uses Cassandra / Astra DB as a backend.
 
+    Example:
+
+        .. code-block:: python
+
+            import cassio
+
+            from langchain_community.cache import CassandraCache
+            from langchain_core.globals import set_llm_cache
+
+            cassio.init(auto=True)  # Requires env. variables, see CassIO docs
+
+            set_llm_cache(CassandraCache())
+
     It uses a single Cassandra table.
     The lookup keys (which get to form the primary key) are:
         - prompt, a string
         - llm_string, a deterministic str representation of the model parameters.
-          (needed to prevent collisions same-prompt-different-model collisions)
+          (needed to prevent same-prompt-different-model collisions)
+
+    Args:
+        session: an open Cassandra session.
+            Leave unspecified to use the global cassio init (see below)
+        keyspace: the keyspace to use for storing the cache.
+            Leave unspecified to use the global cassio init (see below)
+        table_name: name of the Cassandra table to use as cache
+        ttl_seconds: time-to-live for cache entries
+            (default: None, i.e. forever)
+        setup_mode: a value in langchain_community.utilities.cassandra.SetupMode.
+            Choose between SYNC, ASYNC and OFF - the latter if the Cassandra
+            table is guaranteed to exist already, for a faster initialization.
+
+    Note:
+        The session and keyspace parameters, when left out (or passed as None),
+        fall back to the globally-available cassio settings if any are available.
+        In other words, if a previously-run 'cassio.init(...)' has been
+        executed previously anywhere in the code, Cassandra-based objects
+        need not specify the connection parameters at all.
     """
 
     def __init__(
         self,
         session: Optional[CassandraSession] = None,
         keyspace: Optional[str] = None,
         table_name: str = CASSANDRA_CACHE_DEFAULT_TABLE_NAME,
         ttl_seconds: Optional[int] = CASSANDRA_CACHE_DEFAULT_TTL_SECONDS,
         skip_provisioning: bool = False,
+        setup_mode: CassandraSetupMode = CassandraSetupMode.SYNC,
     ):
-        """
-        Initialize with a ready session and a keyspace name.
-        Args:
-            session (cassandra.cluster.Session): an open Cassandra session
-            keyspace (str): the keyspace to use for storing the cache
-            table_name (str): name of the Cassandra table to use as cache
-            ttl_seconds (optional int): time-to-live for cache entries
-                (default: None, i.e. forever)
-        """
+        if skip_provisioning:
+            warn_deprecated(
+                "0.0.33",
+                name="skip_provisioning",
+                alternative=(
+                    "setup_mode=langchain_community.utilities.cassandra.SetupMode.OFF"
+                ),
+                pending=True,
+            )
         try:
             from cassio.table import ElasticCassandraTable
         except (ImportError, ModuleNotFoundError):
-            raise ValueError(
+            raise ImportError(
                 "Could not import cassio python package. "
-                "Please install it with `pip install cassio`."
+                "Please install it with `pip install -U cassio`."
             )
 
         self.session = session
         self.keyspace = keyspace
         self.table_name = table_name
         self.ttl_seconds = ttl_seconds
 
+        kwargs = {}
+        if setup_mode == CassandraSetupMode.ASYNC:
+            kwargs["async_setup"] = True
+
         self.kv_cache = ElasticCassandraTable(
             session=self.session,
             keyspace=self.keyspace,
             table=self.table_name,
             keys=["llm_string", "prompt"],
             primary_key_type=["TEXT", "TEXT"],
             ttl_seconds=self.ttl_seconds,
-            skip_provisioning=skip_provisioning,
+            skip_provisioning=skip_provisioning or setup_mode == CassandraSetupMode.OFF,
+            **kwargs,
         )
 
     def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string."""
         item = self.kv_cache.get(
             llm_string=_hash(llm_string),
             prompt=_hash(prompt),
         )
         if item is not None:
-            generations = _loads_generations(item["body_blob"])
-            # this protects against malformed cached items:
-            if generations is not None:
-                return generations
-            else:
-                return None
+            return _loads_generations(item["body_blob"])
+        else:
+            return None
+
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        item = await self.kv_cache.aget(
+            llm_string=_hash(llm_string),
+            prompt=_hash(prompt),
+        )
+        if item is not None:
+            return _loads_generations(item["body_blob"])
         else:
             return None
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string."""
         blob = _dumps_generations(return_val)
         self.kv_cache.put(
             llm_string=_hash(llm_string),
             prompt=_hash(prompt),
             body_blob=blob,
         )
 
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        blob = _dumps_generations(return_val)
+        await self.kv_cache.aput(
+            llm_string=_hash(llm_string),
+            prompt=_hash(prompt),
+            body_blob=blob,
+        )
+
     def delete_through_llm(
         self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
     ) -> None:
         """
         A wrapper around `delete` with the LLM being passed.
-        In case the llm(prompt) calls have a `stop` param, you should pass it here
+        In case the llm.invoke(prompt) calls have a `stop` param, you should
+        pass it here
         """
         llm_string = get_prompts(
             {**llm.dict(), **{"stop": stop}},
             [],
         )[1]
         return self.delete(prompt, llm_string=llm_string)
 
@@ -984,145 +1189,286 @@
             prompt=_hash(prompt),
         )
 
     def clear(self, **kwargs: Any) -> None:
         """Clear cache. This is for all LLMs at once."""
         self.kv_cache.clear()
 
+    async def aclear(self, **kwargs: Any) -> None:
+        """Clear cache. This is for all LLMs at once."""
+        await self.kv_cache.aclear()
+
 
+# This constant is in fact a similarity - the 'distance' name is kept for compatibility:
 CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC = "dot"
 CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD = 0.85
 CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME = "langchain_llm_semantic_cache"
 CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS = None
 CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE = 16
 
 
 class CassandraSemanticCache(BaseCache):
     """
     Cache that uses Cassandra as a vector-store backend for semantic
     (i.e. similarity-based) lookup.
 
+    Example:
+
+        .. code-block:: python
+
+            import cassio
+
+            from langchain_community.cache import CassandraSemanticCache
+            from langchain_core.globals import set_llm_cache
+
+            cassio.init(auto=True)  # Requires env. variables, see CassIO docs
+
+            my_embedding = ...
+
+            set_llm_cache(CassandraSemanticCache(
+                embedding=my_embedding,
+                table_name="my_semantic_cache",
+            ))
+
     It uses a single (vector) Cassandra table and stores, in principle,
     cached values from several LLMs, so the LLM's llm_string is part
     of the rows' primary keys.
 
-    The similarity is based on one of several distance metrics (default: "dot").
-    If choosing another metric, the default threshold is to be re-tuned accordingly.
+    One can choose a similarity measure (default: "dot" for dot-product).
+    Choosing another one ("cos", "l2") almost certainly requires threshold tuning.
+    (which may be in order nevertheless, even if sticking to "dot").
+
+    Args:
+        session: an open Cassandra session.
+            Leave unspecified to use the global cassio init (see below)
+        keyspace: the keyspace to use for storing the cache.
+            Leave unspecified to use the global cassio init (see below)
+        embedding: Embedding provider for semantic
+            encoding and search.
+        table_name: name of the Cassandra (vector) table
+            to use as cache. There is a default for "simple" usage, but
+            remember to explicitly specify different tables if several embedding
+            models coexist in your app (they cannot share one cache table).
+        distance_metric: an alias for the 'similarity_measure' parameter (see below).
+            As the "distance" terminology is misleading, please prefer
+            'similarity_measure' for clarity.
+        score_threshold: numeric value to use as
+            cutoff for the similarity searches
+        ttl_seconds: time-to-live for cache entries
+            (default: None, i.e. forever)
+        similarity_measure: which measure to adopt for similarity searches.
+            Note: this parameter is aliased by 'distance_metric' - however,
+            it is suggested to use the "similarity" terminology since this value
+            is in fact a similarity (i.e. higher means closer).
+            Note that at most one of the two parameters 'distance_metric'
+            and 'similarity_measure' can be provided.
+        setup_mode: a value in langchain_community.utilities.cassandra.SetupMode.
+            Choose between SYNC, ASYNC and OFF - the latter if the Cassandra
+            table is guaranteed to exist already, for a faster initialization.
+
+    Note:
+        The session and keyspace parameters, when left out (or passed as None),
+        fall back to the globally-available cassio settings if any are available.
+        In other words, if a previously-run 'cassio.init(...)' has been
+        executed previously anywhere in the code, Cassandra-based objects
+        need not specify the connection parameters at all.
     """
 
     def __init__(
         self,
-        session: Optional[CassandraSession],
-        keyspace: Optional[str],
-        embedding: Embeddings,
+        session: Optional[CassandraSession] = None,
+        keyspace: Optional[str] = None,
+        embedding: Optional[Embeddings] = None,
         table_name: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TABLE_NAME,
-        distance_metric: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,
+        distance_metric: Optional[str] = None,
         score_threshold: float = CASSANDRA_SEMANTIC_CACHE_DEFAULT_SCORE_THRESHOLD,
         ttl_seconds: Optional[int] = CASSANDRA_SEMANTIC_CACHE_DEFAULT_TTL_SECONDS,
         skip_provisioning: bool = False,
+        similarity_measure: str = CASSANDRA_SEMANTIC_CACHE_DEFAULT_DISTANCE_METRIC,
+        setup_mode: CassandraSetupMode = CassandraSetupMode.SYNC,
     ):
-        """
-        Initialize the cache with all relevant parameters.
-        Args:
-            session (cassandra.cluster.Session): an open Cassandra session
-            keyspace (str): the keyspace to use for storing the cache
-            embedding (Embedding): Embedding provider for semantic
-                encoding and search.
-            table_name (str): name of the Cassandra (vector) table
-                to use as cache
-            distance_metric (str, 'dot'): which measure to adopt for
-                similarity searches
-            score_threshold (optional float): numeric value to use as
-                cutoff for the similarity searches
-            ttl_seconds (optional int): time-to-live for cache entries
-                (default: None, i.e. forever)
-        The default score threshold is tuned to the default metric.
-        Tune it carefully yourself if switching to another distance metric.
-        """
+        if skip_provisioning:
+            warn_deprecated(
+                "0.0.33",
+                name="skip_provisioning",
+                alternative=(
+                    "setup_mode=langchain_community.utilities.cassandra.SetupMode.OFF"
+                ),
+                pending=True,
+            )
         try:
             from cassio.table import MetadataVectorCassandraTable
         except (ImportError, ModuleNotFoundError):
-            raise ValueError(
+            raise ImportError(
                 "Could not import cassio python package. "
-                "Please install it with `pip install cassio`."
+                "Please install it with `pip install -U cassio`."
+            )
+
+        if not embedding:
+            raise ValueError("Missing required parameter 'embedding'.")
+
+        # detect if legacy 'distance_metric' parameter used
+        if distance_metric is not None:
+            # if passed, takes precedence over 'similarity_measure', but we warn:
+            warn_deprecated(
+                "0.0.33",
+                name="distance_metric",
+                alternative="similarity_measure",
+                pending=True,
             )
+            similarity_measure = distance_metric
+
         self.session = session
         self.keyspace = keyspace
         self.embedding = embedding
         self.table_name = table_name
-        self.distance_metric = distance_metric
+        self.similarity_measure = similarity_measure
         self.score_threshold = score_threshold
         self.ttl_seconds = ttl_seconds
 
         # The contract for this class has separate lookup and update:
         # in order to spare some embedding calculations we cache them between
         # the two calls.
         # Note: each instance of this class has its own `_get_embedding` with
         # its own lru.
         @lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)
         def _cache_embedding(text: str) -> List[float]:
             return self.embedding.embed_query(text=text)
 
         self._get_embedding = _cache_embedding
-        self.embedding_dimension = self._get_embedding_dimension()
+
+        @_async_lru_cache(maxsize=CASSANDRA_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)
+        async def _acache_embedding(text: str) -> List[float]:
+            return await self.embedding.aembed_query(text=text)
+
+        self._aget_embedding = _acache_embedding
+
+        embedding_dimension: Union[int, Awaitable[int], None] = None
+        if setup_mode == CassandraSetupMode.ASYNC:
+            embedding_dimension = self._aget_embedding_dimension()
+        elif setup_mode == CassandraSetupMode.SYNC:
+            embedding_dimension = self._get_embedding_dimension()
+
+        kwargs = {}
+        if setup_mode == CassandraSetupMode.ASYNC:
+            kwargs["async_setup"] = True
 
         self.table = MetadataVectorCassandraTable(
             session=self.session,
             keyspace=self.keyspace,
             table=self.table_name,
             primary_key_type=["TEXT"],
-            vector_dimension=self.embedding_dimension,
+            vector_dimension=embedding_dimension,
             ttl_seconds=self.ttl_seconds,
             metadata_indexing=("allow", {"_llm_string_hash"}),
-            skip_provisioning=skip_provisioning,
+            skip_provisioning=skip_provisioning or setup_mode == CassandraSetupMode.OFF,
+            **kwargs,
         )
 
     def _get_embedding_dimension(self) -> int:
         return len(self._get_embedding(text="This is a sample sentence."))
 
+    async def _aget_embedding_dimension(self) -> int:
+        return len(await self._aget_embedding(text="This is a sample sentence."))
+
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string."""
         embedding_vector = self._get_embedding(text=prompt)
         llm_string_hash = _hash(llm_string)
         body = _dumps_generations(return_val)
         metadata = {
             "_prompt": prompt,
             "_llm_string_hash": llm_string_hash,
         }
         row_id = f"{_hash(prompt)}-{llm_string_hash}"
-        #
+
         self.table.put(
             body_blob=body,
             vector=embedding_vector,
             row_id=row_id,
             metadata=metadata,
         )
 
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        embedding_vector = await self._aget_embedding(text=prompt)
+        llm_string_hash = _hash(llm_string)
+        body = _dumps_generations(return_val)
+        metadata = {
+            "_prompt": prompt,
+            "_llm_string_hash": llm_string_hash,
+        }
+        row_id = f"{_hash(prompt)}-{llm_string_hash}"
+
+        await self.table.aput(
+            body_blob=body,
+            vector=embedding_vector,
+            row_id=row_id,
+            metadata=metadata,
+        )
+
     def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string."""
         hit_with_id = self.lookup_with_id(prompt, llm_string)
         if hit_with_id is not None:
             return hit_with_id[1]
         else:
             return None
 
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        hit_with_id = await self.alookup_with_id(prompt, llm_string)
+        if hit_with_id is not None:
+            return hit_with_id[1]
+        else:
+            return None
+
     def lookup_with_id(
         self, prompt: str, llm_string: str
     ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
         """
         Look up based on prompt and llm_string.
         If there are hits, return (document_id, cached_entry)
         """
         prompt_embedding: List[float] = self._get_embedding(text=prompt)
         hits = list(
             self.table.metric_ann_search(
                 vector=prompt_embedding,
                 metadata={"_llm_string_hash": _hash(llm_string)},
                 n=1,
-                metric=self.distance_metric,
+                metric=self.similarity_measure,
+                metric_threshold=self.score_threshold,
+            )
+        )
+        if hits:
+            hit = hits[0]
+            generations = _loads_generations(hit["body_blob"])
+            if generations is not None:
+                # this protects against malformed cached items:
+                return (
+                    hit["row_id"],
+                    generations,
+                )
+            else:
+                return None
+        else:
+            return None
+
+    async def alookup_with_id(
+        self, prompt: str, llm_string: str
+    ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
+        """
+        Look up based on prompt and llm_string.
+        If there are hits, return (document_id, cached_entry)
+        """
+        prompt_embedding: List[float] = await self._aget_embedding(text=prompt)
+        hits = list(
+            await self.table.ametric_ann_search(
+                vector=prompt_embedding,
+                metadata={"_llm_string_hash": _hash(llm_string)},
+                n=1,
+                metric=self.similarity_measure,
                 metric_threshold=self.score_threshold,
             )
         )
         if hits:
             hit = hits[0]
             generations = _loads_generations(hit["body_blob"])
             if generations is not None:
@@ -1141,26 +1487,49 @@
     ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
         llm_string = get_prompts(
             {**llm.dict(), **{"stop": stop}},
             [],
         )[1]
         return self.lookup_with_id(prompt, llm_string=llm_string)
 
+    async def alookup_with_id_through_llm(
+        self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
+    ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
+        llm_string = (
+            await aget_prompts(
+                {**llm.dict(), **{"stop": stop}},
+                [],
+            )
+        )[1]
+        return await self.alookup_with_id(prompt, llm_string=llm_string)
+
     def delete_by_document_id(self, document_id: str) -> None:
         """
         Given this is a "similarity search" cache, an invalidation pattern
         that makes sense is first a lookup to get an ID, and then deleting
         with that ID. This is for the second step.
         """
         self.table.delete(row_id=document_id)
 
+    async def adelete_by_document_id(self, document_id: str) -> None:
+        """
+        Given this is a "similarity search" cache, an invalidation pattern
+        that makes sense is first a lookup to get an ID, and then deleting
+        with that ID. This is for the second step.
+        """
+        await self.table.adelete(row_id=document_id)
+
     def clear(self, **kwargs: Any) -> None:
         """Clear the *whole* semantic cache."""
         self.table.clear()
 
+    async def aclear(self, **kwargs: Any) -> None:
+        """Clear the *whole* semantic cache."""
+        await self.table.aclear()
+
 
 class FullMd5LLMCache(Base):  # type: ignore
     """SQLite table for full LLM Cache (all generations)."""
 
     __tablename__ = "full_md5_llm_cache"
     id = Column(String, primary_key=True)
     prompt_md5 = Column(String, index=True)
@@ -1186,45 +1555,41 @@
         rows = self._search_rows(prompt, llm_string)
         if rows:
             return [loads(row[0]) for row in rows]
         return None
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
         """Update based on prompt and llm_string."""
-        self._delete_previous(prompt, llm_string)
-        prompt_md5 = self.get_md5(prompt)
-        items = [
-            self.cache_schema(
-                id=str(uuid.uuid1()),
-                prompt=prompt,
-                prompt_md5=prompt_md5,
-                llm=llm_string,
-                response=dumps(gen),
-                idx=i,
-            )
-            for i, gen in enumerate(return_val)
-        ]
         with Session(self.engine) as session, session.begin():
+            self._delete_previous(session, prompt, llm_string)
+            prompt_md5 = self.get_md5(prompt)
+            items = [
+                self.cache_schema(
+                    id=str(uuid.uuid1()),
+                    prompt=prompt,
+                    prompt_md5=prompt_md5,
+                    llm=llm_string,
+                    response=dumps(gen),
+                    idx=i,
+                )
+                for i, gen in enumerate(return_val)
+            ]
             for item in items:
                 session.merge(item)
 
-    def _delete_previous(self, prompt: str, llm_string: str) -> None:
+    def _delete_previous(self, session: Session, prompt: str, llm_string: str) -> None:
         stmt = (
-            select(self.cache_schema.response)
+            delete(self.cache_schema)
             .where(self.cache_schema.prompt_md5 == self.get_md5(prompt))  # type: ignore
             .where(self.cache_schema.llm == llm_string)
             .where(self.cache_schema.prompt == prompt)
-            .order_by(self.cache_schema.idx)
         )
-        with Session(self.engine) as session, session.begin():
-            rows = session.execute(stmt).fetchall()
-            for item in rows:
-                session.delete(item)
+        session.execute(stmt)
 
-    def _search_rows(self, prompt: str, llm_string: str) -> List[Row]:
+    def _search_rows(self, prompt: str, llm_string: str) -> Sequence[Row]:
         prompt_pd5 = self.get_md5(prompt)
         stmt = (
             select(self.cache_schema.response)
             .where(self.cache_schema.prompt_md5 == prompt_pd5)  # type: ignore
             .where(self.cache_schema.llm == llm_string)
             .where(self.cache_schema.prompt == prompt)
             .order_by(self.cache_schema.idx)
@@ -1241,282 +1606,383 @@
     def get_md5(input_string: str) -> str:
         return hashlib.md5(input_string.encode()).hexdigest()
 
 
 ASTRA_DB_CACHE_DEFAULT_COLLECTION_NAME = "langchain_astradb_cache"
 
 
+@deprecated(
+    since="0.0.28",
+    removal="0.3.0",
+    alternative_import="langchain_astradb.AstraDBCache",
+)
 class AstraDBCache(BaseCache):
-    """
-    Cache that uses Astra DB as a backend.
-
-    It uses a single collection as a kv store
-    The lookup keys, combined in the _id of the documents, are:
-        - prompt, a string
-        - llm_string, a deterministic str representation of the model parameters.
-          (needed to prevent same-prompt-different-model collisions)
-    """
+    @staticmethod
+    def _make_id(prompt: str, llm_string: str) -> str:
+        return f"{_hash(prompt)}#{_hash(llm_string)}"
 
     def __init__(
         self,
         *,
         collection_name: str = ASTRA_DB_CACHE_DEFAULT_COLLECTION_NAME,
         token: Optional[str] = None,
         api_endpoint: Optional[str] = None,
-        astra_db_client: Optional[Any] = None,  # 'astrapy.db.AstraDB' if passed
+        astra_db_client: Optional[AstraDB] = None,
+        async_astra_db_client: Optional[AsyncAstraDB] = None,
         namespace: Optional[str] = None,
+        pre_delete_collection: bool = False,
+        setup_mode: AstraSetupMode = AstraSetupMode.SYNC,
     ):
         """
-        Create an AstraDB cache using a collection for storage.
+        Cache that uses Astra DB as a backend.
 
-        Args (only keyword-arguments accepted):
-            collection_name (str): name of the Astra DB collection to create/use.
-            token (Optional[str]): API token for Astra DB usage.
-            api_endpoint (Optional[str]): full URL to the API endpoint,
-                such as "https://<DB-ID>-us-east1.apps.astra.datastax.com".
-            astra_db_client (Optional[Any]): *alternative to token+api_endpoint*,
+        It uses a single collection as a kv store
+        The lookup keys, combined in the _id of the documents, are:
+            - prompt, a string
+            - llm_string, a deterministic str representation of the model parameters.
+              (needed to prevent same-prompt-different-model collisions)
+
+        Args:
+            collection_name: name of the Astra DB collection to create/use.
+            token: API token for Astra DB usage.
+            api_endpoint: full URL to the API endpoint,
+                such as `https://<DB-ID>-us-east1.apps.astra.datastax.com`.
+            astra_db_client: *alternative to token+api_endpoint*,
                 you can pass an already-created 'astrapy.db.AstraDB' instance.
-            namespace (Optional[str]): namespace (aka keyspace) where the
+            async_astra_db_client: *alternative to token+api_endpoint*,
+                you can pass an already-created 'astrapy.db.AsyncAstraDB' instance.
+            namespace: namespace (aka keyspace) where the
                 collection is created. Defaults to the database's "default namespace".
-        """
-        try:
-            from astrapy.db import (
-                AstraDB as LibAstraDB,
-            )
-        except (ImportError, ModuleNotFoundError):
-            raise ImportError(
-                "Could not import a recent astrapy python package. "
-                "Please install it with `pip install --upgrade astrapy`."
-            )
-        # Conflicting-arg checks:
-        if astra_db_client is not None:
-            if token is not None or api_endpoint is not None:
-                raise ValueError(
-                    "You cannot pass 'astra_db_client' to AstraDB if passing "
-                    "'token' and 'api_endpoint'."
-                )
-
-        self.collection_name = collection_name
-        self.token = token
-        self.api_endpoint = api_endpoint
-        self.namespace = namespace
-
-        if astra_db_client is not None:
-            self.astra_db = astra_db_client
-        else:
-            self.astra_db = LibAstraDB(
-                token=self.token,
-                api_endpoint=self.api_endpoint,
-                namespace=self.namespace,
-            )
-        self.collection = self.astra_db.create_collection(
-            collection_name=self.collection_name,
+            setup_mode: mode used to create the Astra DB collection (SYNC, ASYNC or
+                OFF).
+            pre_delete_collection: whether to delete the collection
+                before creating it. If False and the collection already exists,
+                the collection will be used as is.
+        """
+        self.astra_env = _AstraDBCollectionEnvironment(
+            collection_name=collection_name,
+            token=token,
+            api_endpoint=api_endpoint,
+            astra_db_client=astra_db_client,
+            async_astra_db_client=async_astra_db_client,
+            namespace=namespace,
+            setup_mode=setup_mode,
+            pre_delete_collection=pre_delete_collection,
         )
-
-    @staticmethod
-    def _make_id(prompt: str, llm_string: str) -> str:
-        return f"{_hash(prompt)}#{_hash(llm_string)}"
+        self.collection = self.astra_env.collection
+        self.async_collection = self.astra_env.async_collection
 
     def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string."""
+        self.astra_env.ensure_db_setup()
         doc_id = self._make_id(prompt, llm_string)
         item = self.collection.find_one(
             filter={
                 "_id": doc_id,
             },
             projection={
                 "body_blob": 1,
             },
         )["data"]["document"]
-        if item is not None:
-            generations = _loads_generations(item["body_blob"])
-            # this protects against malformed cached items:
-            if generations is not None:
-                return generations
-            else:
-                return None
-        else:
-            return None
+        return _loads_generations(item["body_blob"]) if item is not None else None
+
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        await self.astra_env.aensure_db_setup()
+        doc_id = self._make_id(prompt, llm_string)
+        item = (
+            await self.async_collection.find_one(
+                filter={
+                    "_id": doc_id,
+                },
+                projection={
+                    "body_blob": 1,
+                },
+            )
+        )["data"]["document"]
+        return _loads_generations(item["body_blob"]) if item is not None else None
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string."""
+        self.astra_env.ensure_db_setup()
         doc_id = self._make_id(prompt, llm_string)
         blob = _dumps_generations(return_val)
         self.collection.upsert(
             {
                 "_id": doc_id,
                 "body_blob": blob,
             },
         )
 
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        await self.astra_env.aensure_db_setup()
+        doc_id = self._make_id(prompt, llm_string)
+        blob = _dumps_generations(return_val)
+        await self.async_collection.upsert(
+            {
+                "_id": doc_id,
+                "body_blob": blob,
+            },
+        )
+
     def delete_through_llm(
         self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
     ) -> None:
         """
         A wrapper around `delete` with the LLM being passed.
-        In case the llm(prompt) calls have a `stop` param, you should pass it here
+        In case the llm.invoke(prompt) calls have a `stop` param, you should
+        pass it here
         """
         llm_string = get_prompts(
             {**llm.dict(), **{"stop": stop}},
             [],
         )[1]
         return self.delete(prompt, llm_string=llm_string)
 
+    async def adelete_through_llm(
+        self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
+    ) -> None:
+        """
+        A wrapper around `adelete` with the LLM being passed.
+        In case the llm.invoke(prompt) calls have a `stop` param, you should
+        pass it here
+        """
+        llm_string = (
+            await aget_prompts(
+                {**llm.dict(), **{"stop": stop}},
+                [],
+            )
+        )[1]
+        return await self.adelete(prompt, llm_string=llm_string)
+
     def delete(self, prompt: str, llm_string: str) -> None:
         """Evict from cache if there's an entry."""
+        self.astra_env.ensure_db_setup()
         doc_id = self._make_id(prompt, llm_string)
-        return self.collection.delete_one(doc_id)
+        self.collection.delete_one(doc_id)
+
+    async def adelete(self, prompt: str, llm_string: str) -> None:
+        """Evict from cache if there's an entry."""
+        await self.astra_env.aensure_db_setup()
+        doc_id = self._make_id(prompt, llm_string)
+        await self.async_collection.delete_one(doc_id)
 
     def clear(self, **kwargs: Any) -> None:
-        """Clear cache. This is for all LLMs at once."""
-        self.astra_db.truncate_collection(self.collection_name)
+        self.astra_env.ensure_db_setup()
+        self.collection.clear()
+
+    async def aclear(self, **kwargs: Any) -> None:
+        await self.astra_env.aensure_db_setup()
+        await self.async_collection.clear()
 
 
 ASTRA_DB_SEMANTIC_CACHE_DEFAULT_THRESHOLD = 0.85
 ASTRA_DB_CACHE_DEFAULT_COLLECTION_NAME = "langchain_astradb_semantic_cache"
 ASTRA_DB_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE = 16
 
 
-class AstraDBSemanticCache(BaseCache):
-    """
-    Cache that uses Astra DB as a vector-store backend for semantic
-    (i.e. similarity-based) lookup.
+_unset = ["unset"]
 
-    It uses a single (vector) collection and can store
-    cached values from several LLMs, so the LLM's 'llm_string' is stored
-    in the document metadata.
 
-    You can choose the preferred similarity (or use the API default) --
-    remember the threshold might require metric-dependend tuning.
-    """
+class _CachedAwaitable:
+    """Caches the result of an awaitable so it can be awaited multiple times"""
+
+    def __init__(self, awaitable: Awaitable[Any]):
+        self.awaitable = awaitable
+        self.result = _unset
+
+    def __await__(self) -> Generator:
+        if self.result is _unset:
+            self.result = yield from self.awaitable.__await__()
+        return self.result
+
+
+def _reawaitable(func: Callable) -> Callable:
+    """Makes an async function result awaitable multiple times"""
+
+    @wraps(func)
+    def wrapper(*args: Any, **kwargs: Any) -> _CachedAwaitable:
+        return _CachedAwaitable(func(*args, **kwargs))
+
+    return wrapper
+
+
+def _async_lru_cache(maxsize: int = 128, typed: bool = False) -> Callable:
+    """Least-recently-used async cache decorator.
+    Equivalent to functools.lru_cache for async functions"""
+
+    def decorating_function(user_function: Callable) -> Callable:
+        return lru_cache(maxsize, typed)(_reawaitable(user_function))
 
+    return decorating_function
+
+
+@deprecated(
+    since="0.0.28",
+    removal="0.3.0",
+    alternative_import="langchain_astradb.AstraDBSemanticCache",
+)
+class AstraDBSemanticCache(BaseCache):
     def __init__(
         self,
         *,
         collection_name: str = ASTRA_DB_CACHE_DEFAULT_COLLECTION_NAME,
         token: Optional[str] = None,
         api_endpoint: Optional[str] = None,
-        astra_db_client: Optional[Any] = None,  # 'astrapy.db.AstraDB' if passed
+        astra_db_client: Optional[AstraDB] = None,
+        async_astra_db_client: Optional[AsyncAstraDB] = None,
         namespace: Optional[str] = None,
+        setup_mode: AstraSetupMode = AstraSetupMode.SYNC,
+        pre_delete_collection: bool = False,
         embedding: Embeddings,
         metric: Optional[str] = None,
         similarity_threshold: float = ASTRA_DB_SEMANTIC_CACHE_DEFAULT_THRESHOLD,
     ):
         """
-        Initialize the cache with all relevant parameters.
-        Args:
+        Cache that uses Astra DB as a vector-store backend for semantic
+        (i.e. similarity-based) lookup.
+
+        It uses a single (vector) collection and can store
+        cached values from several LLMs, so the LLM's 'llm_string' is stored
+        in the document metadata.
+
+        You can choose the preferred similarity (or use the API default).
+        The default score threshold is tuned to the default metric.
+        Tune it carefully yourself if switching to another distance metric.
 
-            collection_name (str): name of the Astra DB collection to create/use.
-            token (Optional[str]): API token for Astra DB usage.
-            api_endpoint (Optional[str]): full URL to the API endpoint,
-                such as "https://<DB-ID>-us-east1.apps.astra.datastax.com".
-            astra_db_client (Optional[Any]): *alternative to token+api_endpoint*,
+        Args:
+            collection_name: name of the Astra DB collection to create/use.
+            token: API token for Astra DB usage.
+            api_endpoint: full URL to the API endpoint,
+                such as `https://<DB-ID>-us-east1.apps.astra.datastax.com`.
+            astra_db_client: *alternative to token+api_endpoint*,
                 you can pass an already-created 'astrapy.db.AstraDB' instance.
-            namespace (Optional[str]): namespace (aka keyspace) where the
+            async_astra_db_client: *alternative to token+api_endpoint*,
+                you can pass an already-created 'astrapy.db.AsyncAstraDB' instance.
+            namespace: namespace (aka keyspace) where the
                 collection is created. Defaults to the database's "default namespace".
-            embedding (Embedding): Embedding provider for semantic
-                encoding and search.
+            setup_mode: mode used to create the Astra DB collection (SYNC, ASYNC or
+                OFF).
+            pre_delete_collection: whether to delete the collection
+                before creating it. If False and the collection already exists,
+                the collection will be used as is.
+            embedding: Embedding provider for semantic encoding and search.
             metric: the function to use for evaluating similarity of text embeddings.
                 Defaults to 'cosine' (alternatives: 'euclidean', 'dot_product')
-            similarity_threshold (float, optional): the minimum similarity
-                for accepting a (semantic-search) match.
-
-        The default score threshold is tuned to the default metric.
-        Tune it carefully yourself if switching to another distance metric.
+            similarity_threshold: the minimum similarity for accepting a
+                (semantic-search) match.
         """
-        try:
-            from astrapy.db import (
-                AstraDB as LibAstraDB,
-            )
-        except (ImportError, ModuleNotFoundError):
-            raise ImportError(
-                "Could not import a recent astrapy python package. "
-                "Please install it with `pip install --upgrade astrapy`."
-            )
-        # Conflicting-arg checks:
-        if astra_db_client is not None:
-            if token is not None or api_endpoint is not None:
-                raise ValueError(
-                    "You cannot pass 'astra_db_client' to AstraDB if passing "
-                    "'token' and 'api_endpoint'."
-                )
-
         self.embedding = embedding
         self.metric = metric
         self.similarity_threshold = similarity_threshold
+        self.collection_name = collection_name
 
         # The contract for this class has separate lookup and update:
         # in order to spare some embedding calculations we cache them between
         # the two calls.
         # Note: each instance of this class has its own `_get_embedding` with
         # its own lru.
         @lru_cache(maxsize=ASTRA_DB_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)
         def _cache_embedding(text: str) -> List[float]:
             return self.embedding.embed_query(text=text)
 
         self._get_embedding = _cache_embedding
-        self.embedding_dimension = self._get_embedding_dimension()
 
-        self.collection_name = collection_name
-        self.token = token
-        self.api_endpoint = api_endpoint
-        self.namespace = namespace
-
-        if astra_db_client is not None:
-            self.astra_db = astra_db_client
-        else:
-            self.astra_db = LibAstraDB(
-                token=self.token,
-                api_endpoint=self.api_endpoint,
-                namespace=self.namespace,
-            )
-        self.collection = self.astra_db.create_collection(
-            collection_name=self.collection_name,
-            dimension=self.embedding_dimension,
-            metric=self.metric,
+        @_async_lru_cache(maxsize=ASTRA_DB_SEMANTIC_CACHE_EMBEDDING_CACHE_SIZE)
+        async def _acache_embedding(text: str) -> List[float]:
+            return await self.embedding.aembed_query(text=text)
+
+        self._aget_embedding = _acache_embedding
+
+        embedding_dimension: Union[int, Awaitable[int], None] = None
+        if setup_mode == AstraSetupMode.ASYNC:
+            embedding_dimension = self._aget_embedding_dimension()
+        elif setup_mode == AstraSetupMode.SYNC:
+            embedding_dimension = self._get_embedding_dimension()
+
+        self.astra_env = _AstraDBCollectionEnvironment(
+            collection_name=collection_name,
+            token=token,
+            api_endpoint=api_endpoint,
+            astra_db_client=astra_db_client,
+            async_astra_db_client=async_astra_db_client,
+            namespace=namespace,
+            setup_mode=setup_mode,
+            pre_delete_collection=pre_delete_collection,
+            embedding_dimension=embedding_dimension,
+            metric=metric,
         )
+        self.collection = self.astra_env.collection
+        self.async_collection = self.astra_env.async_collection
 
     def _get_embedding_dimension(self) -> int:
         return len(self._get_embedding(text="This is a sample sentence."))
 
+    async def _aget_embedding_dimension(self) -> int:
+        return len(await self._aget_embedding(text="This is a sample sentence."))
+
     @staticmethod
     def _make_id(prompt: str, llm_string: str) -> str:
         return f"{_hash(prompt)}#{_hash(llm_string)}"
 
     def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
-        """Update cache based on prompt and llm_string."""
+        self.astra_env.ensure_db_setup()
         doc_id = self._make_id(prompt, llm_string)
         llm_string_hash = _hash(llm_string)
         embedding_vector = self._get_embedding(text=prompt)
         body = _dumps_generations(return_val)
         #
         self.collection.upsert(
             {
                 "_id": doc_id,
                 "body_blob": body,
                 "llm_string_hash": llm_string_hash,
                 "$vector": embedding_vector,
             }
         )
 
+    async def aupdate(
+        self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE
+    ) -> None:
+        await self.astra_env.aensure_db_setup()
+        doc_id = self._make_id(prompt, llm_string)
+        llm_string_hash = _hash(llm_string)
+        embedding_vector = await self._aget_embedding(text=prompt)
+        body = _dumps_generations(return_val)
+        #
+        await self.async_collection.upsert(
+            {
+                "_id": doc_id,
+                "body_blob": body,
+                "llm_string_hash": llm_string_hash,
+                "$vector": embedding_vector,
+            }
+        )
+
     def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
-        """Look up based on prompt and llm_string."""
         hit_with_id = self.lookup_with_id(prompt, llm_string)
         if hit_with_id is not None:
             return hit_with_id[1]
         else:
             return None
 
+    async def alookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        hit_with_id = await self.alookup_with_id(prompt, llm_string)
+        if hit_with_id is not None:
+            return hit_with_id[1]
+        else:
+            return None
+
     def lookup_with_id(
         self, prompt: str, llm_string: str
     ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
         """
         Look up based on prompt and llm_string.
         If there are hits, return (document_id, cached_entry) for the top hit
         """
+        self.astra_env.ensure_db_setup()
         prompt_embedding: List[float] = self._get_embedding(text=prompt)
         llm_string_hash = _hash(llm_string)
 
         hit = self.collection.vector_find_one(
             vector=prompt_embedding,
             filter={
                 "llm_string_hash": llm_string_hash,
@@ -1527,31 +1993,383 @@
 
         if hit is None or hit["$similarity"] < self.similarity_threshold:
             return None
         else:
             generations = _loads_generations(hit["body_blob"])
             if generations is not None:
                 # this protects against malformed cached items:
-                return (hit["_id"], generations)
+                return hit["_id"], generations
+            else:
+                return None
+
+    async def alookup_with_id(
+        self, prompt: str, llm_string: str
+    ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
+        """
+        Look up based on prompt and llm_string.
+        If there are hits, return (document_id, cached_entry) for the top hit
+        """
+        await self.astra_env.aensure_db_setup()
+        prompt_embedding: List[float] = await self._aget_embedding(text=prompt)
+        llm_string_hash = _hash(llm_string)
+
+        hit = await self.async_collection.vector_find_one(
+            vector=prompt_embedding,
+            filter={
+                "llm_string_hash": llm_string_hash,
+            },
+            fields=["body_blob", "_id"],
+            include_similarity=True,
+        )
+
+        if hit is None or hit["$similarity"] < self.similarity_threshold:
+            return None
+        else:
+            generations = _loads_generations(hit["body_blob"])
+            if generations is not None:
+                # this protects against malformed cached items:
+                return hit["_id"], generations
             else:
                 return None
 
     def lookup_with_id_through_llm(
         self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
     ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
         llm_string = get_prompts(
             {**llm.dict(), **{"stop": stop}},
             [],
         )[1]
         return self.lookup_with_id(prompt, llm_string=llm_string)
 
+    async def alookup_with_id_through_llm(
+        self, prompt: str, llm: LLM, stop: Optional[List[str]] = None
+    ) -> Optional[Tuple[str, RETURN_VAL_TYPE]]:
+        llm_string = (
+            await aget_prompts(
+                {**llm.dict(), **{"stop": stop}},
+                [],
+            )
+        )[1]
+        return await self.alookup_with_id(prompt, llm_string=llm_string)
+
     def delete_by_document_id(self, document_id: str) -> None:
         """
         Given this is a "similarity search" cache, an invalidation pattern
         that makes sense is first a lookup to get an ID, and then deleting
         with that ID. This is for the second step.
         """
+        self.astra_env.ensure_db_setup()
         self.collection.delete_one(document_id)
 
+    async def adelete_by_document_id(self, document_id: str) -> None:
+        """
+        Given this is a "similarity search" cache, an invalidation pattern
+        that makes sense is first a lookup to get an ID, and then deleting
+        with that ID. This is for the second step.
+        """
+        await self.astra_env.aensure_db_setup()
+        await self.async_collection.delete_one(document_id)
+
     def clear(self, **kwargs: Any) -> None:
-        """Clear the *whole* semantic cache."""
-        self.astra_db.truncate_collection(self.collection_name)
+        self.astra_env.ensure_db_setup()
+        self.collection.clear()
+
+    async def aclear(self, **kwargs: Any) -> None:
+        await self.astra_env.aensure_db_setup()
+        await self.async_collection.clear()
+
+
+class AzureCosmosDBSemanticCache(BaseCache):
+    """Cache that uses Cosmos DB Mongo vCore vector-store backend"""
+
+    DEFAULT_DATABASE_NAME = "CosmosMongoVCoreCacheDB"
+    DEFAULT_COLLECTION_NAME = "CosmosMongoVCoreCacheColl"
+
+    def __init__(
+        self,
+        cosmosdb_connection_string: str,
+        database_name: str,
+        collection_name: str,
+        embedding: Embeddings,
+        *,
+        cosmosdb_client: Optional[Any] = None,
+        num_lists: int = 100,
+        similarity: CosmosDBSimilarityType = CosmosDBSimilarityType.COS,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        dimensions: int = 1536,
+        m: int = 16,
+        ef_construction: int = 64,
+        ef_search: int = 40,
+        score_threshold: Optional[float] = None,
+        application_name: str = "LANGCHAIN_CACHING_PYTHON",
+    ):
+        """
+        Args:
+            cosmosdb_connection_string: Cosmos DB Mongo vCore connection string
+            cosmosdb_client: Cosmos DB Mongo vCore client
+            embedding (Embedding): Embedding provider for semantic encoding and search.
+            database_name: Database name for the CosmosDBMongoVCoreSemanticCache
+            collection_name: Collection name for the CosmosDBMongoVCoreSemanticCache
+            num_lists: This integer is the number of clusters that the
+                inverted file (IVF) index uses to group the vector data.
+                We recommend that numLists is set to documentCount/1000
+                for up to 1 million documents and to sqrt(documentCount)
+                for more than 1 million documents.
+                Using a numLists value of 1 is akin to performing
+                brute-force search, which has limited performance
+            dimensions: Number of dimensions for vector similarity.
+                The maximum number of supported dimensions is 2000
+            similarity: Similarity metric to use with the IVF index.
+
+                Possible options are:
+                    - CosmosDBSimilarityType.COS (cosine distance),
+                    - CosmosDBSimilarityType.L2 (Euclidean distance), and
+                    - CosmosDBSimilarityType.IP (inner product).
+            kind: Type of vector index to create.
+                Possible options are:
+                    - vector-ivf
+                    - vector-hnsw: available as a preview feature only,
+                                   to enable visit https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/preview-features
+            m: The max number of connections per layer (16 by default, minimum
+               value is 2, maximum value is 100). Higher m is suitable for datasets
+               with high dimensionality and/or high accuracy requirements.
+            ef_construction: the size of the dynamic candidate list for constructing
+                            the graph (64 by default, minimum value is 4, maximum
+                            value is 1000). Higher ef_construction will result in
+                            better index quality and higher accuracy, but it will
+                            also increase the time required to build the index.
+                            ef_construction has to be at least 2 * m
+            ef_search: The size of the dynamic candidate list for search
+                       (40 by default). A higher value provides better
+                       recall at the cost of speed.
+            score_threshold: Maximum score used to filter the vector search documents.
+            application_name: Application name for the client for tracking and logging
+        """
+
+        self._validate_enum_value(similarity, CosmosDBSimilarityType)
+        self._validate_enum_value(kind, CosmosDBVectorSearchType)
+
+        if not cosmosdb_connection_string:
+            raise ValueError(" CosmosDB connection string can be empty.")
+
+        self.cosmosdb_connection_string = cosmosdb_connection_string
+        self.cosmosdb_client = cosmosdb_client
+        self.embedding = embedding
+        self.database_name = database_name or self.DEFAULT_DATABASE_NAME
+        self.collection_name = collection_name or self.DEFAULT_COLLECTION_NAME
+        self.num_lists = num_lists
+        self.dimensions = dimensions
+        self.similarity = similarity
+        self.kind = kind
+        self.m = m
+        self.ef_construction = ef_construction
+        self.ef_search = ef_search
+        self.score_threshold = score_threshold
+        self._cache_dict: Dict[str, AzureCosmosDBVectorSearch] = {}
+        self.application_name = application_name
+
+    def _index_name(self, llm_string: str) -> str:
+        hashed_index = _hash(llm_string)
+        return f"cache:{hashed_index}"
+
+    def _get_llm_cache(self, llm_string: str) -> AzureCosmosDBVectorSearch:
+        index_name = self._index_name(llm_string)
+
+        namespace = self.database_name + "." + self.collection_name
+
+        # return vectorstore client for the specific llm string
+        if index_name in self._cache_dict:
+            return self._cache_dict[index_name]
+
+        # create new vectorstore client for the specific llm string
+        if self.cosmosdb_client:
+            collection = self.cosmosdb_client[self.database_name][self.collection_name]
+            self._cache_dict[index_name] = AzureCosmosDBVectorSearch(
+                collection=collection,
+                embedding=self.embedding,
+                index_name=index_name,
+            )
+        else:
+            self._cache_dict[
+                index_name
+            ] = AzureCosmosDBVectorSearch.from_connection_string(
+                connection_string=self.cosmosdb_connection_string,
+                namespace=namespace,
+                embedding=self.embedding,
+                index_name=index_name,
+                application_name=self.application_name,
+            )
+
+        # create index for the vectorstore
+        vectorstore = self._cache_dict[index_name]
+        if not vectorstore.index_exists():
+            vectorstore.create_index(
+                self.num_lists,
+                self.dimensions,
+                self.similarity,
+                self.kind,
+                self.m,
+                self.ef_construction,
+            )
+
+        return vectorstore
+
+    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        """Look up based on prompt and llm_string."""
+        llm_cache = self._get_llm_cache(llm_string)
+        generations: List = []
+        # Read from a Hash
+        results = llm_cache.similarity_search(
+            query=prompt,
+            k=1,
+            kind=self.kind,
+            ef_search=self.ef_search,
+            score_threshold=self.score_threshold,  # type: ignore[arg-type]
+        )
+        if results:
+            for document in results:
+                try:
+                    generations.extend(loads(document.metadata["return_val"]))
+                except Exception:
+                    logger.warning(
+                        "Retrieving a cache value that could not be deserialized "
+                        "properly. This is likely due to the cache being in an "
+                        "older format. Please recreate your cache to avoid this "
+                        "error."
+                    )
+                    # In a previous life we stored the raw text directly
+                    # in the table, so assume it's in that format.
+                    generations.extend(
+                        _load_generations_from_json(document.metadata["return_val"])
+                    )
+        return generations if generations else None
+
+    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
+        """Update cache based on prompt and llm_string."""
+        for gen in return_val:
+            if not isinstance(gen, Generation):
+                raise ValueError(
+                    "CosmosDBMongoVCoreSemanticCache only supports caching of "
+                    f"normal LLM generations, got {type(gen)}"
+                )
+
+        llm_cache = self._get_llm_cache(llm_string)
+        metadata = {
+            "llm_string": llm_string,
+            "prompt": prompt,
+            "return_val": dumps([g for g in return_val]),
+        }
+        llm_cache.add_texts(texts=[prompt], metadatas=[metadata])
+
+    def clear(self, **kwargs: Any) -> None:
+        """Clear semantic cache for a given llm_string."""
+        index_name = self._index_name(kwargs["llm_string"])
+        if index_name in self._cache_dict:
+            self._cache_dict[index_name].get_collection().delete_many({})
+            # self._cache_dict[index_name].clear_collection()
+
+    @staticmethod
+    def _validate_enum_value(value: Any, enum_type: Type[Enum]) -> None:
+        if not isinstance(value, enum_type):
+            raise ValueError(f"Invalid enum value: {value}. Expected {enum_type}.")
+
+
+class OpenSearchSemanticCache(BaseCache):
+    """Cache that uses OpenSearch vector store backend"""
+
+    def __init__(
+        self, opensearch_url: str, embedding: Embeddings, score_threshold: float = 0.2
+    ):
+        """
+        Args:
+            opensearch_url (str): URL to connect to OpenSearch.
+            embedding (Embedding): Embedding provider for semantic encoding and search.
+            score_threshold (float, 0.2):
+        Example:
+        .. code-block:: python
+            import langchain
+            from langchain.cache import OpenSearchSemanticCache
+            from langchain.embeddings import OpenAIEmbeddings
+            langchain.llm_cache = OpenSearchSemanticCache(
+                opensearch_url="http//localhost:9200",
+                embedding=OpenAIEmbeddings()
+            )
+        """
+        self._cache_dict: Dict[str, OpenSearchVectorStore] = {}
+        self.opensearch_url = opensearch_url
+        self.embedding = embedding
+        self.score_threshold = score_threshold
+
+    def _index_name(self, llm_string: str) -> str:
+        hashed_index = _hash(llm_string)
+        return f"cache_{hashed_index}"
+
+    def _get_llm_cache(self, llm_string: str) -> OpenSearchVectorStore:
+        index_name = self._index_name(llm_string)
+
+        # return vectorstore client for the specific llm string
+        if index_name in self._cache_dict:
+            return self._cache_dict[index_name]
+
+        # create new vectorstore client for the specific llm string
+        self._cache_dict[index_name] = OpenSearchVectorStore(
+            opensearch_url=self.opensearch_url,
+            index_name=index_name,
+            embedding_function=self.embedding,
+        )
+
+        # create index for the vectorstore
+        vectorstore = self._cache_dict[index_name]
+        if not vectorstore.index_exists():
+            _embedding = self.embedding.embed_query(text="test")
+            vectorstore.create_index(len(_embedding), index_name)
+        return vectorstore
+
+    def lookup(self, prompt: str, llm_string: str) -> Optional[RETURN_VAL_TYPE]:
+        """Look up based on prompt and llm_string."""
+        llm_cache = self._get_llm_cache(llm_string)
+        generations: List = []
+        # Read from a Hash
+        results = llm_cache.similarity_search(
+            query=prompt,
+            k=1,
+            score_threshold=self.score_threshold,
+        )
+        if results:
+            for document in results:
+                try:
+                    generations.extend(loads(document.metadata["return_val"]))
+                except Exception:
+                    logger.warning(
+                        "Retrieving a cache value that could not be deserialized "
+                        "properly. This is likely due to the cache being in an "
+                        "older format. Please recreate your cache to avoid this "
+                        "error."
+                    )
+
+                    generations.extend(
+                        _load_generations_from_json(document.metadata["return_val"])
+                    )
+        return generations if generations else None
+
+    def update(self, prompt: str, llm_string: str, return_val: RETURN_VAL_TYPE) -> None:
+        """Update cache based on prompt and llm_string."""
+        for gen in return_val:
+            if not isinstance(gen, Generation):
+                raise ValueError(
+                    "OpenSearchSemanticCache only supports caching of "
+                    f"normal LLM generations, got {type(gen)}"
+                )
+        llm_cache = self._get_llm_cache(llm_string)
+        metadata = {
+            "llm_string": llm_string,
+            "prompt": prompt,
+            "return_val": dumps([g for g in return_val]),
+        }
+        llm_cache.add_texts(texts=[prompt], metadatas=[metadata])
+
+    def clear(self, **kwargs: Any) -> None:
+        """Clear semantic cache for a given llm_string."""
+        index_name = self._index_name(kwargs["llm_string"])
+        if index_name in self._cache_dict:
+            self._cache_dict[index_name].delete_index(index_name=index_name)
+            del self._cache_dict[index_name]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/aim_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/aim_callback.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,30 +1,23 @@
 from copy import deepcopy
 from typing import Any, Dict, List, Optional
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
+from langchain_core.utils import guard_import
 
 
 def import_aim() -> Any:
     """Import the aim python package and raise an error if it is not installed."""
-    try:
-        import aim
-    except ImportError:
-        raise ImportError(
-            "To use the Aim callback manager you need to have the"
-            " `aim` python package installed."
-            "Please install it with `pip install aim`"
-        )
-    return aim
+    return guard_import("aim")
 
 
 class BaseMetadataCallbackHandler:
-    """This class handles the metadata and associated function states for callbacks.
+    """Callback handler for the metadata and associated function states for callbacks.
 
     Attributes:
         step (int): The current step.
         starts (int): The number of times the start method has been called.
         ends (int): The number of times the end method has been called.
         errors (int): The number of times the error method has been called.
         text_ctr (int): The number of times the text method has been called.
@@ -310,16 +303,17 @@
         self.starts += 1
 
         resp = {"action": "on_tool_start"}
         resp.update(self.get_custom_callback_meta())
 
         self._run.track(aim.Text(input_str), name="on_tool_start", context=resp)
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         aim = import_aim()
         self.step += 1
         self.tool_ends += 1
         self.ends += 1
 
         resp = {"action": "on_tool_end"}
         resp.update(self.get_custom_callback_meta())
@@ -412,19 +406,29 @@
             except Exception:
                 pass
 
         if finish or reset:
             self._run.close()
             self.reset_callback_meta()
         if reset:
-            self.__init__(  # type: ignore
-                repo=repo if repo else self.repo,
-                experiment_name=experiment_name
-                if experiment_name
-                else self.experiment_name,
-                system_tracking_interval=system_tracking_interval
+            aim = import_aim()
+            self.repo = repo if repo else self.repo
+            self.experiment_name = (
+                experiment_name if experiment_name else self.experiment_name
+            )
+            self.system_tracking_interval = (
+                system_tracking_interval
                 if system_tracking_interval
-                else self.system_tracking_interval,
-                log_system_params=log_system_params
-                if log_system_params
-                else self.log_system_params,
+                else self.system_tracking_interval
+            )
+            self.log_system_params = (
+                log_system_params if log_system_params else self.log_system_params
+            )
+
+            self._run = aim.Run(
+                repo=self.repo,
+                experiment=self.experiment_name,
+                system_tracking_interval=self.system_tracking_interval,
+                log_system_params=self.log_system_params,
             )
+            self._run_hash = self._run.hash
+            self.action_records = []
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/argilla_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/argilla_callback.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 import os
 import warnings
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List, Optional, cast
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
 from packaging.version import parse
 
 
 class ArgillaCallbackHandler(BaseCallbackHandler):
     """Callback Handler that logs into Argilla.
 
     Args:
         dataset_name: name of the `FeedbackDataset` in Argilla. Note that it must
             exist in advance. If you need help on how to create a `FeedbackDataset` in
             Argilla, please visit
-            https://docs.argilla.io/en/latest/guides/llms/practical_guides/use_argilla_callback_in_langchain.html.
+            https://docs.argilla.io/en/latest/tutorials_and_integrations/integrations/use_argilla_callback_in_langchain.html.
         workspace_name: name of the workspace in Argilla where the specified
             `FeedbackDataset` lives in. Defaults to `None`, which means that the
             default workspace will be used.
         api_url: URL of the Argilla Server that we want to use, and where the
             `FeedbackDataset` lives in. Defaults to `None`, which means that either
             `ARGILLA_API_URL` environment variable or the default will be used.
         api_key: API Key to connect to the Argilla Server. Defaults to `None`, which
@@ -50,15 +50,15 @@
         ...     "What is the best NLP-annotation tool out there? (no bias at all)",
         ... ])
         "Argilla, no doubt about it."
     """
 
     REPO_URL: str = "https://github.com/argilla-io/argilla"
     ISSUES_URL: str = f"{REPO_URL}/issues"
-    BLOG_URL: str = "https://docs.argilla.io/en/latest/guides/llms/practical_guides/use_argilla_callback_in_langchain.html"  # noqa: E501
+    BLOG_URL: str = "https://docs.argilla.io/en/latest/tutorials_and_integrations/integrations/use_argilla_callback_in_langchain.html"
 
     DEFAULT_API_URL: str = "http://localhost:6900"
 
     def __init__(
         self,
         dataset_name: str,
         workspace_name: Optional[str] = None,
@@ -67,15 +67,15 @@
     ) -> None:
         """Initializes the `ArgillaCallbackHandler`.
 
         Args:
             dataset_name: name of the `FeedbackDataset` in Argilla. Note that it must
                 exist in advance. If you need help on how to create a `FeedbackDataset`
                 in Argilla, please visit
-                https://docs.argilla.io/en/latest/guides/llms/practical_guides/use_argilla_callback_in_langchain.html.
+                https://docs.argilla.io/en/latest/tutorials_and_integrations/integrations/use_argilla_callback_in_langchain.html.
             workspace_name: name of the workspace in Argilla where the specified
                 `FeedbackDataset` lives in. Defaults to `None`, which means that the
                 default workspace will be used.
             api_url: URL of the Argilla Server that we want to use, and where the
                 `FeedbackDataset` lives in. Defaults to `None`, which means that either
                 `ARGILLA_API_URL` environment variable or the default will be used.
             api_key: API Key to connect to the Argilla Server. Defaults to `None`, which
@@ -88,15 +88,15 @@
             FileNotFoundError: if the `FeedbackDataset` retrieval from Argilla fails.
         """
 
         super().__init__()
 
         # Import Argilla (not via `import_argilla` to keep hints in IDEs)
         try:
-            import argilla as rg  # noqa: F401
+            import argilla as rg
 
             self.ARGILLA_VERSION = rg.__version__
         except ImportError:
             raise ImportError(
                 "To use the Argilla callback manager you need to have the `argilla` "
                 "Python package installed. Please install it with `pip install argilla`"
             )
@@ -130,15 +130,15 @@
             warnings.warn(
                 (
                     "Since `api_key` is None, and the env var `ARGILLA_API_KEY` is not"
                     f" set, it will default to `{self.DEFAULT_API_KEY}`, which is the"
                     " default API key in Argilla Quickstart."
                 ),
             )
-            api_url = self.DEFAULT_API_URL
+            api_key = self.DEFAULT_API_KEY
 
         # Connect to Argilla with the provided credentials, if applicable
         try:
             rg.init(api_key=api_key, api_url=api_url)
         except Exception as e:
             raise ConnectionError(
                 f"Could not connect to Argilla with exception: '{e}'.\n"
@@ -265,41 +265,38 @@
         differs if the output is a list or not.
         """
         if not any(
             key in self.prompts
             for key in [str(kwargs["parent_run_id"]), str(kwargs["run_id"])]
         ):
             return
-        prompts = self.prompts.get(str(kwargs["parent_run_id"])) or self.prompts.get(
-            str(kwargs["run_id"])
+        prompts: List = self.prompts.get(str(kwargs["parent_run_id"])) or cast(
+            List, self.prompts.get(str(kwargs["run_id"]), [])
         )
         for chain_output_key, chain_output_val in outputs.items():
             if isinstance(chain_output_val, list):
                 # Creates the records and adds them to the `FeedbackDataset`
                 self.dataset.add_records(
                     records=[
                         {
                             "fields": {
                                 "prompt": prompt,
                                 "response": output["text"].strip(),
                             },
                         }
-                        for prompt, output in zip(
-                            prompts,  # type: ignore
-                            chain_output_val,
-                        )
+                        for prompt, output in zip(prompts, chain_output_val)
                     ]
                 )
             else:
                 # Creates the records and adds them to the `FeedbackDataset`
                 self.dataset.add_records(
                     records=[
                         {
                             "fields": {
-                                "prompt": " ".join(prompts),  # type: ignore
+                                "prompt": " ".join(prompts),
                                 "response": chain_output_val.strip(),
                             },
                         }
                     ]
                 )
 
         # Pop current run from `self.runs`
@@ -327,15 +324,15 @@
 
     def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
         """Do nothing when agent takes a specific action."""
         pass
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
         """Do nothing when tool ends."""
         pass
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/arize_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/arize_callback.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,15 +45,15 @@
             tokenizer_max_length=512,
             batch_size=256,
         )
         self.arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)
         if SPACE_KEY == "SPACE_KEY" or API_KEY == "API_KEY":
             raise ValueError(" CHANGE SPACE AND API KEYS")
         else:
-            print(" Arize client setup done! Now you can start using Arize!")
+            print(" Arize client setup done! Now you can start using Arize!")  # noqa: T201
 
     def on_llm_start(
         self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
     ) -> None:
         for prompt in prompts:
             self.prompt_records.append(prompt.replace("\n", ""))
 
@@ -157,17 +157,17 @@
                     schema=schema,
                     model_id=self.model_id,
                     model_version=self.model_version,
                     model_type=ModelTypes.GENERATIVE_LLM,
                     environment=Environments.PRODUCTION,
                 )
                 if response_from_arize.status_code == 200:
-                    print(" Successfully logged data to Arize!")
+                    print(" Successfully logged data to Arize!")  # noqa: T201
                 else:
-                    print(f' Logging failed "{response_from_arize.text}"')
+                    print(f' Logging failed "{response_from_arize.text}"')  # noqa: T201
 
     def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
         """Do nothing."""
         pass
 
     def on_chain_start(
         self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
@@ -192,15 +192,15 @@
 
     def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
         """Do nothing."""
         pass
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
         pass
 
     def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/arthur_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/arthur_callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """ArthurAI's Callback Handler."""
+
 from __future__ import annotations
 
 import os
 import uuid
 from collections import defaultdict
 from datetime import datetime
 from time import time
@@ -158,15 +159,15 @@
         run_id = kwargs["run_id"]
         self.run_map[run_id]["input_texts"] = prompts
         self.run_map[run_id]["start_time"] = time()
 
     def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
         """On LLM end, send data to Arthur."""
         try:
-            import pytz  # type: ignore[import]
+            import pytz
         except ImportError as e:
             raise ImportError(
                 "Could not import pytz. Please install it with 'pip install pytz'."
             ) from e
 
         run_id = kwargs["run_id"]
 
@@ -275,15 +276,15 @@
         """Do nothing when tool starts."""
 
     def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
         """Do nothing when agent takes a specific action."""
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
         """Do nothing when tool ends."""
 
     def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/clearml_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/clearml_callback.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 from copy import deepcopy
 from pathlib import Path
 from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Sequence
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
+from langchain_core.utils import guard_import
 
 from langchain_community.callbacks.utils import (
     BaseMetadataCallbackHandler,
     flatten_dict,
     hash_string,
     import_pandas,
     import_spacy,
@@ -21,22 +22,15 @@
 
 if TYPE_CHECKING:
     import pandas as pd
 
 
 def import_clearml() -> Any:
     """Import the clearml python package and raise an error if it is not installed."""
-    try:
-        import clearml  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the clearml callback manager you need to have the `clearml` python "
-            "package installed. Please install it with `pip install clearml`"
-        )
-    return clearml
+    return guard_import("clearml")
 
 
 class ClearMLCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):
     """Callback Handler that logs to ClearML.
 
     Parameters:
         job_type (str): The type of clearml task such as "inference", "testing" or "qc"
@@ -79,15 +73,15 @@
 
         self.temp_dir = tempfile.TemporaryDirectory()
 
         # Check if ClearML task already exists (e.g. in pipeline)
         if clearml.Task.current_task():
             self.task = clearml.Task.current_task()
         else:
-            self.task = clearml.Task.init(  # type: ignore
+            self.task = clearml.Task.init(
                 task_type=self.task_type,
                 project_name=self.project_name,
                 tags=self.tags,
                 task_name=self.task_name,
                 output_uri=True,
             )
         self.logger = self.task.get_logger()
@@ -239,16 +233,17 @@
         resp.update(self.get_custom_callback_meta())
 
         self.on_tool_start_records.append(resp)
         self.action_records.append(resp)
         if self.stream_logs:
             self.logger.report_text(resp)
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         self.step += 1
         self.tool_ends += 1
         self.ends += 1
 
         resp = self._init_resp()
         resp.update({"action": "on_tool_end", "output": output})
         resp.update(self.get_custom_callback_meta())
@@ -357,25 +352,21 @@
                 "osman": textstat.osman(text),
             }
             resp.update(text_complexity_metrics)
 
         if self.visualize and self.nlp and self.temp_dir.name is not None:
             doc = self.nlp(text)
 
-            dep_out = spacy.displacy.render(  # type: ignore
-                doc, style="dep", jupyter=False, page=True
-            )
+            dep_out = spacy.displacy.render(doc, style="dep", jupyter=False, page=True)
             dep_output_path = Path(
                 self.temp_dir.name, hash_string(f"dep-{text}") + ".html"
             )
             dep_output_path.open("w", encoding="utf-8").write(dep_out)
 
-            ent_out = spacy.displacy.render(  # type: ignore
-                doc, style="ent", jupyter=False, page=True
-            )
+            ent_out = spacy.displacy.render(doc, style="ent", jupyter=False, page=True)
             ent_output_path = Path(
                 self.temp_dir.name, hash_string(f"ent-{text}") + ".html"
             )
             ent_output_path.open("w", encoding="utf-8").write(ent_out)
 
             self.logger.report_media(
                 "Dependencies Plot", text, local_path=dep_output_path
@@ -509,16 +500,16 @@
                 )
                 output_model.update_weights(
                     weights_filename=str(langchain_asset_path),
                     auto_delete_file=False,
                     target_filename=name,
                 )
             except NotImplementedError as e:
-                print("Could not save model.")
-                print(repr(e))
+                print("Could not save model.")  # noqa: T201
+                print(repr(e))  # noqa: T201
                 pass
 
         # Cleanup after adding everything to ClearML
         self.task.flush(wait_for_uploads=True)
         self.temp_dir.cleanup()
         self.temp_dir = tempfile.TemporaryDirectory()
         self.reset_callback_meta()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/comet_ml_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/comet_ml_callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Optional, Sequence
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import Generation, LLMResult
+from langchain_core.utils import guard_import
 
 import langchain_community
 from langchain_community.callbacks.utils import (
     BaseMetadataCallbackHandler,
     flatten_dict,
     import_pandas,
     import_spacy,
@@ -17,31 +18,23 @@
 )
 
 LANGCHAIN_MODEL_NAME = "langchain-model"
 
 
 def import_comet_ml() -> Any:
     """Import comet_ml and raise an error if it is not installed."""
-    try:
-        import comet_ml  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the comet_ml callback manager you need to have the "
-            "`comet_ml` python package installed. Please install it with"
-            " `pip install comet_ml`"
-        )
-    return comet_ml
+    return guard_import("comet_ml")
 
 
 def _get_experiment(
     workspace: Optional[str] = None, project_name: Optional[str] = None
 ) -> Any:
     comet_ml = import_comet_ml()
 
-    experiment = comet_ml.Experiment(  # type: ignore
+    experiment = comet_ml.Experiment(
         workspace=workspace,
         project_name=project_name,
     )
 
     return experiment
 
 
@@ -299,16 +292,17 @@
         resp.update(self.get_custom_callback_meta())
         if self.stream_logs:
             self._log_stream(input_str, resp, self.step)
 
         resp.update({"input_str": input_str})
         self.action_records.append(resp)
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         self.step += 1
         self.tool_ends += 1
         self.ends += 1
 
         resp = self._init_resp()
         resp.update({"action": "on_tool_end"})
         resp.update(self.get_custom_callback_meta())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/confident_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/confident_callback.py`

 * *Files 3% similar despite different names*

```diff
@@ -112,21 +112,21 @@
                 output = generation[0].text
                 query = self.prompts[i]
                 if isinstance(metric, AnswerRelevancy):
                     result = metric.measure(
                         output=output,
                         query=query,
                     )
-                    print(f"Answer Relevancy: {result}")
+                    print(f"Answer Relevancy: {result}")  # noqa: T201
                 elif isinstance(metric, UnBiasedMetric):
                     score = metric.measure(output)
-                    print(f"Bias Score: {score}")
+                    print(f"Bias Score: {score}")  # noqa: T201
                 elif isinstance(metric, NonToxicMetric):
                     score = metric.measure(output)
-                    print(f"Toxic Score: {score}")
+                    print(f"Toxic Score: {score}")  # noqa: T201
                 else:
                     raise ValueError(
                         f"""Metric {metric.__name__} is not supported by deepeval 
                         callbacks."""
                     )
 
     def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
@@ -158,15 +158,15 @@
 
     def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
         """Do nothing when agent takes a specific action."""
         pass
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
         """Do nothing when tool ends."""
         pass
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/context_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/context_callback.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,35 +1,33 @@
 """Callback handler for Context AI"""
+
 import os
 from typing import Any, Dict, List
 from uuid import UUID
 
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.messages import BaseMessage
 from langchain_core.outputs import LLMResult
+from langchain_core.utils import guard_import
 
 
 def import_context() -> Any:
     """Import the `getcontext` package."""
-    try:
-        import getcontext  # noqa: F401
-        from getcontext.generated.models import (
-            Conversation,
-            Message,
-            MessageRole,
-            Rating,
-        )
-        from getcontext.token import Credential  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the context callback manager you need to have the "
-            "`getcontext` python package installed (version >=0.3.0). "
-            "Please install it with `pip install --upgrade python-context`"
-        )
-    return getcontext, Credential, Conversation, Message, MessageRole, Rating
+    return (
+        guard_import("getcontext", pip_name="python-context"),
+        guard_import("getcontext.token", pip_name="python-context").Credential,
+        guard_import(
+            "getcontext.generated.models", pip_name="python-context"
+        ).Conversation,
+        guard_import("getcontext.generated.models", pip_name="python-context").Message,
+        guard_import(
+            "getcontext.generated.models", pip_name="python-context"
+        ).MessageRole,
+        guard_import("getcontext.generated.models", pip_name="python-context").Rating,
+    )
 
 
 class ContextCallbackHandler(BaseCallbackHandler):
     """Callback Handler that records transcripts to the Context service.
 
      (https://context.ai).
 
@@ -54,15 +52,15 @@
         ...     callbacks=[context_callback],
         ...     openai_api_key="API_KEY_HERE",
         ... )
         >>> messages = [
         ...     SystemMessage(content="You translate English to French."),
         ...     HumanMessage(content="I love programming with LangChain."),
         ... ]
-        >>> chat(messages)
+        >>> chat.invoke(messages)
 
     Chain Example:
         >>> from langchain.chains import LLMChain
         >>> from langchain_community.chat_models import ChatOpenAI
         >>> from langchain_community.callbacks import ContextCallbackHandler
         >>> context_callback = ContextCallbackHandler(
         ...     token="<CONTEXT_TOKEN_HERE>",
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/flyte_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/flyte_callback.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 """FlyteKit callback handler."""
+
 from __future__ import annotations
 
 import logging
 from copy import deepcopy
 from typing import TYPE_CHECKING, Any, Dict, List, Tuple
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
+from langchain_core.utils import guard_import
 
 from langchain_community.callbacks.utils import (
     BaseMetadataCallbackHandler,
     flatten_dict,
     import_pandas,
     import_spacy,
     import_textstat,
@@ -22,25 +24,20 @@
     from flytekitplugins.deck import renderer
 
 logger = logging.getLogger(__name__)
 
 
 def import_flytekit() -> Tuple[flytekit, renderer]:
     """Import flytekit and flytekitplugins-deck-standard."""
-    try:
-        import flytekit  # noqa: F401
-        from flytekitplugins.deck import renderer  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the flyte callback manager you need"
-            "to have the `flytekit` and `flytekitplugins-deck-standard`"
-            "packages installed. Please install them with `pip install flytekit`"
-            "and `pip install flytekitplugins-deck-standard`."
-        )
-    return flytekit, renderer
+    return (
+        guard_import("flytekit"),
+        guard_import(
+            "flytekitplugins.deck", pip_name="flytekitplugins-deck-standard"
+        ).renderer,
+    )
 
 
 def analyze_text(
     text: str,
     nlp: Any = None,
     textstat: Any = None,
 ) -> dict:
@@ -75,31 +72,27 @@
         }
         resp.update({"text_complexity_metrics": text_complexity_metrics})
         resp.update(text_complexity_metrics)
 
     if nlp is not None:
         spacy = import_spacy()
         doc = nlp(text)
-        dep_out = spacy.displacy.render(  # type: ignore
-            doc, style="dep", jupyter=False, page=True
-        )
-        ent_out = spacy.displacy.render(  # type: ignore
-            doc, style="ent", jupyter=False, page=True
-        )
+        dep_out = spacy.displacy.render(doc, style="dep", jupyter=False, page=True)
+        ent_out = spacy.displacy.render(doc, style="ent", jupyter=False, page=True)
         text_visualizations = {
             "dependency_tree": dep_out,
             "entities": ent_out,
         }
         resp.update(text_visualizations)
 
     return resp
 
 
 class FlyteCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):
-    """This callback handler that is used within a Flyte task."""
+    """Callback handler that is used within a Flyte task."""
 
     def __init__(self) -> None:
         """Initialize callback handler."""
         flytekit, renderer = import_flytekit()
         self.pandas = import_pandas()
 
         self.textstat = None
@@ -195,15 +188,15 @@
                         analyze_text(
                             generation.text, nlp=self.nlp, textstat=self.textstat
                         )
                     )
 
                     complexity_metrics: Dict[str, float] = generation_resp.pop(
                         "text_complexity_metrics"
-                    )  # type: ignore  # noqa: E501
+                    )
                     self.deck.append(
                         self.markdown_renderer().to_html("#### Text Complexity Metrics")
                     )
                     self.deck.append(
                         self.table_renderer().to_html(
                             self.pandas.DataFrame([complexity_metrics])
                         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/human.py` & `gigachain_community-0.2.0/langchain_community/callbacks/human.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/infino_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/infino_callback.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,40 +1,25 @@
 import time
 from typing import Any, Dict, List, Optional, cast
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.messages import BaseMessage
-from langchain_core.outputs import LLMResult
+from langchain_core.outputs import ChatGeneration, LLMResult
+from langchain_core.utils import guard_import
 
 
 def import_infino() -> Any:
     """Import the infino client."""
-    try:
-        from infinopy import InfinoClient
-    except ImportError:
-        raise ImportError(
-            "To use the Infino callbacks manager you need to have the"
-            " `infinopy` python package installed."
-            "Please install it with `pip install infinopy`"
-        )
-    return InfinoClient()
+    return guard_import("infinopy").InfinoClient()
 
 
 def import_tiktoken() -> Any:
     """Import tiktoken for counting tokens for OpenAI models."""
-    try:
-        import tiktoken
-    except ImportError:
-        raise ImportError(
-            "To use the ChatOpenAI model with Infino callback manager, you need to "
-            "have the `tiktoken` python package installed."
-            "Please install it with `pip install tiktoken`"
-        )
-    return tiktoken
+    return guard_import("tiktoken")
 
 
 def get_num_tokens(string: str, openai_model_name: str) -> int:
     """Calculate num tokens for OpenAI with tiktoken package.
 
     Official documentation: https://github.com/openai/openai-cookbook/blob/main
                             /examples/How_to_count_tokens_with_tiktoken.ipynb
@@ -82,15 +67,15 @@
             key: value,
             "labels": {
                 "model_id": self.model_id,
                 "model_version": self.model_version,
             },
         }
         if self.verbose:
-            print(f"Tracking {key} with Infino: {payload}")
+            print(f"Tracking {key} with Infino: {payload}")  # noqa: T201
 
         # Append to Infino time series only if is_ts is True, otherwise
         # append to Infino log.
         if is_ts:
             self.client.append_ts(payload)
         else:
             self.client.append_log(payload)
@@ -142,15 +127,15 @@
                 self._send_to_infino("prompt_tokens", prompt_tokens)
                 self._send_to_infino("total_tokens", total_tokens)
                 self._send_to_infino("completion_tokens", completion_tokens)
 
         # Track completion token usage (for openai chat models).
         if self.is_chat_openai_model:
             messages = " ".join(
-                generation.message.content  # type: ignore[attr-defined]
+                cast(str, cast(ChatGeneration, generation).message.content)
                 for generation in generations
             )
             completion_tokens = get_num_tokens(
                 messages, openai_model_name=self.chat_openai_model_name
             )
             self._send_to_infino("completion_tokens", completion_tokens)
 
@@ -241,15 +226,15 @@
                             openai_model_name=self.chat_openai_model_name,
                         )
                         prompt_tokens += num_tokens
 
                     self._send_to_infino("prompt_tokens", prompt_tokens)
 
         if self.verbose:
-            print(
+            print(  # noqa: T201
                 f"on_chat_model_start: is_chat_openai_model= \
                   {self.is_chat_openai_model}, \
                   chat_openai_model_name={self.chat_openai_model_name}"
             )
 
         # Send the prompt to infino
         prompt = " ".join(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/labelstudio_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/labelstudio_callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -94,15 +94,15 @@
         >>> handler = LabelStudioCallbackHandler(
         ...             api_key='<your_key_here>',
         ...             url='http://localhost:8080',
         ...             project_name='LangChain-%Y-%m-%d',
         ...             mode='prompt'
         ... )
         >>> llm = OpenAI(callbacks=[handler])
-        >>> llm.predict('Tell me a story about a dog.')
+        >>> llm.invoke('Tell me a story about a dog.')
     """
 
     DEFAULT_PROJECT_NAME: str = "LangChain-%Y-%m-%d"
 
     def __init__(
         self,
         api_key: Optional[str] = None,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/llmonitor_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/llmonitor_callback.py`

 * *Files 1% similar despite different names*

```diff
@@ -200,15 +200,15 @@
     ```python
     from langchain_community.llms import OpenAI
     from langchain_community.callbacks import LLMonitorCallbackHandler
 
     llmonitor_callback = LLMonitorCallbackHandler()
     llm = OpenAI(callbacks=[llmonitor_callback],
                  metadata={"userId": "user-123"})
-    llm.predict("Hello, how are you?")
+    llm.invoke("Hello, how are you?")
     ```
     """
 
     __api_url: str
     __app_id: str
     __verbose: bool
     __llmonitor_version: str
@@ -461,21 +461,22 @@
                 app_id=self.__app_id,
             )
         except Exception as e:
             logger.error(f"[LLMonitor] An error occurred in on_tool_start: {e}")
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         *,
         run_id: UUID,
         parent_run_id: Union[UUID, None] = None,
         tags: Union[List[str], None] = None,
         **kwargs: Any,
     ) -> None:
+        output = str(output)
         if self.__has_valid_config is False:
             return
         try:
             self.__track_event(
                 "tool",
                 "end",
                 run_id=str(run_id),
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/mlflow_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/mlflow_callback.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,91 +1,114 @@
+import logging
 import os
 import random
 import string
 import tempfile
 import traceback
 from copy import deepcopy
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Dict, List, Optional, Sequence, Union
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
+from langchain_core.documents import Document
 from langchain_core.outputs import LLMResult
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.utils import get_from_dict_or_env, guard_import
 
 from langchain_community.callbacks.utils import (
     BaseMetadataCallbackHandler,
     flatten_dict,
     hash_string,
     import_pandas,
     import_spacy,
     import_textstat,
 )
 
+logger = logging.getLogger(__name__)
+
 
 def import_mlflow() -> Any:
     """Import the mlflow python package and raise an error if it is not installed."""
-    try:
-        import mlflow
-    except ImportError:
-        raise ImportError(
-            "To use the mlflow callback manager you need to have the `mlflow` python "
-            "package installed. Please install it with `pip install mlflow>=2.3.0`"
-        )
-    return mlflow
+    return guard_import("mlflow")
+
+
+def mlflow_callback_metrics() -> List[str]:
+    """Get the metrics to log to MLFlow."""
+    return [
+        "step",
+        "starts",
+        "ends",
+        "errors",
+        "text_ctr",
+        "chain_starts",
+        "chain_ends",
+        "llm_starts",
+        "llm_ends",
+        "llm_streams",
+        "tool_starts",
+        "tool_ends",
+        "agent_ends",
+        "retriever_starts",
+        "retriever_ends",
+    ]
+
+
+def get_text_complexity_metrics() -> List[str]:
+    """Get the text complexity metrics from textstat."""
+    return [
+        "flesch_reading_ease",
+        "flesch_kincaid_grade",
+        "smog_index",
+        "coleman_liau_index",
+        "automated_readability_index",
+        "dale_chall_readability_score",
+        "difficult_words",
+        "linsear_write_formula",
+        "gunning_fog",
+        # "text_standard"
+        "fernandez_huerta",
+        "szigriszt_pazos",
+        "gutierrez_polini",
+        "crawford",
+        "gulpease_index",
+        "osman",
+    ]
 
 
 def analyze_text(
     text: str,
     nlp: Any = None,
+    textstat: Any = None,
 ) -> dict:
     """Analyze text using textstat and spacy.
 
     Parameters:
         text (str): The text to analyze.
         nlp (spacy.lang): The spacy language model to use for visualization.
+        textstat: The textstat library to use for complexity metrics calculation.
 
     Returns:
         (dict): A dictionary containing the complexity metrics and visualization
             files serialized to  HTML string.
     """
     resp: Dict[str, Any] = {}
-    textstat = import_textstat()
-    spacy = import_spacy()
-    text_complexity_metrics = {
-        "flesch_reading_ease": textstat.flesch_reading_ease(text),
-        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
-        "smog_index": textstat.smog_index(text),
-        "coleman_liau_index": textstat.coleman_liau_index(text),
-        "automated_readability_index": textstat.automated_readability_index(text),
-        "dale_chall_readability_score": textstat.dale_chall_readability_score(text),
-        "difficult_words": textstat.difficult_words(text),
-        "linsear_write_formula": textstat.linsear_write_formula(text),
-        "gunning_fog": textstat.gunning_fog(text),
-        # "text_standard": textstat.text_standard(text),
-        "fernandez_huerta": textstat.fernandez_huerta(text),
-        "szigriszt_pazos": textstat.szigriszt_pazos(text),
-        "gutierrez_polini": textstat.gutierrez_polini(text),
-        "crawford": textstat.crawford(text),
-        "gulpease_index": textstat.gulpease_index(text),
-        "osman": textstat.osman(text),
-    }
-    resp.update({"text_complexity_metrics": text_complexity_metrics})
-    resp.update(text_complexity_metrics)
+    if textstat is not None:
+        text_complexity_metrics = {
+            key: getattr(textstat, key)(text) for key in get_text_complexity_metrics()
+        }
+        resp.update({"text_complexity_metrics": text_complexity_metrics})
+        resp.update(text_complexity_metrics)
 
     if nlp is not None:
+        spacy = import_spacy()
         doc = nlp(text)
 
-        dep_out = spacy.displacy.render(  # type: ignore
-            doc, style="dep", jupyter=False, page=True
-        )
+        dep_out = spacy.displacy.render(doc, style="dep", jupyter=False, page=True)
 
-        ent_out = spacy.displacy.render(  # type: ignore
-            doc, style="ent", jupyter=False, page=True
-        )
+        ent_out = spacy.displacy.render(doc, style="ent", jupyter=False, page=True)
 
         text_visualizations = {
             "dependency_tree": dep_out,
             "entities": ent_out,
         }
 
         resp.update(text_visualizations)
@@ -136,97 +159,95 @@
             self.mlf_exp = self.mlflow.get_experiment(self.mlf_expid)
         else:
             tracking_uri = get_from_dict_or_env(
                 kwargs, "tracking_uri", "MLFLOW_TRACKING_URI", ""
             )
             self.mlflow.set_tracking_uri(tracking_uri)
 
-            # User can set other env variables described here
-            # > https://www.mlflow.org/docs/latest/tracking.html#logging-to-a-tracking-server
-
-            experiment_name = get_from_dict_or_env(
-                kwargs, "experiment_name", "MLFLOW_EXPERIMENT_NAME"
-            )
-            self.mlf_exp = self.mlflow.get_experiment_by_name(experiment_name)
-            if self.mlf_exp is not None:
-                self.mlf_expid = self.mlf_exp.experiment_id
+            if run_id := kwargs.get("run_id"):
+                self.mlf_expid = self.mlflow.get_run(run_id).info.experiment_id
             else:
-                self.mlf_expid = self.mlflow.create_experiment(experiment_name)
+                # User can set other env variables described here
+                # > https://www.mlflow.org/docs/latest/tracking.html#logging-to-a-tracking-server
 
-        self.start_run(kwargs["run_name"], kwargs["run_tags"])
+                experiment_name = get_from_dict_or_env(
+                    kwargs, "experiment_name", "MLFLOW_EXPERIMENT_NAME"
+                )
+                self.mlf_exp = self.mlflow.get_experiment_by_name(experiment_name)
+                if self.mlf_exp is not None:
+                    self.mlf_expid = self.mlf_exp.experiment_id
+                else:
+                    self.mlf_expid = self.mlflow.create_experiment(experiment_name)
 
-    def start_run(self, name: str, tags: Dict[str, str]) -> None:
-        """To start a new run, auto generates the random suffix for name"""
-        if name.endswith("-%"):
-            rname = "".join(random.choices(string.ascii_uppercase + string.digits, k=7))
-            name = name.replace("%", rname)
-        self.run = self.mlflow.MlflowClient().create_run(
-            self.mlf_expid, run_name=name, tags=tags
+        self.start_run(
+            kwargs["run_name"], kwargs["run_tags"], kwargs.get("run_id", None)
         )
+        self.dir = kwargs.get("artifacts_dir", "")
+
+    def start_run(
+        self, name: str, tags: Dict[str, str], run_id: Optional[str] = None
+    ) -> None:
+        """
+        If run_id is provided, it will reuse the run with the given run_id.
+        Otherwise, it starts a new run, auto generates the random suffix for name.
+        """
+        if run_id is None:
+            if name.endswith("-%"):
+                rname = "".join(
+                    random.choices(string.ascii_uppercase + string.digits, k=7)
+                )
+                name = name[:-1] + rname
+            run = self.mlflow.MlflowClient().create_run(
+                self.mlf_expid, run_name=name, tags=tags
+            )
+            run_id = run.info.run_id
+        self.run_id = run_id
 
     def finish_run(self) -> None:
         """To finish the run."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.end_run()
+        self.mlflow.end_run()
 
     def metric(self, key: str, value: float) -> None:
         """To log metric to mlflow server."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_metric(key, value)
+        self.mlflow.log_metric(key, value, run_id=self.run_id)
 
     def metrics(
         self, data: Union[Dict[str, float], Dict[str, int]], step: Optional[int] = 0
     ) -> None:
         """To log all metrics in the input dict."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_metrics(data)
+        self.mlflow.log_metrics(data, run_id=self.run_id)
 
     def jsonf(self, data: Dict[str, Any], filename: str) -> None:
         """To log the input data as json file artifact."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_dict(data, f"{filename}.json")
+        self.mlflow.log_dict(
+            data, os.path.join(self.dir, f"{filename}.json"), run_id=self.run_id
+        )
 
-    def table(self, name: str, dataframe) -> None:  # type: ignore
+    def table(self, name: str, dataframe: Any) -> None:
         """To log the input pandas dataframe as a html table"""
         self.html(dataframe.to_html(), f"table_{name}")
 
     def html(self, html: str, filename: str) -> None:
         """To log the input html string as html file artifact."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_text(html, f"{filename}.html")
+        self.mlflow.log_text(
+            html, os.path.join(self.dir, f"{filename}.html"), run_id=self.run_id
+        )
 
     def text(self, text: str, filename: str) -> None:
         """To log the input text as text file artifact."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_text(text, f"{filename}.txt")
+        self.mlflow.log_text(
+            text, os.path.join(self.dir, f"{filename}.txt"), run_id=self.run_id
+        )
 
     def artifact(self, path: str) -> None:
         """To upload the file from given path as artifact."""
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.log_artifact(path)
+        self.mlflow.log_artifact(path, run_id=self.run_id)
 
     def langchain_artifact(self, chain: Any) -> None:
-        with self.mlflow.start_run(
-            run_id=self.run.info.run_id, experiment_id=self.mlf_expid
-        ):
-            self.mlflow.langchain.log_model(chain, "langchain-model")
+        self.mlflow.langchain.log_model(chain, "langchain-model", run_id=self.run_id)
 
 
 class MlflowCallbackHandler(BaseMetadataCallbackHandler, BaseCallbackHandler):
     """Callback Handler that logs metrics and artifacts to mlflow server.
 
     Parameters:
         name (str): Name of the run.
@@ -242,66 +263,76 @@
 
     def __init__(
         self,
         name: Optional[str] = "langchainrun-%",
         experiment: Optional[str] = "langchain",
         tags: Optional[Dict] = None,
         tracking_uri: Optional[str] = None,
+        run_id: Optional[str] = None,
+        artifacts_dir: str = "",
     ) -> None:
         """Initialize callback handler."""
         import_pandas()
-        import_textstat()
         import_mlflow()
-        spacy = import_spacy()
         super().__init__()
 
         self.name = name
         self.experiment = experiment
         self.tags = tags or {}
         self.tracking_uri = tracking_uri
+        self.run_id = run_id
+        self.artifacts_dir = artifacts_dir
 
         self.temp_dir = tempfile.TemporaryDirectory()
 
         self.mlflg = MlflowLogger(
             tracking_uri=self.tracking_uri,
             experiment_name=self.experiment,
             run_name=self.name,
             run_tags=self.tags,
+            run_id=self.run_id,
+            artifacts_dir=self.artifacts_dir,
         )
 
         self.action_records: list = []
-        self.nlp = spacy.load("en_core_web_sm")
+        self.nlp = None
+        try:
+            spacy = import_spacy()
+        except ImportError as e:
+            logger.warning(e.msg)
+        else:
+            try:
+                self.nlp = spacy.load("en_core_web_sm")
+            except OSError:
+                logger.warning(
+                    "Run `python -m spacy download en_core_web_sm` "
+                    "to download en_core_web_sm model for text visualization."
+                )
 
-        self.metrics = {
-            "step": 0,
-            "starts": 0,
-            "ends": 0,
-            "errors": 0,
-            "text_ctr": 0,
-            "chain_starts": 0,
-            "chain_ends": 0,
-            "llm_starts": 0,
-            "llm_ends": 0,
-            "llm_streams": 0,
-            "tool_starts": 0,
-            "tool_ends": 0,
-            "agent_ends": 0,
-        }
+        try:
+            self.textstat = import_textstat()
+        except ImportError as e:
+            logger.warning(e.msg)
+            self.textstat = None
+
+        self.metrics = {key: 0 for key in mlflow_callback_metrics()}
 
         self.records: Dict[str, Any] = {
             "on_llm_start_records": [],
             "on_llm_token_records": [],
             "on_llm_end_records": [],
             "on_chain_start_records": [],
             "on_chain_end_records": [],
             "on_tool_start_records": [],
             "on_tool_end_records": [],
             "on_text_records": [],
             "on_agent_finish_records": [],
             "on_agent_action_records": [],
+            "on_retriever_start_records": [],
+            "on_retriever_end_records": [],
             "action_records": [],
         }
 
     def _reset(self) -> None:
         for k, v in self.metrics.items():
             self.metrics[k] = 0
         for k, v in self.records.items():
@@ -367,30 +398,36 @@
             for idx, generation in enumerate(generations):
                 generation_resp = deepcopy(resp)
                 generation_resp.update(flatten_dict(generation.dict()))
                 generation_resp.update(
                     analyze_text(
                         generation.text,
                         nlp=self.nlp,
+                        textstat=self.textstat,
                     )
                 )
-                complexity_metrics: Dict[str, float] = generation_resp.pop(
-                    "text_complexity_metrics"
-                )  # type: ignore  # noqa: E501
-                self.mlflg.metrics(
-                    complexity_metrics,
-                    step=self.metrics["step"],
-                )
+                if "text_complexity_metrics" in generation_resp:
+                    complexity_metrics: Dict[str, float] = generation_resp.pop(
+                        "text_complexity_metrics"
+                    )
+                    self.mlflg.metrics(
+                        complexity_metrics,
+                        step=self.metrics["step"],
+                    )
                 self.records["on_llm_end_records"].append(generation_resp)
                 self.records["action_records"].append(generation_resp)
                 self.mlflg.jsonf(resp, f"llm_end_{llm_ends}_generation_{idx}")
-                dependency_tree = generation_resp["dependency_tree"]
-                entities = generation_resp["entities"]
-                self.mlflg.html(dependency_tree, "dep-" + hash_string(generation.text))
-                self.mlflg.html(entities, "ent-" + hash_string(generation.text))
+                if "dependency_tree" in generation_resp:
+                    dependency_tree = generation_resp["dependency_tree"]
+                    self.mlflg.html(
+                        dependency_tree, "dep-" + hash_string(generation.text)
+                    )
+                if "entities" in generation_resp:
+                    entities = generation_resp["entities"]
+                    self.mlflg.html(entities, "ent-" + hash_string(generation.text))
 
     def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
         """Run when LLM errors."""
         self.metrics["step"] += 1
         self.metrics["errors"] += 1
 
     def on_chain_start(
@@ -406,31 +443,43 @@
         resp: Dict[str, Any] = {}
         resp.update({"action": "on_chain_start"})
         resp.update(flatten_dict(serialized))
         resp.update(self.metrics)
 
         self.mlflg.metrics(self.metrics, step=self.metrics["step"])
 
-        chain_input = ",".join([f"{k}={v}" for k, v in inputs.items()])
+        if isinstance(inputs, dict):
+            chain_input = ",".join([f"{k}={v}" for k, v in inputs.items()])
+        elif isinstance(inputs, list):
+            chain_input = ",".join([str(input) for input in inputs])
+        else:
+            chain_input = str(inputs)
         input_resp = deepcopy(resp)
         input_resp["inputs"] = chain_input
         self.records["on_chain_start_records"].append(input_resp)
         self.records["action_records"].append(input_resp)
         self.mlflg.jsonf(input_resp, f"chain_start_{chain_starts}")
 
-    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
+    def on_chain_end(
+        self, outputs: Union[Dict[str, Any], str, List[str]], **kwargs: Any
+    ) -> None:
         """Run when chain ends running."""
         self.metrics["step"] += 1
         self.metrics["chain_ends"] += 1
         self.metrics["ends"] += 1
 
         chain_ends = self.metrics["chain_ends"]
 
         resp: Dict[str, Any] = {}
-        chain_output = ",".join([f"{k}={v}" for k, v in outputs.items()])
+        if isinstance(outputs, dict):
+            chain_output = ",".join([f"{k}={v}" for k, v in outputs.items()])
+        elif isinstance(outputs, list):
+            chain_output = ",".join(map(str, outputs))
+        else:
+            chain_output = str(outputs)
         resp.update({"action": "on_chain_end", "outputs": chain_output})
         resp.update(self.metrics)
 
         self.mlflg.metrics(self.metrics, step=self.metrics["step"])
 
         self.records["on_chain_end_records"].append(resp)
         self.records["action_records"].append(resp)
@@ -458,16 +507,17 @@
 
         self.mlflg.metrics(self.metrics, step=self.metrics["step"])
 
         self.records["on_tool_start_records"].append(resp)
         self.records["action_records"].append(resp)
         self.mlflg.jsonf(resp, f"tool_start_{tool_starts}")
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         self.metrics["step"] += 1
         self.metrics["tool_ends"] += 1
         self.metrics["ends"] += 1
 
         tool_ends = self.metrics["tool_ends"]
 
         resp: Dict[str, Any] = {}
@@ -483,15 +533,15 @@
     def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
         """Run when tool errors."""
         self.metrics["step"] += 1
         self.metrics["errors"] += 1
 
     def on_text(self, text: str, **kwargs: Any) -> None:
         """
-        Run when agent is ending.
+        Run when text is received.
         """
         self.metrics["step"] += 1
         self.metrics["text_ctr"] += 1
 
         text_ctr = self.metrics["text_ctr"]
 
         resp: Dict[str, Any] = {}
@@ -545,14 +595,79 @@
         )
         resp.update(self.metrics)
         self.mlflg.metrics(self.metrics, step=self.metrics["step"])
         self.records["on_agent_action_records"].append(resp)
         self.records["action_records"].append(resp)
         self.mlflg.jsonf(resp, f"agent_action_{tool_starts}")
 
+    def on_retriever_start(
+        self,
+        serialized: Dict[str, Any],
+        query: str,
+        **kwargs: Any,
+    ) -> Any:
+        """Run when Retriever starts running."""
+        self.metrics["step"] += 1
+        self.metrics["retriever_starts"] += 1
+        self.metrics["starts"] += 1
+
+        retriever_starts = self.metrics["retriever_starts"]
+
+        resp: Dict[str, Any] = {}
+        resp.update({"action": "on_retriever_start", "query": query})
+        resp.update(flatten_dict(serialized))
+        resp.update(self.metrics)
+
+        self.mlflg.metrics(self.metrics, step=self.metrics["step"])
+
+        self.records["on_retriever_start_records"].append(resp)
+        self.records["action_records"].append(resp)
+        self.mlflg.jsonf(resp, f"retriever_start_{retriever_starts}")
+
+    def on_retriever_end(
+        self,
+        documents: Sequence[Document],
+        **kwargs: Any,
+    ) -> Any:
+        """Run when Retriever ends running."""
+        self.metrics["step"] += 1
+        self.metrics["retriever_ends"] += 1
+        self.metrics["ends"] += 1
+
+        retriever_ends = self.metrics["retriever_ends"]
+
+        resp: Dict[str, Any] = {}
+        retriever_documents = [
+            {
+                "page_content": doc.page_content,
+                "metadata": {
+                    k: (
+                        str(v)
+                        if not isinstance(v, list)
+                        else ",".join(str(x) for x in v)
+                    )
+                    for k, v in doc.metadata.items()
+                },
+            }
+            for doc in documents
+        ]
+        resp.update({"action": "on_retriever_end", "documents": retriever_documents})
+        resp.update(self.metrics)
+
+        self.mlflg.metrics(self.metrics, step=self.metrics["step"])
+
+        self.records["on_retriever_end_records"].append(resp)
+        self.records["action_records"].append(resp)
+        self.mlflg.jsonf(resp, f"retriever_end_{retriever_ends}")
+
+    def on_retriever_error(self, error: BaseException, **kwargs: Any) -> Any:
+        """Run when Retriever errors."""
+        self.metrics["step"] += 1
+        self.metrics["errors"] += 1
+
     def _create_session_analysis_df(self) -> Any:
         """Create a dataframe with all the information from the session."""
         pd = import_pandas()
         on_llm_start_records_df = pd.DataFrame(self.records["on_llm_start_records"])
         on_llm_end_records_df = pd.DataFrame(self.records["on_llm_end_records"])
 
         llm_input_columns = ["step", "prompt"]
@@ -566,47 +681,37 @@
             )
             llm_input_columns.append("name")
         llm_input_prompts_df = (
             on_llm_start_records_df[llm_input_columns]
             .dropna(axis=1)
             .rename({"step": "prompt_step"}, axis=1)
         )
-        complexity_metrics_columns = []
-        visualizations_columns = []
+        complexity_metrics_columns = (
+            get_text_complexity_metrics() if self.textstat is not None else []
+        )
+        visualizations_columns = (
+            ["dependency_tree", "entities"] if self.nlp is not None else []
+        )
 
-        complexity_metrics_columns = [
-            "flesch_reading_ease",
-            "flesch_kincaid_grade",
-            "smog_index",
-            "coleman_liau_index",
-            "automated_readability_index",
-            "dale_chall_readability_score",
-            "difficult_words",
-            "linsear_write_formula",
-            "gunning_fog",
-            # "text_standard",
-            "fernandez_huerta",
-            "szigriszt_pazos",
-            "gutierrez_polini",
-            "crawford",
-            "gulpease_index",
-            "osman",
+        token_usage_columns = [
+            "token_usage_total_tokens",
+            "token_usage_prompt_tokens",
+            "token_usage_completion_tokens",
+        ]
+        token_usage_columns = [
+            x for x in token_usage_columns if x in on_llm_end_records_df.columns
         ]
-
-        visualizations_columns = ["dependency_tree", "entities"]
 
         llm_outputs_df = (
             on_llm_end_records_df[
                 [
                     "step",
                     "text",
-                    "token_usage_total_tokens",
-                    "token_usage_prompt_tokens",
-                    "token_usage_completion_tokens",
                 ]
+                + token_usage_columns
                 + complexity_metrics_columns
                 + visualizations_columns
             ]
             .dropna(axis=1)
             .rename({"step": "output_step", "text": "output"}, axis=1)
         )
         session_analysis_df = pd.concat([llm_input_prompts_df, llm_outputs_df], axis=1)
@@ -616,22 +721,26 @@
             lambda row: construct_html_from_prompt_and_generation(
                 row["prompt"], row["output"]
             ),
             axis=1,
         )
         return session_analysis_df
 
+    def _contain_llm_records(self) -> bool:
+        return bool(self.records["on_llm_start_records"])
+
     def flush_tracker(self, langchain_asset: Any = None, finish: bool = False) -> None:
         pd = import_pandas()
         self.mlflg.table("action_records", pd.DataFrame(self.records["action_records"]))
-        session_analysis_df = self._create_session_analysis_df()
-        chat_html = session_analysis_df.pop("chat_html")
-        chat_html = chat_html.replace("\n", "", regex=True)
-        self.mlflg.table("session_analysis", pd.DataFrame(session_analysis_df))
-        self.mlflg.html("".join(chat_html.tolist()), "chat_html")
+        if self._contain_llm_records():
+            session_analysis_df = self._create_session_analysis_df()
+            chat_html = session_analysis_df.pop("chat_html")
+            chat_html = chat_html.replace("\n", "", regex=True)
+            self.mlflg.table("session_analysis", pd.DataFrame(session_analysis_df))
+            self.mlflg.html("".join(chat_html.tolist()), "chat_html")
 
         if langchain_asset:
             # To avoid circular import error
             # mlflow only supports LLMChain asset
             if "langchain.chains.llm.LLMChain" in str(type(langchain_asset)):
                 self.mlflg.langchain_artifact(langchain_asset)
             else:
@@ -640,21 +749,21 @@
                     langchain_asset.save(langchain_asset_path)
                     self.mlflg.artifact(langchain_asset_path)
                 except ValueError:
                     try:
                         langchain_asset.save_agent(langchain_asset_path)
                         self.mlflg.artifact(langchain_asset_path)
                     except AttributeError:
-                        print("Could not save model.")
+                        print("Could not save model.")  # noqa: T201
                         traceback.print_exc()
                         pass
                     except NotImplementedError:
-                        print("Could not save model.")
+                        print("Could not save model.")  # noqa: T201
                         traceback.print_exc()
                         pass
                 except NotImplementedError:
-                    print("Could not save model.")
+                    print("Could not save model.")  # noqa: T201
                     traceback.print_exc()
                     pass
         if finish:
             self.mlflg.finish_run()
             self._reset()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/openai_info.py` & `gigachain_community-0.2.0/langchain_community/callbacks/openai_info.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,64 @@
 """Callback Handler that prints to std out."""
+
+import threading
 from typing import Any, Dict, List
 
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
 
 MODEL_COST_PER_1K_TOKENS = {
+    # GPT-4o input
+    "gpt-4o": 0.005,
+    "gpt-4o-2024-05-13": 0.005,
+    # GPT-4o output
+    "gpt-4o-completion": 0.015,
+    "gpt-4o-2024-05-13-completion": 0.015,
     # GPT-4 input
     "gpt-4": 0.03,
     "gpt-4-0314": 0.03,
     "gpt-4-0613": 0.03,
     "gpt-4-32k": 0.06,
     "gpt-4-32k-0314": 0.06,
     "gpt-4-32k-0613": 0.06,
     "gpt-4-vision-preview": 0.01,
     "gpt-4-1106-preview": 0.01,
+    "gpt-4-0125-preview": 0.01,
+    "gpt-4-turbo-preview": 0.01,
+    "gpt-4-turbo": 0.01,
+    "gpt-4-turbo-2024-04-09": 0.01,
     # GPT-4 output
     "gpt-4-completion": 0.06,
     "gpt-4-0314-completion": 0.06,
     "gpt-4-0613-completion": 0.06,
     "gpt-4-32k-completion": 0.12,
     "gpt-4-32k-0314-completion": 0.12,
     "gpt-4-32k-0613-completion": 0.12,
     "gpt-4-vision-preview-completion": 0.03,
     "gpt-4-1106-preview-completion": 0.03,
+    "gpt-4-0125-preview-completion": 0.03,
+    "gpt-4-turbo-preview-completion": 0.03,
+    "gpt-4-turbo-completion": 0.03,
+    "gpt-4-turbo-2024-04-09-completion": 0.03,
     # GPT-3.5 input
+    # gpt-3.5-turbo points at gpt-3.5-turbo-0613 until Feb 16, 2024.
+    # Switches to gpt-3.5-turbo-0125 after.
     "gpt-3.5-turbo": 0.0015,
+    "gpt-3.5-turbo-0125": 0.0005,
     "gpt-3.5-turbo-0301": 0.0015,
     "gpt-3.5-turbo-0613": 0.0015,
     "gpt-3.5-turbo-1106": 0.001,
     "gpt-3.5-turbo-instruct": 0.0015,
     "gpt-3.5-turbo-16k": 0.003,
     "gpt-3.5-turbo-16k-0613": 0.003,
     # GPT-3.5 output
+    # gpt-3.5-turbo points at gpt-3.5-turbo-0613 until Feb 16, 2024.
+    # Switches to gpt-3.5-turbo-0125 after.
     "gpt-3.5-turbo-completion": 0.002,
+    "gpt-3.5-turbo-0125-completion": 0.0015,
     "gpt-3.5-turbo-0301-completion": 0.002,
     "gpt-3.5-turbo-0613-completion": 0.002,
     "gpt-3.5-turbo-1106-completion": 0.002,
     "gpt-3.5-turbo-instruct-completion": 0.002,
     "gpt-3.5-turbo-16k-completion": 0.004,
     "gpt-3.5-turbo-16k-0613-completion": 0.004,
     # Azure GPT-35 input
@@ -63,18 +85,20 @@
     "text-davinci-003": 0.02,
     "text-davinci-002": 0.02,
     "code-davinci-002": 0.02,
     # Fine Tuned input
     "babbage-002-finetuned": 0.0016,
     "davinci-002-finetuned": 0.012,
     "gpt-3.5-turbo-0613-finetuned": 0.012,
+    "gpt-3.5-turbo-1106-finetuned": 0.012,
     # Fine Tuned output
     "babbage-002-finetuned-completion": 0.0016,
     "davinci-002-finetuned-completion": 0.012,
     "gpt-3.5-turbo-0613-finetuned-completion": 0.016,
+    "gpt-3.5-turbo-1106-finetuned-completion": 0.016,
     # Azure Fine Tuned input
     "babbage-002-azure-finetuned": 0.0004,
     "davinci-002-azure-finetuned": 0.002,
     "gpt-35-turbo-0613-azure-finetuned": 0.0015,
     # Azure Fine Tuned output
     "babbage-002-azure-finetuned-completion": 0.0004,
     "davinci-002-azure-finetuned-completion": 0.002,
@@ -150,14 +174,18 @@
 
     total_tokens: int = 0
     prompt_tokens: int = 0
     completion_tokens: int = 0
     successful_requests: int = 0
     total_cost: float = 0.0
 
+    def __init__(self) -> None:
+        super().__init__()
+        self._lock = threading.Lock()
+
     def __repr__(self) -> str:
         return (
             f"Tokens Used: {self.total_tokens}\n"
             f"\tPrompt Tokens: {self.prompt_tokens}\n"
             f"\tCompletion Tokens: {self.completion_tokens}\n"
             f"Successful Requests: {self.successful_requests}\n"
             f"Total Cost (USD): ${self.total_cost}"
@@ -178,30 +206,41 @@
         """Print out the token."""
         pass
 
     def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
         """Collect token usage."""
         if response.llm_output is None:
             return None
-        self.successful_requests += 1
+
         if "token_usage" not in response.llm_output:
+            with self._lock:
+                self.successful_requests += 1
             return None
+
+        # compute tokens and cost for this request
         token_usage = response.llm_output["token_usage"]
         completion_tokens = token_usage.get("completion_tokens", 0)
         prompt_tokens = token_usage.get("prompt_tokens", 0)
         model_name = standardize_model_name(response.llm_output.get("model_name", ""))
         if model_name in MODEL_COST_PER_1K_TOKENS:
             completion_cost = get_openai_token_cost_for_model(
                 model_name, completion_tokens, is_completion=True
             )
             prompt_cost = get_openai_token_cost_for_model(model_name, prompt_tokens)
+        else:
+            completion_cost = 0
+            prompt_cost = 0
+
+        # update shared state behind lock
+        with self._lock:
             self.total_cost += prompt_cost + completion_cost
-        self.total_tokens += token_usage.get("total_tokens", 0)
-        self.prompt_tokens += prompt_tokens
-        self.completion_tokens += completion_tokens
+            self.total_tokens += token_usage.get("total_tokens", 0)
+            self.prompt_tokens += prompt_tokens
+            self.completion_tokens += completion_tokens
+            self.successful_requests += 1
 
     def __copy__(self) -> "OpenAICallbackHandler":
         """Return a copy of the callback handler."""
         return self
 
     def __deepcopy__(self, memo: Any) -> "OpenAICallbackHandler":
         """Return a deep copy of the callback handler."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/promptlayer_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/promptlayer_callback.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Callback handler for promptlayer."""
+
 from __future__ import annotations
 
 import datetime
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple
 from uuid import UUID
 
 from langchain_core.callbacks import BaseCallbackHandler
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/sagemaker_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/sagemaker_callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -182,16 +182,17 @@
         resp: Dict[str, Any] = {}
         resp.update({"action": "on_tool_start", "input_str": input_str})
         resp.update(flatten_dict(serialized))
         resp.update(self.metrics)
 
         self.jsonf(resp, self.temp_dir, f"tool_start_{tool_starts}")
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         self.metrics["step"] += 1
         self.metrics["tool_ends"] += 1
         self.metrics["ends"] += 1
 
         tool_ends = self.metrics["tool_ends"]
 
         resp: Dict[str, Any] = {}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/__init__.py` & `gigachain_community-0.2.0/langchain_community/callbacks/streamlit/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,15 +58,15 @@
 
     """
     # If we're using a version of Streamlit that implements StreamlitCallbackHandler,
     # delegate to it instead of using our built-in handler. The official handler is
     # guaranteed to support the same set of kwargs.
     try:
         from streamlit.external.langchain import (
-            StreamlitCallbackHandler as OfficialStreamlitCallbackHandler,  # type: ignore # noqa: 501
+            StreamlitCallbackHandler as OfficialStreamlitCallbackHandler,
         )
 
         return OfficialStreamlitCallbackHandler(
             parent_container,
             max_thought_containers=max_thought_containers,
             expand_new_thoughts=expand_new_thoughts,
             collapse_completed_thoughts=collapse_completed_thoughts,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/mutable_expander.py` & `gigachain_community-0.2.0/langchain_community/callbacks/streamlit/mutable_expander.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,30 +5,30 @@
 
 if TYPE_CHECKING:
     from streamlit.delta_generator import DeltaGenerator
     from streamlit.type_util import SupportsStr
 
 
 class ChildType(Enum):
-    """The enumerator of the child type."""
+    """Enumerator of the child type."""
 
     MARKDOWN = "MARKDOWN"
     EXCEPTION = "EXCEPTION"
 
 
 class ChildRecord(NamedTuple):
-    """The child record as a NamedTuple."""
+    """Child record as a NamedTuple."""
 
     type: ChildType
     kwargs: Dict[str, Any]
     dg: DeltaGenerator
 
 
 class MutableExpander:
-    """A Streamlit expander that can be renamed and dynamically expanded/collapsed."""
+    """Streamlit expander that can be renamed and dynamically expanded/collapsed."""
 
     def __init__(self, parent_container: DeltaGenerator, label: str, expanded: bool):
         """Create a new MutableExpander.
 
         Parameters
         ----------
         parent_container
@@ -47,15 +47,15 @@
         self._expanded = expanded
         self._parent_cursor = parent_container.empty()
         self._container = self._parent_cursor.expander(label, expanded)
         self._child_records: List[ChildRecord] = []
 
     @property
     def label(self) -> str:
-        """The expander's label string."""
+        """Expander's label string."""
         return self._label
 
     @property
     def expanded(self) -> bool:
         """True if the expander was created with `expanded=True`."""
         return self._expanded
 
@@ -104,15 +104,15 @@
         unsafe_allow_html: bool = False,
         *,
         help: Optional[str] = None,
         index: Optional[int] = None,
     ) -> int:
         """Add a Markdown element to the container and return its index."""
         kwargs = {"body": body, "unsafe_allow_html": unsafe_allow_html, "help": help}
-        new_dg = self._get_dg(index).markdown(**kwargs)  # type: ignore[arg-type]
+        new_dg = self._get_dg(index).markdown(**kwargs)
         record = ChildRecord(ChildType.MARKDOWN, kwargs, new_dg)
         return self._add_record(record, index)
 
     def exception(
         self, exception: BaseException, *, index: Optional[int] = None
     ) -> int:
         """Add an Exception element to the container and return its index."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/streamlit/streamlit_callback_handler.py` & `gigachain_community-0.2.0/langchain_community/callbacks/streamlit/streamlit_callback_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,15 +36,15 @@
     # The LLM has decided to run a tool. We don't have results from the tool yet.
     RUNNING_TOOL = "RUNNING_TOOL"
     # We have results from the tool.
     COMPLETE = "COMPLETE"
 
 
 class ToolRecord(NamedTuple):
-    """The tool record as a NamedTuple."""
+    """Tool record as a NamedTuple."""
 
     name: str
     input_str: str
 
 
 class LLMThoughtLabeler:
     """
@@ -179,21 +179,21 @@
         self._last_tool = ToolRecord(name=tool_name, input_str=input_str)
         self._container.update(
             new_label=self._labeler.get_tool_label(self._last_tool, is_complete=False)
         )
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         color: Optional[str] = None,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
-        self._container.markdown(f"**{output}**")
+        self._container.markdown(f"**{str(output)}**")
 
     def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
         self._container.markdown("**Tool encountered an error...**")
         self._container.exception(error)
 
     def on_agent_action(
         self, action: AgentAction, color: Optional[str] = None, **kwargs: Any
@@ -221,15 +221,15 @@
 
     def clear(self) -> None:
         """Remove the thought from the screen. A cleared thought can't be reused."""
         self._container.clear()
 
 
 class StreamlitCallbackHandler(BaseCallbackHandler):
-    """A callback handler that writes to a Streamlit app."""
+    """Callback handler that writes to a Streamlit app."""
 
     def __init__(
         self,
         parent_container: DeltaGenerator,
         *,
         max_thought_containers: int = 4,
         expand_new_thoughts: bool = True,
@@ -359,20 +359,21 @@
         self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
     ) -> None:
         self._require_current_thought().on_tool_start(serialized, input_str, **kwargs)
         self._prune_old_thought_containers()
 
     def on_tool_end(
         self,
-        output: str,
+        output: Any,
         color: Optional[str] = None,
         observation_prefix: Optional[str] = None,
         llm_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> None:
+        output = str(output)
         self._require_current_thought().on_tool_end(
             output, color, observation_prefix, llm_prefix, **kwargs
         )
         self._complete_current_thought()
 
     def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:
         self._require_current_thought().on_tool_error(error, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/tracers/comet.py` & `gigachain_community-0.2.0/langchain_community/callbacks/tracers/comet.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from types import ModuleType, SimpleNamespace
 from typing import TYPE_CHECKING, Any, Callable, Dict
 
 from langchain_core.tracers import BaseTracer
+from langchain_core.utils import guard_import
 
 if TYPE_CHECKING:
     from uuid import UUID
 
     from comet_llm import Span
     from comet_llm.chains.chain import Chain
 
@@ -19,37 +20,23 @@
         return run.run_type.value
     else:
         return str(run.run_type)
 
 
 def import_comet_llm_api() -> SimpleNamespace:
     """Import comet_llm api and raise an error if it is not installed."""
-    try:
-        from comet_llm import (
-            experiment_info,  # noqa: F401
-            flush,  # noqa: F401
-        )
-        from comet_llm.chains import api as chain_api  # noqa: F401
-        from comet_llm.chains import (
-            chain,  # noqa: F401
-            span,  # noqa: F401
-        )
-
-    except ImportError:
-        raise ImportError(
-            "To use the CometTracer you need to have the "
-            "`comet_llm>=2.0.0` python package installed. Please install it with"
-            " `pip install -U comet_llm`"
-        )
+    comet_llm = guard_import("comet_llm")
+    comet_llm_chains = guard_import("comet_llm.chains")
+
     return SimpleNamespace(
-        chain=chain,
-        span=span,
-        chain_api=chain_api,
-        experiment_info=experiment_info,
-        flush=flush,
+        chain=comet_llm_chains.chain,
+        span=comet_llm_chains.span,
+        chain_api=comet_llm_chains.api,
+        experiment_info=comet_llm.experiment_info,
+        flush=comet_llm.flush,
     )
 
 
 class CometTracer(BaseTracer):
     """Comet Tracer."""
 
     def __init__(self, **kwargs: Any) -> None:
@@ -66,45 +53,48 @@
         self._chain: ModuleType = comet_llm_api.chain
         self._span: ModuleType = comet_llm_api.span
         self._chain_api: ModuleType = comet_llm_api.chain_api
         self._experiment_info: ModuleType = comet_llm_api.experiment_info
         self._flush: Callable[[], None] = comet_llm_api.flush
 
     def _persist_run(self, run: "Run") -> None:
+        run_dict: Dict[str, Any] = run.dict()
         chain_ = self._chains_map[run.id]
-        chain_.set_outputs(outputs=run.outputs)
+        chain_.set_outputs(outputs=run_dict["outputs"])
         self._chain_api.log_chain(chain_)
 
     def _process_start_trace(self, run: "Run") -> None:
+        run_dict: Dict[str, Any] = run.dict()
         if not run.parent_run_id:
             # This is the first run, which maps to a chain
             chain_: "Chain" = self._chain.Chain(
-                inputs=run.inputs,
+                inputs=run_dict["inputs"],
                 metadata=None,
                 experiment_info=self._experiment_info.get(),
             )
             self._chains_map[run.id] = chain_
         else:
             span: "Span" = self._span.Span(
-                inputs=run.inputs,
+                inputs=run_dict["inputs"],
                 category=_get_run_type(run),
-                metadata=run.extra,
+                metadata=run_dict["extra"],
                 name=run.name,
             )
             span.__api__start__(self._chains_map[run.parent_run_id])
             self._chains_map[run.id] = self._chains_map[run.parent_run_id]
             self._span_map[run.id] = span
 
     def _process_end_trace(self, run: "Run") -> None:
+        run_dict: Dict[str, Any] = run.dict()
         if not run.parent_run_id:
             pass
             # Langchain will call _persist_run for us
         else:
             span = self._span_map[run.id]
-            span.set_outputs(outputs=run.outputs)
+            span.set_outputs(outputs=run_dict["outputs"])
             span.__api__end__()
 
     def flush(self) -> None:
         self._flush()
 
     def _on_llm_start(self, run: "Run") -> None:
         """Process the LLM Run upon start."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/tracers/wandb.py` & `gigachain_community-0.2.0/langchain_community/callbacks/tracers/wandb.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """A Tracer Implementation that records activity to Weights & Biases."""
+
 from __future__ import annotations
 
 import json
 from typing import (
     TYPE_CHECKING,
     Any,
     Dict,
@@ -485,19 +486,18 @@
 
     def _ensure_run(self, should_print_url: bool = False) -> None:
         """Ensures an active W&B run exists.
 
         If not, will start a new run with the provided run_args.
         """
         if self._wandb.run is None:
-            run_args = self._run_args or {}  # type: ignore
-            run_args: dict = {**run_args}  # type: ignore
+            run_args: Dict = {**(self._run_args or {})}
 
-            if "settings" not in run_args:  # type: ignore
-                run_args["settings"] = {"silent": True}  # type: ignore
+            if "settings" not in run_args:
+                run_args["settings"] = {"silent": True}
 
             self._wandb.init(**run_args)
         if self._wandb.run is not None:
             if should_print_url:
                 run_url = self._wandb.run.settings.run_url
                 self._wandb.termlog(
                     f"Streaming LangChain activity to W&B at {run_url}\n"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/trubrics_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/trubrics_callback.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/utils.py` & `gigachain_community-0.2.0/langchain_community/callbacks/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,46 +1,27 @@
 import hashlib
 from pathlib import Path
 from typing import Any, Dict, Iterable, Tuple, Union
 
+from langchain_core.utils import guard_import
+
 
 def import_spacy() -> Any:
     """Import the spacy python package and raise an error if it is not installed."""
-    try:
-        import spacy
-    except ImportError:
-        raise ImportError(
-            "This callback manager requires the `spacy` python "
-            "package installed. Please install it with `pip install spacy`"
-        )
-    return spacy
+    return guard_import("spacy")
 
 
 def import_pandas() -> Any:
     """Import the pandas python package and raise an error if it is not installed."""
-    try:
-        import pandas
-    except ImportError:
-        raise ImportError(
-            "This callback manager requires the `pandas` python "
-            "package installed. Please install it with `pip install pandas`"
-        )
-    return pandas
+    return guard_import("pandas")
 
 
 def import_textstat() -> Any:
     """Import the textstat python package and raise an error if it is not installed."""
-    try:
-        import textstat
-    except ImportError:
-        raise ImportError(
-            "This callback manager requires the `textstat` python "
-            "package installed. Please install it with `pip install textstat`"
-        )
-    return textstat
+    return guard_import("textstat")
 
 
 def _flatten_dict(
     nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"
 ) -> Iterable[Tuple[str, Any]]:
     """
     Generator that yields flattened items from a nested dictionary for a flat dict.
@@ -61,15 +42,15 @@
         else:
             yield new_key, value
 
 
 def flatten_dict(
     nested_dict: Dict[str, Any], parent_key: str = "", sep: str = "_"
 ) -> Dict[str, Any]:
-    """Flattens a nested dictionary into a flat dictionary.
+    """Flatten a nested dictionary into a flat dictionary.
 
     Parameters:
         nested_dict (dict): The nested dictionary to flatten.
         parent_key (str): The prefix to prepend to the keys of the flattened dict.
         sep (str): The separator to use between the parent key and the key of the
             flattened dictionary.
 
@@ -104,15 +85,15 @@
     """
     with open(json_path, "r") as f:
         data = f.read()
     return data
 
 
 class BaseMetadataCallbackHandler:
-    """This class handles the metadata and associated function states for callbacks.
+    """Handle the metadata and associated function states for callbacks.
 
     Attributes:
         step (int): The current step.
         starts (int): The number of times the start method has been called.
         ends (int): The number of times the end method has been called.
         errors (int): The number of times the error method has been called.
         text_ctr (int): The number of times the text method has been called.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/wandb_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/wandb_callback.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,35 +3,29 @@
 from copy import deepcopy
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Sequence, Union
 
 from langchain_core.agents import AgentAction, AgentFinish
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
+from langchain_core.utils import guard_import
 
 from langchain_community.callbacks.utils import (
     BaseMetadataCallbackHandler,
     flatten_dict,
     hash_string,
     import_pandas,
     import_spacy,
     import_textstat,
 )
 
 
 def import_wandb() -> Any:
     """Import the wandb python package and raise an error if it is not installed."""
-    try:
-        import wandb  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the wandb callback manager you need to have the `wandb` python "
-            "package installed. Please install it with `pip install wandb`"
-        )
-    return wandb
+    return guard_import("wandb")
 
 
 def load_json_to_dict(json_path: Union[str, Path]) -> dict:
     """Load json file to a dictionary.
 
     Parameters:
         json_path (str): The path to the json file.
@@ -88,23 +82,19 @@
             "osman": textstat.osman(text),
         }
         resp.update(text_complexity_metrics)
 
     if visualize and nlp and output_dir is not None:
         doc = nlp(text)
 
-        dep_out = spacy.displacy.render(  # type: ignore
-            doc, style="dep", jupyter=False, page=True
-        )
+        dep_out = spacy.displacy.render(doc, style="dep", jupyter=False, page=True)
         dep_output_path = Path(output_dir, hash_string(f"dep-{text}") + ".html")
         dep_output_path.open("w", encoding="utf-8").write(dep_out)
 
-        ent_out = spacy.displacy.render(  # type: ignore
-            doc, style="ent", jupyter=False, page=True
-        )
+        ent_out = spacy.displacy.render(doc, style="ent", jupyter=False, page=True)
         ent_output_path = Path(output_dir, hash_string(f"ent-{text}") + ".html")
         ent_output_path.open("w", encoding="utf-8").write(ent_out)
 
         text_visualizations = {
             "dependency_tree": wandb.Html(str(dep_output_path)),
             "entities": wandb.Html(str(ent_output_path)),
         }
@@ -189,15 +179,15 @@
         self.name = name
         self.notes = notes
         self.visualize = visualize
         self.complexity_metrics = complexity_metrics
         self.stream_logs = stream_logs
 
         self.temp_dir = tempfile.TemporaryDirectory()
-        self.run: wandb.sdk.wandb_run.Run = wandb.init(  # type: ignore
+        self.run = wandb.init(
             job_type=self.job_type,
             project=self.project,
             entity=self.entity,
             tags=self.tags,
             group=self.group,
             name=self.name,
             notes=self.notes,
@@ -356,16 +346,17 @@
         resp.update(self.get_custom_callback_meta())
 
         self.on_tool_start_records.append(resp)
         self.action_records.append(resp)
         if self.stream_logs:
             self.run.log(resp)
 
-    def on_tool_end(self, output: str, **kwargs: Any) -> None:
+    def on_tool_end(self, output: Any, **kwargs: Any) -> None:
         """Run when tool ends running."""
+        output = str(output)
         self.step += 1
         self.tool_ends += 1
         self.ends += 1
 
         resp = self._init_resp()
         resp.update({"action": "on_tool_end", "output": output})
         resp.update(self.get_custom_callback_meta())
@@ -558,16 +549,16 @@
                 model_artifact.add_file(str(langchain_asset_path))
                 model_artifact.metadata = load_json_to_dict(langchain_asset_path)
             except ValueError:
                 langchain_asset.save_agent(langchain_asset_path)
                 model_artifact.add_file(str(langchain_asset_path))
                 model_artifact.metadata = load_json_to_dict(langchain_asset_path)
             except NotImplementedError as e:
-                print("Could not save model.")
-                print(repr(e))
+                print("Could not save model.")  # noqa: T201
+                print(repr(e))  # noqa: T201
                 pass
             self.run.log_artifact(model_artifact)
 
         if finish or reset:
             self.run.finish()
             self.temp_dir.cleanup()
             self.reset_callback_meta()
@@ -577,11 +568,13 @@
                 project=project if project else self.project,
                 entity=entity if entity else self.entity,
                 tags=tags if tags else self.tags,
                 group=group if group else self.group,
                 name=name if name else self.name,
                 notes=notes if notes else self.notes,
                 visualize=visualize if visualize else self.visualize,
-                complexity_metrics=complexity_metrics
-                if complexity_metrics
-                else self.complexity_metrics,
+                complexity_metrics=(
+                    complexity_metrics
+                    if complexity_metrics
+                    else self.complexity_metrics
+                ),
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/callbacks/whylabs_callback.py` & `gigachain_community-0.2.0/langchain_community/callbacks/whylabs_callback.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import logging
 from typing import TYPE_CHECKING, Any, Optional
 
 from langchain_core.callbacks import BaseCallbackHandler
-from langchain_core.utils import get_from_env
+from langchain_core.utils import get_from_env, guard_import
 
 if TYPE_CHECKING:
     from whylogs.api.logger.logger import Logger
 
 diagnostic_logger = logging.getLogger(__name__)
 
 
@@ -23,30 +23,23 @@
         sentiment: Whether to import the langkit.sentiment module. Defaults to False.
         toxicity: Whether to import the langkit.toxicity module. Defaults to False.
         themes: Whether to import the langkit.themes module. Defaults to False.
 
     Returns:
         The imported langkit module.
     """
-    try:
-        import langkit  # noqa: F401
-        import langkit.regexes  # noqa: F401
-        import langkit.textstat  # noqa: F401
-
-        if sentiment:
-            import langkit.sentiment  # noqa: F401
-        if toxicity:
-            import langkit.toxicity  # noqa: F401
-        if themes:
-            import langkit.themes  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "To use the whylabs callback manager you need to have the `langkit` python "
-            "package installed. Please install it with `pip install langkit`."
-        )
+    langkit = guard_import("langkit")
+    guard_import("langkit.regexes")
+    guard_import("langkit.textstat")
+    if sentiment:
+        guard_import("langkit.sentiment")
+    if toxicity:
+        guard_import("langkit.toxicity")
+    if themes:
+        guard_import("langkit.themes")
     return langkit
 
 
 class WhyLabsCallbackHandler(BaseCallbackHandler):
     """
     Callback Handler for logging to WhyLabs. This callback handler utilizes
     `langkit` to extract features from the prompts & responses when interacting with
@@ -157,18 +150,20 @@
             logger (Optional[Logger]): If specified will bind the configured logger as
                 the telemetry gathering agent. Defaults to LangKit schema with periodic
                 WhyLabs writer.
         """
         # langkit library will import necessary whylogs libraries
         import_langkit(sentiment=sentiment, toxicity=toxicity, themes=themes)
 
-        import whylogs as why
-        from langkit.callback_handler import get_callback_instance
-        from whylogs.api.writer.whylabs import WhyLabsWriter
-        from whylogs.experimental.core.udf_schema import udf_schema
+        why = guard_import("whylogs")
+        get_callback_instance = guard_import(
+            "langkit.callback_handler"
+        ).get_callback_instance
+        WhyLabsWriter = guard_import("whylogs.api.writer.whylabs").WhyLabsWriter
+        udf_schema = guard_import("whylogs.experimental.core.udf_schema").udf_schema
 
         if logger is None:
             api_key = api_key or get_from_env("api_key", "WHYLABS_API_KEY")
             org_id = org_id or get_from_env("org_id", "WHYLABS_DEFAULT_ORG_ID")
             dataset_id = dataset_id or get_from_env(
                 "dataset_id", "WHYLABS_DEFAULT_DATASET_ID"
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/facebook_messenger.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/facebook_messenger.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 import json
 import logging
 from pathlib import Path
 from typing import Iterator, Union
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 logger = logging.getLogger(__file__)
 
 
 class SingleFileFacebookMessengerChatLoader(BaseChatLoader):
     """Load `Facebook Messenger` chat data from a single file.
 
     Args:
@@ -33,15 +32,21 @@
             ChatSession: A chat session containing the loaded messages.
 
         """
         with open(self.file_path) as f:
             data = json.load(f)
         sorted_data = sorted(data["messages"], key=lambda x: x["timestamp_ms"])
         messages = []
-        for m in sorted_data:
+        for index, m in enumerate(sorted_data):
+            if "content" not in m:
+                logger.info(
+                    f"""Skipping Message No.
+                    {index+1} as no content is present in the message"""
+                )
+                continue
             messages.append(
                 HumanMessage(
                     content=m["content"], additional_kwargs={"sender": m["sender_name"]}
                 )
             )
         yield ChatSession(messages=messages)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/gmail.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/gmail.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 import base64
 import re
 from typing import Any, Iterator
 
+from langchain_core._api.deprecation import deprecated
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 
 def _extract_email_content(msg: Any) -> HumanMessage:
     from_email = None
     for values in msg["payload"]["headers"]:
         name = values["name"]
         if name == "From":
             from_email = values["value"]
@@ -59,14 +59,19 @@
                     response_email = message
     if response_email is None:
         raise ValueError
     starter_content = _extract_email_content(response_email)
     return ChatSession(messages=[starter_content, message_content])
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GMailLoader",
+)
 class GMailLoader(BaseChatLoader):
     """Load data from `GMail`.
 
     There are many ways you could want to load data from GMail.
     This loader is currently fairly opinionated in how to do so.
     The way it does it is it first looks for all messages that you have sent.
     It then looks for messages where you are responding to a previous email.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/imessage.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/imessage.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 from __future__ import annotations
 
 from datetime import datetime
 from pathlib import Path
 from typing import TYPE_CHECKING, Iterator, List, Optional, Union
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 if TYPE_CHECKING:
     import sqlite3
 
 
 def nanoseconds_from_2001_to_datetime(nanoseconds: int) -> datetime:
     # Convert nanoseconds to seconds (1 second = 1e9 nanoseconds)
     timestamp_in_seconds = nanoseconds / 1e9
@@ -87,50 +86,73 @@
         """
         content = attributedBody.split(b"NSString")[1][5:]
         length, start = content[0], 1
         if content[0] == 129:
             length, start = int.from_bytes(content[1:3], "little"), 3
         return content[start : start + length].decode("utf-8", errors="ignore")
 
+    def _get_session_query(self, use_chat_handle_table: bool) -> str:
+        # Messages sent pre OSX 12 require a join through the chat_handle_join table
+        # However, the table doesn't exist if database created with OSX 12 or above.
+
+        joins_w_chat_handle = """
+            JOIN chat_handle_join ON
+                 chat_message_join.chat_id = chat_handle_join.chat_id
+            JOIN handle ON
+                 handle.ROWID = chat_handle_join.handle_id"""
+
+        joins_no_chat_handle = """
+            JOIN handle ON message.handle_id = handle.ROWID
+        """
+
+        joins = joins_w_chat_handle if use_chat_handle_table else joins_no_chat_handle
+
+        return f"""
+            SELECT  message.date,
+                    handle.id,
+                    message.text,
+                    message.is_from_me,
+                    message.attributedBody
+            FROM message
+            JOIN chat_message_join ON
+                 message.ROWID = chat_message_join.message_id
+            {joins}
+            WHERE chat_message_join.chat_id = ?
+            ORDER BY message.date ASC;
+        """
+
     def _load_single_chat_session(
-        self, cursor: "sqlite3.Cursor", chat_id: int
+        self, cursor: "sqlite3.Cursor", use_chat_handle_table: bool, chat_id: int
     ) -> ChatSession:
         """
         Load a single chat session from the iMessage chat.db.
 
         Args:
             cursor: SQLite cursor object.
             chat_id (int): ID of the chat session to load.
 
         Returns:
             ChatSession: Loaded chat session.
         """
         results: List[HumanMessage] = []
 
-        query = """
-        SELECT message.date, handle.id, message.text, message.is_from_me, message.attributedBody
-        FROM message
-        JOIN chat_message_join ON message.ROWID = chat_message_join.message_id
-        JOIN handle ON message.handle_id = handle.ROWID
-        WHERE chat_message_join.chat_id = ?
-        ORDER BY message.date ASC;
-        """  # noqa: E501
+        query = self._get_session_query(use_chat_handle_table)
         cursor.execute(query, (chat_id,))
         messages = cursor.fetchall()
 
         for date, sender, text, is_from_me, attributedBody in messages:
             if text:
                 content = text
             elif attributedBody:
                 content = self._parse_attributedBody(attributedBody)
             else:  # Skip messages with no content
                 continue
 
             results.append(
-                HumanMessage(
+                HumanMessage(  # type: ignore[call-arg]
                     role=sender,
                     content=content,
                     additional_kwargs={
                         "message_time": date,
                         "message_time_as_datetime": nanoseconds_from_2001_to_datetime(
                             date
                         ),
@@ -161,20 +183,29 @@
                 "   You can either copy the DB file to an accessible location"
                 " or grant full disk access for your terminal emulator."
                 "  You can grant full disk access for your terminal emulator"
                 " in System Settings > Security and Privacy > Full Disk Access."
             ) from e
         cursor = conn.cursor()
 
+        # See if chat_handle_join table exists:
+        query = """SELECT name FROM sqlite_master
+                   WHERE type='table' AND name='chat_handle_join';"""
+
+        cursor.execute(query)
+        is_chat_handle_join_exists = cursor.fetchone()
+
         # Fetch the list of chat IDs sorted by time (most recent first)
         query = """SELECT chat_id
         FROM message
         JOIN chat_message_join ON message.ROWID = chat_message_join.message_id
         GROUP BY chat_id
         ORDER BY MAX(date) DESC;"""
         cursor.execute(query)
         chat_ids = [row[0] for row in cursor.fetchall()]
 
         for chat_id in chat_ids:
-            yield self._load_single_chat_session(cursor, chat_id)
+            yield self._load_single_chat_session(
+                cursor, is_chat_handle_join_exists, chat_id
+            )
 
         conn.close()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/langsmith.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/langsmith.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 from __future__ import annotations
 
 import logging
 from typing import TYPE_CHECKING, Dict, Iterable, Iterator, List, Optional, Union, cast
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
-from langchain_core.load import load
-
-from langchain_community.chat_loaders.base import BaseChatLoader
+from langchain_core.load.load import load
 
 if TYPE_CHECKING:
     from langsmith.client import Client
     from langsmith.schemas import Run
 
 logger = logging.getLogger(__name__)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/slack.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/slack.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 import json
 import logging
 import re
 import zipfile
 from pathlib import Path
 from typing import Dict, Iterator, List, Union
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import AIMessage, HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 logger = logging.getLogger(__name__)
 
 
 class SlackChatLoader(BaseChatLoader):
     """Load `Slack` conversations from a dump zip file."""
 
     def __init__(
@@ -48,15 +47,15 @@
             if sender == previous_sender:
                 results[-1].content += "\n\n" + text
                 results[-1].additional_kwargs["events"].append(
                     {"message_time": timestamp}
                 )
             else:
                 results.append(
-                    HumanMessage(
+                    HumanMessage(  # type: ignore[call-arg]
                         role=sender,
                         content=text,
                         additional_kwargs={
                             "sender": sender,
                             "events": [{"message_time": timestamp}],
                         },
                     )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/telegram.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/telegram.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,19 +2,18 @@
 import logging
 import os
 import tempfile
 import zipfile
 from pathlib import Path
 from typing import Iterator, List, Union
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 logger = logging.getLogger(__name__)
 
 
 class TelegramChatLoader(BaseChatLoader):
     """Load `telegram` conversations to LangChain chat messages.
 
     To export, use the Telegram Desktop app from
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/utils.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Utilities for chat loaders."""
+
 from copy import deepcopy
 from typing import Iterable, Iterator, List
 
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import AIMessage, BaseMessage
 
 
@@ -73,15 +74,15 @@
     messages = []
     num_converted = 0
     for message in chat_sessions["messages"]:
         if message.additional_kwargs.get("sender") == sender:
             message = AIMessage(
                 content=message.content,
                 additional_kwargs=message.additional_kwargs.copy(),
-                example=getattr(message, "example", None),
+                example=getattr(message, "example", None),  # type: ignore[arg-type]
             )
             num_converted += 1
         messages.append(message)
     return ChatSession(messages=messages)
 
 
 def map_ai_messages(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_loaders/whatsapp.py` & `gigachain_community-0.2.0/langchain_community/chat_loaders/whatsapp.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,17 @@
 import logging
 import os
 import re
 import zipfile
 from typing import Iterator, List, Union
 
+from langchain_core.chat_loaders import BaseChatLoader
 from langchain_core.chat_sessions import ChatSession
 from langchain_core.messages import AIMessage, HumanMessage
 
-from langchain_community.chat_loaders.base import BaseChatLoader
-
 logger = logging.getLogger(__name__)
 
 
 class WhatsAppChatLoader(BaseChatLoader):
     """Load `WhatsApp` conversations from a dump zip file or directory."""
 
     def __init__(self, path: str):
@@ -70,15 +69,15 @@
         results: List[Union[HumanMessage, AIMessage]] = []
         for line in chat_lines:
             result = self._message_line_regex.match(line.strip())
             if result:
                 timestamp, sender, text = result.groups()
                 if not self._ignore_lines.match(text.strip()):
                     results.append(
-                        HumanMessage(
+                        HumanMessage(  # type: ignore[call-arg]
                             role=sender,
                             content=text,
                             additional_kwargs={
                                 "sender": sender,
                                 "events": [{"message_time": timestamp}],
                             },
                         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/astradb.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/sql.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,114 +1,140 @@
-"""Astra DB - based chat message history, based on astrapy."""
-from __future__ import annotations
-
 import json
-import time
-import typing
-from typing import List, Optional
-
-if typing.TYPE_CHECKING:
-    from astrapy.db import AstraDB as LibAstraDB
-
+import logging
+from abc import ABC, abstractmethod
+from typing import Any, List, Optional
+
+from sqlalchemy import Column, Integer, Text, create_engine
+
+try:
+    from sqlalchemy.orm import declarative_base
+except ImportError:
+    from sqlalchemy.ext.declarative import declarative_base
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
 )
+from sqlalchemy.orm import sessionmaker
+
+logger = logging.getLogger(__name__)
 
-DEFAULT_COLLECTION_NAME = "langchain_message_store"
 
+class BaseMessageConverter(ABC):
+    """Convert BaseMessage to the SQLAlchemy model."""
 
-class AstraDBChatMessageHistory(BaseChatMessageHistory):
-    """Chat message history that stores history in Astra DB.
+    @abstractmethod
+    def from_sql_model(self, sql_message: Any) -> BaseMessage:
+        """Convert a SQLAlchemy model to a BaseMessage instance."""
+        raise NotImplementedError
 
-    Args (only keyword-arguments accepted):
-        session_id: arbitrary key that is used to store the messages
-            of a single chat session.
-        collection_name (str): name of the Astra DB collection to create/use.
-        token (Optional[str]): API token for Astra DB usage.
-        api_endpoint (Optional[str]): full URL to the API endpoint,
-            such as "https://<DB-ID>-us-east1.apps.astra.datastax.com".
-        astra_db_client (Optional[Any]): *alternative to token+api_endpoint*,
-            you can pass an already-created 'astrapy.db.AstraDB' instance.
-        namespace (Optional[str]): namespace (aka keyspace) where the
-            collection is created. Defaults to the database's "default namespace".
+    @abstractmethod
+    def to_sql_model(self, message: BaseMessage, session_id: str) -> Any:
+        """Convert a BaseMessage instance to a SQLAlchemy model."""
+        raise NotImplementedError
+
+    @abstractmethod
+    def get_sql_model_class(self) -> Any:
+        """Get the SQLAlchemy model class."""
+        raise NotImplementedError
+
+
+def create_message_model(table_name: str, DynamicBase: Any) -> Any:
     """
+    Create a message model for a given table name.
+
+    Args:
+        table_name: The name of the table to use.
+        DynamicBase: The base class to use for the model.
+
+    Returns:
+        The model class.
+
+    """
+
+    # Model declared inside a function to have a dynamic table name.
+    class Message(DynamicBase):  # type: ignore[valid-type, misc]
+        __tablename__ = table_name
+        id = Column(Integer, primary_key=True)
+        session_id = Column(Text)
+        message = Column(Text)
+
+    return Message
+
+
+class DefaultMessageConverter(BaseMessageConverter):
+    """The default message converter for SQLChatMessageHistory."""
+
+    def __init__(self, table_name: str):
+        self.model_class = create_message_model(table_name, declarative_base())
+
+    def from_sql_model(self, sql_message: Any) -> BaseMessage:
+        return messages_from_dict([json.loads(sql_message.message)])[0]
+
+    def to_sql_model(self, message: BaseMessage, session_id: str) -> Any:
+        return self.model_class(
+            session_id=session_id, message=json.dumps(message_to_dict(message))
+        )
+
+    def get_sql_model_class(self) -> Any:
+        return self.model_class
+
+
+class SQLChatMessageHistory(BaseChatMessageHistory):
+    """Chat message history stored in an SQL database."""
 
     def __init__(
         self,
-        *,
         session_id: str,
-        collection_name: str = DEFAULT_COLLECTION_NAME,
-        token: Optional[str] = None,
-        api_endpoint: Optional[str] = None,
-        astra_db_client: Optional[LibAstraDB] = None,  # type 'astrapy.db.AstraDB'
-        namespace: Optional[str] = None,
-    ) -> None:
-        """Create an Astra DB chat message history."""
-        try:
-            from astrapy.db import AstraDB as LibAstraDB
-        except (ImportError, ModuleNotFoundError):
-            raise ImportError(
-                "Could not import a recent astrapy python package. "
-                "Please install it with `pip install --upgrade astrapy`."
-            )
-
-        # Conflicting-arg checks:
-        if astra_db_client is not None:
-            if token is not None or api_endpoint is not None:
-                raise ValueError(
-                    "You cannot pass 'astra_db_client' to AstraDB if passing "
-                    "'token' and 'api_endpoint'."
-                )
+        connection_string: str,
+        table_name: str = "message_store",
+        session_id_field_name: str = "session_id",
+        custom_message_converter: Optional[BaseMessageConverter] = None,
+    ):
+        self.connection_string = connection_string
+        self.engine = create_engine(connection_string, echo=False)
+        self.session_id_field_name = session_id_field_name
+        self.converter = custom_message_converter or DefaultMessageConverter(table_name)
+        self.sql_model_class = self.converter.get_sql_model_class()
+        if not hasattr(self.sql_model_class, session_id_field_name):
+            raise ValueError("SQL model class must have session_id column")
+        self._create_table_if_not_exists()
 
         self.session_id = session_id
-        self.collection_name = collection_name
-        self.token = token
-        self.api_endpoint = api_endpoint
-        self.namespace = namespace
-        if astra_db_client is not None:
-            self.astra_db = astra_db_client
-        else:
-            self.astra_db = LibAstraDB(
-                token=self.token,
-                api_endpoint=self.api_endpoint,
-                namespace=self.namespace,
-            )
-        self.collection = self.astra_db.create_collection(self.collection_name)
+        self.Session = sessionmaker(self.engine)
+
+    def _create_table_if_not_exists(self) -> None:
+        self.sql_model_class.metadata.create_all(self.engine)
 
     @property
     def messages(self) -> List[BaseMessage]:  # type: ignore
-        """Retrieve all session messages from DB"""
-        message_blobs = [
-            doc["body_blob"]
-            for doc in sorted(
-                self.collection.paginated_find(
-                    filter={
-                        "session_id": self.session_id,
-                    },
-                    projection={
-                        "timestamp": 1,
-                        "body_blob": 1,
-                    },
-                ),
-                key=lambda _doc: _doc["timestamp"],
+        """Retrieve all messages from db"""
+        with self.Session() as session:
+            result = (
+                session.query(self.sql_model_class)
+                .where(
+                    getattr(self.sql_model_class, self.session_id_field_name)
+                    == self.session_id
+                )
+                .order_by(self.sql_model_class.id.asc())
             )
-        ]
-        items = [json.loads(message_blob) for message_blob in message_blobs]
-        messages = messages_from_dict(items)
-        return messages
+            messages = []
+            for record in result:
+                messages.append(self.converter.from_sql_model(record))
+            return messages
 
     def add_message(self, message: BaseMessage) -> None:
-        """Write a message to the table"""
-        self.collection.insert_one(
-            {
-                "timestamp": time.time(),
-                "session_id": self.session_id,
-                "body_blob": json.dumps(message_to_dict(message)),
-            }
-        )
+        """Append the message to the record in db"""
+        with self.Session() as session:
+            session.add(self.converter.to_sql_model(message, self.session_id))
+            session.commit()
 
     def clear(self) -> None:
-        """Clear session memory from DB"""
-        self.collection.delete_many(filter={"session_id": self.session_id})
+        """Clear session memory from db"""
+
+        with self.Session() as session:
+            session.query(self.sql_model_class).filter(
+                getattr(self.sql_model_class, self.session_id_field_name)
+                == self.session_id
+            ).delete()
+            session.commit()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/cassandra.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/cassandra.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 """Cassandra-based chat message history, based on cassIO."""
+
 from __future__ import annotations
 
 import json
-import typing
-from typing import List
+import uuid
+from typing import TYPE_CHECKING, List, Optional
 
-if typing.TYPE_CHECKING:
+if TYPE_CHECKING:
     from cassandra.cluster import Session
 
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
@@ -21,52 +22,71 @@
 
 class CassandraChatMessageHistory(BaseChatMessageHistory):
     """Chat message history that stores history in Cassandra.
 
     Args:
         session_id: arbitrary key that is used to store the messages
             of a single chat session.
-        session: a Cassandra `Session` object (an open DB connection)
-        keyspace: name of the keyspace to use.
+        session: Cassandra driver session. If not provided, it is resolved from cassio.
+        keyspace: Cassandra key space. If not provided, it is resolved from cassio.
         table_name: name of the table to use.
         ttl_seconds: time-to-live (seconds) for automatic expiration
             of stored entries. None (default) for no expiration.
     """
 
     def __init__(
         self,
         session_id: str,
-        session: Session,
-        keyspace: str,
+        session: Optional[Session] = None,
+        keyspace: Optional[str] = None,
         table_name: str = DEFAULT_TABLE_NAME,
-        ttl_seconds: typing.Optional[int] = DEFAULT_TTL_SECONDS,
+        ttl_seconds: Optional[int] = DEFAULT_TTL_SECONDS,
     ) -> None:
         try:
-            from cassio.history import StoredBlobHistory
+            from cassio.table import ClusteredCassandraTable
         except (ImportError, ModuleNotFoundError):
             raise ImportError(
                 "Could not import cassio python package. "
                 "Please install it with `pip install cassio`."
             )
         self.session_id = session_id
         self.ttl_seconds = ttl_seconds
-        self.blob_history = StoredBlobHistory(session, keyspace, table_name)
+        self.table = ClusteredCassandraTable(
+            session=session,
+            keyspace=keyspace,
+            table=table_name,
+            ttl_seconds=ttl_seconds,
+            primary_key_type=["TEXT", "TIMEUUID"],
+            ordering_in_partition="DESC",
+        )
 
     @property
     def messages(self) -> List[BaseMessage]:  # type: ignore
         """Retrieve all session messages from DB"""
-        message_blobs = self.blob_history.retrieve(
-            self.session_id,
-        )
+        # The latest are returned, in chronological order
+        message_blobs = [
+            row["body_blob"]
+            for row in self.table.get_partition(
+                partition_id=self.session_id,
+            )
+        ][::-1]
         items = [json.loads(message_blob) for message_blob in message_blobs]
         messages = messages_from_dict(items)
         return messages
 
     def add_message(self, message: BaseMessage) -> None:
-        """Write a message to the table"""
-        self.blob_history.store(
-            self.session_id, json.dumps(message_to_dict(message)), self.ttl_seconds
+        """Write a message to the table
+
+        Args:
+            message: A message to write.
+        """
+        this_row_id = uuid.uuid1()
+        self.table.put(
+            partition_id=self.session_id,
+            row_id=this_row_id,
+            body_blob=json.dumps(message_to_dict(message)),
+            ttl_seconds=self.ttl_seconds,
         )
 
     def clear(self) -> None:
         """Clear session memory from DB"""
-        self.blob_history.clear_session_id(self.session_id)
+        self.table.delete_partition(self.session_id)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/cosmos_db.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/cosmos_db.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Azure CosmosDB Memory History."""
+
 from __future__ import annotations
 
 import logging
 from types import TracebackType
 from typing import TYPE_CHECKING, Any, List, Optional, Type
 
 from langchain_core.chat_history import BaseChatMessageHistory
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/dynamodb.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/dynamodb.py`

 * *Files 18% similar despite different names*

```diff
@@ -34,25 +34,35 @@
             is optional, defaulting to "SessionId".
         key: an optional dictionary with a custom primary and secondary key.
             This argument is optional, but useful when using composite dynamodb keys, or
             isolating records based off of application details such as a user id.
             This may also contain global and local secondary index keys.
         kms_key_id: an optional AWS KMS Key ID, AWS KMS Key ARN, or AWS KMS Alias for
             client-side encryption
+        ttl: Optional Time-to-live (TTL) in seconds. Allows you to define a per-item
+            expiration timestamp that indicates when an item can be deleted from the
+            table. DynamoDB handles deletion of expired items without consuming
+            write throughput. To enable this feature on the table, follow the
+            [AWS DynamoDB documentation](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html)
+        history_size: Maximum number of messages to store. If None then there is no
+            limit. If not None then only the latest `history_size` messages are stored.
     """
 
     def __init__(
         self,
         table_name: str,
         session_id: str,
         endpoint_url: Optional[str] = None,
         primary_key_name: str = "SessionId",
         key: Optional[Dict[str, str]] = None,
         boto3_session: Optional[Session] = None,
         kms_key_id: Optional[str] = None,
+        ttl: Optional[int] = None,
+        ttl_key_name: str = "expireAt",
+        history_size: Optional[int] = None,
     ):
         if boto3_session:
             client = boto3_session.resource("dynamodb", endpoint_url=endpoint_url)
         else:
             try:
                 import boto3
             except ImportError as e:
@@ -62,14 +72,17 @@
             if endpoint_url:
                 client = boto3.resource("dynamodb", endpoint_url=endpoint_url)
             else:
                 client = boto3.resource("dynamodb")
         self.table = client.Table(table_name)
         self.session_id = session_id
         self.key: Dict = key or {primary_key_name: session_id}
+        self.ttl = ttl
+        self.ttl_key_name = ttl_key_name
+        self.history_size = history_size
 
         if kms_key_id:
             try:
                 from dynamodb_encryption_sdk.encrypted.table import EncryptedTable
                 from dynamodb_encryption_sdk.identifiers import CryptoAction
                 from dynamodb_encryption_sdk.material_providers.aws_kms import (
                     AwsKmsCryptographicMaterialsProvider,
@@ -90,15 +103,15 @@
                 table=self.table,
                 materials_provider=aws_kms_cmp,
                 attribute_actions=actions,
                 auto_refresh_table_indexes=False,
             )
 
     @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore
+    def messages(self) -> List[BaseMessage]:
         """Retrieve the messages from DynamoDB"""
         try:
             from botocore.exceptions import ClientError
         except ImportError as e:
             raise ImportError(
                 "Unable to import botocore, please install with `pip install botocore`."
             ) from e
@@ -116,29 +129,47 @@
             items = response["Item"]["History"]
         else:
             items = []
 
         messages = messages_from_dict(items)
         return messages
 
+    @messages.setter
+    def messages(self, messages: List[BaseMessage]) -> None:
+        raise NotImplementedError(
+            "Direct assignment to 'messages' is not allowed."
+            " Use the 'add_messages' instead."
+        )
+
     def add_message(self, message: BaseMessage) -> None:
         """Append the message to the record in DynamoDB"""
         try:
             from botocore.exceptions import ClientError
         except ImportError as e:
             raise ImportError(
                 "Unable to import botocore, please install with `pip install botocore`."
             ) from e
 
         messages = messages_to_dict(self.messages)
         _message = message_to_dict(message)
         messages.append(_message)
 
+        if self.history_size:
+            messages = messages[-self.history_size :]
+
         try:
-            self.table.put_item(Item={**self.key, "History": messages})
+            if self.ttl:
+                import time
+
+                expireAt = int(time.time()) + self.ttl
+                self.table.put_item(
+                    Item={**self.key, "History": messages, self.ttl_key_name: expireAt}
+                )
+            else:
+                self.table.put_item(Item={**self.key, "History": messages})
         except ClientError as err:
             logger.error(err)
 
     def clear(self) -> None:
         """Clear session memory from DynamoDB"""
         try:
             from botocore.exceptions import ClientError
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/elasticsearch.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/elasticsearch.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 import json
 import logging
 from time import time
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
+from langchain_core._api import deprecated
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
 )
 
 if TYPE_CHECKING:
     from elasticsearch import Elasticsearch
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated("0.0.27", alternative="Use langchain-elasticsearch package", pending=True)
 class ElasticsearchChatMessageHistory(BaseChatMessageHistory):
     """Chat message history that stores history in Elasticsearch.
 
     Args:
         es_url: URL of the Elasticsearch instance to connect to.
         es_cloud_id: Cloud ID of the Elasticsearch instance to connect to.
         es_user: Username to use when connecting to Elasticsearch.
@@ -43,15 +45,15 @@
         es_user: Optional[str] = None,
         es_api_key: Optional[str] = None,
         es_password: Optional[str] = None,
         esnsure_ascii: Optional[bool] = True,
     ):
         self.index: str = index
         self.session_id: str = session_id
-        self.ensure_ascii: bool = esnsure_ascii
+        self.ensure_ascii = esnsure_ascii
 
         # Initialize Elasticsearch client from passed client arg or connection info
         if es_connection is not None:
             self.client = es_connection.options(
                 headers={"user-agent": self.get_user_agent()}
             )
         elif es_url is not None or es_cloud_id is not None:
@@ -137,15 +139,15 @@
         except Exception as err:
             logger.error(f"Error connecting to Elasticsearch: {err}")
             raise err
 
         return es_client
 
     @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore[override]
+    def messages(self) -> List[BaseMessage]:
         """Retrieve the messages from Elasticsearch"""
         try:
             from elasticsearch import ApiError
 
             result = self.client.search(
                 index=self.index,
                 query={"term": {"session_id": self.session_id}},
@@ -161,27 +163,34 @@
                 for document in result["hits"]["hits"]
             ]
         else:
             items = []
 
         return messages_from_dict(items)
 
+    @messages.setter
+    def messages(self, messages: List[BaseMessage]) -> None:
+        raise NotImplementedError(
+            "Direct assignment to 'messages' is not allowed."
+            " Use the 'add_messages' instead."
+        )
+
     def add_message(self, message: BaseMessage) -> None:
         """Add a message to the chat session in Elasticsearch"""
         try:
             from elasticsearch import ApiError
 
             self.client.index(
                 index=self.index,
                 document={
                     "session_id": self.session_id,
                     "created_at": round(time() * 1000),
                     "history": json.dumps(
                         message_to_dict(message),
-                        ensure_ascii=self.ensure_ascii,
+                        ensure_ascii=bool(self.ensure_ascii),
                     ),
                 },
                 refresh=True,
             )
         except ApiError as err:
             logger.error(f"Could not add message to Elasticsearch: {err}")
             raise err
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/file.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/file.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,31 +1,26 @@
 import json
-import logging
 from pathlib import Path
 from typing import List
 
-from langchain_core.chat_history import BaseChatMessageHistory
-from langchain_core.messages import (
-    BaseMessage,
-    messages_from_dict,
-    messages_to_dict,
+from langchain_core.chat_history import (
+    BaseChatMessageHistory,
 )
-
-logger = logging.getLogger(__name__)
+from langchain_core.messages import BaseMessage, messages_from_dict, messages_to_dict
 
 
 class FileChatMessageHistory(BaseChatMessageHistory):
-    """
-    Chat message history that stores history in a local file.
+    """Chat message history that stores history in a local file."""
 
-    Args:
-        file_path: path of the local file to store the messages.
-    """
+    def __init__(self, file_path: str) -> None:
+        """Initialize the file path for the chat history.
 
-    def __init__(self, file_path: str):
+        Args:
+            file_path: The path to the local file to store the chat history.
+        """
         self.file_path = Path(file_path)
         if not self.file_path.exists():
             self.file_path.touch()
             self.file_path.write_text(json.dumps([]))
 
     @property
     def messages(self) -> List[BaseMessage]:  # type: ignore
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/firestore.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/firestore.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Firestore Chat Message History."""
+
 from __future__ import annotations
 
 import logging
 from typing import TYPE_CHECKING, List, Optional
 
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/momento.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/momento.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/mongodb.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/mongodb.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,41 +1,50 @@
 import json
 import logging
 from typing import List
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
 )
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_DBNAME = "chat_history"
 DEFAULT_COLLECTION_NAME = "message_store"
 
 
+@deprecated(
+    since="0.0.25",
+    removal="0.3.0",
+    alternative_import="langchain_mongodb.MongoDBChatMessageHistory",
+)
 class MongoDBChatMessageHistory(BaseChatMessageHistory):
     """Chat message history that stores history in MongoDB.
 
     Args:
         connection_string: connection string to connect to MongoDB
         session_id: arbitrary key that is used to store the messages
             of a single chat session.
         database_name: name of the database to use
         collection_name: name of the collection to use
+        create_index: whether to create an index with name SessionId. Set to False if
+            such an index already exists.
     """
 
     def __init__(
         self,
         connection_string: str,
         session_id: str,
         database_name: str = DEFAULT_DBNAME,
         collection_name: str = DEFAULT_COLLECTION_NAME,
+        create_index: bool = True,
     ):
         from pymongo import MongoClient, errors
 
         self.connection_string = connection_string
         self.session_id = session_id
         self.database_name = database_name
         self.collection_name = collection_name
@@ -43,15 +52,16 @@
         try:
             self.client: MongoClient = MongoClient(connection_string)
         except errors.ConnectionFailure as error:
             logger.error(error)
 
         self.db = self.client[database_name]
         self.collection = self.db[collection_name]
-        self.collection.create_index("SessionId")
+        if create_index:
+            self.collection.create_index("SessionId")
 
     @property
     def messages(self) -> List[BaseMessage]:  # type: ignore
         """Retrieve the messages from MongoDB"""
         from pymongo import errors
 
         try:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/neo4j.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/neo4j.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,71 +1,86 @@
 from typing import List, Optional, Union
 
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import BaseMessage, messages_from_dict
-from langchain_core.utils import get_from_env
+from langchain_core.utils import get_from_dict_or_env
+
+from langchain_community.graphs import Neo4jGraph
 
 
 class Neo4jChatMessageHistory(BaseChatMessageHistory):
     """Chat message history stored in a Neo4j database."""
 
     def __init__(
         self,
         session_id: Union[str, int],
         url: Optional[str] = None,
         username: Optional[str] = None,
         password: Optional[str] = None,
         database: str = "neo4j",
         node_label: str = "Session",
         window: int = 3,
+        *,
+        graph: Optional[Neo4jGraph] = None,
     ):
         try:
             import neo4j
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import neo4j python package. "
                 "Please install it with `pip install neo4j`."
             )
 
         # Make sure session id is not null
         if not session_id:
             raise ValueError("Please ensure that the session_id parameter is provided")
 
-        url = get_from_env("url", "NEO4J_URI", url)
-        username = get_from_env("username", "NEO4J_USERNAME", username)
-        password = get_from_env("password", "NEO4J_PASSWORD", password)
-        database = get_from_env("database", "NEO4J_DATABASE", database)
+        # Graph object takes precedent over env or input params
+        if graph:
+            self._driver = graph._driver
+            self._database = graph._database
+        else:
+            # Handle if the credentials are environment variables
+            url = get_from_dict_or_env({"url": url}, "url", "NEO4J_URI")
+            username = get_from_dict_or_env(
+                {"username": username}, "username", "NEO4J_USERNAME"
+            )
+            password = get_from_dict_or_env(
+                {"password": password}, "password", "NEO4J_PASSWORD"
+            )
+            database = get_from_dict_or_env(
+                {"database": database}, "database", "NEO4J_DATABASE", "neo4j"
+            )
 
-        self._driver = neo4j.GraphDatabase.driver(url, auth=(username, password))
-        self._database = database
+            self._driver = neo4j.GraphDatabase.driver(url, auth=(username, password))
+            self._database = database
+            # Verify connection
+            try:
+                self._driver.verify_connectivity()
+            except neo4j.exceptions.ServiceUnavailable:
+                raise ValueError(
+                    "Could not connect to Neo4j database. "
+                    "Please ensure that the url is correct"
+                )
+            except neo4j.exceptions.AuthError:
+                raise ValueError(
+                    "Could not connect to Neo4j database. "
+                    "Please ensure that the username and password are correct"
+                )
         self._session_id = session_id
         self._node_label = node_label
         self._window = window
-
-        # Verify connection
-        try:
-            self._driver.verify_connectivity()
-        except neo4j.exceptions.ServiceUnavailable:
-            raise ValueError(
-                "Could not connect to Neo4j database. "
-                "Please ensure that the url is correct"
-            )
-        except neo4j.exceptions.AuthError:
-            raise ValueError(
-                "Could not connect to Neo4j database. "
-                "Please ensure that the username and password are correct"
-            )
         # Create session node
         self._driver.execute_query(
             f"MERGE (s:`{self._node_label}` {{id:$session_id}})",
             {"session_id": self._session_id},
         ).summary
 
     @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore
+    def messages(self) -> List[BaseMessage]:
         """Retrieve the messages from Neo4j"""
         query = (
             f"MATCH (s:`{self._node_label}`)-[:LAST_MESSAGE]->(last_message) "
             "WHERE s.id = $session_id MATCH p=(last_message)<-[:NEXT*0.."
             f"{self._window*2}]-() WITH p, length(p) AS length "
             "ORDER BY length DESC LIMIT 1 UNWIND reverse(nodes(p)) AS node "
             "RETURN {data:{content: node.content}, type:node.type} AS result"
@@ -73,14 +88,21 @@
         records, _, _ = self._driver.execute_query(
             query, {"session_id": self._session_id}
         )
 
         messages = messages_from_dict([el["result"] for el in records])
         return messages
 
+    @messages.setter
+    def messages(self, messages: List[BaseMessage]) -> None:
+        raise NotImplementedError(
+            "Direct assignment to 'messages' is not allowed."
+            " Use the 'add_messages' instead."
+        )
+
     def add_message(self, message: BaseMessage) -> None:
         """Append the message to the record in Neo4j"""
         query = (
             f"MATCH (s:`{self._node_label}`) WHERE s.id = $session_id "
             "OPTIONAL MATCH (s)-[lm:LAST_MESSAGE]->(last_message) "
             "CREATE (s)-[:LAST_MESSAGE]->(new:Message) "
             "SET new += {type:$type, content:$content} "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/postgres.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/redis.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,82 +1,72 @@
 import json
 import logging
-from typing import List
+from typing import List, Optional
 
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
 )
 
-logger = logging.getLogger(__name__)
+from langchain_community.utilities.redis import get_client
 
-DEFAULT_CONNECTION_STRING = "postgresql://postgres:mypassword@localhost/chat_history"
+logger = logging.getLogger(__name__)
 
 
-class PostgresChatMessageHistory(BaseChatMessageHistory):
-    """Chat message history stored in a Postgres database."""
+class RedisChatMessageHistory(BaseChatMessageHistory):
+    """Chat message history stored in a Redis database."""
 
     def __init__(
         self,
         session_id: str,
-        connection_string: str = DEFAULT_CONNECTION_STRING,
-        table_name: str = "message_store",
+        url: str = "redis://localhost:6379/0",
+        key_prefix: str = "message_store:",
+        ttl: Optional[int] = None,
     ):
-        import psycopg
-        from psycopg.rows import dict_row
+        try:
+            import redis
+        except ImportError:
+            raise ImportError(
+                "Could not import redis python package. "
+                "Please install it with `pip install redis`."
+            )
 
         try:
-            self.connection = psycopg.connect(connection_string)
-            self.cursor = self.connection.cursor(row_factory=dict_row)
-        except psycopg.OperationalError as error:
+            self.redis_client = get_client(redis_url=url)
+        except redis.exceptions.ConnectionError as error:
             logger.error(error)
 
         self.session_id = session_id
-        self.table_name = table_name
+        self.key_prefix = key_prefix
+        self.ttl = ttl
 
-        self._create_table_if_not_exists()
-
-    def _create_table_if_not_exists(self) -> None:
-        create_table_query = f"""CREATE TABLE IF NOT EXISTS {self.table_name} (
-            id SERIAL PRIMARY KEY,
-            session_id TEXT NOT NULL,
-            message JSONB NOT NULL
-        );"""
-        self.cursor.execute(create_table_query)
-        self.connection.commit()
+    @property
+    def key(self) -> str:
+        """Construct the record key to use"""
+        return self.key_prefix + self.session_id
 
     @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore
-        """Retrieve the messages from PostgreSQL"""
-        query = (
-            f"SELECT message FROM {self.table_name} WHERE session_id = %s ORDER BY id;"
-        )
-        self.cursor.execute(query, (self.session_id,))
-        items = [record["message"] for record in self.cursor.fetchall()]
+    def messages(self) -> List[BaseMessage]:
+        """Retrieve the messages from Redis"""
+        _items = self.redis_client.lrange(self.key, 0, -1)
+        items = [json.loads(m.decode("utf-8")) for m in _items[::-1]]
         messages = messages_from_dict(items)
         return messages
 
-    def add_message(self, message: BaseMessage) -> None:
-        """Append the message to the record in PostgreSQL"""
-        from psycopg import sql
-
-        query = sql.SQL("INSERT INTO {} (session_id, message) VALUES (%s, %s);").format(
-            sql.Identifier(self.table_name)
-        )
-        self.cursor.execute(
-            query, (self.session_id, json.dumps(message_to_dict(message)))
+    @messages.setter
+    def messages(self, messages: List[BaseMessage]) -> None:
+        raise NotImplementedError(
+            "Direct assignment to 'messages' is not allowed."
+            " Use the 'add_messages' instead."
         )
-        self.connection.commit()
+
+    def add_message(self, message: BaseMessage) -> None:
+        """Append the message to the record in Redis"""
+        self.redis_client.lpush(self.key, json.dumps(message_to_dict(message)))
+        if self.ttl:
+            self.redis_client.expire(self.key, self.ttl)
 
     def clear(self) -> None:
-        """Clear session memory from PostgreSQL"""
-        query = f"DELETE FROM {self.table_name} WHERE session_id = %s;"
-        self.cursor.execute(query, (self.session_id,))
-        self.connection.commit()
-
-    def __del__(self) -> None:
-        if self.cursor:
-            self.cursor.close()
-        if self.connection:
-            self.connection.close()
+        """Clear session memory from Redis"""
+        self.redis_client.delete(self.key)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/redis.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/upstash_redis.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,61 +5,65 @@
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     BaseMessage,
     message_to_dict,
     messages_from_dict,
 )
 
-from langchain_community.utilities.redis import get_client
-
 logger = logging.getLogger(__name__)
 
 
-class RedisChatMessageHistory(BaseChatMessageHistory):
-    """Chat message history stored in a Redis database."""
+class UpstashRedisChatMessageHistory(BaseChatMessageHistory):
+    """Chat message history stored in an Upstash Redis database."""
 
     def __init__(
         self,
         session_id: str,
-        url: str = "redis://localhost:6379/0",
+        url: str = "",
+        token: str = "",
         key_prefix: str = "message_store:",
         ttl: Optional[int] = None,
     ):
         try:
-            import redis
+            from upstash_redis import Redis
         except ImportError:
             raise ImportError(
-                "Could not import redis python package. "
-                "Please install it with `pip install redis`."
+                "Could not import upstash redis python package. "
+                "Please install it with `pip install upstash_redis`."
+            )
+
+        if url == "" or token == "":
+            raise ValueError(
+                "UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN are needed."
             )
 
         try:
-            self.redis_client = get_client(redis_url=url)
-        except redis.exceptions.ConnectionError as error:
-            logger.error(error)
+            self.redis_client = Redis(url=url, token=token)
+        except Exception:
+            logger.error("Upstash Redis instance could not be initiated.")
 
         self.session_id = session_id
         self.key_prefix = key_prefix
         self.ttl = ttl
 
     @property
     def key(self) -> str:
         """Construct the record key to use"""
         return self.key_prefix + self.session_id
 
     @property
     def messages(self) -> List[BaseMessage]:  # type: ignore
-        """Retrieve the messages from Redis"""
+        """Retrieve the messages from Upstash Redis"""
         _items = self.redis_client.lrange(self.key, 0, -1)
-        items = [json.loads(m.decode("utf-8")) for m in _items[::-1]]
+        items = [json.loads(m) for m in _items[::-1]]
         messages = messages_from_dict(items)
         return messages
 
     def add_message(self, message: BaseMessage) -> None:
-        """Append the message to the record in Redis"""
+        """Append the message to the record in Upstash Redis"""
         self.redis_client.lpush(self.key, json.dumps(message_to_dict(message)))
         if self.ttl:
             self.redis_client.expire(self.key, self.ttl)
 
     def clear(self) -> None:
-        """Clear session memory from Redis"""
+        """Clear session memory from Upstash Redis"""
         self.redis_client.delete(self.key)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/rocksetdb.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/rocksetdb.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,15 @@
                 collection="langchain_demo",
                 sync=True
             )
 
             history.add_user_message("hi!")
             history.add_ai_message("whats up?")
 
-            print(history.messages)
+            print(history.messages)  # noqa: T201
     """
 
     # You should set these values based on your VI.
     # These values are configured for the typical
     # free VI. Read more about VIs here:
     # https://rockset.com/docs/instances
     SLEEP_INTERVAL_MS: int = 5
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/singlestoredb.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/singlestoredb.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/sql.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/tidb.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,140 +1,148 @@
 import json
 import logging
-from abc import ABC, abstractmethod
-from typing import Any, List, Optional
+from datetime import datetime
+from typing import List, Optional
 
-from sqlalchemy import Column, Integer, Text, create_engine
-
-try:
-    from sqlalchemy.orm import declarative_base
-except ImportError:
-    from sqlalchemy.ext.declarative import declarative_base
 from langchain_core.chat_history import BaseChatMessageHistory
-from langchain_core.messages import (
-    BaseMessage,
-    message_to_dict,
-    messages_from_dict,
-)
+from langchain_core.messages import BaseMessage, message_to_dict, messages_from_dict
+from sqlalchemy import create_engine, text
+from sqlalchemy.exc import SQLAlchemyError
 from sqlalchemy.orm import sessionmaker
 
 logger = logging.getLogger(__name__)
 
 
-class BaseMessageConverter(ABC):
-    """The class responsible for converting BaseMessage to your SQLAlchemy model."""
-
-    @abstractmethod
-    def from_sql_model(self, sql_message: Any) -> BaseMessage:
-        """Convert a SQLAlchemy model to a BaseMessage instance."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def to_sql_model(self, message: BaseMessage, session_id: str) -> Any:
-        """Convert a BaseMessage instance to a SQLAlchemy model."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def get_sql_model_class(self) -> Any:
-        """Get the SQLAlchemy model class."""
-        raise NotImplementedError
-
-
-def create_message_model(table_name, DynamicBase):  # type: ignore
+class TiDBChatMessageHistory(BaseChatMessageHistory):
     """
-    Create a message model for a given table name.
-
-    Args:
-        table_name: The name of the table to use.
-        DynamicBase: The base class to use for the model.
-
-    Returns:
-        The model class.
-
+    Represents a chat message history stored in a TiDB database.
     """
 
-    # Model decleared inside a function to have a dynamic table name
-    class Message(DynamicBase):
-        __tablename__ = table_name
-        id = Column(Integer, primary_key=True)
-        session_id = Column(Text)
-        message = Column(Text)
-
-    return Message
-
-
-class DefaultMessageConverter(BaseMessageConverter):
-    """The default message converter for SQLChatMessageHistory."""
-
-    def __init__(self, table_name: str):
-        self.model_class = create_message_model(table_name, declarative_base())
-
-    def from_sql_model(self, sql_message: Any) -> BaseMessage:
-        return messages_from_dict([json.loads(sql_message.message)])[0]
-
-    def to_sql_model(self, message: BaseMessage, session_id: str) -> Any:
-        return self.model_class(
-            session_id=session_id, message=json.dumps(message_to_dict(message))
-        )
-
-    def get_sql_model_class(self) -> Any:
-        return self.model_class
-
-
-class SQLChatMessageHistory(BaseChatMessageHistory):
-    """Chat message history stored in an SQL database."""
-
     def __init__(
         self,
         session_id: str,
         connection_string: str,
-        table_name: str = "message_store",
-        session_id_field_name: str = "session_id",
-        custom_message_converter: Optional[BaseMessageConverter] = None,
+        table_name: str = "langchain_message_store",
+        earliest_time: Optional[datetime] = None,
     ):
-        self.connection_string = connection_string
-        self.engine = create_engine(connection_string, echo=False)
-        self.session_id_field_name = session_id_field_name
-        self.converter = custom_message_converter or DefaultMessageConverter(table_name)
-        self.sql_model_class = self.converter.get_sql_model_class()
-        if not hasattr(self.sql_model_class, session_id_field_name):
-            raise ValueError("SQL model class must have session_id column")
-        self._create_table_if_not_exists()
+        """
+        Initializes a new instance of the TiDBChatMessageHistory class.
+
+        Args:
+            session_id (str): The ID of the chat session.
+            connection_string (str): The connection string for the TiDB database.
+                format: mysql+pymysql://<host>:<PASSWORD>@<host>:4000/<db>?ssl_ca=/etc/ssl/cert.pem&ssl_verify_cert=true&ssl_verify_identity=true
+            table_name (str, optional): the table name to store the chat messages.
+                Defaults to "langchain_message_store".
+            earliest_time (Optional[datetime], optional): The earliest time to retrieve messages from.
+                Defaults to None.
+        """  # noqa
 
         self.session_id = session_id
-        self.Session = sessionmaker(self.engine)
+        self.table_name = table_name
+        self.earliest_time = earliest_time
+        self.cache: List = []
+
+        # Set up SQLAlchemy engine and session
+        self.engine = create_engine(connection_string)
+        Session = sessionmaker(bind=self.engine)
+        self.session = Session()
+
+        self._create_table_if_not_exists()
+        self._load_messages_to_cache()
 
     def _create_table_if_not_exists(self) -> None:
-        self.sql_model_class.metadata.create_all(self.engine)
+        """
+        Creates a table if it does not already exist in the database.
+        """
+
+        create_table_query = text(
+            f"""
+            CREATE TABLE IF NOT EXISTS {self.table_name} (
+                id INT AUTO_INCREMENT PRIMARY KEY,
+                session_id VARCHAR(255) NOT NULL,
+                message JSON NOT NULL,
+                create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
+                INDEX session_idx (session_id)
+            );"""
+        )
+        try:
+            self.session.execute(create_table_query)
+            self.session.commit()
+        except SQLAlchemyError as e:
+            logger.error(f"Error creating table: {e}")
+            self.session.rollback()
+
+    def _load_messages_to_cache(self) -> None:
+        """
+        Loads messages from the database into the cache.
+
+        This method retrieves messages from the database table. The retrieved messages
+        are then stored in the cache for faster access.
+
+        Raises:
+            SQLAlchemyError: If there is an error executing the database query.
+
+        """
+        time_condition = (
+            f"AND create_time >= '{self.earliest_time}'" if self.earliest_time else ""
+        )
+        query = text(
+            f"""
+            SELECT message FROM {self.table_name} 
+            WHERE session_id = :session_id {time_condition} 
+            ORDER BY id;
+        """
+        )
+        try:
+            result = self.session.execute(query, {"session_id": self.session_id})
+            for record in result.fetchall():
+                message_dict = json.loads(record[0])
+                self.cache.append(messages_from_dict([message_dict])[0])
+        except SQLAlchemyError as e:
+            logger.error(f"Error loading messages to cache: {e}")
 
     @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore
-        """Retrieve all messages from db"""
-        with self.Session() as session:
-            result = (
-                session.query(self.sql_model_class)
-                .where(
-                    getattr(self.sql_model_class, self.session_id_field_name)
-                    == self.session_id
-                )
-                .order_by(self.sql_model_class.id.asc())
-            )
-            messages = []
-            for record in result:
-                messages.append(self.converter.from_sql_model(record))
-            return messages
+    def messages(self) -> List[BaseMessage]:  # type: ignore[override]
+        """returns all messages"""
+        if len(self.cache) == 0:
+            self.reload_cache()
+        return self.cache
 
     def add_message(self, message: BaseMessage) -> None:
-        """Append the message to the record in db"""
-        with self.Session() as session:
-            session.add(self.converter.to_sql_model(message, self.session_id))
-            session.commit()
+        """adds a message to the database and cache"""
+        query = text(
+            f"INSERT INTO {self.table_name} (session_id, message) VALUES (:session_id, :message);"  # noqa
+        )
+        try:
+            self.session.execute(
+                query,
+                {
+                    "session_id": self.session_id,
+                    "message": json.dumps(message_to_dict(message)),
+                },
+            )
+            self.session.commit()
+            self.cache.append(message)
+        except SQLAlchemyError as e:
+            logger.error(f"Error adding message: {e}")
+            self.session.rollback()
 
     def clear(self) -> None:
-        """Clear session memory from db"""
-
-        with self.Session() as session:
-            session.query(self.sql_model_class).filter(
-                getattr(self.sql_model_class, self.session_id_field_name)
-                == self.session_id
-            ).delete()
-            session.commit()
+        """clears all messages"""
+        query = text(f"DELETE FROM {self.table_name} WHERE session_id = :session_id;")
+        try:
+            self.session.execute(query, {"session_id": self.session_id})
+            self.session.commit()
+            self.cache.clear()
+        except SQLAlchemyError as e:
+            logger.error(f"Error clearing messages: {e}")
+            self.session.rollback()
+
+    def reload_cache(self) -> None:
+        """reloads messages from database to cache"""
+        self.cache.clear()
+        self._load_messages_to_cache()
+
+    def __del__(self) -> None:
+        """closes the session"""
+        self.session.close()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/upstash_redis.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/streamlit.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,69 +1,47 @@
-import json
-import logging
-from typing import List, Optional
+from typing import List
 
 from langchain_core.chat_history import BaseChatMessageHistory
-from langchain_core.messages import (
-    BaseMessage,
-    message_to_dict,
-    messages_from_dict,
-)
-
-logger = logging.getLogger(__name__)
-
-
-class UpstashRedisChatMessageHistory(BaseChatMessageHistory):
-    """Chat message history stored in an Upstash Redis database."""
-
-    def __init__(
-        self,
-        session_id: str,
-        url: str = "",
-        token: str = "",
-        key_prefix: str = "message_store:",
-        ttl: Optional[int] = None,
-    ):
+from langchain_core.messages import BaseMessage
+
+
+class StreamlitChatMessageHistory(BaseChatMessageHistory):
+    """
+    Chat message history that stores messages in Streamlit session state.
+
+    Args:
+        key: The key to use in Streamlit session state for storing messages.
+    """
+
+    def __init__(self, key: str = "langchain_messages"):
         try:
-            from upstash_redis import Redis
-        except ImportError:
+            import streamlit as st
+        except ImportError as e:
             raise ImportError(
-                "Could not import upstash redis python package. "
-                "Please install it with `pip install upstash_redis`."
-            )
-
-        if url == "" or token == "":
-            raise ValueError(
-                "UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN are needed."
-            )
+                "Unable to import streamlit, please run `pip install streamlit`."
+            ) from e
 
-        try:
-            self.redis_client = Redis(url=url, token=token)
-        except Exception:
-            logger.error("Upstash Redis instance could not be initiated.")
-
-        self.session_id = session_id
-        self.key_prefix = key_prefix
-        self.ttl = ttl
+        if key not in st.session_state:
+            st.session_state[key] = []
+        self._messages = st.session_state[key]
+        self._key = key
 
     @property
-    def key(self) -> str:
-        """Construct the record key to use"""
-        return self.key_prefix + self.session_id
+    def messages(self) -> List[BaseMessage]:
+        """Retrieve the current list of messages"""
+        return self._messages
+
+    @messages.setter
+    def messages(self, value: List[BaseMessage]) -> None:
+        """Set the messages list with a new value"""
+        import streamlit as st
 
-    @property
-    def messages(self) -> List[BaseMessage]:  # type: ignore
-        """Retrieve the messages from Upstash Redis"""
-        _items = self.redis_client.lrange(self.key, 0, -1)
-        items = [json.loads(m) for m in _items[::-1]]
-        messages = messages_from_dict(items)
-        return messages
+        st.session_state[self._key] = value
+        self._messages = st.session_state[self._key]
 
     def add_message(self, message: BaseMessage) -> None:
-        """Append the message to the record in Upstash Redis"""
-        self.redis_client.lpush(self.key, json.dumps(message_to_dict(message)))
-        if self.ttl:
-            self.redis_client.expire(self.key, self.ttl)
+        """Add a message to the session memory"""
+        self.messages.append(message)
 
     def clear(self) -> None:
-        """Clear session memory from Upstash Redis"""
-        self.redis_client.delete(self.key)
+        """Clear session memory"""
+        self.messages.clear()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/xata.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/xata.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,17 +19,17 @@
         api_key: str,
         branch_name: str = "main",
         table_name: str = "messages",
         create_table: bool = True,
     ) -> None:
         """Initialize with Xata client."""
         try:
-            from xata.client import XataClient  # noqa: F401
+            from xata.client import XataClient
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import xata python package. "
                 "Please install it with `pip install xata`."
             )
         self._client = XataClient(
             api_key=api_key, db_url=db_url, branch_name=branch_name
         )
         self._table_name = table_name
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_message_histories/zep.py` & `gigachain_community-0.2.0/langchain_community/chat_message_histories/zep.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 import logging
-from typing import TYPE_CHECKING, Any, Dict, List, Optional
+from enum import Enum
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence
 
 from langchain_core.chat_history import BaseChatMessageHistory
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
     HumanMessage,
     SystemMessage,
@@ -13,14 +14,32 @@
 
 if TYPE_CHECKING:
     from zep_python import Memory, MemorySearchResult, Message, NotFoundError
 
 logger = logging.getLogger(__name__)
 
 
+class SearchScope(str, Enum):
+    """Scope for the document search. Messages or Summaries?"""
+
+    messages = "messages"
+    """Search chat history messages."""
+    summary = "summary"
+    """Search chat history summaries."""
+
+
+class SearchType(str, Enum):
+    """Enumerator of the types of search to perform."""
+
+    similarity = "similarity"
+    """Similarity search."""
+    mmr = "mmr"
+    """Maximal Marginal Relevance reranking of similarity search."""
+
+
 class ZepChatMessageHistory(BaseChatMessageHistory):
     """Chat message history that uses Zep as a backend.
 
     Recommended usage::
 
         # Set up Zep Chat History
         zep_chat_history = ZepChatMessageHistory(
@@ -126,26 +145,26 @@
         except NotFoundError:
             logger.warning(
                 f"Session {self.session_id} not found in Zep. Returning None"
             )
             return None
         return zep_memory
 
-    def add_user_message(
+    def add_user_message(  # type: ignore[override]
         self, message: str, metadata: Optional[Dict[str, Any]] = None
     ) -> None:
         """Convenience method for adding a human message string to the store.
 
         Args:
             message: The string contents of a human message.
             metadata: Optional metadata to attach to the message.
         """
         self.add_message(HumanMessage(content=message), metadata=metadata)
 
-    def add_ai_message(
+    def add_ai_message(  # type: ignore[override]
         self, message: str, metadata: Optional[Dict[str, Any]] = None
     ) -> None:
         """Convenience method for adding an AI message string to the store.
 
         Args:
             message: The string contents of an AI message.
             metadata: Optional metadata to attach to the message.
@@ -161,22 +180,64 @@
         zep_message = Message(
             content=message.content, role=message.type, metadata=metadata
         )
         zep_memory = Memory(messages=[zep_message])
 
         self.zep_client.memory.add_memory(self.session_id, zep_memory)
 
+    def add_messages(self, messages: Sequence[BaseMessage]) -> None:
+        """Append the messages to the Zep memory history"""
+        from zep_python import Memory, Message
+
+        zep_messages = [
+            Message(
+                content=message.content,
+                role=message.type,
+                metadata=message.additional_kwargs.get("metadata", None),
+            )
+            for message in messages
+        ]
+        zep_memory = Memory(messages=zep_messages)
+
+        self.zep_client.memory.add_memory(self.session_id, zep_memory)
+
+    async def aadd_messages(self, messages: Sequence[BaseMessage]) -> None:
+        """Append the messages to the Zep memory history asynchronously"""
+        from zep_python import Memory, Message
+
+        zep_messages = [
+            Message(
+                content=message.content,
+                role=message.type,
+                metadata=message.additional_kwargs.get("metadata", None),
+            )
+            for message in messages
+        ]
+        zep_memory = Memory(messages=zep_messages)
+
+        await self.zep_client.memory.aadd_memory(self.session_id, zep_memory)
+
     def search(
-        self, query: str, metadata: Optional[Dict] = None, limit: Optional[int] = None
+        self,
+        query: str,
+        metadata: Optional[Dict] = None,
+        search_scope: SearchScope = SearchScope.messages,
+        search_type: SearchType = SearchType.similarity,
+        mmr_lambda: Optional[float] = None,
+        limit: Optional[int] = None,
     ) -> List[MemorySearchResult]:
         """Search Zep memory for messages matching the query"""
         from zep_python import MemorySearchPayload
 
-        payload: MemorySearchPayload = MemorySearchPayload(
-            text=query, metadata=metadata
+        payload = MemorySearchPayload(
+            text=query,
+            metadata=metadata,
+            search_scope=search_scope,
+            search_type=search_type,
+            mmr_lambda=mmr_lambda,
         )
 
         return self.zep_client.memory.search_memory(
             self.session_id, payload, limit=limit
         )
 
     def clear(self) -> None:
@@ -185,7 +246,19 @@
         """
         try:
             self.zep_client.memory.delete_memory(self.session_id)
         except NotFoundError:
             logger.warning(
                 f"Session {self.session_id} not found in Zep. Skipping delete."
             )
+
+    async def aclear(self) -> None:
+        """Clear session memory from Zep asynchronously.
+        Note that Zep is long-term storage for memory and this is not advised
+        unless you have specific data retention requirements.
+        """
+        try:
+            await self.zep_client.memory.adelete_memory(self.session_id)
+        except NotFoundError:
+            logger.warning(
+                f"Session {self.session_id} not found in Zep. Skipping delete."
+            )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/anthropic.py` & `gigachain_community-0.2.0/langchain_community/chat_models/anthropic.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, cast
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     agenerate_from_stream,
@@ -66,14 +67,19 @@
         for message in messages
     )
 
     # trim off the trailing ' ' that might come from the "Assistant: "
     return text.rstrip()
 
 
+@deprecated(
+    since="0.0.28",
+    removal="0.3",
+    alternative_import="langchain_anthropic.ChatAnthropic",
+)
 class ChatAnthropic(BaseChatModel, _AnthropicCommon):
     """`Anthropic` chat large language models.
 
     To use, you should have the ``anthropic`` python package installed, and the
     environment variable ``ANTHROPIC_API_KEY`` set with your API key, or pass
     it as a named parameter to the constructor.
 
@@ -138,17 +144,18 @@
         params: Dict[str, Any] = {"prompt": prompt, **self._default_params, **kwargs}
         if stop:
             params["stop_sequences"] = stop
 
         stream_resp = self.client.completions.create(**params, stream=True)
         for data in stream_resp:
             delta = data.completion
-            yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+            chunk = ChatGenerationChunk(message=AIMessageChunk(content=delta))
             if run_manager:
-                run_manager.on_llm_new_token(delta)
+                run_manager.on_llm_new_token(delta, chunk=chunk)
+            yield chunk
 
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -157,17 +164,18 @@
         params: Dict[str, Any] = {"prompt": prompt, **self._default_params, **kwargs}
         if stop:
             params["stop_sequences"] = stop
 
         stream_resp = await self.async_client.completions.create(**params, stream=True)
         async for data in stream_resp:
             delta = data.completion
-            yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+            chunk = ChatGenerationChunk(message=AIMessageChunk(content=delta))
             if run_manager:
-                await run_manager.on_llm_new_token(delta)
+                await run_manager.on_llm_new_token(delta, chunk=chunk)
+            yield chunk
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/anyscale.py` & `gigachain_community-0.2.0/langchain_community/chat_models/anyscale.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Anyscale Endpoints chat wrapper. Relies heavily on ChatOpenAI."""
+
 from __future__ import annotations
 
 import logging
 import os
 import sys
 from typing import TYPE_CHECKING, Dict, Optional, Set
 
@@ -19,15 +20,14 @@
 from langchain_community.utils.openai import is_openai_v1
 
 if TYPE_CHECKING:
     import tiktoken
 
 logger = logging.getLogger(__name__)
 
-
 DEFAULT_API_BASE = "https://api.endpoints.anyscale.com/v1"
 DEFAULT_MODEL = "meta-llama/Llama-2-7b-chat-hf"
 
 
 class ChatAnyscale(ChatOpenAI):
     """`Anyscale` Chat large language models.
 
@@ -56,15 +56,15 @@
     def lc_secrets(self) -> Dict[str, str]:
         return {"anyscale_api_key": "ANYSCALE_API_KEY"}
 
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return False
 
-    anyscale_api_key: SecretStr
+    anyscale_api_key: SecretStr = Field(default=None)
     """AnyScale Endpoints API keys."""
     model_name: str = Field(default=DEFAULT_MODEL, alias="model")
     """Model name to use."""
     anyscale_api_base: str = Field(default=DEFAULT_API_BASE)
     """Base URL path for API requests,
     leave blank if not using a proxy or service emulator."""
     anyscale_proxy: Optional[str] = None
@@ -98,30 +98,25 @@
             raise ValueError(
                 f"Error getting models from {models_url}: "
                 f"{models_response.status_code}",
             )
 
         return {model["id"] for model in models_response.json()["data"]}
 
-    @root_validator(pre=True)
-    def validate_environment_override(cls, values: dict) -> dict:
+    @root_validator()
+    def validate_environment(cls, values: dict) -> dict:
         """Validate that api key and python package exists in environment."""
-        values["openai_api_key"] = get_from_dict_or_env(
-            values,
-            "anyscale_api_key",
-            "ANYSCALE_API_KEY",
-        )
         values["anyscale_api_key"] = convert_to_secret_str(
             get_from_dict_or_env(
                 values,
                 "anyscale_api_key",
                 "ANYSCALE_API_KEY",
             )
         )
-        values["openai_api_base"] = get_from_dict_or_env(
+        values["anyscale_api_base"] = get_from_dict_or_env(
             values,
             "anyscale_api_base",
             "ANYSCALE_API_BASE",
             default=DEFAULT_API_BASE,
         )
         values["openai_proxy"] = get_from_dict_or_env(
             values,
@@ -129,49 +124,55 @@
             "ANYSCALE_PROXY",
             default="",
         )
         try:
             import openai
 
         except ImportError as e:
-            raise ValueError(
+            raise ImportError(
                 "Could not import openai python package. "
                 "Please install it with `pip install openai`.",
             ) from e
         try:
             if is_openai_v1():
                 client_params = {
-                    "api_key": values["openai_api_key"],
-                    "base_url": values["openai_api_base"],
+                    "api_key": values["anyscale_api_key"].get_secret_value(),
+                    "base_url": values["anyscale_api_base"],
                     # To do: future support
                     # "organization": values["openai_organization"],
                     # "timeout": values["request_timeout"],
                     # "max_retries": values["max_retries"],
                     # "default_headers": values["default_headers"],
                     # "default_query": values["default_query"],
                     # "http_client": values["http_client"],
                 }
-                values["client"] = openai.OpenAI(**client_params).chat.completions
+                if not values.get("client"):
+                    values["client"] = openai.OpenAI(**client_params).chat.completions
+                if not values.get("async_client"):
+                    values["async_client"] = openai.AsyncOpenAI(
+                        **client_params
+                    ).chat.completions
             else:
+                values["openai_api_base"] = values["anyscale_api_base"]
+                values["openai_api_key"] = values["anyscale_api_key"].get_secret_value()
                 values["client"] = openai.ChatCompletion
         except AttributeError as exc:
             raise ValueError(
                 "`openai` has no `ChatCompletion` attribute, this is likely "
                 "due to an old version of the openai package. Try upgrading it "
                 "with `pip install --upgrade openai`.",
             ) from exc
 
         if "model_name" not in values.keys():
             values["model_name"] = DEFAULT_MODEL
 
         model_name = values["model_name"]
-
         available_models = cls.get_available_models(
-            values["openai_api_key"],
-            values["openai_api_base"],
+            values["anyscale_api_key"].get_secret_value(),
+            values["anyscale_api_base"],
         )
 
         if model_name not in available_models:
             raise ValueError(
                 f"Model name {model_name} not found in available models: "
                 f"{available_models}.",
             )
@@ -193,17 +194,16 @@
             logger.warning("Warning: model not found. Using cl100k_base encoding.")
             model = "cl100k_base"
             encoding = tiktoken_.get_encoding(model)
         return model, encoding
 
     def get_num_tokens_from_messages(self, messages: list[BaseMessage]) -> int:
         """Calculate num tokens with tiktoken package.
-
-        Official documentation: https://github.com/openai/openai-cookbook/blob/
-        main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb"""
+        Official documentation: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb
+        """
         if sys.version_info[1] <= 7:
             return super().get_num_tokens_from_messages(messages)
         model, encoding = self._get_encoding_model()
         tokens_per_message = 3
         tokens_per_name = 1
         num_tokens = 0
         messages_dict = [convert_message_to_dict(m) for m in messages]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/azure_openai.py` & `gigachain_community-0.2.0/langchain_community/chat_models/azure_openai.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,32 @@
 """Azure OpenAI chat wrapper."""
+
 from __future__ import annotations
 
 import logging
 import os
 import warnings
 from typing import Any, Callable, Dict, List, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.outputs import ChatResult
 from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.chat_models.openai import ChatOpenAI
 from langchain_community.utils.openai import is_openai_v1
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.10",
+    removal="0.3.0",
+    alternative_import="langchain_openai.AzureChatOpenAI",
+)
 class AzureChatOpenAI(ChatOpenAI):
     """`Azure OpenAI` Chat Completion API.
 
     To use this class you
     must have a deployed model on Azure OpenAI. Use `deployment_name` in the
     constructor to refer to the "Model deployment name" in the Azure portal.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/azureml_endpoint.py` & `gigachain_community-0.2.0/langchain_community/chat_models/dappier.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,169 +1,161 @@
-import json
-from typing import Any, Dict, List, Optional, cast
+from typing import Any, Dict, List, Optional, Union
 
-from langchain_core.callbacks import CallbackManagerForLLMRun
-from langchain_core.language_models.chat_models import SimpleChatModel
+from aiohttp import ClientSession
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForLLMRun,
+    CallbackManagerForLLMRun,
+)
+from langchain_core.language_models.chat_models import (
+    BaseChatModel,
+)
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
-    ChatMessage,
-    HumanMessage,
-    SystemMessage,
 )
-from langchain_core.pydantic_v1 import SecretStr, validator
+from langchain_core.outputs import ChatGeneration, ChatResult
+from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
-from langchain_community.llms.azureml_endpoint import (
-    AzureMLEndpointClient,
-    ContentFormatterBase,
-)
+from langchain_community.utilities.requests import Requests
 
 
-class LlamaContentFormatter(ContentFormatterBase):
-    """Content formatter for `LLaMA`."""
+def _format_dappier_messages(
+    messages: List[BaseMessage],
+) -> List[Dict[str, Union[str, List[Union[str, Dict[Any, Any]]]]]]:
+    formatted_messages = []
 
-    SUPPORTED_ROLES: List[str] = ["user", "assistant", "system"]
+    for message in messages:
+        if message.type == "human":
+            formatted_messages.append({"role": "user", "content": message.content})
+        elif message.type == "system":
+            formatted_messages.append({"role": "system", "content": message.content})
 
-    @staticmethod
-    def _convert_message_to_dict(message: BaseMessage) -> Dict:
-        """Converts message to a dict according to role"""
-        content = cast(str, message.content)
-        if isinstance(message, HumanMessage):
-            return {
-                "role": "user",
-                "content": ContentFormatterBase.escape_special_characters(content),
-            }
-        elif isinstance(message, AIMessage):
-            return {
-                "role": "assistant",
-                "content": ContentFormatterBase.escape_special_characters(content),
-            }
-        elif isinstance(message, SystemMessage):
-            return {
-                "role": "system",
-                "content": ContentFormatterBase.escape_special_characters(content),
-            }
-        elif (
-            isinstance(message, ChatMessage)
-            and message.role in LlamaContentFormatter.SUPPORTED_ROLES
-        ):
-            return {
-                "role": message.role,
-                "content": ContentFormatterBase.escape_special_characters(content),
-            }
-        else:
-            supported = ",".join(
-                [role for role in LlamaContentFormatter.SUPPORTED_ROLES]
-            )
-            raise ValueError(
-                f"""Received unsupported role. 
-                Supported roles for the LLaMa Foundation Model: {supported}"""
-            )
-
-    def _format_request_payload(
-        self, messages: List[BaseMessage], model_kwargs: Dict
-    ) -> bytes:
-        chat_messages = [
-            LlamaContentFormatter._convert_message_to_dict(message)
-            for message in messages
-        ]
-        prompt = json.dumps(
-            {"input_data": {"input_string": chat_messages, "parameters": model_kwargs}}
-        )
-        return self.format_request_payload(prompt=prompt, model_kwargs=model_kwargs)
+    return formatted_messages
 
-    def format_request_payload(self, prompt: str, model_kwargs: Dict) -> bytes:
-        """Formats the request according to the chosen api"""
-        return str.encode(prompt)
 
-    def format_response_payload(self, output: bytes) -> str:
-        """Formats response"""
-        return json.loads(output)["output"]
+class ChatDappierAI(BaseChatModel):
+    """`Dappier` chat large language models.
 
+    `Dappier` is a platform enabling access to diverse, real-time data models.
+    Enhance your AI applications with Dappier's pre-trained, LLM-ready data models
+    and ensure accurate, current responses with reduced inaccuracies.
 
-class AzureMLChatOnlineEndpoint(SimpleChatModel):
-    """`AzureML` Chat models API.
+    To use one of our Dappier AI Data Models, you will need an API key.
+    Please visit Dappier Platform (https://platform.dappier.com/) to log in
+    and create an API key in your profile.
 
     Example:
         .. code-block:: python
 
-            azure_chat = AzureMLChatOnlineEndpoint(
-                endpoint_url="https://<your-endpoint>.<your_region>.inference.ml.azure.com/score",
-                endpoint_api_key="my-api-key",
-                content_formatter=content_formatter,
-            )
-    """
-
-    endpoint_url: str = ""
-    """URL of pre-existing Endpoint. Should be passed to constructor or specified as 
-        env var `AZUREML_ENDPOINT_URL`."""
-
-    endpoint_api_key: SecretStr = convert_to_secret_str("")
-    """Authentication Key for Endpoint. Should be passed to constructor or specified as
-        env var `AZUREML_ENDPOINT_API_KEY`."""
-
-    http_client: Any = None  #: :meta private:
-
-    content_formatter: Any = None
-    """The content formatter that provides an input and output
-    transform function to handle formats between the LLM and
-    the endpoint"""
-
-    model_kwargs: Optional[dict] = None
-    """Keyword arguments to pass to the model."""
-
-    @validator("http_client", always=True, allow_reuse=True)
-    @classmethod
-    def validate_client(cls, field_value: Any, values: Dict) -> AzureMLEndpointClient:
-        """Validate that api key and python package exist in environment."""
-        values["endpoint_api_key"] = convert_to_secret_str(
-            get_from_dict_or_env(values, "endpoint_api_key", "AZUREML_ENDPOINT_API_KEY")
-        )
-        endpoint_url = get_from_dict_or_env(
-            values, "endpoint_url", "AZUREML_ENDPOINT_URL"
-        )
-        http_client = AzureMLEndpointClient(
-            endpoint_url, values["endpoint_api_key"].get_secret_value()
+            from langchain_community.chat_models import ChatDappierAI
+            from langchain_core.messages import HumanMessage
+
+            # Initialize `ChatDappierAI` with the desired configuration
+            chat = ChatDappierAI(
+                dappier_endpoint="https://api.dappier.com/app/datamodel/dm_01hpsxyfm2fwdt2zet9cg6fdxt",
+                dappier_api_key="<YOUR_KEY>")
+
+            # Create a list of messages to interact with the model
+            messages = [HumanMessage(content="hello")]
+
+            # Invoke the model with the provided messages
+            chat.invoke(messages)
+
+
+    you can find more details here : https://docs.dappier.com/introduction"""
+
+    dappier_endpoint: str = "https://api.dappier.com/app/datamodelconversation"
+
+    dappier_model: str = "dm_01hpsxyfm2fwdt2zet9cg6fdxt"
+
+    dappier_api_key: Optional[SecretStr] = Field(None, description="Dappier API Token")
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        extra = Extra.forbid
+
+    @root_validator()
+    def validate_environment(cls, values: Dict) -> Dict:
+        """Validate that api key exists in environment."""
+        values["dappier_api_key"] = convert_to_secret_str(
+            get_from_dict_or_env(values, "dappier_api_key", "DAPPIER_API_KEY")
         )
-        return http_client
+        return values
 
-    @property
-    def _identifying_params(self) -> Dict[str, Any]:
-        """Get the identifying parameters."""
-        _model_kwargs = self.model_kwargs or {}
-        return {
-            **{"model_kwargs": _model_kwargs},
-        }
+    @staticmethod
+    def get_user_agent() -> str:
+        from langchain_community import __version__
+
+        return f"langchain/{__version__}"
 
     @property
     def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "azureml_chat_endpoint"
+        """Return type of chat model."""
+        return "dappier-realtimesearch-chat"
+
+    @property
+    def _api_key(self) -> str:
+        if self.dappier_api_key:
+            return self.dappier_api_key.get_secret_value()
+        return ""
 
-    def _call(
+    def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> str:
-        """Call out to an AzureML Managed Online endpoint.
-        Args:
-            messages: The messages in the conversation with the chat model.
-            stop: Optional list of stop words to use when generating.
-        Returns:
-            The string generated by the model.
-        Example:
-            .. code-block:: python
-                response = azureml_model("Tell me a joke.")
-        """
-        _model_kwargs = self.model_kwargs or {}
+    ) -> ChatResult:
+        url = f"{self.dappier_endpoint}"
+        headers = {
+            "Authorization": f"Bearer {self._api_key}",
+            "User-Agent": self.get_user_agent(),
+        }
+        user_query = _format_dappier_messages(messages=messages)
+        payload: Dict[str, Any] = {
+            "model": self.dappier_model,
+            "conversation": user_query,
+        }
 
-        request_payload = self.content_formatter._format_request_payload(
-            messages, _model_kwargs
-        )
-        response_payload = self.http_client.call(request_payload, **kwargs)
-        generated_text = self.content_formatter.format_response_payload(
-            response_payload
+        request = Requests(headers=headers)
+        response = request.post(url=url, data=payload)
+        response.raise_for_status()
+
+        data = response.json()
+
+        message_response = data["message"]
+
+        return ChatResult(
+            generations=[ChatGeneration(message=AIMessage(content=message_response))]
         )
-        return generated_text
+
+    async def _agenerate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> ChatResult:
+        url = f"{self.dappier_endpoint}"
+        headers = {
+            "Authorization": f"Bearer {self._api_key}",
+            "User-Agent": self.get_user_agent(),
+        }
+        user_query = _format_dappier_messages(messages=messages)
+        payload: Dict[str, Any] = {
+            "model": self.dappier_model,
+            "conversation": user_query,
+        }
+
+        async with ClientSession() as session:
+            async with session.post(url, json=payload, headers=headers) as response:
+                response.raise_for_status()
+                data = await response.json()
+                message_response = data["message"]
+
+                return ChatResult(
+                    generations=[
+                        ChatGeneration(message=AIMessage(content=message_response))
+                    ]
+                )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/baichuan.py` & `gigachain_community-0.2.0/langchain_community/chat_models/baichuan.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,9 @@
-import hashlib
 import json
 import logging
-import time
 from typing import Any, Dict, Iterator, List, Mapping, Optional, Type
 
 import requests
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     generate_from_stream,
@@ -26,15 +24,15 @@
     convert_to_secret_str,
     get_from_dict_or_env,
     get_pydantic_field_names,
 )
 
 logger = logging.getLogger(__name__)
 
-DEFAULT_API_BASE = "https://api.baichuan-ai.com/v1"
+DEFAULT_API_BASE = "https://api.baichuan-ai.com/v1/chat/completions"
 
 
 def _convert_message_to_dict(message: BaseMessage) -> dict:
     message_dict: Dict[str, Any]
     if isinstance(message, ChatMessage):
         message_dict = {"role": message.role, "content": message.content}
     elif isinstance(message, HumanMessage):
@@ -64,58 +62,49 @@
     content = _dict.get("content") or ""
 
     if role == "user" or default_class == HumanMessageChunk:
         return HumanMessageChunk(content=content)
     elif role == "assistant" or default_class == AIMessageChunk:
         return AIMessageChunk(content=content)
     elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
     else:
-        return default_class(content=content)
-
-
-# signature generation
-def _signature(secret_key: SecretStr, payload: Dict[str, Any], timestamp: int) -> str:
-    input_str = secret_key.get_secret_value() + json.dumps(payload) + str(timestamp)
-    md5 = hashlib.md5()
-    md5.update(input_str.encode("utf-8"))
-    return md5.hexdigest()
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
 class ChatBaichuan(BaseChatModel):
     """Baichuan chat models API by Baichuan Intelligent Technology.
 
     For more information, see https://platform.baichuan-ai.com/docs/api
     """
 
     @property
     def lc_secrets(self) -> Dict[str, str]:
         return {
             "baichuan_api_key": "BAICHUAN_API_KEY",
-            "baichuan_secret_key": "BAICHUAN_SECRET_KEY",
         }
 
     @property
     def lc_serializable(self) -> bool:
         return True
 
     baichuan_api_base: str = Field(default=DEFAULT_API_BASE)
     """Baichuan custom endpoints"""
-    baichuan_api_key: Optional[SecretStr] = None
+    baichuan_api_key: Optional[SecretStr] = Field(default=None, alias="api_key")
     """Baichuan API Key"""
     baichuan_secret_key: Optional[SecretStr] = None
-    """Baichuan Secret Key"""
+    """[DEPRECATED, keeping it for for backward compatibility] Baichuan Secret Key"""
     streaming: bool = False
     """Whether to stream the results or not."""
-    request_timeout: int = 60
+    request_timeout: int = Field(default=60, alias="timeout")
     """request timeout for chat http requests"""
-
-    model = "Baichuan2-53B"
-    """model name of Baichuan, default is `Baichuan2-53B`."""
-    temperature: float = 0.3
+    model = "Baichuan2-Turbo-192K"
+    """model name of Baichuan, default is `Baichuan2-Turbo-192K`,
+    other options include `Baichuan2-Turbo`"""
+    temperature: Optional[float] = Field(default=0.3)
     """What sampling temperature to use."""
     top_k: int = 5
     """What search sampling control to use."""
     top_p: float = 0.85
     """What probability mass to use."""
     with_search_enhance: bool = False
     """Whether to use search enhance, default is False."""
@@ -164,33 +153,27 @@
         values["baichuan_api_key"] = convert_to_secret_str(
             get_from_dict_or_env(
                 values,
                 "baichuan_api_key",
                 "BAICHUAN_API_KEY",
             )
         )
-        values["baichuan_secret_key"] = convert_to_secret_str(
-            get_from_dict_or_env(
-                values,
-                "baichuan_secret_key",
-                "BAICHUAN_SECRET_KEY",
-            )
-        )
 
         return values
 
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Baichuan API."""
         normal_params = {
             "model": self.model,
             "temperature": self.temperature,
             "top_p": self.top_p,
             "top_k": self.top_k,
             "with_search_enhance": self.with_search_enhance,
+            "stream": self.streaming,
         }
 
         return {**normal_params, **self.model_kwargs}
 
     def _generate(
         self,
         messages: List[BaseMessage],
@@ -201,95 +184,92 @@
         if self.streaming:
             stream_iter = self._stream(
                 messages=messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return generate_from_stream(stream_iter)
 
         res = self._chat(messages, **kwargs)
-
+        if res.status_code != 200:
+            raise ValueError(f"Error from Baichuan api response: {res}")
         response = res.json()
-
-        if response.get("code") != 0:
-            raise ValueError(f"Error from Baichuan api response: {response}")
-
         return self._create_chat_result(response)
 
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         res = self._chat(messages, **kwargs)
-
+        if res.status_code != 200:
+            raise ValueError(f"Error from Baichuan api response: {res}")
         default_chunk_class = AIMessageChunk
         for chunk in res.iter_lines():
+            chunk = chunk.decode("utf-8").strip("\r\n")
+            parts = chunk.split("data: ", 1)
+            chunk = parts[1] if len(parts) > 1 else None
+            if chunk is None:
+                continue
+            if chunk == "[DONE]":
+                break
             response = json.loads(chunk)
-            if response.get("code") != 0:
-                raise ValueError(f"Error from Baichuan api response: {response}")
-
-            data = response.get("data")
-            for m in data.get("messages"):
-                chunk = _convert_delta_to_message_chunk(m, default_chunk_class)
+            for m in response.get("choices"):
+                chunk = _convert_delta_to_message_chunk(
+                    m.get("delta"), default_chunk_class
+                )
                 default_chunk_class = chunk.__class__
-                yield ChatGenerationChunk(message=chunk)
+                cg_chunk = ChatGenerationChunk(message=chunk)
                 if run_manager:
-                    run_manager.on_llm_new_token(chunk.content)
+                    run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+                yield cg_chunk
 
     def _chat(self, messages: List[BaseMessage], **kwargs: Any) -> requests.Response:
-        if self.baichuan_secret_key is None:
-            raise ValueError("Baichuan secret key is not set.")
-
         parameters = {**self._default_params, **kwargs}
 
         model = parameters.pop("model")
         headers = parameters.pop("headers", {})
+        temperature = parameters.pop("temperature", 0.3)
+        top_k = parameters.pop("top_k", 5)
+        top_p = parameters.pop("top_p", 0.85)
+        with_search_enhance = parameters.pop("with_search_enhance", False)
+        stream = parameters.pop("stream", False)
 
         payload = {
             "model": model,
             "messages": [_convert_message_to_dict(m) for m in messages],
-            "parameters": parameters,
+            "top_k": top_k,
+            "top_p": top_p,
+            "temperature": temperature,
+            "with_search_enhance": with_search_enhance,
+            "stream": stream,
         }
 
-        timestamp = int(time.time())
-
         url = self.baichuan_api_base
-        if self.streaming:
-            url = f"{url}/stream"
-        url = f"{url}/chat"
-
         api_key = ""
         if self.baichuan_api_key:
             api_key = self.baichuan_api_key.get_secret_value()
 
         res = requests.post(
             url=url,
             timeout=self.request_timeout,
             headers={
                 "Content-Type": "application/json",
                 "Authorization": f"Bearer {api_key}",
-                "X-BC-Timestamp": str(timestamp),
-                "X-BC-Signature": _signature(
-                    secret_key=self.baichuan_secret_key,
-                    payload=payload,
-                    timestamp=timestamp,
-                ),
-                "X-BC-Sign-Algo": "MD5",
                 **headers,
             },
             json=payload,
             stream=self.streaming,
         )
         return res
 
     def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:
         generations = []
-        for m in response["data"]["messages"]:
-            message = _convert_dict_to_message(m)
+        for c in response["choices"]:
+            message = _convert_dict_to_message(c["message"])
             gen = ChatGeneration(message=message)
             generations.append(gen)
 
         token_usage = response["usage"]
         llm_output = {"token_usage": token_usage, "model": self.model}
         return ChatResult(generations=generations, llm_output=llm_output)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/baidu_qianfan_endpoint.py` & `gigachain_community-0.2.0/langchain_community/chat_models/maritalk.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,348 +1,374 @@
-from __future__ import annotations
-
-import logging
-from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional, cast
+import json
+from http import HTTPStatus
+from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
 
+import requests
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
     AIMessageChunk,
     BaseMessage,
-    ChatMessage,
-    FunctionMessage,
     HumanMessage,
     SystemMessage,
 )
 from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
-from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
-
-logger = logging.getLogger(__name__)
+from langchain_core.pydantic_v1 import Field
+from requests import Response
+from requests.exceptions import HTTPError
 
 
-def convert_message_to_dict(message: BaseMessage) -> dict:
-    """Convert a message to a dictionary that can be passed to the API."""
-    message_dict: Dict[str, Any]
-    if isinstance(message, ChatMessage):
-        message_dict = {"role": message.role, "content": message.content}
-    elif isinstance(message, HumanMessage):
-        message_dict = {"role": "user", "content": message.content}
-    elif isinstance(message, AIMessage):
-        message_dict = {"role": "assistant", "content": message.content}
-        if "function_call" in message.additional_kwargs:
-            message_dict["function_call"] = message.additional_kwargs["function_call"]
-            # If function call only, content is None not empty string
-            if message_dict["content"] == "":
-                message_dict["content"] = None
-    elif isinstance(message, FunctionMessage):
-        message_dict = {
-            "role": "function",
-            "content": message.content,
-            "name": message.name,
-        }
-    else:
-        raise TypeError(f"Got unknown type {message}")
-
-    return message_dict
+class MaritalkHTTPError(HTTPError):
+    def __init__(self, request_obj: Response) -> None:
+        self.request_obj = request_obj
+        try:
+            response_json = request_obj.json()
+            if "detail" in response_json:
+                api_message = response_json["detail"]
+            elif "message" in response_json:
+                api_message = response_json["message"]
+            else:
+                api_message = response_json
+        except Exception:
+            api_message = request_obj.text
+
+        self.message = api_message
+        self.status_code = request_obj.status_code
+
+    def __str__(self) -> str:
+        status_code_meaning = HTTPStatus(self.status_code).phrase
+        formatted_message = f"HTTP Error: {self.status_code} - {status_code_meaning}"
+        formatted_message += f"\nDetail: {self.message}"
+        return formatted_message
 
 
-def _convert_dict_to_message(_dict: Mapping[str, Any]) -> AIMessage:
-    content = _dict.get("result", "") or ""
-    if _dict.get("function_call"):
-        additional_kwargs = {"function_call": dict(_dict["function_call"])}
-        if "thoughts" in additional_kwargs["function_call"]:
-            # align to api sample, which affects the llm function_call output
-            additional_kwargs["function_call"].pop("thoughts")
-    else:
-        additional_kwargs = {}
-    return AIMessage(
-        content=content,
-        additional_kwargs={**_dict.get("body", {}), **additional_kwargs},
-    )
-
-
-class QianfanChatEndpoint(BaseChatModel):
-    """Baidu Qianfan chat models.
-
-    To use, you should have the ``qianfan`` python package installed, and
-    the environment variable ``qianfan_ak`` and ``qianfan_sk`` set with your
-    API key and Secret Key.
+class ChatMaritalk(BaseChatModel):
+    """`MariTalk` Chat models API.
 
-    ak, sk are required parameters
-    which you could get from  https://cloud.baidu.com/product/wenxinworkshop
+    This class allows interacting with the MariTalk chatbot API.
+    To use it, you must provide an API key either through the constructor.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.chat_models import QianfanChatEndpoint
-            qianfan_chat = QianfanChatEndpoint(model="ERNIE-Bot",
-                endpoint="your_endpoint", qianfan_ak="your_ak", qianfan_sk="your_sk")
+            from langchain_community.chat_models import ChatMaritalk
+            chat = ChatMaritalk(api_key="your_api_key_here")
     """
 
-    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    api_key: str
+    """Your MariTalk API key."""
 
-    client: Any
+    model: str
+    """Chose one of the available models: 
+    - `sabia-2-medium`
+    - `sabia-2-small`
+    - `sabia-2-medium-2024-03-13`
+    - `sabia-2-small-2024-03-13`
+    - `maritalk-2024-01-08` (deprecated)"""
+
+    temperature: float = Field(default=0.7, gt=0.0, lt=1.0)
+    """Run inference with this temperature. 
+    Must be in the closed interval [0.0, 1.0]."""
+
+    max_tokens: int = Field(default=512, gt=0)
+    """The maximum number of tokens to generate in the reply."""
+
+    do_sample: bool = Field(default=True)
+    """Whether or not to use sampling; use `True` to enable."""
+
+    top_p: float = Field(default=0.95, gt=0.0, lt=1.0)
+    """Nucleus sampling parameter controlling the size of 
+    the probability mass considered for sampling."""
 
-    qianfan_ak: Optional[SecretStr] = None
-    qianfan_sk: Optional[SecretStr] = None
+    @property
+    def _llm_type(self) -> str:
+        """Identifies the LLM type as 'maritalk'."""
+        return "maritalk"
 
-    streaming: Optional[bool] = False
-    """Whether to stream the results or not."""
+    def parse_messages_for_model(
+        self, messages: List[BaseMessage]
+    ) -> List[Dict[str, Union[str, List[Union[str, Dict[Any, Any]]]]]]:
+        """
+        Parses messages from LangChain's format to the format expected by
+        the MariTalk API.
 
-    request_timeout: Optional[int] = 60
-    """request timeout for chat http requests"""
+        Parameters:
+            messages (List[BaseMessage]): A list of messages in LangChain
+            format to be parsed.
 
-    top_p: Optional[float] = 0.8
-    temperature: Optional[float] = 0.95
-    penalty_score: Optional[float] = 1
-    """Model params, only supported in ERNIE-Bot and ERNIE-Bot-turbo.
-    In the case of other model, passing these params will not affect the result.
-    """
+        Returns:
+            A list of messages formatted for the MariTalk API.
+        """
+        parsed_messages = []
 
-    model: str = "ERNIE-Bot-turbo"
-    """Model name.
-    you could get from https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu
-    
-    preset models are mapping to an endpoint.
-    `model` will be ignored if `endpoint` is set.
-    Default is ERNIE-Bot-turbo.
-    """
+        for message in messages:
+            if isinstance(message, HumanMessage):
+                role = "user"
+            elif isinstance(message, AIMessage):
+                role = "assistant"
+            elif isinstance(message, SystemMessage):
+                role = "system"
 
-    endpoint: Optional[str] = None
-    """Endpoint of the Qianfan LLM, required if custom model used."""
+            parsed_messages.append({"role": role, "content": message.content})
+        return parsed_messages
 
-    @root_validator()
-    def validate_environment(cls, values: Dict) -> Dict:
-        values["qianfan_ak"] = convert_to_secret_str(
-            get_from_dict_or_env(
-                values,
-                "qianfan_ak",
-                "QIANFAN_AK",
-                default="",
-            )
-        )
-        values["qianfan_sk"] = convert_to_secret_str(
-            get_from_dict_or_env(
-                values,
-                "qianfan_sk",
-                "QIANFAN_SK",
-                default="",
-            )
-        )
-        params = {
-            "model": values["model"],
-            "stream": values["streaming"],
-        }
-        if values["qianfan_ak"].get_secret_value() != "":
-            params["ak"] = values["qianfan_ak"].get_secret_value()
-        if values["qianfan_sk"].get_secret_value() != "":
-            params["sk"] = values["qianfan_sk"].get_secret_value()
-        if values["endpoint"] is not None and values["endpoint"] != "":
-            params["endpoint"] = values["endpoint"]
-        try:
-            import qianfan
+    def _call(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> str:
+        """
+        Sends the parsed messages to the MariTalk API and returns the generated
+        response or an error message.
 
-            values["client"] = qianfan.ChatCompletion(**params)
-        except ImportError:
-            raise ValueError(
-                "qianfan package not found, please install it with "
-                "`pip install qianfan`"
-            )
-        return values
+        This method makes an HTTP POST request to the MariTalk API with the
+        provided messages and other parameters.
+        If the request is successful and the API returns a response,
+        this method returns a string containing the answer.
+        If the request is rate-limited or encounters another error,
+        it returns a string with the error message.
+
+        Parameters:
+            messages (List[BaseMessage]): Messages to send to the model.
+            stop (Optional[List[str]]): Tokens that will signal the model
+                to stop generating further tokens.
 
-    @property
-    def _identifying_params(self) -> Dict[str, Any]:
-        return {
-            **{"endpoint": self.endpoint, "model": self.model},
-            **super()._identifying_params,
-        }
+        Returns:
+            str: If the API call is successful, returns the answer.
+                 If an error occurs (e.g., rate limiting), returns a string
+                 describing the error.
+        """
+        url = "https://chat.maritaca.ai/api/chat/inference"
+        headers = {"authorization": f"Key {self.api_key}"}
+        stopping_tokens = stop if stop is not None else []
 
-    @property
-    def _llm_type(self) -> str:
-        """Return type of chat_model."""
-        return "baidu-qianfan-chat"
+        parsed_messages = self.parse_messages_for_model(messages)
 
-    @property
-    def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for calling Qianfan API."""
-        normal_params = {
+        data = {
+            "messages": parsed_messages,
             "model": self.model,
-            "endpoint": self.endpoint,
-            "stream": self.streaming,
-            "request_timeout": self.request_timeout,
-            "top_p": self.top_p,
+            "do_sample": self.do_sample,
+            "max_tokens": self.max_tokens,
             "temperature": self.temperature,
-            "penalty_score": self.penalty_score,
+            "top_p": self.top_p,
+            "stopping_tokens": stopping_tokens,
+            **kwargs,
         }
 
-        return {**normal_params, **self.model_kwargs}
+        response = requests.post(url, json=data, headers=headers)
+
+        if response.ok:
+            return response.json().get("answer", "No answer found")
+        else:
+            raise MaritalkHTTPError(response)  # type: ignore
 
-    def _convert_prompt_msg_params(
+    async def _acall(
         self,
         messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Dict[str, Any]:
+    ) -> str:
         """
-        Converts a list of messages into a dictionary containing the message content
-        and default parameters.
+        Asynchronously sends the parsed messages to the MariTalk API and returns
+        the generated response or an error message.
 
-        Args:
-            messages (List[BaseMessage]): The list of messages.
-            **kwargs (Any): Optional arguments to add additional parameters to the
-            resulting dictionary.
+        This method makes an HTTP POST request to the MariTalk API with the
+        provided messages and other parameters using async I/O.
+        If the request is successful and the API returns a response,
+        this method returns a string containing the answer.
+        If the request is rate-limited or encounters another error,
+        it returns a string with the error message.
+        """
+        try:
+            import httpx
 
-        Returns:
-            Dict[str, Any]: A dictionary containing the message content and default
-            parameters.
+            url = "https://chat.maritaca.ai/api/chat/inference"
+            headers = {"authorization": f"Key {self.api_key}"}
+            stopping_tokens = stop if stop is not None else []
+
+            parsed_messages = self.parse_messages_for_model(messages)
+
+            data = {
+                "messages": parsed_messages,
+                "model": self.model,
+                "do_sample": self.do_sample,
+                "max_tokens": self.max_tokens,
+                "temperature": self.temperature,
+                "top_p": self.top_p,
+                "stopping_tokens": stopping_tokens,
+                **kwargs,
+            }
+
+            async with httpx.AsyncClient() as client:
+                response = await client.post(
+                    url, json=data, headers=headers, timeout=None
+                )
 
-        """
-        messages_dict: Dict[str, Any] = {
-            "messages": [
-                convert_message_to_dict(m)
-                for m in messages
-                if not isinstance(m, SystemMessage)
-            ]
-        }
-        for i in [i for i, m in enumerate(messages) if isinstance(m, SystemMessage)]:
-            if "system" not in messages_dict:
-                messages_dict["system"] = ""
-            messages_dict["system"] += cast(str, messages[i].content) + "\n"
+            if response.status_code == 200:
+                return response.json().get("answer", "No answer found")
+            else:
+                raise MaritalkHTTPError(response)  # type: ignore
 
-        return {
-            **messages_dict,
-            **self._default_params,
-            **kwargs,
-        }
+        except ImportError:
+            raise ImportError(
+                "Could not import httpx python package. "
+                "Please install it with `pip install httpx`."
+            )
 
-    def _generate(
+    def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> ChatResult:
-        """Call out to an qianfan models endpoint for each generation with a prompt.
-        Args:
-            messages: The messages to pass into the model.
-            stop: Optional list of stop words to use when generating.
-        Returns:
-            The string generated by the model.
+    ) -> Iterator[ChatGenerationChunk]:
+        headers = {"Authorization": f"Key {self.api_key}"}
+        stopping_tokens = stop if stop is not None else []
 
-        Example:
-            .. code-block:: python
-                response = qianfan_model("Tell me a joke.")
-        """
-        if self.streaming:
-            completion = ""
-            for chunk in self._stream(messages, stop, run_manager, **kwargs):
-                completion += chunk.text
-            lc_msg = AIMessage(content=completion, additional_kwargs={})
-            gen = ChatGeneration(
-                message=lc_msg,
-                generation_info=dict(finish_reason="stop"),
-            )
-            return ChatResult(
-                generations=[gen],
-                llm_output={"token_usage": {}, "model_name": self.model},
-            )
-        params = self._convert_prompt_msg_params(messages, **kwargs)
-        response_payload = self.client.do(**params)
-        lc_msg = _convert_dict_to_message(response_payload)
-        gen = ChatGeneration(
-            message=lc_msg,
-            generation_info={
-                "finish_reason": "stop",
-                **response_payload.get("body", {}),
-            },
+        parsed_messages = self.parse_messages_for_model(messages)
+
+        data = {
+            "messages": parsed_messages,
+            "model": self.model,
+            "do_sample": self.do_sample,
+            "max_tokens": self.max_tokens,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            "stopping_tokens": stopping_tokens,
+            "stream": True,
+            **kwargs,
+        }
+
+        response = requests.post(
+            "https://chat.maritaca.ai/api/chat/inference",
+            data=json.dumps(data),
+            headers=headers,
+            stream=True,
         )
-        token_usage = response_payload.get("usage", {})
-        llm_output = {"token_usage": token_usage, "model_name": self.model}
-        return ChatResult(generations=[gen], llm_output=llm_output)
 
-    async def _agenerate(
+        if response.ok:
+            for line in response.iter_lines():
+                if line.startswith(b"data: "):
+                    response_data = line.replace(b"data: ", b"").decode("utf-8")
+                    if response_data:
+                        parsed_data = json.loads(response_data)
+                        if "text" in parsed_data:
+                            delta = parsed_data["text"]
+                            chunk = ChatGenerationChunk(
+                                message=AIMessageChunk(content=delta)
+                            )
+                            if run_manager:
+                                run_manager.on_llm_new_token(delta, chunk=chunk)
+                            yield chunk
+
+        else:
+            raise MaritalkHTTPError(response)  # type: ignore
+
+    async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> ChatResult:
-        if self.streaming:
-            completion = ""
-            token_usage = {}
-            async for chunk in self._astream(messages, stop, run_manager, **kwargs):
-                completion += chunk.text
-
-            lc_msg = AIMessage(content=completion, additional_kwargs={})
-            gen = ChatGeneration(
-                message=lc_msg,
-                generation_info=dict(finish_reason="stop"),
-            )
-            return ChatResult(
-                generations=[gen],
-                llm_output={"token_usage": {}, "model_name": self.model},
+    ) -> AsyncIterator[ChatGenerationChunk]:
+        try:
+            import httpx
+
+            headers = {"Authorization": f"Key {self.api_key}"}
+            stopping_tokens = stop if stop is not None else []
+
+            parsed_messages = self.parse_messages_for_model(messages)
+
+            data = {
+                "messages": parsed_messages,
+                "model": self.model,
+                "do_sample": self.do_sample,
+                "max_tokens": self.max_tokens,
+                "temperature": self.temperature,
+                "top_p": self.top_p,
+                "stopping_tokens": stopping_tokens,
+                "stream": True,
+                **kwargs,
+            }
+
+            async with httpx.AsyncClient() as client:
+                async with client.stream(
+                    "POST",
+                    "https://chat.maritaca.ai/api/chat/inference",
+                    data=json.dumps(data),  # type: ignore
+                    headers=headers,
+                    timeout=None,
+                ) as response:
+                    if response.status_code == 200:
+                        async for line in response.aiter_lines():
+                            if line.startswith("data: "):
+                                response_data = line.replace("data: ", "")
+                                if response_data:
+                                    parsed_data = json.loads(response_data)
+                                    if "text" in parsed_data:
+                                        delta = parsed_data["text"]
+                                        chunk = ChatGenerationChunk(
+                                            message=AIMessageChunk(content=delta)
+                                        )
+                                        if run_manager:
+                                            await run_manager.on_llm_new_token(
+                                                delta, chunk=chunk
+                                            )
+                                        yield chunk
+
+                    else:
+                        raise MaritalkHTTPError(response)  # type: ignore
+
+        except ImportError:
+            raise ImportError(
+                "Could not import httpx python package. "
+                "Please install it with `pip install httpx`."
             )
-        params = self._convert_prompt_msg_params(messages, **kwargs)
-        response_payload = await self.client.ado(**params)
-        lc_msg = _convert_dict_to_message(response_payload)
-        generations = []
-        gen = ChatGeneration(
-            message=lc_msg,
-            generation_info={
-                "finish_reason": "stop",
-                **response_payload.get("body", {}),
-            },
-        )
-        generations.append(gen)
-        token_usage = response_payload.get("usage", {})
-        llm_output = {"token_usage": token_usage, "model_name": self.model}
-        return ChatResult(generations=generations, llm_output=llm_output)
 
-    def _stream(
+    def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Iterator[ChatGenerationChunk]:
-        params = self._convert_prompt_msg_params(messages, **kwargs)
-        for res in self.client.do(**params):
-            if res:
-                msg = _convert_dict_to_message(res)
-                chunk = ChatGenerationChunk(
-                    text=res["result"],
-                    message=AIMessageChunk(
-                        content=msg.content,
-                        role="assistant",
-                        additional_kwargs=msg.additional_kwargs,
-                    ),
-                )
-                yield chunk
-                if run_manager:
-                    run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+    ) -> ChatResult:
+        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)
+        message = AIMessage(content=output_str)
+        generation = ChatGeneration(message=message)
+        return ChatResult(generations=[generation])
 
-    async def _astream(
+    async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> AsyncIterator[ChatGenerationChunk]:
-        params = self._convert_prompt_msg_params(messages, **kwargs)
-        async for res in await self.client.ado(**params):
-            if res:
-                msg = _convert_dict_to_message(res)
-                chunk = ChatGenerationChunk(
-                    text=res["result"],
-                    message=AIMessageChunk(
-                        content=msg.content,
-                        role="assistant",
-                        additional_kwargs=msg.additional_kwargs,
-                    ),
-                )
-                yield chunk
-                if run_manager:
-                    await run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+    ) -> ChatResult:
+        output_str = await self._acall(
+            messages, stop=stop, run_manager=run_manager, **kwargs
+        )
+        message = AIMessage(content=output_str)
+        generation = ChatGeneration(message=message)
+        return ChatResult(generations=[generation])
+
+    @property
+    def _identifying_params(self) -> Dict[str, Any]:
+        """
+        Identifies the key parameters of the chat model for logging
+        or tracking purposes.
+
+        Returns:
+            A dictionary of the key configuration parameters.
+        """
+        return {
+            "model": self.model,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            "max_tokens": self.max_tokens,
+        }
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/cohere.py` & `gigachain_community-0.2.0/langchain_community/chat_models/cohere.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     agenerate_from_stream,
@@ -76,42 +77,48 @@
 
     # by enabling automatic prompt truncation, the probability of request failure is
     # reduced with minimal impact on response quality
     prompt_truncation = (
         "AUTO" if documents is not None or connectors is not None else None
     )
 
-    return {
+    req = {
         "message": messages[-1].content,
         "chat_history": [
             {"role": get_role(x), "message": x.content} for x in messages[:-1]
         ],
         "documents": documents,
         "connectors": maybe_connectors,
         "prompt_truncation": prompt_truncation,
         **kwargs,
     }
 
+    return {k: v for k, v in req.items() if v is not None}
 
+
+@deprecated(
+    since="0.0.30", removal="0.3.0", alternative_import="langchain_cohere.ChatCohere"
+)
 class ChatCohere(BaseChatModel, BaseCohere):
     """`Cohere` chat large language models.
 
     To use, you should have the ``cohere`` python package installed, and the
     environment variable ``COHERE_API_KEY`` set with your API key, or pass
     it as a named parameter to the constructor.
 
     Example:
         .. code-block:: python
 
             from langchain_community.chat_models import ChatCohere
             from langchain_core.messages import HumanMessage
 
-            chat = ChatCohere(model="foo")
-            result = chat([HumanMessage(content="Hello")])
-            print(result.content)
+            chat = ChatCohere(model="command", max_tokens=256, temperature=0.75)
+
+            messages = [HumanMessage(content="knock knock")]
+            chat.invoke(messages)
     """
 
     class Config:
         """Configuration for this pydantic object."""
 
         allow_population_by_field_name = True
         arbitrary_types_allowed = True
@@ -137,39 +144,49 @@
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         request = get_cohere_chat_request(messages, **self._default_params, **kwargs)
-        stream = self.client.chat(**request, stream=True)
+
+        if hasattr(self.client, "chat_stream"):  # detect and support sdk v5
+            stream = self.client.chat_stream(**request)
+        else:
+            stream = self.client.chat(**request, stream=True)
 
         for data in stream:
             if data.event_type == "text-generation":
                 delta = data.text
-                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+                chunk = ChatGenerationChunk(message=AIMessageChunk(content=delta))
                 if run_manager:
-                    run_manager.on_llm_new_token(delta)
+                    run_manager.on_llm_new_token(delta, chunk=chunk)
+                yield chunk
 
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[ChatGenerationChunk]:
         request = get_cohere_chat_request(messages, **self._default_params, **kwargs)
-        stream = await self.async_client.chat(**request, stream=True)
+
+        if hasattr(self.async_client, "chat_stream"):  # detect and support sdk v5
+            stream = await self.async_client.chat_stream(**request)
+        else:
+            stream = await self.async_client.chat(**request, stream=True)
 
         async for data in stream:
             if data.event_type == "text-generation":
                 delta = data.text
-                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+                chunk = ChatGenerationChunk(message=AIMessageChunk(content=delta))
                 if run_manager:
-                    await run_manager.on_llm_new_token(delta)
+                    await run_manager.on_llm_new_token(delta, chunk=chunk)
+                yield chunk
 
     def _get_generation_info(self, response: Any) -> Dict[str, Any]:
         """Get the generation info from cohere API response."""
         return {
             "documents": response.documents,
             "citations": response.citations,
             "search_results": response.search_results,
@@ -213,22 +230,22 @@
         if self.streaming:
             stream_iter = self._astream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return await agenerate_from_stream(stream_iter)
 
         request = get_cohere_chat_request(messages, **self._default_params, **kwargs)
-        response = self.client.chat(**request, stream=False)
+        response = self.client.chat(**request)
 
         message = AIMessage(content=response.text)
         generation_info = None
         if hasattr(response, "documents"):
             generation_info = self._get_generation_info(response)
         return ChatResult(
             generations=[
                 ChatGeneration(message=message, generation_info=generation_info)
             ]
         )
 
     def get_num_tokens(self, text: str) -> int:
         """Calculate number of tokens."""
-        return len(self.client.tokenize(text).tokens)
+        return len(self.client.tokenize(text=text).tokens)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/databricks.py` & `gigachain_community-0.2.0/langchain_community/chat_models/databricks.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,26 +6,33 @@
 logger = logging.getLogger(__name__)
 
 
 class ChatDatabricks(ChatMlflow):
     """`Databricks` chat models API.
 
     To use, you should have the ``mlflow`` python package installed.
-    For more information, see https://mlflow.org/docs/latest/llms/deployments/databricks.html.
+    For more information, see https://mlflow.org/docs/latest/llms/deployments.
 
     Example:
         .. code-block:: python
 
             from langchain_community.chat_models import ChatDatabricks
 
             chat = ChatDatabricks(
                 target_uri="databricks",
-                endpoint="chat",
+                endpoint="databricks-llama-2-70b-chat",
                 temperature-0.1,
             )
+
+            # single input invocation
+            print(chat_model.invoke("What is MLflow?").content)
+
+            # single input invocation with streaming response
+            for chunk in chat_model.stream("What is MLflow?"):
+                print(chunk.content, end="|")
     """
 
     target_uri: str = "databricks"
     """The target URI to use. Defaults to ``databricks``."""
 
     @property
     def _llm_type(self) -> str:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/ernie.py` & `gigachain_community-0.2.0/langchain_community/chat_models/ernie.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import logging
 import threading
 from typing import Any, Dict, List, Mapping, Optional
 
 import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
     ChatMessage,
     HumanMessage,
@@ -26,14 +27,18 @@
     elif isinstance(message, AIMessage):
         message_dict = {"role": "assistant", "content": message.content}
     else:
         raise ValueError(f"Got unknown type {message}")
     return message_dict
 
 
+@deprecated(
+    since="0.0.13",
+    alternative="langchain_community.chat_models.QianfanChatEndpoint",
+)
 class ErnieBotChat(BaseChatModel):
     """`ERNIE-Bot` large language model.
 
     ERNIE-Bot is a large language model developed by Baidu,
     covering a huge amount of Chinese data.
 
     To use, you should have the `ernie_client_id` and `ernie_client_secret` set,
@@ -205,15 +210,15 @@
                 "function_call": dict(response.get("function_call", {}))
             }
         else:
             additional_kwargs = {}
         generations = [
             ChatGeneration(
                 message=AIMessage(
-                    content=response.get("result"),
+                    content=response.get("result", ""),
                     additional_kwargs={**additional_kwargs},
                 )
             )
         ]
         token_usage = response.get("usage", {})
         llm_output = {"token_usage": token_usage, "model_name": self.model_name}
         return ChatResult(generations=generations, llm_output=llm_output)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/everlyai.py` & `gigachain_community-0.2.0/langchain_community/chat_models/everlyai.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """EverlyAI Endpoints chat wrapper. Relies heavily on ChatOpenAI."""
+
 from __future__ import annotations
 
 import logging
 import sys
 from typing import TYPE_CHECKING, Dict, Optional, Set
 
 from langchain_core.messages import BaseMessage
@@ -85,15 +86,15 @@
         )
         values["openai_api_base"] = DEFAULT_API_BASE
 
         try:
             import openai
 
         except ImportError as e:
-            raise ValueError(
+            raise ImportError(
                 "Could not import openai python package. "
                 "Please install it with `pip install openai`.",
             ) from e
         try:
             values["client"] = openai.ChatCompletion
         except AttributeError as exc:
             raise ValueError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/fake.py` & `gigachain_community-0.2.0/langchain_community/chat_models/fake.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Fake ChatModel for testing purposes."""
+
 import asyncio
 import time
 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/fireworks.py` & `gigachain_community-0.2.0/langchain_community/chat_models/fireworks.py`

 * *Files 3% similar despite different names*

```diff
@@ -6,14 +6,15 @@
     Iterator,
     List,
     Optional,
     Type,
     Union,
 )
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.language_models.llms import create_base_retry_decorator
 from langchain_core.messages import (
@@ -53,15 +54,15 @@
     elif role == "system" or default_class == SystemMessageChunk:
         return SystemMessageChunk(content=content)
     elif role == "function" or default_class == FunctionMessageChunk:
         return FunctionMessageChunk(content=content, name=_dict.name)
     elif role or default_class == ChatMessageChunk:
         return ChatMessageChunk(content=content, role=role)
     else:
-        return default_class(content=content)
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
 def convert_dict_to_message(_dict: Any) -> BaseMessage:
     """Convert a dict response to a message."""
     role = _dict.role
     content = _dict.content or ""
     if role == "user":
@@ -74,14 +75,19 @@
         return SystemMessage(content=content)
     elif role == "function":
         return FunctionMessage(content=content, name=_dict.name)
     else:
         return ChatMessage(content=content, role=role)
 
 
+@deprecated(
+    since="0.0.26",
+    removal="0.3",
+    alternative_import="langchain_fireworks.ChatFireworks",
+)
 class ChatFireworks(BaseChatModel):
     """Fireworks Chat models."""
 
     model: str = "accounts/fireworks/models/llama-v2-7b-chat"
     model_kwargs: dict = Field(
         default_factory=lambda: {
             "temperature": 0.7,
@@ -215,18 +221,20 @@
             choice = chunk.choices[0]
             chunk = _convert_delta_to_message_chunk(choice.delta, default_chunk_class)
             finish_reason = choice.finish_reason
             generation_info = (
                 dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
             default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
+            cg_chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info
+            )
             if run_manager:
-                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+                run_manager.on_llm_new_token(cg_chunk.text, chunk=cg_chunk)
+            yield cg_chunk
 
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -246,18 +254,20 @@
             choice = chunk.choices[0]
             chunk = _convert_delta_to_message_chunk(choice.delta, default_chunk_class)
             finish_reason = choice.finish_reason
             generation_info = (
                 dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
             default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
+            cg_chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info
+            )
             if run_manager:
-                await run_manager.on_llm_new_token(token=chunk.text, chunk=chunk)
+                await run_manager.on_llm_new_token(token=chunk.text, chunk=cg_chunk)
+            yield cg_chunk
 
 
 def conditional_decorator(
     condition: bool, decorator: Callable[[Any], Any]
 ) -> Callable[[Any], Any]:
     """Define conditional decorator.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/gigachat.py` & `gigachain_community-0.2.0/langchain_community/chat_models/friendli.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-import logging
-from typing import Any, AsyncIterator, Iterator, List, Optional
+from __future__ import annotations
+
+from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
@@ -16,163 +17,201 @@
     BaseMessage,
     ChatMessage,
     HumanMessage,
     SystemMessage,
 )
 from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
 
-from langchain_community.llms.gigachat import _BaseGigaChat
+from langchain_community.llms.friendli import BaseFriendli
 
-logger = logging.getLogger(__name__)
 
+def get_role(message: BaseMessage) -> str:
+    """Get role of the message.
 
-def _convert_dict_to_message(message: Any) -> BaseMessage:
-    from gigachat.models import MessagesRole
+    Args:
+        message (BaseMessage): The message object.
 
-    if message.role == MessagesRole.SYSTEM:
-        return SystemMessage(content=message.content)
-    elif message.role == MessagesRole.USER:
-        return HumanMessage(content=message.content)
-    elif message.role == MessagesRole.ASSISTANT:
-        return AIMessage(content=message.content)
-    else:
-        raise TypeError(f"Got unknown role {message.role} {message}")
+    Raises:
+        ValueError: Raised when the message is of an unknown type.
 
+    Returns:
+        str: The role of the message.
+    """
+    if isinstance(message, ChatMessage) or isinstance(message, HumanMessage):
+        return "user"
+    if isinstance(message, AIMessage):
+        return "assistant"
+    if isinstance(message, SystemMessage):
+        return "system"
+    raise ValueError(f"Got unknown type {message}")
 
-def _convert_message_to_dict(message: BaseMessage) -> Any:
-    from gigachat.models import Messages, MessagesRole
 
-    if isinstance(message, SystemMessage):
-        return Messages(role=MessagesRole.SYSTEM, content=message.content)
-    elif isinstance(message, HumanMessage):
-        return Messages(role=MessagesRole.USER, content=message.content)
-    elif isinstance(message, AIMessage):
-        return Messages(role=MessagesRole.ASSISTANT, content=message.content)
-    elif isinstance(message, ChatMessage):
-        return Messages(role=MessagesRole(message.role), content=message.content)
-    else:
-        raise TypeError(f"Got unknown type {message}")
+def get_chat_request(messages: List[BaseMessage]) -> Dict[str, Any]:
+    """Get a request of the Friendli chat API.
 
+    Args:
+        messages (List[BaseMessage]): Messages comprising the conversation so far.
+
+    Returns:
+        Dict[str, Any]: The request for the Friendli chat API.
+    """
+    return {
+        "messages": [
+            {"role": get_role(message), "content": message.content}
+            for message in messages
+        ]
+    }
 
-class GigaChat(_BaseGigaChat, BaseChatModel):
-    """`GigaChat` large language models API.
 
-    To use, you should pass login and password to access GigaChat API or use token.
+class ChatFriendli(BaseChatModel, BaseFriendli):
+    """Friendli LLM for chat.
+
+    ``friendli-client`` package should be installed with `pip install friendli-client`.
+    You must set ``FRIENDLI_TOKEN`` environment variable or provide the value of your
+    personal access token for the ``friendli_token`` argument.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.chat_models import GigaChat
-            giga = GigaChat(credentials=..., verify_ssl_certs=False)
+            from langchain_community.chat_models import FriendliChat
+
+            chat = Friendli(
+                model="llama-2-13b-chat", friendli_token="YOUR FRIENDLI TOKEN"
+            )
+            chat.invoke("What is generative AI?")
     """
 
-    def _build_payload(self, messages: List[BaseMessage]) -> Any:
-        from gigachat.models import Chat
+    model: str = "llama-2-13b-chat"
+
+    @property
+    def lc_secrets(self) -> Dict[str, str]:
+        return {"friendli_token": "FRIENDLI_TOKEN"}
+
+    @property
+    def _default_params(self) -> Dict[str, Any]:
+        """Get the default parameters for calling Friendli completions API."""
+        return {
+            "frequency_penalty": self.frequency_penalty,
+            "presence_penalty": self.presence_penalty,
+            "max_tokens": self.max_tokens,
+            "stop": self.stop,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+        }
+
+    @property
+    def _identifying_params(self) -> Dict[str, Any]:
+        """Get the identifying parameters."""
+        return {"model": self.model, **self._default_params}
+
+    @property
+    def _llm_type(self) -> str:
+        return "friendli-chat"
+
+    def _get_invocation_params(
+        self, stop: Optional[List[str]] = None, **kwargs: Any
+    ) -> Dict[str, Any]:
+        """Get the parameters used to invoke the model."""
+        params = self._default_params
+        if self.stop is not None and stop is not None:
+            raise ValueError("`stop` found in both the input and default params.")
+        elif self.stop is not None:
+            params["stop"] = self.stop
+        else:
+            params["stop"] = stop
+        return {**params, **kwargs}
 
-        payload = Chat(
-            messages=[_convert_message_to_dict(m) for m in messages],
-            profanity_check=self.profanity_check,
+    def _stream(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> Iterator[ChatGenerationChunk]:
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        stream = self.client.chat.completions.create(
+            **get_chat_request(messages), stream=True, model=self.model, **params
         )
-        if self.temperature is not None:
-            payload.temperature = self.temperature
-        if self.max_tokens is not None:
-            payload.max_tokens = self.max_tokens
-
-        if self.verbose:
-            logger.info("Giga request: %s", payload.dict())
-
-        return payload
-
-    def _create_chat_result(self, response: Any) -> ChatResult:
-        generations = []
-        for res in response.choices:
-            message = _convert_dict_to_message(res.message)
-            finish_reason = res.finish_reason
-            gen = ChatGeneration(
-                message=message,
-                generation_info={"finish_reason": finish_reason},
-            )
-            generations.append(gen)
-            if finish_reason != "stop":
-                logger.warning(
-                    "Giga generation stopped with reason: %s",
-                    finish_reason,
-                )
-            if self.verbose:
-                logger.info("Giga response: %s", message.content)
-        llm_output = {"token_usage": response.usage, "model_name": response.model}
-        return ChatResult(generations=generations, llm_output=llm_output)
+        for chunk in stream:
+            delta = chunk.choices[0].delta.content
+            if delta:
+                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+                if run_manager:
+                    run_manager.on_llm_new_token(delta)
+
+    async def _astream(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> AsyncIterator[ChatGenerationChunk]:
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        stream = await self.async_client.chat.completions.create(
+            **get_chat_request(messages), stream=True, model=self.model, **params
+        )
+        async for chunk in stream:
+            delta = chunk.choices[0].delta.content
+            if delta:
+                yield ChatGenerationChunk(message=AIMessageChunk(content=delta))
+                if run_manager:
+                    await run_manager.on_llm_new_token(delta)
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
-        stream: Optional[bool] = None,
         **kwargs: Any,
     ) -> ChatResult:
-        should_stream = stream if stream is not None else self.streaming
-        if should_stream:
+        if self.streaming:
             stream_iter = self._stream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return generate_from_stream(stream_iter)
 
-        payload = self._build_payload(messages)
-        response = self._client.chat(payload)
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        response = self.client.chat.completions.create(
+            messages=[
+                {
+                    "role": get_role(message),
+                    "content": message.content,
+                }
+                for message in messages
+            ],
+            stream=False,
+            model=self.model,
+            **params,
+        )
 
-        return self._create_chat_result(response)
+        message = AIMessage(content=response.choices[0].message.content)
+        return ChatResult(generations=[ChatGeneration(message=message)])
 
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        stream: Optional[bool] = None,
         **kwargs: Any,
     ) -> ChatResult:
-        should_stream = stream if stream is not None else self.streaming
-        if should_stream:
+        if self.streaming:
             stream_iter = self._astream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return await agenerate_from_stream(stream_iter)
 
-        payload = self._build_payload(messages)
-        response = await self._client.achat(payload)
-
-        return self._create_chat_result(response)
-
-    def _stream(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> Iterator[ChatGenerationChunk]:
-        payload = self._build_payload(messages)
-
-        for chunk in self._client.stream(payload):
-            if chunk.choices:
-                content = chunk.choices[0].delta.content
-                yield ChatGenerationChunk(message=AIMessageChunk(content=content))
-                if run_manager:
-                    run_manager.on_llm_new_token(content)
-
-    async def _astream(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[ChatGenerationChunk]:
-        payload = self._build_payload(messages)
-
-        async for chunk in self._client.astream(payload):
-            if chunk.choices:
-                content = chunk.choices[0].delta.content
-                yield ChatGenerationChunk(message=AIMessageChunk(content=content))
-                if run_manager:
-                    await run_manager.on_llm_new_token(content)
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        response = await self.async_client.chat.completions.create(
+            messages=[
+                {
+                    "role": get_role(message),
+                    "content": message.content,
+                }
+                for message in messages
+            ],
+            stream=False,
+            model=self.model,
+            **params,
+        )
 
-    class Config:
-        extra = "allow"
+        message = AIMessage(content=response.choices[0].message.content)
+        return ChatResult(generations=[ChatGeneration(message=message)])
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/google_palm.py` & `gigachain_community-0.2.0/langchain_community/chat_models/google_palm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around Google's PaLM Chat API."""
+
 from __future__ import annotations
 
 import logging
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, cast
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
@@ -16,16 +17,16 @@
     HumanMessage,
     SystemMessage,
 )
 from langchain_core.outputs import (
     ChatGeneration,
     ChatResult,
 )
-from langchain_core.pydantic_v1 import BaseModel, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     retry_if_exception_type,
     stop_after_attempt,
     wait_exponential,
 )
@@ -229,15 +230,15 @@
             chat = ChatGooglePalm()
 
     """
 
     client: Any  #: :meta private:
     model_name: str = "models/chat-bison-001"
     """Model name to use."""
-    google_api_key: Optional[str] = None
+    google_api_key: Optional[SecretStr] = None
     temperature: Optional[float] = None
     """Run inference with this temperature. Must by in the closed
        interval [0.0, 1.0]."""
     top_p: Optional[float] = None
     """Decode using nucleus sampling: consider the smallest set of tokens whose
        probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]."""
     top_k: Optional[int] = None
@@ -259,21 +260,21 @@
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "chat_models", "google_palm"]
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate api key, python package exists, temperature, top_p, and top_k."""
-        google_api_key = get_from_dict_or_env(
-            values, "google_api_key", "GOOGLE_API_KEY"
+        google_api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "google_api_key", "GOOGLE_API_KEY")
         )
         try:
             import google.generativeai as genai
 
-            genai.configure(api_key=google_api_key)
+            genai.configure(api_key=google_api_key.get_secret_value())
         except ImportError:
             raise ChatGooglePalmError(
                 "Could not import google.generativeai python package. "
                 "Please install it with `pip install google-generativeai`"
             )
 
         values["client"] = genai
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/gpt_router.py` & `gigachain_community-0.2.0/langchain_community/chat_models/gpt_router.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,31 +10,32 @@
     Dict,
     Generator,
     Iterator,
     List,
     Mapping,
     Optional,
     Tuple,
+    Type,
     Union,
 )
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     agenerate_from_stream,
     generate_from_stream,
 )
 from langchain_core.language_models.llms import create_base_retry_decorator
-from langchain_core.messages import AIMessageChunk, BaseMessage
+from langchain_core.messages import AIMessageChunk, BaseMessage, BaseMessageChunk
 from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
-from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.adapters.openai import (
     convert_dict_to_message,
     convert_message_to_dict,
 )
 from langchain_community.chat_models.openai import _convert_delta_to_message_chunk
 
@@ -48,21 +49,23 @@
 
 
 class GPTRouterException(Exception):
     """Error with the `GPTRouter APIs`"""
 
 
 class GPTRouterModel(BaseModel):
+    """GPTRouter model."""
+
     name: str
     provider_name: str
 
 
 def get_ordered_generation_requests(
-    models_priority_list: List[GPTRouterModel], **kwargs
-):
+    models_priority_list: List[GPTRouterModel], **kwargs: Any
+) -> List:
     """
     Return the body for the model router input.
     """
 
     from gpt_router.models import GenerationParams, ModelGenerationRequest
 
     return [
@@ -96,15 +99,15 @@
 
 
 def completion_with_retry(
     llm: GPTRouter,
     models_priority_list: List[GPTRouterModel],
     run_manager: Optional[CallbackManagerForLLMRun] = None,
     **kwargs: Any,
-) -> Union[GenerationResponse, Generator[ChunkedGenerationResponse]]:
+) -> Union[GenerationResponse, Generator[ChunkedGenerationResponse, None, None]]:
     """Use tenacity to retry the completion call."""
     retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)
 
     @retry_decorator
     def _completion_with_retry(**kwargs: Any) -> Any:
         ordered_generation_requests = get_ordered_generation_requests(
             models_priority_list, **kwargs
@@ -118,15 +121,15 @@
 
 
 async def acompletion_with_retry(
     llm: GPTRouter,
     models_priority_list: List[GPTRouterModel],
     run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
     **kwargs: Any,
-) -> Union[GenerationResponse, AsyncGenerator[ChunkedGenerationResponse]]:
+) -> Union[GenerationResponse, AsyncGenerator[ChunkedGenerationResponse, None]]:
     """Use tenacity to retry the async completion call."""
 
     retry_decorator = _create_retry_decorator(llm, run_manager=run_manager)
 
     @retry_decorator
     async def _completion_with_retry(**kwargs: Any) -> Any:
         ordered_generation_requests = get_ordered_generation_requests(
@@ -146,15 +149,15 @@
     For more information, see https://gpt-router.writesonic.com/docs
     """
 
     client: Any = Field(default=None, exclude=True)  #: :meta private:
     models_priority_list: List[GPTRouterModel] = Field(min_items=1)
     gpt_router_api_base: str = Field(default=None)
     """WriteSonic GPTRouter custom endpoint"""
-    gpt_router_api_key: Optional[str] = None
+    gpt_router_api_key: Optional[SecretStr] = None
     """WriteSonic GPTRouter API Key"""
     temperature: float = 0.7
     """What sampling temperature to use."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any model parameters valid for `create` call not explicitly specified."""
     max_retries: int = 4
     """Maximum number of retries to make when generating."""
@@ -169,41 +172,42 @@
         values["gpt_router_api_base"] = get_from_dict_or_env(
             values,
             "gpt_router_api_base",
             "GPT_ROUTER_API_BASE",
             DEFAULT_API_BASE_URL,
         )
 
-        values["gpt_router_api_key"] = get_from_dict_or_env(
-            values,
-            "gpt_router_api_key",
-            "GPT_ROUTER_API_KEY",
+        values["gpt_router_api_key"] = convert_to_secret_str(
+            get_from_dict_or_env(
+                values,
+                "gpt_router_api_key",
+                "GPT_ROUTER_API_KEY",
+            )
         )
 
         try:
             from gpt_router.client import GPTRouterClient
 
         except ImportError:
             raise GPTRouterException(
                 "Could not import GPTRouter python package. "
                 "Please install it with `pip install GPTRouter`."
             )
 
         gpt_router_client = GPTRouterClient(
-            values["gpt_router_api_base"], values["gpt_router_api_key"]
+            values["gpt_router_api_base"],
+            values["gpt_router_api_key"].get_secret_value(),
         )
         values["client"] = gpt_router_client
 
         return values
 
     @property
     def lc_secrets(self) -> Dict[str, str]:
-        return {
-            "gpt_router_api_key": "GPT_ROUTER_API_KEY",
-        }
+        return {"gpt_router_api_key": "GPT_ROUTER_API_KEY"}
 
     @property
     def lc_serializable(self) -> bool:
         return True
 
     @property
     def _llm_type(self) -> str:
@@ -278,38 +282,38 @@
             models_priority_list=self.models_priority_list,
             run_manager=run_manager,
             **params,
         )
         return self._create_chat_result(response)
 
     def _create_chat_generation_chunk(
-        self, data: Mapping[str, Any], default_chunk_class
-    ):
+        self, data: Mapping[str, Any], default_chunk_class: Type[BaseMessageChunk]
+    ) -> Tuple[ChatGenerationChunk, Type[BaseMessageChunk]]:
         chunk = _convert_delta_to_message_chunk(
             {"content": data.get("text", "")}, default_chunk_class
         )
         finish_reason = data.get("finish_reason")
         generation_info = (
             dict(finish_reason=finish_reason) if finish_reason is not None else None
         )
         default_chunk_class = chunk.__class__
-        chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-        return chunk, default_chunk_class
+        gen_chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
+        return gen_chunk, default_chunk_class
 
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         message_dicts, params = self._create_message_dicts(messages, stop)
         params = {**params, **kwargs, "stream": True}
 
-        default_chunk_class = AIMessageChunk
+        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk
         generator_response = completion_with_retry(
             self,
             messages=message_dicts,
             models_priority_list=self.models_priority_list,
             run_manager=run_manager,
             **params,
         )
@@ -317,32 +321,32 @@
             if chunk.event != "update":
                 continue
 
             chunk, default_chunk_class = self._create_chat_generation_chunk(
                 chunk.data, default_chunk_class
             )
 
-            yield chunk
-
             if run_manager:
                 run_manager.on_llm_new_token(
                     token=chunk.message.content, chunk=chunk.message
                 )
 
+            yield chunk
+
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[ChatGenerationChunk]:
         message_dicts, params = self._create_message_dicts(messages, stop)
         params = {**params, **kwargs, "stream": True}
 
-        default_chunk_class = AIMessageChunk
+        default_chunk_class: Type[BaseMessageChunk] = AIMessageChunk
         generator_response = acompletion_with_retry(
             self,
             messages=message_dicts,
             models_priority_list=self.models_priority_list,
             run_manager=run_manager,
             **params,
         )
@@ -350,21 +354,21 @@
             if chunk.event != "update":
                 continue
 
             chunk, default_chunk_class = self._create_chat_generation_chunk(
                 chunk.data, default_chunk_class
             )
 
-            yield chunk
-
             if run_manager:
                 await run_manager.on_llm_new_token(
                     token=chunk.message.content, chunk=chunk.message
                 )
 
+            yield chunk
+
     def _create_message_dicts(
         self, messages: List[BaseMessage], stop: Optional[List[str]]
     ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
         params = self._default_params
         if stop is not None:
             if "stop" in params:
                 raise ValueError("`stop` found in both the input and default params.")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/huggingface.py` & `gigachain_community-0.2.0/langchain_community/chat_models/mlx.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,66 +1,62 @@
-"""Hugging Face Chat Wrapper."""
-from typing import Any, List, Optional, Union
+"""MLX Chat Wrapper."""
+
+from typing import Any, Iterator, List, Optional
 
 from langchain_core.callbacks.manager import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
+    AIMessageChunk,
     BaseMessage,
     HumanMessage,
     SystemMessage,
 )
 from langchain_core.outputs import (
     ChatGeneration,
+    ChatGenerationChunk,
     ChatResult,
     LLMResult,
 )
 
-from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint
-from langchain_community.llms.huggingface_hub import HuggingFaceHub
-from langchain_community.llms.huggingface_text_gen_inference import (
-    HuggingFaceTextGenInference,
-)
+from langchain_community.llms.mlx_pipeline import MLXPipeline
 
 DEFAULT_SYSTEM_PROMPT = """You are a helpful, respectful, and honest assistant."""
 
 
-class ChatHuggingFace(BaseChatModel):
-    """
-    Wrapper for using Hugging Face LLM's as ChatModels.
+class ChatMLX(BaseChatModel):
+    """MLX chat models.
+
+    Works with `MLXPipeline` LLM.
 
-    Works with `HuggingFaceTextGenInference`, `HuggingFaceEndpoint`,
-    and `HuggingFaceHub` LLMs.
+    To use, you should have the ``mlx-lm`` python package installed.
 
-    Upon instantiating this class, the model_id is resolved from the url
-    provided to the LLM, and the appropriate tokenizer is loaded from
-    the HuggingFace Hub.
+    Example:
+        .. code-block:: python
+
+            from langchain_community.chat_models import chatMLX
+            from langchain_community.llms import MLXPipeline
+
+            llm = MLXPipeline.from_model_id(
+                model_id="mlx-community/quantized-gemma-2b-it",
+            )
+            chat = chatMLX(llm=llm)
 
-    Adapted from: https://python.langchain.com/docs/integrations/chat/llama2_chat
     """
 
-    llm: Union[HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub]
+    llm: MLXPipeline
     system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)
     tokenizer: Any = None
-    model_id: str = None  # type: ignore
 
     def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
-
-        from transformers import AutoTokenizer
-
-        self._resolve_model_id()
-        self.tokenizer = (
-            AutoTokenizer.from_pretrained(self.model_id)
-            if self.tokenizer is None
-            else self.tokenizer
-        )
+        self.tokenizer = self.llm.tokenizer
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -83,26 +79,31 @@
             prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs
         )
         return self._to_chat_result(llm_result)
 
     def _to_chat_prompt(
         self,
         messages: List[BaseMessage],
+        tokenize: bool = False,
+        return_tensors: Optional[str] = None,
     ) -> str:
         """Convert a list of messages into a prompt format expected by wrapped LLM."""
         if not messages:
-            raise ValueError("at least one HumanMessage must be provided")
+            raise ValueError("At least one HumanMessage must be provided!")
 
         if not isinstance(messages[-1], HumanMessage):
-            raise ValueError("last message must be a HumanMessage")
+            raise ValueError("Last message must be a HumanMessage!")
 
         messages_dicts = [self._to_chatml_format(m) for m in messages]
 
         return self.tokenizer.apply_chat_template(
-            messages_dicts, tokenize=False, add_generation_prompt=True
+            messages_dicts,
+            tokenize=tokenize,
+            add_generation_prompt=True,
+            return_tensors=return_tensors,
         )
 
     def _to_chatml_format(self, message: BaseMessage) -> dict:
         """Convert LangChain message to ChatML format."""
 
         if isinstance(message, SystemMessage):
             role = "system"
@@ -125,42 +126,67 @@
             )
             chat_generations.append(chat_generation)
 
         return ChatResult(
             generations=chat_generations, llm_output=llm_result.llm_output
         )
 
-    def _resolve_model_id(self) -> None:
-        """Resolve the model_id from the LLM's inference_server_url"""
-
-        from huggingface_hub import list_inference_endpoints
-
-        available_endpoints = list_inference_endpoints("*")
-
-        if isinstance(self.llm, HuggingFaceTextGenInference):
-            endpoint_url = self.llm.inference_server_url
+    @property
+    def _llm_type(self) -> str:
+        return "mlx-chat-wrapper"
 
-        elif isinstance(self.llm, HuggingFaceEndpoint):
-            endpoint_url = self.llm.endpoint_url
+    def _stream(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> Iterator[ChatGenerationChunk]:
+        try:
+            import mlx.core as mx
+            from mlx_lm.utils import generate_step
+
+        except ImportError:
+            raise ImportError(
+                "Could not import mlx_lm python package. "
+                "Please install it with `pip install mlx_lm`."
+            )
+        model_kwargs = kwargs.get("model_kwargs", self.llm.pipeline_kwargs)
+        temp: float = model_kwargs.get("temp", 0.0)
+        max_new_tokens: int = model_kwargs.get("max_tokens", 100)
+        repetition_penalty: Optional[float] = model_kwargs.get(
+            "repetition_penalty", None
+        )
+        repetition_context_size: Optional[int] = model_kwargs.get(
+            "repetition_context_size", None
+        )
 
-        elif isinstance(self.llm, HuggingFaceHub):
-            # no need to look up model_id for HuggingFaceHub LLM
-            self.model_id = self.llm.repo_id
-            return
+        llm_input = self._to_chat_prompt(messages, tokenize=True, return_tensors="np")
 
-        else:
-            raise ValueError(f"Unknown LLM type: {type(self.llm)}")
+        prompt_tokens = mx.array(llm_input[0])
 
-        for endpoint in available_endpoints:
-            if endpoint.url == endpoint_url:
-                self.model_id = endpoint.repository
-
-        if not self.model_id:
-            raise ValueError(
-                "Failed to resolve model_id"
-                f"Could not find model id for inference server provided: {endpoint_url}"
-                "Make sure that your Hugging Face token has access to the endpoint."
-            )
+        eos_token_id = self.tokenizer.eos_token_id
 
-    @property
-    def _llm_type(self) -> str:
-        return "huggingface-chat-wrapper"
+        for (token, prob), n in zip(
+            generate_step(
+                prompt_tokens,
+                self.llm.model,
+                temp,
+                repetition_penalty,
+                repetition_context_size,
+            ),
+            range(max_new_tokens),
+        ):
+            # identify text to yield
+            text: Optional[str] = None
+            text = self.tokenizer.decode(token.item())
+
+            # yield text, if any
+            if text:
+                chunk = ChatGenerationChunk(message=AIMessageChunk(content=text))
+                yield chunk
+                if run_manager:
+                    run_manager.on_llm_new_token(text, chunk=chunk)
+
+            # break if stop sequence found
+            if token == eos_token_id or (stop is not None and text in stop):
+                break
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/hunyuan.py` & `gigachain_community-0.2.0/langchain_community/chat_models/hunyuan.py`

 * *Files 1% similar despite different names*

```diff
@@ -68,17 +68,17 @@
     content = _dict.get("content") or ""
 
     if role == "user" or default_class == HumanMessageChunk:
         return HumanMessageChunk(content=content)
     elif role == "assistant" or default_class == AIMessageChunk:
         return AIMessageChunk(content=content)
     elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
     else:
-        return default_class(content=content)
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
 # signature generation
 # https://cloud.tencent.com/document/product/1729/97732#532252ce-e960-48a7-8821-940a9ce2ccf3
 def _signature(secret_key: SecretStr, url: str, payload: Dict[str, Any]) -> str:
     sorted_keys = sorted(payload.keys())
 
@@ -271,17 +271,18 @@
                 raise ValueError(f"Error from Hunyuan api response: {response}")
 
             for choice in response["choices"]:
                 chunk = _convert_delta_to_message_chunk(
                     choice["delta"], default_chunk_class
                 )
                 default_chunk_class = chunk.__class__
-                yield ChatGenerationChunk(message=chunk)
+                cg_chunk = ChatGenerationChunk(message=chunk)
                 if run_manager:
-                    run_manager.on_llm_new_token(chunk.content)
+                    run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+                yield cg_chunk
 
     def _chat(self, messages: List[BaseMessage], **kwargs: Any) -> requests.Response:
         if self.hunyuan_secret_key is None:
             raise ValueError("Hunyuan secret key is not set.")
 
         parameters = {**self._default_params, **kwargs}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/javelin_ai_gateway.py` & `gigachain_community-0.2.0/langchain_community/chat_models/javelin_ai_gateway.py`

 * *Files 1% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr
 
 logger = logging.getLogger(__name__)
 
 
 # Ignoring type because below is valid pydantic code
 # Unexpected keyword argument "extra" for "__init_subclass__" of "object"  [call-arg]
-class ChatParams(BaseModel, extra=Extra.allow):  # type: ignore[call-arg]
+class ChatParams(BaseModel, extra=Extra.allow):
     """Parameters for the `Javelin AI Gateway` LLM."""
 
     temperature: float = 0.0
     stop: Optional[List[str]] = None
     max_tokens: Optional[int] = None
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/jinachat.py` & `gigachain_community-0.2.0/langchain_community/chat_models/jinachat.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """JinaChat wrapper."""
+
 from __future__ import annotations
 
 import logging
 from typing import (
     Any,
     AsyncIterator,
     Callable,
@@ -99,17 +100,17 @@
     if role == "user" or default_class == HumanMessageChunk:
         return HumanMessageChunk(content=content)
     elif role == "assistant" or default_class == AIMessageChunk:
         return AIMessageChunk(content=content)
     elif role == "system" or default_class == SystemMessageChunk:
         return SystemMessageChunk(content=content)
     elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
     else:
-        return default_class(content=content)
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
 def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
     role = _dict["role"]
     if role == "user":
         return HumanMessage(content=_dict["content"])
     elif role == "assistant":
@@ -223,15 +224,15 @@
         values["jinachat_api_key"] = convert_to_secret_str(
             get_from_dict_or_env(values, "jinachat_api_key", "JINACHAT_API_KEY")
         )
         try:
             import openai
 
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import openai python package. "
                 "Please install it with `pip install openai`."
             )
         try:
             values["client"] = openai.ChatCompletion
         except AttributeError:
             raise ValueError(
@@ -308,17 +309,18 @@
         params = {**params, **kwargs, "stream": True}
 
         default_chunk_class = AIMessageChunk
         for chunk in self.completion_with_retry(messages=message_dicts, **params):
             delta = chunk["choices"][0]["delta"]
             chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)
             default_chunk_class = chunk.__class__
-            yield ChatGenerationChunk(message=chunk)
+            cg_chunk = ChatGenerationChunk(message=chunk)
             if run_manager:
-                run_manager.on_llm_new_token(chunk.content)
+                run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -367,17 +369,18 @@
         default_chunk_class = AIMessageChunk
         async for chunk in await acompletion_with_retry(
             self, messages=message_dicts, **params
         ):
             delta = chunk["choices"][0]["delta"]
             chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)
             default_chunk_class = chunk.__class__
-            yield ChatGenerationChunk(message=chunk)
+            cg_chunk = ChatGenerationChunk(message=chunk)
             if run_manager:
-                await run_manager.on_llm_new_token(chunk.content)
+                await run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
 
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/konko.py` & `gigachain_community-0.2.0/langchain_community/chat_models/konko.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,50 +1,52 @@
 """KonkoAI chat wrapper."""
+
 from __future__ import annotations
 
 import logging
 import os
+import warnings
 from typing import (
     Any,
     Dict,
     Iterator,
     List,
-    Mapping,
     Optional,
     Set,
     Tuple,
     Union,
+    cast,
 )
 
 import requests
 from langchain_core.callbacks import (
     CallbackManagerForLLMRun,
 )
-from langchain_core.language_models.chat_models import (
-    BaseChatModel,
-    generate_from_stream,
-)
 from langchain_core.messages import AIMessageChunk, BaseMessage
-from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
-from langchain_core.pydantic_v1 import Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.outputs import ChatGenerationChunk, ChatResult
+from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.adapters.openai import (
-    convert_dict_to_message,
     convert_message_to_dict,
 )
-from langchain_community.chat_models.openai import _convert_delta_to_message_chunk
+from langchain_community.chat_models.openai import (
+    ChatOpenAI,
+    _convert_delta_to_message_chunk,
+    generate_from_stream,
+)
+from langchain_community.utils.openai import is_openai_v1
 
 DEFAULT_API_BASE = "https://api.konko.ai/v1"
 DEFAULT_MODEL = "meta-llama/Llama-2-13b-chat-hf"
 
 logger = logging.getLogger(__name__)
 
 
-class ChatKonko(BaseChatModel):
+class ChatKonko(ChatOpenAI):
     """`ChatKonko` Chat large language models API.
 
     To use, you should have the ``konko`` python package installed, and the
     environment variable ``KONKO_API_KEY`` and ``OPENAI_API_KEY`` set with your API key.
 
     Any parameters that are valid to be passed to the konko.create call can be passed
     in, even if not explicitly saved on this class.
@@ -70,99 +72,112 @@
     """Model name to use."""
     temperature: float = 0.7
     """What sampling temperature to use."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any model parameters valid for `create` call not explicitly specified."""
     openai_api_key: Optional[str] = None
     konko_api_key: Optional[str] = None
-    request_timeout: Optional[Union[float, Tuple[float, float]]] = None
-    """Timeout for requests to Konko completion API."""
     max_retries: int = 6
     """Maximum number of retries to make when generating."""
     streaming: bool = False
     """Whether to stream the results or not."""
     n: int = 1
     """Number of chat completions to generate for each prompt."""
     max_tokens: int = 20
     """Maximum number of tokens to generate."""
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        values["konko_api_key"] = get_from_dict_or_env(
-            values, "konko_api_key", "KONKO_API_KEY"
+        values["konko_api_key"] = convert_to_secret_str(
+            get_from_dict_or_env(values, "konko_api_key", "KONKO_API_KEY")
         )
         try:
             import konko
 
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import konko python package. "
                 "Please install it with `pip install konko`."
             )
         try:
-            values["client"] = konko.ChatCompletion
+            if is_openai_v1():
+                values["client"] = konko.chat.completions
+            else:
+                values["client"] = konko.ChatCompletion
         except AttributeError:
             raise ValueError(
                 "`konko` has no `ChatCompletion` attribute, this is likely "
                 "due to an old version of the konko package. Try upgrading it "
                 "with `pip install --upgrade konko`."
             )
+
+        if not hasattr(konko, "_is_legacy_openai"):
+            warnings.warn(
+                "You are using an older version of the 'konko' package. "
+                "Please consider upgrading to access new features."
+            )
+
         if values["n"] < 1:
             raise ValueError("n must be at least 1.")
         if values["n"] > 1 and values["streaming"]:
             raise ValueError("n must be 1 when streaming.")
         return values
 
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Konko API."""
         return {
             "model": self.model,
-            "request_timeout": self.request_timeout,
             "max_tokens": self.max_tokens,
             "stream": self.streaming,
             "n": self.n,
             "temperature": self.temperature,
             **self.model_kwargs,
         }
 
     @staticmethod
     def get_available_models(
-        konko_api_key: Optional[str] = None,
-        openai_api_key: Optional[str] = None,
+        konko_api_key: Union[str, SecretStr, None] = None,
+        openai_api_key: Union[str, SecretStr, None] = None,
         konko_api_base: str = DEFAULT_API_BASE,
     ) -> Set[str]:
         """Get available models from Konko API."""
 
         # Try to retrieve the OpenAI API key if it's not passed as an argument
         if not openai_api_key:
             try:
-                openai_api_key = os.environ["OPENAI_API_KEY"]
+                openai_api_key = convert_to_secret_str(os.environ["OPENAI_API_KEY"])
             except KeyError:
                 pass  # It's okay if it's not set, we just won't use it
+        elif isinstance(openai_api_key, str):
+            openai_api_key = convert_to_secret_str(openai_api_key)
 
         # Try to retrieve the Konko API key if it's not passed as an argument
         if not konko_api_key:
             try:
-                konko_api_key = os.environ["KONKO_API_KEY"]
+                konko_api_key = convert_to_secret_str(os.environ["KONKO_API_KEY"])
             except KeyError:
                 raise ValueError(
                     "Konko API key must be passed as keyword argument or "
                     "set in environment variable KONKO_API_KEY."
                 )
+        elif isinstance(konko_api_key, str):
+            konko_api_key = convert_to_secret_str(konko_api_key)
 
         models_url = f"{konko_api_base}/models"
 
         headers = {
-            "Authorization": f"Bearer {konko_api_key}",
+            "Authorization": f"Bearer {konko_api_key.get_secret_value()}",
         }
 
         if openai_api_key:
-            headers["X-OpenAI-Api-Key"] = openai_api_key
+            headers["X-OpenAI-Api-Key"] = cast(
+                SecretStr, openai_api_key
+            ).get_secret_value()
 
         models_response = requests.get(models_url, headers=headers)
 
         if models_response.status_code != 200:
             raise ValueError(
                 f"Error getting models from {models_url}: "
                 f"{models_response.status_code}"
@@ -174,28 +189,14 @@
         self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any
     ) -> Any:
         def _completion_with_retry(**kwargs: Any) -> Any:
             return self.client.create(**kwargs)
 
         return _completion_with_retry(**kwargs)
 
-    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:
-        overall_token_usage: dict = {}
-        for output in llm_outputs:
-            if output is None:
-                # Happens in streaming
-                continue
-            token_usage = output["token_usage"]
-            for k, v in token_usage.items():
-                if k in overall_token_usage:
-                    overall_token_usage[k] += v
-                else:
-                    overall_token_usage[k] = v
-        return {"token_usage": overall_token_usage, "model_name": self.model}
-
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
@@ -213,18 +214,20 @@
                 choice["delta"], default_chunk_class
             )
             finish_reason = choice.get("finish_reason")
             generation_info = (
                 dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
             default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
+            cg_chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info
+            )
             if run_manager:
-                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+                run_manager.on_llm_new_token(cg_chunk.text, chunk=cg_chunk)
+            yield cg_chunk
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         stream: Optional[bool] = None,
@@ -251,27 +254,14 @@
         if stop is not None:
             if "stop" in params:
                 raise ValueError("`stop` found in both the input and default params.")
             params["stop"] = stop
         message_dicts = [convert_message_to_dict(m) for m in messages]
         return message_dicts, params
 
-    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:
-        generations = []
-        for res in response["choices"]:
-            message = convert_dict_to_message(res["message"])
-            gen = ChatGeneration(
-                message=message,
-                generation_info=dict(finish_reason=res.get("finish_reason")),
-            )
-            generations.append(gen)
-        token_usage = response.get("usage", {})
-        llm_output = {"token_usage": token_usage, "model_name": self.model}
-        return ChatResult(generations=generations, llm_output=llm_output)
-
     @property
     def _identifying_params(self) -> Dict[str, Any]:
         """Get the identifying parameters."""
         return {**{"model_name": self.model}, **self._default_params}
 
     @property
     def _client_params(self) -> Dict[str, Any]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/litellm.py` & `gigachain_community-0.2.0/langchain_community/chat_models/litellm.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around LiteLLM's model I/O library."""
+
 from __future__ import annotations
 
 import logging
 from typing import (
     Any,
     AsyncIterator,
     Callable,
@@ -127,17 +128,17 @@
     elif role == "assistant" or default_class == AIMessageChunk:
         return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)
     elif role == "system" or default_class == SystemMessageChunk:
         return SystemMessageChunk(content=content)
     elif role == "function" or default_class == FunctionMessageChunk:
         return FunctionMessageChunk(content=content, name=_dict["name"])
     elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
     else:
-        return default_class(content=content)
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
 def _convert_message_to_dict(message: BaseMessage) -> dict:
     if isinstance(message, ChatMessage):
         message_dict = {"role": message.role, "content": message.content}
     elif isinstance(message, HumanMessage):
         message_dict = {"role": "user", "content": message.content}
@@ -157,15 +158,15 @@
         raise ValueError(f"Got unknown type {message}")
     if "name" in message.additional_kwargs:
         message_dict["name"] = message.additional_kwargs["name"]
     return message_dict
 
 
 class ChatLiteLLM(BaseChatModel):
-    """A chat model that uses the LiteLLM API."""
+    """Chat model that uses the LiteLLM API."""
 
     client: Any  #: :meta private:
     model: str = "gpt-3.5-turbo"
     model_name: Optional[str] = None
     """Model name to use."""
     openai_api_key: Optional[str] = None
     azure_api_key: Optional[str] = None
@@ -219,14 +220,15 @@
         if self.model_name is not None:
             set_model_value = self.model_name
         self.client.api_base = self.api_base
         self.client.organization = self.organization
         creds: Dict[str, Any] = {
             "model": set_model_value,
             "force_timeout": self.request_timeout,
+            "api_base": self.api_base,
         }
         return {**self._default_params, **creds}
 
     def completion_with_retry(
         self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any
     ) -> Any:
         """Use tenacity to retry the completion call."""
@@ -241,16 +243,16 @@
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate api key, python package exists, temperature, top_p, and top_k."""
         try:
             import litellm
         except ImportError:
             raise ChatLiteLLMException(
-                "Could not import google.generativeai python package. "
-                "Please install it with `pip install google-generativeai`"
+                "Could not import litellm python package. "
+                "Please install it with `pip install litellm`"
             )
 
         values["openai_api_key"] = get_from_dict_or_env(
             values, "openai_api_key", "OPENAI_API_KEY", default=""
         )
         values["azure_api_key"] = get_from_dict_or_env(
             values, "azure_api_key", "AZURE_API_KEY", default=""
@@ -350,17 +352,18 @@
             messages=message_dicts, run_manager=run_manager, **params
         ):
             if len(chunk["choices"]) == 0:
                 continue
             delta = chunk["choices"][0]["delta"]
             chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)
             default_chunk_class = chunk.__class__
-            yield ChatGenerationChunk(message=chunk)
+            cg_chunk = ChatGenerationChunk(message=chunk)
             if run_manager:
-                run_manager.on_llm_new_token(chunk.content)
+                run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
 
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -373,17 +376,18 @@
             self, messages=message_dicts, run_manager=run_manager, **params
         ):
             if len(chunk["choices"]) == 0:
                 continue
             delta = chunk["choices"][0]["delta"]
             chunk = _convert_delta_to_message_chunk(delta, default_chunk_class)
             default_chunk_class = chunk.__class__
-            yield ChatGenerationChunk(message=chunk)
+            cg_chunk = ChatGenerationChunk(message=chunk)
             if run_manager:
-                await run_manager.on_llm_new_token(chunk.content)
+                await run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
 
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         stream: Optional[bool] = None,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/meta.py` & `gigachain_community-0.2.0/langchain_community/chat_models/meta.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/minimax.py` & `gigachain_community-0.2.0/langchain_community/chat_models/minimax.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 """Wrapper around Minimax chat models."""
+
 import logging
 from typing import Any, Dict, List, Optional, cast
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
     HumanMessage,
 )
-from langchain_core.outputs import ChatResult
+from langchain_core.outputs import ChatGeneration, ChatResult
 
 from langchain_community.llms.minimax import MinimaxCommon
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
@@ -33,15 +34,15 @@
             chat_history.append(_parse_message("USER", content))
         if isinstance(message, AIMessage):
             chat_history.append(_parse_message("BOT", content))
     return chat_history
 
 
 class MiniMaxChat(MinimaxCommon, BaseChatModel):
-    """Wrapper around Minimax large language models.
+    """MiniMax large language models.
 
     To use, you should have the environment variable ``MINIMAX_GROUP_ID`` and
     ``MINIMAX_API_KEY`` set with your API token, or pass it as a named parameter to
     the constructor.
 
     Example:
         .. code-block:: python
@@ -77,15 +78,16 @@
             )
         history = _parse_chat_history(messages)
         payload = self._default_params
         payload["messages"] = history
         text = self._client.post(payload)
 
         # This is required since the stop are not enforced by the model parameters
-        return text if stop is None else enforce_stop_tokens(text, stop)
+        text = text if stop is None else enforce_stop_tokens(text, stop)
+        return ChatResult(generations=[ChatGeneration(message=AIMessage(text))])  # type: ignore[misc]
 
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/mlflow.py` & `gigachain_community-0.2.0/langchain_community/chat_models/mlflow.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,40 +1,42 @@
-import asyncio
 import logging
-from functools import partial
-from typing import Any, Dict, List, Mapping, Optional
+from typing import Any, Dict, Iterator, List, Mapping, Optional, cast
 from urllib.parse import urlparse
 
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
-)
+from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models import BaseChatModel
+from langchain_core.language_models.base import LanguageModelInput
 from langchain_core.messages import (
     AIMessage,
+    AIMessageChunk,
     BaseMessage,
+    BaseMessageChunk,
     ChatMessage,
+    ChatMessageChunk,
     FunctionMessage,
     HumanMessage,
+    HumanMessageChunk,
     SystemMessage,
+    SystemMessageChunk,
 )
-from langchain_core.outputs import ChatGeneration, ChatResult
+from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
 from langchain_core.pydantic_v1 import (
     Field,
     PrivateAttr,
 )
+from langchain_core.runnables import RunnableConfig
 
 logger = logging.getLogger(__name__)
 
 
 class ChatMlflow(BaseChatModel):
     """`MLflow` chat models API.
 
     To use, you should have the `mlflow[genai]` python package installed.
-    For more information, see https://mlflow.org/docs/latest/llms/deployments/server.html.
+    For more information, see https://mlflow.org/docs/latest/llms/deployments.
 
     Example:
         .. code-block:: python
 
             from langchain_community.chat_models import ChatMlflow
 
             chat = ChatMlflow(
@@ -97,49 +99,103 @@
             "n": self.n,
             "stop": self.stop,
             "max_tokens": self.max_tokens,
             "extra_params": self.extra_params,
         }
         return params
 
-    def _generate(
+    def _prepare_inputs(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> ChatResult:
+    ) -> Dict[str, Any]:
         message_dicts = [
             ChatMlflow._convert_message_to_dict(message) for message in messages
         ]
         data: Dict[str, Any] = {
             "messages": message_dicts,
             "temperature": self.temperature,
             "n": self.n,
             **self.extra_params,
             **kwargs,
         }
         if stop := self.stop or stop:
             data["stop"] = stop
         if self.max_tokens is not None:
             data["max_tokens"] = self.max_tokens
+
+        return data
+
+    def _generate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> ChatResult:
+        data = self._prepare_inputs(
+            messages,
+            stop,
+            **kwargs,
+        )
         resp = self._client.predict(endpoint=self.endpoint, inputs=data)
         return ChatMlflow._create_chat_result(resp)
 
-    async def _agenerate(
+    def stream(
+        self,
+        input: LanguageModelInput,
+        config: Optional[RunnableConfig] = None,
+        *,
+        stop: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> Iterator[BaseMessageChunk]:
+        # We need to override `stream` to handle the case
+        # that `self._client` does not implement `predict_stream`
+        if not hasattr(self._client, "predict_stream"):
+            # MLflow deployment client does not implement streaming,
+            # so use default implementation
+            yield cast(
+                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
+            )
+        else:
+            yield from super().stream(input, config, stop=stop, **kwargs)
+
+    def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> ChatResult:
-        func = partial(
-            self._generate, messages, stop=stop, run_manager=run_manager, **kwargs
+    ) -> Iterator[ChatGenerationChunk]:
+        data = self._prepare_inputs(
+            messages,
+            stop,
+            **kwargs,
         )
-        return await asyncio.get_event_loop().run_in_executor(None, func)
+        # TODO: check if `_client.predict_stream` is available.
+        chunk_iter = self._client.predict_stream(endpoint=self.endpoint, inputs=data)
+        for chunk in chunk_iter:
+            choice = chunk["choices"][0]
+            chunk = ChatMlflow._convert_delta_to_message_chunk(choice["delta"])
+
+            generation_info = {}
+            if finish_reason := choice.get("finish_reason"):
+                generation_info["finish_reason"] = finish_reason
+            if logprobs := choice.get("logprobs"):
+                generation_info["logprobs"] = logprobs
+
+            chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info or None
+            )
+
+            if run_manager:
+                run_manager.on_llm_new_token(chunk.text, chunk=chunk, logprobs=logprobs)
+
+            yield chunk
 
     @property
     def _identifying_params(self) -> Dict[str, Any]:
         return self._default_params
 
     def _get_invocation_params(
         self, stop: Optional[List[str]] = None, **kwargs: Any
@@ -165,14 +221,27 @@
             return AIMessage(content=content)
         elif role == "system":
             return SystemMessage(content=content)
         else:
             return ChatMessage(content=content, role=role)
 
     @staticmethod
+    def _convert_delta_to_message_chunk(_dict: Mapping[str, Any]) -> BaseMessageChunk:
+        role = _dict["role"]
+        content = _dict["content"]
+        if role == "user":
+            return HumanMessageChunk(content=content)
+        elif role == "assistant":
+            return AIMessageChunk(content=content)
+        elif role == "system":
+            return SystemMessageChunk(content=content)
+        else:
+            return ChatMessageChunk(content=content, role=role)
+
+    @staticmethod
     def _raise_functions_not_supported() -> None:
         raise ValueError(
             "Function messages are not supported by Databricks. Please"
             " create a feature request at https://github.com/mlflow/mlflow/issues."
         )
 
     @staticmethod
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/mlflow_ai_gateway.py` & `gigachain_community-0.2.0/langchain_community/chat_models/mlflow_ai_gateway.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,12 @@
-import asyncio
 import logging
 import warnings
-from functools import partial
 from typing import Any, Dict, List, Mapping, Optional
 
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
     ChatMessage,
@@ -24,15 +21,15 @@
 from langchain_core.pydantic_v1 import BaseModel, Extra
 
 logger = logging.getLogger(__name__)
 
 
 # Ignoring type because below is valid pydantic code
 # Unexpected keyword argument "extra" for "__init_subclass__" of "object"  [call-arg]
-class ChatParams(BaseModel, extra=Extra.allow):  # type: ignore[call-arg]
+class ChatParams(BaseModel, extra=Extra.allow):
     """Parameters for the `MLflow AI Gateway` LLM."""
 
     temperature: float = 0.0
     candidate_count: int = 1
     """The number of candidates to return."""
     stop: Optional[List[str]] = None
     max_tokens: Optional[int] = None
@@ -112,26 +109,14 @@
             "messages": message_dicts,
             **(self.params.dict() if self.params else {}),
         }
 
         resp = mlflow.gateway.query(self.route, data=data)
         return ChatMLflowAIGateway._create_chat_result(resp)
 
-    async def _agenerate(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        func = partial(
-            self._generate, messages, stop=stop, run_manager=run_manager, **kwargs
-        )
-        return await asyncio.get_event_loop().run_in_executor(None, func)
-
     @property
     def _identifying_params(self) -> Dict[str, Any]:
         return self._default_params
 
     def _get_invocation_params(
         self, stop: Optional[List[str]] = None, **kwargs: Any
     ) -> Dict[str, Any]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/ollama.py` & `gigachain_community-0.2.0/langchain_community/chat_models/ollama.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import json
-from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
+from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union, cast
 
 from langchain_core._api import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
@@ -70,18 +70,23 @@
         return False
 
     @deprecated("0.0.3", alternative="_convert_messages_to_ollama_messages")
     def _format_message_as_text(self, message: BaseMessage) -> str:
         if isinstance(message, ChatMessage):
             message_text = f"\n\n{message.role.capitalize()}: {message.content}"
         elif isinstance(message, HumanMessage):
-            if message.content[0].get("type") == "text":
-                message_text = f"[INST] {message.content[0]['text']} [/INST]"
-            elif message.content[0].get("type") == "image_url":
-                message_text = message.content[0]["image_url"]["url"]
+            if isinstance(message.content, List):
+                first_content = cast(List[Dict], message.content)[0]
+                content_type = first_content.get("type")
+                if content_type == "text":
+                    message_text = f"[INST] {first_content['text']} [/INST]"
+                elif content_type == "image_url":
+                    message_text = first_content["image_url"]["url"]
+            else:
+                message_text = f"[INST] {message.content} [/INST]"
         elif isinstance(message, AIMessage):
             message_text = f"{message.content}"
         elif isinstance(message, SystemMessage):
             message_text = f"<<SYS>> {message.content} <</SYS>>"
         else:
             raise ValueError(f"Got unknown type {message}")
         return message_text
@@ -90,15 +95,15 @@
         return "\n".join(
             [self._format_message_as_text(message) for message in messages]
         )
 
     def _convert_messages_to_ollama_messages(
         self, messages: List[BaseMessage]
     ) -> List[Dict[str, Union[str, List[str]]]]:
-        ollama_messages = []
+        ollama_messages: List = []
         for message in messages:
             role = ""
             if isinstance(message, HumanMessage):
                 role = "user"
             elif isinstance(message, AIMessage):
                 role = "assistant"
             elif isinstance(message, SystemMessage):
@@ -107,15 +112,15 @@
                 raise ValueError("Received unsupported message type for Ollama.")
 
             content = ""
             images = []
             if isinstance(message.content, str):
                 content = message.content
             else:
-                for content_part in message.content:
+                for content_part in cast(List[Dict], message.content):
                     if content_part.get("type") == "text":
                         content += f"\n{content_part['text']}"
                     elif content_part.get("type") == "image_url":
                         if isinstance(content_part.get("image_url"), str):
                             image_url_components = content_part["image_url"].split(",")
                             # Support data:image/jpeg;base64,<image> format
                             # and base64 strings
@@ -147,31 +152,33 @@
     def _create_chat_stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         payload = {
+            "model": self.model,
             "messages": self._convert_messages_to_ollama_messages(messages),
         }
         yield from self._create_stream(
-            payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
+            payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat", **kwargs
         )
 
     async def _acreate_chat_stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> AsyncIterator[str]:
         payload = {
+            "model": self.model,
             "messages": self._convert_messages_to_ollama_messages(messages),
         }
         async for stream_resp in self._acreate_stream(
-            payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
+            payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat", **kwargs
         ):
             yield stream_resp
 
     def _chat_stream_with_aggregation(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
@@ -186,14 +193,15 @@
                 if final_chunk is None:
                     final_chunk = chunk
                 else:
                     final_chunk += chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
                         chunk.text,
+                        chunk=chunk,
                         verbose=verbose,
                     )
         if final_chunk is None:
             raise ValueError("No data received from Ollama stream.")
 
         return final_chunk
 
@@ -212,14 +220,15 @@
                 if final_chunk is None:
                     final_chunk = chunk
                 else:
                     final_chunk += chunk
                 if run_manager:
                     await run_manager.on_llm_new_token(
                         chunk.text,
+                        chunk=chunk,
                         verbose=verbose,
                     )
         if final_chunk is None:
             raise ValueError("No data received from Ollama stream.")
 
         return final_chunk
 
@@ -304,57 +313,54 @@
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         try:
             for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
                 if stream_resp:
                     chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
-                    yield chunk
                     if run_manager:
                         run_manager.on_llm_new_token(
                             chunk.text,
+                            chunk=chunk,
                             verbose=self.verbose,
                         )
+                    yield chunk
         except OllamaEndpointNotFoundError:
             yield from self._legacy_stream(messages, stop, **kwargs)
 
     async def _astream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[ChatGenerationChunk]:
-        try:
-            async for stream_resp in self._create_async_chat_stream(
-                messages, stop, **kwargs
-            ):
-                if stream_resp:
-                    chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
-                    yield chunk
-                    if run_manager:
-                        await run_manager.on_llm_new_token(
-                            chunk.text,
-                            verbose=self.verbose,
-                        )
-        except OllamaEndpointNotFoundError:
-            async for chunk in self._legacy_astream(messages, stop, **kwargs):
+        async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):
+            if stream_resp:
+                chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
+                if run_manager:
+                    await run_manager.on_llm_new_token(
+                        chunk.text,
+                        chunk=chunk,
+                        verbose=self.verbose,
+                    )
                 yield chunk
 
     @deprecated("0.0.3", alternative="_stream")
     def _legacy_stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         prompt = self._format_messages_as_text(messages)
         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
             if stream_resp:
                 chunk = _stream_response_to_chat_generation_chunk(stream_resp)
-                yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
                         chunk.text,
+                        chunk=chunk,
                         verbose=self.verbose,
                     )
+                yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/openai.py` & `gigachain_community-0.2.0/langchain_community/chat_models/openai.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """OpenAI chat wrapper."""
+
 from __future__ import annotations
 
 import logging
 import os
 import sys
 from typing import (
     TYPE_CHECKING,
@@ -16,14 +17,15 @@
     Optional,
     Sequence,
     Tuple,
     Type,
     Union,
 )
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models import LanguageModelInput
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
@@ -62,15 +64,15 @@
 logger = logging.getLogger(__name__)
 
 
 def _import_tiktoken() -> Any:
     try:
         import tiktoken
     except ImportError:
-        raise ValueError(
+        raise ImportError(
             "Could not import tiktoken python package. "
             "This is needed in order to calculate get_token_ids. "
             "Please install it with `pip install tiktoken`."
         )
     return tiktoken
 
 
@@ -134,33 +136,36 @@
     elif role == "system" or default_class == SystemMessageChunk:
         return SystemMessageChunk(content=content)
     elif role == "function" or default_class == FunctionMessageChunk:
         return FunctionMessageChunk(content=content, name=_dict["name"])
     elif role == "tool" or default_class == ToolMessageChunk:
         return ToolMessageChunk(content=content, tool_call_id=_dict["tool_call_id"])
     elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
     else:
-        return default_class(content=content)
+        return default_class(content=content)  # type: ignore[call-arg]
 
 
+@deprecated(
+    since="0.0.10", removal="0.3.0", alternative_import="langchain_openai.ChatOpenAI"
+)
 class ChatOpenAI(BaseChatModel):
     """`OpenAI` Chat large language models API.
 
     To use, you should have the ``openai`` python package installed, and the
     environment variable ``OPENAI_API_KEY`` set with your API key.
 
     Any parameters that are valid to be passed to the openai.create call can be passed
     in, even if not explicitly saved on this class.
 
     Example:
         .. code-block:: python
 
             from langchain_community.chat_models import ChatOpenAI
-            openai = ChatOpenAI(model_name="gpt-3.5-turbo")
+            openai = ChatOpenAI(model="gpt-3.5-turbo")
     """
 
     @property
     def lc_secrets(self) -> Dict[str, str]:
         return {"openai_api_key": "OPENAI_API_KEY"}
 
     @classmethod
@@ -209,15 +214,15 @@
     # to support explicit proxy for OpenAI
     openai_proxy: Optional[str] = None
     request_timeout: Union[float, Tuple[float, float], Any, None] = Field(
         default=None, alias="timeout"
     )
     """Timeout for requests to OpenAI completion API. Can be float, httpx.Timeout or 
         None."""
-    max_retries: int = 2
+    max_retries: int = Field(default=2)
     """Maximum number of retries to make when generating."""
     streaming: bool = False
     """Whether to stream the results or not."""
     n: int = 1
     """Number of chat completions to generate for each prompt."""
     max_tokens: Optional[int] = None
     """Maximum number of tokens to generate."""
@@ -403,18 +408,20 @@
                 choice["delta"], default_chunk_class
             )
             finish_reason = choice.get("finish_reason")
             generation_info = (
                 dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
             default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
+            cg_chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info
+            )
             if run_manager:
-                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+                run_manager.on_llm_new_token(cg_chunk.text, chunk=cg_chunk)
+            yield cg_chunk
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         stream: Optional[bool] = None,
@@ -493,18 +500,20 @@
                 choice["delta"], default_chunk_class
             )
             finish_reason = choice.get("finish_reason")
             generation_info = (
                 dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
             default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
+            cg_chunk = ChatGenerationChunk(
+                message=chunk, generation_info=generation_info
+            )
             if run_manager:
-                await run_manager.on_llm_new_token(token=chunk.text, chunk=chunk)
+                await run_manager.on_llm_new_token(token=cg_chunk.text, chunk=cg_chunk)
+            yield cg_chunk
 
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         stream: Optional[bool] = None,
@@ -546,15 +555,15 @@
                     "api_base": self.openai_api_base,
                     "organization": self.openai_organization,
                 }
             )
         if self.openai_proxy:
             import openai
 
-            openai.proxy = {"http": self.openai_proxy, "https": self.openai_proxy}  # type: ignore[assignment]  # noqa: E501
+            openai.proxy = {"http": self.openai_proxy, "https": self.openai_proxy}
         return {**self._default_params, **openai_creds}
 
     def _get_invocation_params(
         self, stop: Optional[List[str]] = None, **kwargs: Any
     ) -> Dict[str, Any]:
         """Get the parameters used to invoke the model."""
         return {
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/pai_eas_endpoint.py` & `gigachain_community-0.2.0/langchain_community/chat_models/pai_eas_endpoint.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,9 @@
-import asyncio
 import json
 import logging
-from functools import partial
 from typing import Any, AsyncIterator, Dict, List, Optional, cast
 
 import requests
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
@@ -24,15 +22,15 @@
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
 class PaiEasChatEndpoint(BaseChatModel):
-    """Eas LLM Service chat model API.
+    """Alibaba Cloud PAI-EAS LLM Service chat model API.
 
         To use, must have a deployed eas chat llm service on AliCloud. One can set the
     environment variable ``eas_service_url`` and ``eas_service_token`` set with your eas
     service url and service token.
 
     Example:
         .. code-block:: python
@@ -289,36 +287,17 @@
                 if stop_seq_found:
                     content.content = content.content[
                         : content.content.index(stop_seq_found)
                     ]
 
                 # yield text, if any
                 if text:
+                    cg_chunk = ChatGenerationChunk(message=content)
                     if run_manager:
-                        await run_manager.on_llm_new_token(cast(str, content.content))
-                    yield ChatGenerationChunk(message=content)
+                        await run_manager.on_llm_new_token(
+                            cast(str, content.content), chunk=cg_chunk
+                        )
+                    yield cg_chunk
 
                 # break if stop sequence found
                 if stop_seq_found:
                     break
-
-    async def _agenerate(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        stream: Optional[bool] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        if stream if stream is not None else self.streaming:
-            generation: Optional[ChatGenerationChunk] = None
-            async for chunk in self._astream(
-                messages=messages, stop=stop, run_manager=run_manager, **kwargs
-            ):
-                generation = chunk
-            assert generation is not None
-            return ChatResult(generations=[generation])
-
-        func = partial(
-            self._generate, messages, stop=stop, run_manager=run_manager, **kwargs
-        )
-        return await asyncio.get_event_loop().run_in_executor(None, func)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/promptlayer_openai.py` & `gigachain_community-0.2.0/langchain_community/chat_models/promptlayer_openai.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """PromptLayer wrapper."""
+
 import datetime
 from typing import Any, Dict, List, Optional
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
@@ -29,15 +30,15 @@
             returned in the ``generation_info`` field of the
             ``Generation`` object.
 
     Example:
         .. code-block:: python
 
             from langchain_community.chat_models import PromptLayerChatOpenAI
-            openai = PromptLayerChatOpenAI(model_name="gpt-3.5-turbo")
+            openai = PromptLayerChatOpenAI(model="gpt-3.5-turbo")
     """
 
     pl_tags: Optional[List[str]]
     return_pl_id: Optional[bool] = False
 
     @classmethod
     def is_lc_serializable(cls) -> bool:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/tongyi.py` & `gigachain_community-0.2.0/langchain_community/chat_models/deepinfra.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,27 +1,39 @@
+"""deepinfra.com chat models wrapper"""
+
 from __future__ import annotations
 
+import json
 import logging
 from typing import (
     Any,
+    AsyncIterator,
     Callable,
     Dict,
     Iterator,
     List,
     Mapping,
     Optional,
     Tuple,
     Type,
+    Union,
 )
 
-from langchain_core.callbacks import CallbackManagerForLLMRun
+import aiohttp
+import requests
+from langchain_core.callbacks.manager import (
+    AsyncCallbackManagerForLLMRun,
+    CallbackManagerForLLMRun,
+)
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
+    agenerate_from_stream,
     generate_from_stream,
 )
+from langchain_core.language_models.llms import create_base_retry_decorator
 from langchain_core.messages import (
     AIMessage,
     AIMessageChunk,
     BaseMessage,
     BaseMessageChunk,
     ChatMessage,
     ChatMessageChunk,
@@ -32,288 +44,230 @@
     SystemMessage,
     SystemMessageChunk,
 )
 from langchain_core.outputs import (
     ChatGeneration,
     ChatGenerationChunk,
     ChatResult,
-    GenerationChunk,
 )
 from langchain_core.pydantic_v1 import Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
-from requests.exceptions import HTTPError
-from tenacity import (
-    RetryCallState,
-    retry,
-    retry_if_exception_type,
-    stop_after_attempt,
-    wait_exponential,
-)
+
+# from langchain.llms.base import create_base_retry_decorator
+from langchain_community.utilities.requests import Requests
 
 logger = logging.getLogger(__name__)
 
 
-def convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
-    """Convert a dict to a message."""
+class ChatDeepInfraException(Exception):
+    """Exception raised when the DeepInfra API returns an error."""
+
+    pass
+
 
+def _create_retry_decorator(
+    llm: ChatDeepInfra,
+    run_manager: Optional[
+        Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]
+    ] = None,
+) -> Callable[[Any], Any]:
+    """Returns a tenacity retry decorator, preconfigured to handle PaLM exceptions."""
+    return create_base_retry_decorator(
+        error_types=[requests.exceptions.ConnectTimeout, ChatDeepInfraException],
+        max_retries=llm.max_retries,
+        run_manager=run_manager,
+    )
+
+
+def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
     role = _dict["role"]
     if role == "user":
         return HumanMessage(content=_dict["content"])
     elif role == "assistant":
+        # Fix for azure
+        # Also OpenAI returns None for tool invocations
         content = _dict.get("content", "") or ""
         if _dict.get("function_call"):
             additional_kwargs = {"function_call": dict(_dict["function_call"])}
         else:
             additional_kwargs = {}
         return AIMessage(content=content, additional_kwargs=additional_kwargs)
     elif role == "system":
         return SystemMessage(content=_dict["content"])
     elif role == "function":
         return FunctionMessage(content=_dict["content"], name=_dict["name"])
     else:
         return ChatMessage(content=_dict["content"], role=role)
 
 
-def convert_message_to_dict(message: BaseMessage) -> dict:
-    """Convert a message to a dict."""
+def _convert_delta_to_message_chunk(
+    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]
+) -> BaseMessageChunk:
+    role = _dict.get("role")
+    content = _dict.get("content") or ""
+    if _dict.get("function_call"):
+        additional_kwargs = {"function_call": dict(_dict["function_call"])}
+    else:
+        additional_kwargs = {}
+
+    if role == "user" or default_class == HumanMessageChunk:
+        return HumanMessageChunk(content=content)
+    elif role == "assistant" or default_class == AIMessageChunk:
+        return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)
+    elif role == "system" or default_class == SystemMessageChunk:
+        return SystemMessageChunk(content=content)
+    elif role == "function" or default_class == FunctionMessageChunk:
+        return FunctionMessageChunk(content=content, name=_dict["name"])
+    elif role or default_class == ChatMessageChunk:
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
+    else:
+        return default_class(content=content)  # type: ignore[call-arg]
+
 
-    message_dict: Dict[str, Any]
+def _convert_message_to_dict(message: BaseMessage) -> dict:
     if isinstance(message, ChatMessage):
         message_dict = {"role": message.role, "content": message.content}
     elif isinstance(message, HumanMessage):
         message_dict = {"role": "user", "content": message.content}
     elif isinstance(message, AIMessage):
         message_dict = {"role": "assistant", "content": message.content}
         if "function_call" in message.additional_kwargs:
             message_dict["function_call"] = message.additional_kwargs["function_call"]
-            # If function call only, content is None not empty string
-            if message_dict["content"] == "":
-                message_dict["content"] = None
     elif isinstance(message, SystemMessage):
         message_dict = {"role": "system", "content": message.content}
     elif isinstance(message, FunctionMessage):
         message_dict = {
             "role": "function",
             "content": message.content,
             "name": message.name,
         }
     else:
-        raise TypeError(f"Got unknown type {message}")
+        raise ValueError(f"Got unknown type {message}")
     if "name" in message.additional_kwargs:
         message_dict["name"] = message.additional_kwargs["name"]
     return message_dict
 
 
-def _stream_response_to_generation_chunk(
-    stream_response: Dict[str, Any],
-    length: int,
-) -> GenerationChunk:
-    """Convert a stream response to a generation chunk.
-
-    As the low level API implement is different from openai and other llm.
-    Stream response of Tongyi is not split into chunks, but all data generated before.
-    For example, the answer 'Hi Pickle Rick! How can I assist you today?'
-    Other llm will stream answer:
-    'Hi Pickle',
-    ' Rick!',
-    ' How can I assist you today?'.
-
-    Tongyi answer:
-    'Hi Pickle',
-    'Hi Pickle Rick!',
-    'Hi Pickle Rick! How can I assist you today?'.
-
-    As the GenerationChunk is implemented with chunks. Only return full_text[length:]
-    for new chunk.
-    """
-    full_text = stream_response["output"]["text"]
-    text = full_text[length:]
-    finish_reason = stream_response["output"].get("finish_reason", None)
-
-    return GenerationChunk(
-        text=text,
-        generation_info=dict(
-            finish_reason=finish_reason,
-        ),
-    )
-
-
-def _create_retry_decorator(
-    llm: ChatTongyi,
-    run_manager: Optional[CallbackManagerForLLMRun] = None,
-) -> Callable[[Any], Any]:
-    def _before_sleep(retry_state: RetryCallState) -> None:
-        if run_manager:
-            run_manager.on_retry(retry_state)
-        return None
-
-    min_seconds = 1
-    max_seconds = 4
-    # Wait 2^x * 1 second between each retry starting with
-    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
-    return retry(
-        reraise=True,
-        stop=stop_after_attempt(llm.max_retries),
-        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
-        retry=(retry_if_exception_type(HTTPError)),
-        before_sleep=_before_sleep,
-    )
-
-
-def _convert_delta_to_message_chunk(
-    _dict: Mapping[str, Any],
-    default_class: Type[BaseMessageChunk],
-    length: int,
-) -> BaseMessageChunk:
-    role = _dict.get("role")
-    full_content = _dict.get("content") or ""
-    content = full_content[length:]
-    if _dict.get("function_call"):
-        additional_kwargs = {"function_call": dict(_dict["function_call"])}
-    else:
-        additional_kwargs = {}
-
-    if role == "user" or default_class == HumanMessageChunk:
-        return HumanMessageChunk(content=content)
-    elif role == "assistant" or default_class == AIMessageChunk:
-        return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)
-    elif role == "system" or default_class == SystemMessageChunk:
-        return SystemMessageChunk(content=content)
-    elif role == "function" or default_class == FunctionMessageChunk:
-        return FunctionMessageChunk(content=content, name=_dict["name"])
-    elif role or default_class == ChatMessageChunk:
-        return ChatMessageChunk(content=content, role=role)
-    else:
-        return default_class(content=content)
-
-
-class ChatTongyi(BaseChatModel):
-    """Alibaba Tongyi Qwen chat models API.
-
-    To use, you should have the ``dashscope`` python package installed,
-    and set env ``DASHSCOPE_API_KEY`` with your API key, or pass
-    it as a named parameter to the constructor.
-
-    Example:
-        .. code-block:: python
-
-            from langchain_community.chat_models import Tongyi
-            Tongyi_chat = ChatTongyi()
-    """
-
-    @property
-    def lc_secrets(self) -> Dict[str, str]:
-        return {"dashscope_api_key": "DASHSCOPE_API_KEY"}
-
-    @property
-    def lc_serializable(self) -> bool:
-        return True
-
-    client: Any  #: :meta private:
-    model_name: str = Field(default="qwen-turbo", alias="model")
+class ChatDeepInfra(BaseChatModel):
+    """A chat model that uses the DeepInfra API."""
 
+    # client: Any  #: :meta private:
+    model_name: str = Field(default="meta-llama/Llama-2-70b-chat-hf", alias="model")
     """Model name to use."""
+    deepinfra_api_token: Optional[str] = None
+    request_timeout: Optional[float] = Field(default=None, alias="timeout")
+    temperature: Optional[float] = 1
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
-
-    top_p: float = 0.8
-    """Total probability mass of tokens to consider at each step."""
-
-    dashscope_api_key: Optional[str] = None
-    """Dashscope api key provide by alicloud."""
-
+    """Run inference with this temperature. Must by in the closed
+       interval [0.0, 1.0]."""
+    top_p: Optional[float] = None
+    """Decode using nucleus sampling: consider the smallest set of tokens whose
+       probability sum is at least top_p. Must be in the closed interval [0.0, 1.0]."""
+    top_k: Optional[int] = None
+    """Decode using top-k sampling: consider the set of top_k most probable tokens.
+       Must be positive."""
     n: int = 1
-    """How many completions to generate for each prompt."""
-
+    """Number of chat completions to generate for each prompt. Note that the API may
+       not return the full n completions if duplicates are generated."""
+    max_tokens: int = 256
     streaming: bool = False
-    """Whether to stream the results or not."""
-
-    max_retries: int = 10
-    """Maximum number of retries to make when generating."""
-
-    prefix_messages: List = Field(default_factory=list)
-    """Series of messages for Chat input."""
-
-    result_format: str = Field(default="message")
-    """Return result format"""
-
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "tongyi"
-
-    @root_validator()
-    def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that api key and python package exists in environment."""
-        get_from_dict_or_env(values, "dashscope_api_key", "DASHSCOPE_API_KEY")
-        try:
-            import dashscope
-        except ImportError:
-            raise ImportError(
-                "Could not import dashscope python package. "
-                "Please install it with `pip install dashscope --upgrade`."
-            )
-        try:
-            values["client"] = dashscope.Generation
-        except AttributeError:
-            raise ValueError(
-                "`dashscope` has no `Generation` attribute, this is likely "
-                "due to an old version of the dashscope package. Try upgrading it "
-                "with `pip install --upgrade dashscope`."
-            )
-
-        return values
+    max_retries: int = 1
 
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling OpenAI API."""
         return {
             "model": self.model_name,
-            "top_p": self.top_p,
+            "max_tokens": self.max_tokens,
             "stream": self.streaming,
             "n": self.n,
-            "result_format": self.result_format,
+            "temperature": self.temperature,
+            "request_timeout": self.request_timeout,
             **self.model_kwargs,
         }
 
+    @property
+    def _client_params(self) -> Dict[str, Any]:
+        """Get the parameters used for the openai client."""
+        return {**self._default_params}
+
     def completion_with_retry(
         self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any
     ) -> Any:
         """Use tenacity to retry the completion call."""
         retry_decorator = _create_retry_decorator(self, run_manager=run_manager)
 
         @retry_decorator
-        def _completion_with_retry(**_kwargs: Any) -> Any:
-            resp = self.client.call(**_kwargs)
-            if resp.status_code == 200:
-                return resp
-            elif resp.status_code in [400, 401]:
-                raise ValueError(
-                    f"status_code: {resp.status_code} \n "
-                    f"code: {resp.code} \n message: {resp.message}"
-                )
-            else:
-                raise HTTPError(
-                    f"HTTP error occurred: status_code: {resp.status_code} \n "
-                    f"code: {resp.code} \n message: {resp.message}",
-                    response=resp,
+        def _completion_with_retry(**kwargs: Any) -> Any:
+            try:
+                request_timeout = kwargs.pop("request_timeout")
+                request = Requests(headers=self._headers())
+                response = request.post(
+                    url=self._url(), data=self._body(kwargs), timeout=request_timeout
                 )
+                self._handle_status(response.status_code, response.text)
+                return response
+            except Exception as e:
+                # import pdb; pdb.set_trace()
+                print("EX", e)  # noqa: T201
+                raise
 
         return _completion_with_retry(**kwargs)
 
-    def stream_completion_with_retry(
-        self, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any
+    async def acompletion_with_retry(
+        self,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
     ) -> Any:
-        """Use tenacity to retry the completion call."""
+        """Use tenacity to retry the async completion call."""
         retry_decorator = _create_retry_decorator(self, run_manager=run_manager)
 
         @retry_decorator
-        def _stream_completion_with_retry(**_kwargs: Any) -> Any:
-            return self.client.call(**_kwargs)
+        async def _completion_with_retry(**kwargs: Any) -> Any:
+            try:
+                request_timeout = kwargs.pop("request_timeout")
+                request = Requests(headers=self._headers())
+                async with request.apost(
+                    url=self._url(), data=self._body(kwargs), timeout=request_timeout
+                ) as response:
+                    self._handle_status(response.status, response.text)
+                    return await response.json()
+            except Exception as e:
+                print("EX", e)  # noqa: T201
+                raise
 
-        return _stream_completion_with_retry(**kwargs)
+        return await _completion_with_retry(**kwargs)
+
+    @root_validator()
+    def validate_environment(cls, values: Dict) -> Dict:
+        """Validate api key, python package exists, temperature, top_p, and top_k."""
+        # For compatibility with LiteLLM
+        api_key = get_from_dict_or_env(
+            values,
+            "deepinfra_api_key",
+            "DEEPINFRA_API_KEY",
+            default="",
+        )
+        values["deepinfra_api_token"] = get_from_dict_or_env(
+            values,
+            "deepinfra_api_token",
+            "DEEPINFRA_API_TOKEN",
+            default=api_key,
+        )
+
+        if values["temperature"] is not None and not 0 <= values["temperature"] <= 1:
+            raise ValueError("temperature must be in the range [0.0, 1.0]")
+
+        if values["top_p"] is not None and not 0 <= values["top_p"] <= 1:
+            raise ValueError("top_p must be in the range [0.0, 1.0]")
+
+        if values["top_k"] is not None and values["top_k"] <= 0:
+            raise ValueError("top_k must be positive")
+
+        return values
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         stream: Optional[bool] = None,
@@ -322,87 +276,183 @@
         should_stream = stream if stream is not None else self.streaming
         if should_stream:
             stream_iter = self._stream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return generate_from_stream(stream_iter)
 
-        if not messages:
-            raise ValueError("No messages provided.")
-
         message_dicts, params = self._create_message_dicts(messages, stop)
-
-        if message_dicts[-1]["role"] != "user":
-            raise ValueError("Last message should be user message.")
-
         params = {**params, **kwargs}
         response = self.completion_with_retry(
             messages=message_dicts, run_manager=run_manager, **params
         )
-        return self._create_chat_result(response)
+        return self._create_chat_result(response.json())
+
+    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:
+        generations = []
+        for res in response["choices"]:
+            message = _convert_dict_to_message(res["message"])
+            gen = ChatGeneration(
+                message=message,
+                generation_info=dict(finish_reason=res.get("finish_reason")),
+            )
+            generations.append(gen)
+        token_usage = response.get("usage", {})
+        llm_output = {"token_usage": token_usage, "model": self.model_name}
+        res = ChatResult(generations=generations, llm_output=llm_output)
+        return res
+
+    def _create_message_dicts(
+        self, messages: List[BaseMessage], stop: Optional[List[str]]
+    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
+        params = self._client_params
+        if stop is not None:
+            if "stop" in params:
+                raise ValueError("`stop` found in both the input and default params.")
+            params["stop"] = stop
+        message_dicts = [_convert_message_to_dict(m) for m in messages]
+        return message_dicts, params
 
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         message_dicts, params = self._create_message_dicts(messages, stop)
         params = {**params, **kwargs, "stream": True}
-        # Mark current chunk total length
-        length = 0
-        default_chunk_class = AIMessageChunk
-        for chunk in self.stream_completion_with_retry(
+
+        response = self.completion_with_retry(
             messages=message_dicts, run_manager=run_manager, **params
-        ):
-            if len(chunk["output"]["choices"]) == 0:
-                continue
-            choice = chunk["output"]["choices"][0]
+        )
+        for line in _parse_stream(response.iter_lines()):
+            chunk = _handle_sse_line(line)
+            if chunk:
+                cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
+                if run_manager:
+                    run_manager.on_llm_new_token(str(chunk.content), chunk=cg_chunk)
+                yield cg_chunk
 
-            chunk = _convert_delta_to_message_chunk(
-                choice["message"], default_chunk_class, length
-            )
-            finish_reason = choice.get("finish_reason")
-            generation_info = (
-                dict(finish_reason=finish_reason) if finish_reason is not None else None
-            )
-            default_chunk_class = chunk.__class__
-            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
-            yield chunk
-            if run_manager:
-                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
-            length = len(choice["message"]["content"])
+    async def _astream(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> AsyncIterator[ChatGenerationChunk]:
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {"messages": message_dicts, "stream": True, **params, **kwargs}
 
-    def _create_message_dicts(
-        self, messages: List[BaseMessage], stop: Optional[List[str]]
-    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
-        params = self._client_params()
+        request_timeout = params.pop("request_timeout")
+        request = Requests(headers=self._headers())
+        async with request.apost(
+            url=self._url(), data=self._body(params), timeout=request_timeout
+        ) as response:
+            async for line in _parse_stream_async(response.content):
+                chunk = _handle_sse_line(line)
+                if chunk:
+                    cg_chunk = ChatGenerationChunk(message=chunk, generation_info=None)
+                    if run_manager:
+                        await run_manager.on_llm_new_token(
+                            str(chunk.content), chunk=cg_chunk
+                        )
+                    yield cg_chunk
 
-        # Ensure `stop` is a list of strings
-        if stop is not None:
-            if "stop" in params:
-                raise ValueError("`stop` found in both the input and default params.")
-            params["stop"] = stop
+    async def _agenerate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        stream: Optional[bool] = None,
+        **kwargs: Any,
+    ) -> ChatResult:
+        should_stream = stream if stream is not None else self.streaming
+        if should_stream:
+            stream_iter = self._astream(
+                messages, stop=stop, run_manager=run_manager, **kwargs
+            )
+            return await agenerate_from_stream(stream_iter)
 
-        message_dicts = [convert_message_to_dict(m) for m in messages]
-        return message_dicts, params
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {"messages": message_dicts, **params, **kwargs}
 
-    def _client_params(self) -> Dict[str, Any]:
-        """Get the parameters used for the openai client."""
-        creds: Dict[str, Any] = {
-            "api_key": self.dashscope_api_key,
+        res = await self.acompletion_with_retry(run_manager=run_manager, **params)
+        return self._create_chat_result(res)
+
+    @property
+    def _identifying_params(self) -> Dict[str, Any]:
+        """Get the identifying parameters."""
+        return {
+            "model": self.model_name,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            "top_k": self.top_k,
+            "n": self.n,
         }
-        return {**self._default_params, **creds}
 
-    def _create_chat_result(self, response: Mapping[str, Any]) -> ChatResult:
-        generations = []
-        for res in response["output"]["choices"]:
-            message = convert_dict_to_message(res["message"])
-            gen = ChatGeneration(
-                message=message,
-                generation_info=dict(finish_reason=res.get("finish_reason")),
+    @property
+    def _llm_type(self) -> str:
+        return "deepinfra-chat"
+
+    def _handle_status(self, code: int, text: Any) -> None:
+        if code >= 500:
+            raise ChatDeepInfraException(f"DeepInfra Server: Error {code}")
+        elif code >= 400:
+            raise ValueError(f"DeepInfra received an invalid payload: {text}")
+        elif code != 200:
+            raise Exception(
+                f"DeepInfra returned an unexpected response with status "
+                f"{code}: {text}"
             )
-            generations.append(gen)
-        token_usage = response.get("usage", {})
-        llm_output = {"token_usage": token_usage, "model_name": self.model_name}
-        return ChatResult(generations=generations, llm_output=llm_output)
+
+    def _url(self) -> str:
+        return "https://stage.api.deepinfra.com/v1/openai/chat/completions"
+
+    def _headers(self) -> Dict:
+        return {
+            "Authorization": f"bearer {self.deepinfra_api_token}",
+            "Content-Type": "application/json",
+        }
+
+    def _body(self, kwargs: Any) -> Dict:
+        return kwargs
+
+
+def _parse_stream(rbody: Iterator[bytes]) -> Iterator[str]:
+    for line in rbody:
+        _line = _parse_stream_helper(line)
+        if _line is not None:
+            yield _line
+
+
+async def _parse_stream_async(rbody: aiohttp.StreamReader) -> AsyncIterator[str]:
+    async for line in rbody:
+        _line = _parse_stream_helper(line)
+        if _line is not None:
+            yield _line
+
+
+def _parse_stream_helper(line: bytes) -> Optional[str]:
+    if line and line.startswith(b"data:"):
+        if line.startswith(b"data: "):
+            # SSE event may be valid when it contain whitespace
+            line = line[len(b"data: ") :]
+        else:
+            line = line[len(b"data:") :]
+        if line.strip() == b"[DONE]":
+            # return here will cause GeneratorExit exception in urllib3
+            # and it will close http connection with TCP Reset
+            return None
+        else:
+            return line.decode("utf-8")
+    return None
+
+
+def _handle_sse_line(line: str) -> Optional[BaseMessageChunk]:
+    try:
+        obj = json.loads(line)
+        default_chunk_class = AIMessageChunk
+        delta = obj.get("choices", [{}])[0].get("delta", {})
+        return _convert_delta_to_message_chunk(delta, default_chunk_class)
+    except Exception:
+        return None
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/vertexai.py` & `gigachain_community-0.2.0/langchain_community/chat_models/vertexai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """Wrapper around Google VertexAI chat-based models."""
+
 from __future__ import annotations
 
 import base64
 import logging
 import re
 from dataclasses import dataclass, field
 from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union, cast
 from urllib.parse import urlparse
 
 import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     generate_from_stream,
@@ -115,19 +117,18 @@
             return Part.from_text(part["text"])
         elif part["type"] == "image_url":
             path = part["image_url"]["url"]
             if path.startswith("gs://"):
                 image = load_image_from_gcs(path=path, project=project)
             elif path.startswith("data:image/"):
                 # extract base64 component from image uri
-                try:
-                    encoded = re.search(r"data:image/\w{2,4};base64,(.*)", path).group(
-                        1
-                    )
-                except AttributeError:
+                encoded: Any = re.search(r"data:image/\w{2,4};base64,(.*)", path)
+                if encoded:
+                    encoded = encoded.group(1)
+                else:
                     raise ValueError(
                         "Invalid image uri. It should be in the format "
                         "data:image/<image_type>;base64,<base64_encoded_image>."
                     )
                 image = Image.from_bytes(base64.b64decode(encoded))
             elif _is_url(path):
                 response = requests.get(path)
@@ -199,14 +200,19 @@
     if not isinstance(question, HumanMessage):
         raise ValueError(
             f"Last message in the list should be from human, got {question.type}."
         )
     return question
 
 
+@deprecated(
+    since="0.0.12",
+    removal="0.3.0",
+    alternative_import="langchain_google_vertexai.ChatVertexAI",
+)
 class ChatVertexAI(_VertexAICommon, BaseChatModel):
     """`Vertex AI` Chat large language models API."""
 
     model_name: str = "chat-bison"
     "Underlying model name."
     examples: Optional[List[BaseMessage]] = None
 
@@ -367,17 +373,18 @@
             history = _parse_chat_history(messages[:-1])
             examples = kwargs.get("examples", None)
             if examples:
                 params["examples"] = _parse_examples(examples)
             chat = self._start_chat(history, **params)
             responses = chat.send_message_streaming(question.content, **params)
         for response in responses:
+            chunk = ChatGenerationChunk(message=AIMessageChunk(content=response.text))
             if run_manager:
-                run_manager.on_llm_new_token(response.text)
-            yield ChatGenerationChunk(message=AIMessageChunk(content=response.text))
+                run_manager.on_llm_new_token(response.text, chunk=chunk)
+            yield chunk
 
     def _start_chat(
         self, history: _ChatHistory, **kwargs: Any
     ) -> Union[ChatSession, CodeChatSession]:
         if not self.is_codey_model:
             return self.client.start_chat(
                 context=history.context, message_history=history.history, **kwargs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/volcengine_maas.py` & `gigachain_community-0.2.0/langchain_community/chat_models/volcengine_maas.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,16 +35,16 @@
     """Convert a dict to a message."""
 
     content = _dict.get("choice", {}).get("message", {}).get("content", "")
     return AIMessage(content=content)
 
 
 class VolcEngineMaasChat(BaseChatModel, VolcEngineMaasBase):
+    """Volc Engine Maas hosts a plethora of models.
 
-    """volc engine maas hosts a plethora of models.
     You can utilize these models through this class.
 
     To use, you should have the ``volcengine`` python package installed.
     and set access key and secret key by environment variable or direct pass those
     to this class.
     access key, secret key are required parameters which you could get help
     https://www.volcengine.com/docs/6291/65568
@@ -108,34 +108,39 @@
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
+        if stop is not None:
+            kwargs["stop"] = stop
         params = self._convert_prompt_msg_params(messages, **kwargs)
         for res in self.client.stream_chat(params):
             if res:
                 msg = convert_dict_to_message(res)
-                yield ChatGenerationChunk(message=AIMessageChunk(content=msg.content))
+                chunk = ChatGenerationChunk(message=AIMessageChunk(content=msg.content))
                 if run_manager:
-                    run_manager.on_llm_new_token(cast(str, msg.content))
+                    run_manager.on_llm_new_token(cast(str, msg.content), chunk=chunk)
+                yield chunk
 
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> ChatResult:
         completion = ""
         if self.streaming:
             for chunk in self._stream(messages, stop, run_manager, **kwargs):
                 completion += chunk.text
         else:
+            if stop is not None:
+                kwargs["stop"] = stop
             params = self._convert_prompt_msg_params(messages, **kwargs)
             res = self.client.chat(params)
             msg = convert_dict_to_message(res)
             completion = cast(str, msg.content)
 
         message = AIMessage(content=completion)
         return ChatResult(generations=[ChatGeneration(message=message)])
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/chat_models/yandex.py` & `gigachain_community-0.2.0/langchain_community/chat_models/yandex.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around YandexGPT chat models."""
+
 from __future__ import annotations
 
 import logging
 from typing import Any, Callable, Dict, List, Optional, cast
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
@@ -49,15 +50,15 @@
             chat_history.append(_parse_message("assistant", content))
         if isinstance(message, SystemMessage):
             chat_history.append(_parse_message("system", content))
     return chat_history
 
 
 class ChatYandexGPT(_BaseYandexGPT, BaseChatModel):
-    """Wrapper around YandexGPT large language models.
+    """YandexGPT large language models.
 
     There are two authentication options for the service account
     with the ``ai.languageModels.user`` role:
         - You can specify the token in a constructor parameter `iam_token`
         or in an environment variable `YC_IAM_TOKEN`.
         - You can specify the key in a constructor parameter `api_key`
         or in an environment variable `YC_API_KEY`.
@@ -123,27 +124,41 @@
 def _make_request(
     self: ChatYandexGPT,
     messages: List[BaseMessage],
 ) -> str:
     try:
         import grpc
         from google.protobuf.wrappers_pb2 import DoubleValue, Int64Value
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
-            CompletionOptions,
-            Message,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
-            CompletionRequest,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
-            TextGenerationServiceStub,
-        )
+
+        try:
+            from yandex.cloud.ai.foundation_models.v1.text_common_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2_grpc import (  # noqa: E501
+                TextGenerationServiceStub,
+            )
+        except ModuleNotFoundError:
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
+                TextGenerationServiceStub,
+            )
     except ImportError as e:
         raise ImportError(
-            "Please install YandexCloud SDK" " with `pip install yandexcloud`."
+            "Please install YandexCloud SDK  with `pip install yandexcloud` \
+            or upgrade it to recent version."
         ) from e
     if not messages:
         raise ValueError("You should provide at least one message to start the chat!")
     message_history = _parse_chat_history(messages)
     channel_credentials = grpc.ssl_channel_credentials()
     channel = grpc.secure_channel(self.url, channel_credentials)
     request = CompletionRequest(
@@ -161,32 +176,47 @@
 
 async def _amake_request(self: ChatYandexGPT, messages: List[BaseMessage]) -> str:
     try:
         import asyncio
 
         import grpc
         from google.protobuf.wrappers_pb2 import DoubleValue, Int64Value
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
-            CompletionOptions,
-            Message,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
-            CompletionRequest,
-            CompletionResponse,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
-            TextGenerationAsyncServiceStub,
-        )
+
+        try:
+            from yandex.cloud.ai.foundation_models.v1.text_common_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+                CompletionResponse,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2_grpc import (  # noqa: E501
+                TextGenerationAsyncServiceStub,
+            )
+        except ModuleNotFoundError:
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+                CompletionResponse,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
+                TextGenerationAsyncServiceStub,
+            )
         from yandex.cloud.operation.operation_service_pb2 import GetOperationRequest
         from yandex.cloud.operation.operation_service_pb2_grpc import (
             OperationServiceStub,
         )
     except ImportError as e:
         raise ImportError(
-            "Please install YandexCloud SDK" " with `pip install yandexcloud`."
+            "Please install YandexCloud SDK  with `pip install yandexcloud` \
+            or upgrade it to recent version."
         ) from e
     if not messages:
         raise ValueError("You should provide at least one message to start the chat!")
     message_history = _parse_chat_history(messages)
     operation_api_url = "operation.api.cloud.yandex.net:443"
     channel_credentials = grpc.ssl_channel_credentials()
     async with grpc.aio.secure_channel(self.url, channel_credentials) as channel:
@@ -204,26 +234,27 @@
             operation_api_url, channel_credentials
         ) as operation_channel:
             operation_stub = OperationServiceStub(operation_channel)
             while not operation.done:
                 await asyncio.sleep(1)
                 operation_request = GetOperationRequest(operation_id=operation.id)
                 operation = await operation_stub.Get(
-                    operation_request, metadata=self._grpc_metadata
+                    operation_request,
+                    metadata=self._grpc_metadata,
                 )
 
         completion_response = CompletionResponse()
         operation.response.Unpack(completion_response)
         return completion_response.alternatives[0].message.text
 
 
 def _create_retry_decorator(llm: ChatYandexGPT) -> Callable[[Any], Any]:
     from grpc import RpcError
 
-    min_seconds = 1
+    min_seconds = llm.sleep_interval
     max_seconds = 60
     return retry(
         reraise=True,
         stop=stop_after_attempt(llm.max_retries),
         wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
         retry=(retry_if_exception_type((RpcError))),
         before_sleep=before_sleep_log(logger, logging.WARNING),
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/docstore/arbitrary_fn.py` & `gigachain_community-0.2.0/langchain_community/docstore/arbitrary_fn.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 from langchain_core.documents import Document
 
 from langchain_community.docstore.base import Docstore
 
 
 class DocstoreFn(Docstore):
-    """Langchain Docstore via arbitrary lookup function.
+    """Docstore via arbitrary lookup function.
 
     This is useful when:
      * it's expensive to construct an InMemoryDocstore/dict
      * you retrieve documents from remote sources
      * you just want to reuse existing objects
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/docstore/base.py` & `gigachain_community-0.2.0/langchain_community/docstore/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Interface to access to place that stores documents."""
+
 from abc import ABC, abstractmethod
 from typing import Dict, List, Union
 
 from langchain_core.documents import Document
 
 
 class Docstore(ABC):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/docstore/in_memory.py` & `gigachain_community-0.2.0/langchain_community/docstore/in_memory.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Simple in memory docstore in the form of a dict."""
+
 from typing import Dict, List, Optional, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.docstore.base import AddableMixin, Docstore
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/docstore/wikipedia.py` & `gigachain_community-0.2.0/langchain_community/docstore/wikipedia.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 """Wrapper around wikipedia API."""
 
-
 from typing import Union
 
 from langchain_core.documents import Document
 
 from langchain_community.docstore.base import Docstore
 
 
 class Wikipedia(Docstore):
-    """Wrapper around wikipedia API."""
+    """Wikipedia API."""
 
     def __init__(self) -> None:
         """Check that wikipedia package is installed."""
         try:
             import wikipedia  # noqa: F401
         except ImportError:
             raise ImportError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/acreom.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/acreom.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 import re
 from pathlib import Path
-from typing import Iterator, List
+from typing import Iterator, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class AcreomLoader(BaseLoader):
     """Load `acreom` vault from a directory."""
 
     FRONT_MATTER_REGEX = re.compile(r"^---\n(.*?)\n---\n", re.MULTILINE | re.DOTALL)
     """Regex to match front matter metadata in markdown files."""
 
     def __init__(
-        self, path: str, encoding: str = "UTF-8", collect_metadata: bool = True
+        self,
+        path: Union[str, Path],
+        encoding: str = "UTF-8",
+        collect_metadata: bool = True,
     ):
         """Initialize the loader."""
         self.file_path = path
         """Path to the directory containing the markdown files."""
         self.encoding = encoding
         """Encoding to use when reading the files."""
         self.collect_metadata = collect_metadata
@@ -70,10 +73,7 @@
             metadata = {
                 "source": str(p.name),
                 "path": str(p),
                 **front_matter,
             }
 
             yield Document(page_content=text, metadata=metadata)
-
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/airbyte.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/airbyte.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Callable, Iterator, List, Mapping, Optional
+from typing import Any, Callable, Iterator, Mapping, Optional
 
 from langchain_core.documents import Document
 from langchain_core.utils.utils import guard_import
 
 from langchain_community.document_loaders.base import BaseLoader
 
 RecordHandler = Callable[[Any, Optional[str]], Document]
@@ -49,17 +49,14 @@
         self._integration = CDKIntegration(
             config=config,
             runner=CDKRunner(source=source_class(), name=source_class.__name__),
         )
         self._stream_name = stream_name
         self._state = state
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         return self._integration._load_data(
             stream_name=self._stream_name, state=self._state
         )
 
     @property
     def last_state(self) -> Any:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/airbyte_json.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/airbyte_json.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 import json
-from typing import List
+from pathlib import Path
+from typing import List, Union
 
 from langchain_core.documents import Document
 from langchain_core.utils import stringify_dict
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class AirbyteJSONLoader(BaseLoader):
     """Load local `Airbyte` json files."""
 
-    def __init__(self, file_path: str):
+    def __init__(self, file_path: Union[str, Path]):
         """Initialize with a file path. This should start with '/tmp/airbyte_local/'."""
         self.file_path = file_path
         """Path to the directory containing the json files."""
 
     def load(self) -> List[Document]:
         text = ""
         for line in open(self.file_path, "r"):
             data = json.loads(line)["_airbyte_data"]
             text += stringify_dict(data)
-        metadata = {"source": self.file_path}
+        metadata = {"source": str(self.file_path)}
         return [Document(page_content=text, metadata=metadata)]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/airtable.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/airtable.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterator, List
+from typing import Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class AirtableLoader(BaseLoader):
@@ -30,11 +30,7 @@
                 page_content=str(record),
                 metadata={
                     "source": self.base_id + "_" + self.table_id,
                     "base_id": self.base_id,
                     "table_id": self.table_id,
                 },
             )
-
-    def load(self) -> List[Document]:
-        """Load Documents from table."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/apify_dataset.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/apify_dataset.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/arcgis_loader.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/arcgis_loader.py`

 * *Files 11% similar despite different names*

```diff
@@ -37,15 +37,15 @@
         except ImportError as e:
             raise ImportError(
                 "arcgis is required to use the ArcGIS Loader. "
                 "Install it with pip or conda."
             ) from e
 
         try:
-            from bs4 import BeautifulSoup  # type: ignore
+            from bs4 import BeautifulSoup
 
             self.BEAUTIFULSOUP = BeautifulSoup
         except ImportError:
             warnings.warn("BeautifulSoup not found. HTML will not be parsed.")
             self.BEAUTIFULSOUP = None
 
         self.gis = gis or arcgis.gis.GIS()
@@ -144,11 +144,7 @@
                     metadata["geometry"] = feature["geometry"]
                 except KeyError:
                     warnings.warn(
                         "Geometry could not be retrieved from the feature layer."
                     )
 
             yield Document(page_content=page_content, metadata=metadata)
-
-    def load(self) -> List[Document]:
-        """Load all records from FeatureLayer."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/arxiv.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/arxiv.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, List, Optional
+from typing import Any, Iterator, List, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.arxiv import ArxivAPIWrapper
 
 
@@ -19,12 +19,12 @@
         self, query: str, doc_content_chars_max: Optional[int] = None, **kwargs: Any
     ):
         self.query = query
         self.client = ArxivAPIWrapper(
             doc_content_chars_max=doc_content_chars_max, **kwargs
         )
 
-    def load(self) -> List[Document]:
-        return self.client.load(self.query)
+    def lazy_load(self) -> Iterator[Document]:
+        yield from self.client.lazy_load(self.query)
 
     def get_summaries_as_docs(self) -> List[Document]:
         return self.client.get_summaries_as_docs(self.query)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/async_html.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/async_html.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,22 @@
 import asyncio
 import logging
 import warnings
-from concurrent.futures import ThreadPoolExecutor
-from typing import Any, Dict, Iterator, List, Optional, Union, cast
+from concurrent.futures import Future, ThreadPoolExecutor
+from typing import (
+    Any,
+    AsyncIterator,
+    Dict,
+    Iterator,
+    List,
+    Optional,
+    Tuple,
+    Union,
+    cast,
+)
 
 import aiohttp
 import requests
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
@@ -48,14 +58,16 @@
         autoset_encoding: bool = True,
         encoding: Optional[str] = None,
         default_parser: str = "html.parser",
         requests_per_second: int = 2,
         requests_kwargs: Optional[Dict[str, Any]] = None,
         raise_for_status: bool = False,
         ignore_load_errors: bool = False,
+        *,
+        preserve_order: bool = True,
     ):
         """Initialize with a webpage path."""
 
         # TODO: Deprecate web_path in favor of web_paths, and remove this
         # left like this because there are a number of loaders that expect single
         # urls
         if isinstance(web_path, str):
@@ -76,24 +88,26 @@
                     "`pip install fake_useragent`."
                 )
 
         self.session = requests.Session()
         self.session.headers = dict(headers)
         self.session.verify = verify_ssl
 
+        self.proxies = proxies
         if proxies:
             self.session.proxies.update(proxies)
 
         self.requests_per_second = requests_per_second
         self.default_parser = default_parser
         self.requests_kwargs = requests_kwargs or {}
         self.raise_for_status = raise_for_status
         self.autoset_encoding = autoset_encoding
         self.encoding = encoding
         self.ignore_load_errors = ignore_load_errors
+        self.preserve_order = preserve_order
 
     def _fetch_valid_connection_docs(self, url: str) -> Any:
         if self.ignore_load_errors:
             try:
                 return self.session.get(url, **self.requests_kwargs)
             except Exception as e:
                 warnings.warn(str(e))
@@ -106,61 +120,41 @@
         """Check that parser is valid for bs4."""
         valid_parsers = ["html.parser", "lxml", "xml", "lxml-xml", "html5lib"]
         if parser not in valid_parsers:
             raise ValueError(
                 "`parser` must be one of " + ", ".join(valid_parsers) + "."
             )
 
-    def _scrape(
+    async def _fetch(
         self,
         url: str,
-        parser: Union[str, None] = None,
-        bs_kwargs: Optional[dict] = None,
-    ) -> Any:
-        from bs4 import BeautifulSoup
-
-        if parser is None:
-            if url.endswith(".xml"):
-                parser = "xml"
-            else:
-                parser = self.default_parser
-
-        self._check_parser(parser)
-
-        html_doc = self._fetch_valid_connection_docs(url)
-        if not getattr(html_doc, "ok", False):
-            return None
-
-        if self.raise_for_status:
-            html_doc.raise_for_status()
-
-        if self.encoding is not None:
-            html_doc.encoding = self.encoding
-        elif self.autoset_encoding:
-            html_doc.encoding = html_doc.apparent_encoding
-        return BeautifulSoup(html_doc.text, parser, **(bs_kwargs or {}))
-
-    async def _fetch(
-        self, url: str, retries: int = 3, cooldown: int = 2, backoff: float = 1.5
+        retries: int = 2,
+        cooldown: int = 2,
+        backoff: float = 1.5,
+        timeout: aiohttp.ClientTimeout = aiohttp.ClientTimeout(total=3),
     ) -> str:
-        async with aiohttp.ClientSession() as session:
+        async with aiohttp.ClientSession(timeout=timeout) as session:
             for i in range(retries):
                 try:
+                    proxy = None
+                    if self.proxies and "https" in self.proxies:
+                        proxy = self.proxies["https"]
                     async with session.get(
                         url,
                         headers=self.session.headers,
                         ssl=None if self.session.verify else False,
+                        proxy=proxy,
                     ) as response:
                         try:
                             text = await response.text()
                         except UnicodeDecodeError:
                             logger.error(f"Failed to decode content from {url}")
                             text = ""
                         return text
-                except aiohttp.ClientConnectionError as e:
+                except Exception as e:
                     if i == retries - 1 and self.ignore_load_errors:
                         logger.warning(f"Error fetching {url} after {retries} retries.")
                         return ""
                     elif i == retries - 1:
                         raise
                     else:
                         logger.warning(
@@ -168,55 +162,83 @@
                             f"{i + 1}/{retries}: {e}. Retrying..."
                         )
                         await asyncio.sleep(cooldown * backoff**i)
         raise ValueError("retry count exceeded")
 
     async def _fetch_with_rate_limit(
         self, url: str, semaphore: asyncio.Semaphore
-    ) -> str:
+    ) -> Tuple[str, str]:
         async with semaphore:
-            return await self._fetch(url)
+            return url, await self._fetch(url)
 
-    async def fetch_all(self, urls: List[str]) -> Any:
-        """Fetch all urls concurrently with rate limiting."""
+    async def _lazy_fetch_all(
+        self, urls: List[str], preserve_order: bool
+    ) -> AsyncIterator[Tuple[str, str]]:
         semaphore = asyncio.Semaphore(self.requests_per_second)
-        tasks = []
-        for url in urls:
-            task = asyncio.ensure_future(self._fetch_with_rate_limit(url, semaphore))
-            tasks.append(task)
+        tasks = [
+            asyncio.create_task(self._fetch_with_rate_limit(url, semaphore))
+            for url in urls
+        ]
         try:
             from tqdm.asyncio import tqdm_asyncio
 
-            return await tqdm_asyncio.gather(
-                *tasks, desc="Fetching pages", ascii=True, mininterval=1
-            )
+            if preserve_order:
+                for task in tqdm_asyncio(
+                    tasks, desc="Fetching pages", ascii=True, mininterval=1
+                ):
+                    yield await task
+            else:
+                for task in tqdm_asyncio.as_completed(
+                    tasks, desc="Fetching pages", ascii=True, mininterval=1
+                ):
+                    yield await task
         except ImportError:
             warnings.warn("For better logging of progress, `pip install tqdm`")
-            return await asyncio.gather(*tasks)
+            if preserve_order:
+                for result in await asyncio.gather(*tasks):
+                    yield result
+            else:
+                for task in asyncio.as_completed(tasks):
+                    yield await task
 
-    def lazy_load(self) -> Iterator[Document]:
-        """Lazy load text from the url(s) in web_path."""
-        for doc in self.load():
-            yield doc
+    async def fetch_all(self, urls: List[str]) -> List[str]:
+        """Fetch all urls concurrently with rate limiting."""
+        return [doc async for _, doc in self._lazy_fetch_all(urls, True)]
+
+    def _to_document(self, url: str, text: str) -> Document:
+        from bs4 import BeautifulSoup
 
-    def load(self) -> List[Document]:
-        """Load text from the url(s) in web_path."""
+        if url.endswith(".xml"):
+            parser = "xml"
+        else:
+            parser = self.default_parser
+        self._check_parser(parser)
+        soup = BeautifulSoup(text, parser)
+        metadata = _build_metadata(soup, url)
+        return Document(page_content=text, metadata=metadata)
 
+    def lazy_load(self) -> Iterator[Document]:
+        """Lazy load text from the url(s) in web_path."""
+        results: List[str]
         try:
             # Raises RuntimeError if there is no current event loop.
             asyncio.get_running_loop()
             # If there is a current event loop, we need to run the async code
             # in a separate loop, in a separate thread.
             with ThreadPoolExecutor(max_workers=1) as executor:
-                future = executor.submit(asyncio.run, self.fetch_all(self.web_paths))
+                future: Future[List[str]] = executor.submit(
+                    asyncio.run,  # type: ignore[arg-type]
+                    self.fetch_all(self.web_paths),  # type: ignore[arg-type]
+                )
                 results = future.result()
         except RuntimeError:
             results = asyncio.run(self.fetch_all(self.web_paths))
-        docs = []
+
         for i, text in enumerate(cast(List[str], results)):
-            soup = self._scrape(self.web_paths[i])
-            if not soup:
-                continue
-            metadata = _build_metadata(soup, self.web_paths[i])
-            docs.append(Document(page_content=text, metadata=metadata))
+            yield self._to_document(self.web_paths[i], text)
 
-        return docs
+    async def alazy_load(self) -> AsyncIterator[Document]:
+        """Lazy load text from the url(s) in web_path."""
+        async for url, text in self._lazy_fetch_all(
+            self.web_paths, self.preserve_order
+        ):
+            yield self._to_document(url, text)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/azlyrics.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/azlyrics.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_ai_data.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/azure_ai_data.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterator, List, Optional
+from typing import Iterator, Optional
 
 from langchain_community.docstore.document import Document
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import UnstructuredFileIOLoader
 
 
 class AzureAIDataLoader(BaseLoader):
@@ -12,18 +12,14 @@
         """Initialize with URL to a data asset or storage location
         ."""
         self.url = url
         """URL to the data asset or storage location."""
         self.glob_pattern = glob
         """Optional glob pattern to select files. Defaults to None."""
 
-    def load(self) -> List[Document]:
-        """Load documents."""
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """A lazy loader for Documents."""
         try:
             from azureml.fsspec import AzureMachineLearningFileSystem
         except ImportError as exc:
             raise ImportError(
                 "Could not import azureml-fspec package."
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_blob_storage_container.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/azure_blob_storage_container.py`

 * *Files 6% similar despite different names*

```diff
@@ -35,11 +35,11 @@
         )
         docs = []
         blob_list = container.list_blobs(name_starts_with=self.prefix)
         for blob in blob_list:
             loader = AzureBlobStorageFileLoader(
                 self.conn_str,
                 self.container,
-                blob.name,  # type: ignore
+                blob.name,
             )
             docs.extend(loader.load())
         return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/azure_blob_storage_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/azure_blob_storage_file.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/baiducloud_bos_directory.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/baiducloud_bos_directory.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Iterator, List
+from typing import Any, Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class BaiduBOSDirectoryLoader(BaseLoader):
@@ -14,17 +14,14 @@
         :param bucket(str): BOS bucket.
         :param prefix(str): prefix.
         """
         self.conf = conf
         self.bucket = bucket
         self.prefix = prefix
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Load documents."""
         try:
             from baidubce.services.bos.bos_client import BosClient
         except ImportError:
             raise ImportError(
                 "Please install bce-python-sdk with `pip install bce-python-sdk`."
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/baiducloud_bos_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/baiducloud_bos_file.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import logging
 import os
 import tempfile
-from typing import Any, Iterator, List
+from typing import Any, Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
 
 logger = logging.getLogger(__name__)
@@ -20,17 +20,14 @@
         :param bucket(str): BOS bucket.
         :param key(str): BOS file key.
         """
         self.conf = conf
         self.bucket = bucket
         self.key = key
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Load documents."""
         try:
             from baidubce.services.bos.bos_client import BosClient
         except ImportError:
             raise ImportError(
                 "Please using `pip install bce-python-sdk`"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/base_o365.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/base_o365.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Base class for all loaders that uses O365 Package"""
+
 from __future__ import annotations
 
 import logging
 import os
 import tempfile
 from abc import abstractmethod
 from enum import Enum
@@ -66,20 +67,22 @@
             mime_types_mapping[file_type.value] = "application/pdf"
     return mime_types_mapping
 
 
 class O365BaseLoader(BaseLoader, BaseModel):
     """Base class for all loaders that uses O365 Package"""
 
-    settings: _O365Settings = Field(default_factory=_O365Settings)
+    settings: _O365Settings = Field(default_factory=_O365Settings)  # type: ignore[arg-type]
     """Settings for the Office365 API client."""
     auth_with_token: bool = False
     """Whether to authenticate with a token or not. Defaults to False."""
     chunk_size: Union[int, str] = CHUNK_SIZE
     """Number of bytes to retrieve from each api call to the server. int or 'auto'."""
+    recursive: bool = False
+    """Should the loader recursively load subfolders?"""
 
     @property
     @abstractmethod
     def _file_types(self) -> Sequence[_FileType]:
         """Return supported file types."""
 
     @property
@@ -110,14 +113,17 @@
             os.makedirs(os.path.dirname(temp_dir), exist_ok=True)
             for file in items:
                 if file.is_file:
                     if file.mime_type in list(file_mime_types.values()):
                         file.download(to_path=temp_dir, chunk_size=self.chunk_size)
             loader = FileSystemBlobLoader(path=temp_dir)
             yield from loader.yield_blobs()
+        if self.recursive:
+            for subfolder in folder.get_child_folders():
+                yield from self._load_from_folder(subfolder)
 
     def _load_from_object_ids(
         self, drive: Drive, object_ids: List[str]
     ) -> Iterable[Blob]:
         """Lazily load files specified by their object_ids from a drive.
 
         Load files into the system as binary large objects (Blobs) and return Iterable.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/bibtex.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/bibtex.py`

 * *Files 4% similar despite different names*

```diff
@@ -92,20 +92,7 @@
         entries = self.parser.load_bibtex_entries(self.file_path)
         if self.max_docs:
             entries = entries[: self.max_docs]
         for entry in entries:
             doc = self._load_entry(entry)
             if doc:
                 yield doc
-
-    def load(self) -> List[Document]:
-        """Load bibtex file documents from the given bibtex file path.
-
-        See https://bibtexparser.readthedocs.io/en/master/
-
-        Args:
-            file_path: the path to the bibtex file
-
-        Returns:
-            a list of documents with the document.page_content in text format
-        """
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/bigquery.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/bigquery.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,26 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.vertexai import get_client_info
 
 if TYPE_CHECKING:
     from google.auth.credentials import Credentials
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.BigQueryLoader",
+)
 class BigQueryLoader(BaseLoader):
     """Load from the Google Cloud Platform `BigQuery`.
 
     Each document represents one row of the result. The `page_content_columns`
     are written into the `page_content` of the document. The `metadata_columns`
     are written into the `metadata` of the document. By default, all columns
     are written into the `page_content` and none into the `metadata`.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/bilibili.py` & `gigachain_community-0.2.0/langchain_community/retrievers/chatgpt_plugin_retriever.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,83 +1,90 @@
-import json
-import re
-import warnings
-from typing import List, Tuple
+from __future__ import annotations
 
+from typing import List, Optional
+
+import aiohttp
 import requests
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForRetrieverRun,
+    CallbackManagerForRetrieverRun,
+)
 from langchain_core.documents import Document
-
-from langchain_community.document_loaders.base import BaseLoader
+from langchain_core.retrievers import BaseRetriever
 
 
-class BiliBiliLoader(BaseLoader):
-    """Load `BiliBili` video transcripts."""
+class ChatGPTPluginRetriever(BaseRetriever):
+    """`ChatGPT plugin` retriever."""
 
-    def __init__(self, video_urls: List[str]):
-        """Initialize with bilibili url.
-
-        Args:
-            video_urls: List of bilibili urls.
-        """
-        self.video_urls = video_urls
-
-    def load(self) -> List[Document]:
-        """Load Documents from bilibili url."""
-        results = []
-        for url in self.video_urls:
-            transcript, video_info = self._get_bilibili_subs_and_info(url)
-            doc = Document(page_content=transcript, metadata=video_info)
-            results.append(doc)
-
-        return results
-
-    def _get_bilibili_subs_and_info(self, url: str) -> Tuple[str, dict]:
-        try:
-            from bilibili_api import sync, video
-        except ImportError:
-            raise ImportError(
-                "requests package not found, please install it with "
-                "`pip install bilibili-api-python`"
-            )
-
-        bvid = re.search(r"BV\w+", url)
-        if bvid is not None:
-            v = video.Video(bvid=bvid.group())
-        else:
-            aid = re.search(r"av[0-9]+", url)
-            if aid is not None:
-                try:
-                    v = video.Video(aid=int(aid.group()[2:]))
-                except AttributeError:
-                    raise ValueError(f"{url} is not bilibili url.")
-            else:
-                raise ValueError(f"{url} is not bilibili url.")
-
-        video_info = sync(v.get_info())
-        video_info.update({"url": url})
-        sub = sync(v.get_subtitle(video_info["cid"]))
-
-        # Get subtitle url
-        sub_list = sub["subtitles"]
-        if sub_list:
-            sub_url = sub_list[0]["subtitle_url"]
-            if not sub_url.startswith("http"):
-                sub_url = "https:" + sub_url
-            result = requests.get(sub_url)
-            raw_sub_titles = json.loads(result.content)["body"]
-            raw_transcript = " ".join([c["content"] for c in raw_sub_titles])
-
-            raw_transcript_with_meta_info = (
-                f"Video Title: {video_info['title']},"
-                f"description: {video_info['desc']}\n\n"
-                f"Transcript: {raw_transcript}"
-            )
-            return raw_transcript_with_meta_info, video_info
+    url: str
+    """URL of the ChatGPT plugin."""
+    bearer_token: str
+    """Bearer token for the ChatGPT plugin."""
+    top_k: int = 3
+    """Number of documents to return."""
+    filter: Optional[dict] = None
+    """Filter to apply to the results."""
+    aiosession: Optional[aiohttp.ClientSession] = None
+    """Aiohttp session to use for requests."""
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        arbitrary_types_allowed = True
+        """Allow arbitrary types."""
+
+    def _get_relevant_documents(
+        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
+    ) -> List[Document]:
+        url, json, headers = self._create_request(query)
+        response = requests.post(url, json=json, headers=headers)
+        results = response.json()["results"][0]["results"]
+        docs = []
+        for d in results:
+            content = d.pop("text")
+            metadata = d.pop("metadata", d)
+            if metadata.get("source_id"):
+                metadata["source"] = metadata.pop("source_id")
+            docs.append(Document(page_content=content, metadata=metadata))
+        return docs
+
+    async def _aget_relevant_documents(
+        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
+    ) -> List[Document]:
+        url, json, headers = self._create_request(query)
+
+        if not self.aiosession:
+            async with aiohttp.ClientSession() as session:
+                async with session.post(url, headers=headers, json=json) as response:
+                    res = await response.json()
         else:
-            raw_transcript = ""
-            warnings.warn(
-                f"""
-                No subtitles found for video: {url}.
-                Return Empty transcript.
-                """
-            )
-            return raw_transcript, video_info
+            async with self.aiosession.post(
+                url, headers=headers, json=json
+            ) as response:
+                res = await response.json()
+
+        results = res["results"][0]["results"]
+        docs = []
+        for d in results:
+            content = d.pop("text")
+            metadata = d.pop("metadata", d)
+            if metadata.get("source_id"):
+                metadata["source"] = metadata.pop("source_id")
+            docs.append(Document(page_content=content, metadata=metadata))
+        return docs
+
+    def _create_request(self, query: str) -> tuple[str, dict, dict]:
+        url = f"{self.url}/query"
+        json = {
+            "queries": [
+                {
+                    "query": query,
+                    "filter": self.filter,
+                    "top_k": self.top_k,
+                }
+            ]
+        }
+        headers = {
+            "Content-Type": "application/json",
+            "Authorization": f"Bearer {self.bearer_token}",
+        }
+        return url, json, headers
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/blackboard.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/blackboard.py`

 * *Files 3% similar despite different names*

```diff
@@ -105,21 +105,21 @@
         if self.load_all_recursively:
             soup_info = self.scrape()
             self.folder_path = self._get_folder_path(soup_info)
             relative_paths = self._get_paths(soup_info)
             documents = []
             for path in relative_paths:
                 url = self.base_url + path
-                print(f"Fetching documents from {url}")
+                print(f"Fetching documents from {url}")  # noqa: T201
                 soup_info = self._scrape(url)
                 with contextlib.suppress(ValueError):
                     documents.extend(self._get_documents(soup_info))
             return documents
         else:
-            print(f"Fetching documents from {self.web_path}")
+            print(f"Fetching documents from {self.web_path}")  # noqa: T201
             soup_info = self.scrape()
             self.folder_path = self._get_folder_path(soup_info)
             return self._get_documents(soup_info)
 
     def _get_folder_path(self, soup: Any) -> str:
         """Get the folder path to save the Documents in.
 
@@ -172,24 +172,24 @@
 
         Returns:
             List of attachments.
         """
         from bs4 import BeautifulSoup, Tag
 
         # Get content list
+        content_list: BeautifulSoup
         content_list = soup.find("ul", {"class": "contentList"})
         if content_list is None:
             raise ValueError("No content list found.")
-        content_list: BeautifulSoup  # type: ignore
         # Get all attachments
         attachments = []
+        attachment: Tag
         for attachment in content_list.find_all("ul", {"class": "attachments"}):
-            attachment: Tag  # type: ignore
+            link: Tag
             for link in attachment.find_all("a"):
-                link: Tag  # type: ignore
                 href = link.get("href")
                 # Only add if href is not None and does not start with #
                 if href is not None and not href.startswith("#"):
                     attachments.append(href)
         return attachments
 
     def _download_attachments(self, attachments: List[str]) -> None:
@@ -291,8 +291,8 @@
         "https://<YOUR BLACKBOARD URL"
         " HERE>/webapps/blackboard/content/listContent.jsp?course_id=_<YOUR COURSE ID"
         " HERE>_1&content_id=_<YOUR CONTENT ID HERE>_1&mode=reset",
         "<YOUR BBROUTER COOKIE HERE>",
         load_all_recursively=True,
     )
     documents = loader.load()
-    print(f"Loaded {len(documents)} pages of PDFs from {loader.web_path}")
+    print(f"Loaded {len(documents)} pages of PDFs from {loader.web_path}")  # noqa: T201
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/file_system.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/file_system.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 """Use to load blobs from the local file system."""
+
 from pathlib import Path
 from typing import Callable, Iterable, Iterator, Optional, Sequence, TypeVar, Union
 
 from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader
 
 T = TypeVar("T")
 
 
 def _make_iterator(
     length_func: Callable[[], int], show_progress: bool = False
 ) -> Callable[[Iterable[T]], Iterator[T]]:
     """Create a function that optionally wraps an iterable in tqdm."""
+    iterator: Callable[[Iterable[T]], Iterator[T]]
     if show_progress:
         try:
             from tqdm.auto import tqdm
         except ImportError:
             raise ImportError(
                 "You must install tqdm to use show_progress=True."
                 "You can install tqdm with `pip install tqdm`."
@@ -24,15 +26,15 @@
         # a progress bar that takes into account the total number of files.
         def _with_tqdm(iterable: Iterable[T]) -> Iterator[T]:
             """Wrap an iterable in a tqdm progress bar."""
             return tqdm(iterable, total=length_func())
 
         iterator = _with_tqdm
     else:
-        iterator = iter  # type: ignore
+        iterator = iter
 
     return iterator
 
 
 # PUBLIC API
 
 
@@ -42,15 +44,15 @@
     Example:
 
     .. code-block:: python
 
         from langchain_community.document_loaders.blob_loaders import FileSystemBlobLoader
         loader = FileSystemBlobLoader("/path/to/directory")
         for blob in loader.yield_blobs():
-            print(blob)
+            print(blob)  # noqa: T201
     """  # noqa: E501
 
     def __init__(
         self,
         path: Union[str, Path],
         *,
         glob: str = "**/[!.]*",
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/schema.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/json_loader.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,195 +1,182 @@
-"""Schema for Blobs and Blob Loaders.
+import json
+from pathlib import Path
+from typing import Any, Callable, Dict, Iterator, Optional, Union
 
-The goal is to facilitate decoupling of content loading from content parsing code.
+from langchain_core.documents import Document
 
-In addition, content loading code should provide a lazy loading interface by default.
-"""
-from __future__ import annotations
+from langchain_community.document_loaders.base import BaseLoader
 
-import contextlib
-import mimetypes
-from abc import ABC, abstractmethod
-from io import BufferedReader, BytesIO
-from pathlib import PurePath
-from typing import Any, Dict, Generator, Iterable, Mapping, Optional, Union, cast
 
-from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
+class JSONLoader(BaseLoader):
+    """Load a `JSON` file using a `jq` schema.
 
-PathLike = Union[str, PurePath]
-
-
-class Blob(BaseModel):
-    """Blob represents raw data by either reference or value.
-
-    Provides an interface to materialize the blob in different representations, and
-    help to decouple the development of data loaders from the downstream parsing of
-    the raw data.
-
-    Inspired by: https://developer.mozilla.org/en-US/docs/Web/API/Blob
+    Example:
+        [{"text": ...}, {"text": ...}, {"text": ...}] -> schema = .[].text
+        {"key": [{"text": ...}, {"text": ...}, {"text": ...}]} -> schema = .key[].text
+        ["", "", ""] -> schema = .[]
     """
 
-    data: Union[bytes, str, None]
-    """Raw data associated with the blob."""
-    mimetype: Optional[str] = None
-    """MimeType not to be confused with a file extension."""
-    encoding: str = "utf-8"
-    """Encoding to use if decoding the bytes into a string.
-    
-    Use utf-8 as default encoding, if decoding to string.
-    """
-    path: Optional[PathLike] = None
-    """Location where the original content was found."""
-
-    metadata: Dict[str, Any] = Field(default_factory=dict)
-    """Metadata about the blob (e.g., source)"""
-
-    class Config:
-        arbitrary_types_allowed = True
-        frozen = True
-
-    @property
-    def source(self) -> Optional[str]:
-        """The source location of the blob as string if known otherwise none.
-
-        If a path is associated with the blob, it will default to the path location.
+    def __init__(
+        self,
+        file_path: Union[str, Path],
+        jq_schema: str,
+        content_key: Optional[str] = None,
+        is_content_key_jq_parsable: Optional[bool] = False,
+        metadata_func: Optional[Callable[[Dict, Dict], Dict]] = None,
+        text_content: bool = True,
+        json_lines: bool = False,
+    ):
+        """Initialize the JSONLoader.
 
-        Unless explicitly set via a metadata field called "source", in which
-        case that value will be used instead.
+        Args:
+            file_path (Union[str, Path]): The path to the JSON or JSON Lines file.
+            jq_schema (str): The jq schema to use to extract the data or text from
+                the JSON.
+            content_key (str): The key to use to extract the content from
+                the JSON if the jq_schema results to a list of objects (dict).
+                If is_content_key_jq_parsable is True, this has to be a jq compatible
+                schema. If is_content_key_jq_parsable is False, this should be a simple
+                string key.
+            is_content_key_jq_parsable (bool): A flag to determine if
+                content_key is parsable by jq or not. If True, content_key is
+                treated as a jq schema and compiled accordingly. If False or if
+                content_key is None, content_key is used as a simple string.
+                Default is False.
+            metadata_func (Callable[Dict, Dict]): A function that takes in the JSON
+                object extracted by the jq_schema and the default metadata and returns
+                a dict of the updated metadata.
+            text_content (bool): Boolean flag to indicate whether the content is in
+                string format, default to True.
+            json_lines (bool): Boolean flag to indicate whether the input is in
+                JSON Lines format.
         """
-        if self.metadata and "source" in self.metadata:
-            return cast(Optional[str], self.metadata["source"])
-        return str(self.path) if self.path else None
-
-    @root_validator(pre=True)
-    def check_blob_is_valid(cls, values: Mapping[str, Any]) -> Mapping[str, Any]:
-        """Verify that either data or path is provided."""
-        if "data" not in values and "path" not in values:
-            raise ValueError("Either data or path must be provided")
-        return values
-
-    def as_string(self) -> str:
-        """Read data as a string."""
-        if self.data is None and self.path:
-            with open(str(self.path), "r", encoding=self.encoding) as f:
-                return f.read()
-        elif isinstance(self.data, bytes):
-            return self.data.decode(self.encoding)
-        elif isinstance(self.data, str):
-            return self.data
-        else:
-            raise ValueError(f"Unable to get string for blob {self}")
+        try:
+            import jq
 
-    def as_bytes(self) -> bytes:
-        """Read data as bytes."""
-        if isinstance(self.data, bytes):
-            return self.data
-        elif isinstance(self.data, str):
-            return self.data.encode(self.encoding)
-        elif self.data is None and self.path:
-            with open(str(self.path), "rb") as f:
-                return f.read()
+            self.jq = jq
+        except ImportError:
+            raise ImportError(
+                "jq package not found, please install it with `pip install jq`"
+            )
+
+        self.file_path = Path(file_path).resolve()
+        self._jq_schema = jq.compile(jq_schema)
+        self._is_content_key_jq_parsable = is_content_key_jq_parsable
+        self._content_key = content_key
+        self._metadata_func = metadata_func
+        self._text_content = text_content
+        self._json_lines = json_lines
+
+    def lazy_load(self) -> Iterator[Document]:
+        """Load and return documents from the JSON file."""
+        index = 0
+        if self._json_lines:
+            with self.file_path.open(encoding="utf-8") as f:
+                for line in f:
+                    line = line.strip()
+                    if line:
+                        for doc in self._parse(line, index):
+                            yield doc
+                            index += 1
         else:
-            raise ValueError(f"Unable to get bytes for blob {self}")
-
-    @contextlib.contextmanager
-    def as_bytes_io(self) -> Generator[Union[BytesIO, BufferedReader], None, None]:
-        """Read data as a byte stream."""
-        if isinstance(self.data, bytes):
-            yield BytesIO(self.data)
-        elif self.data is None and self.path:
-            with open(str(self.path), "rb") as f:
-                yield f
+            for doc in self._parse(self.file_path.read_text(encoding="utf-8"), index):
+                yield doc
+                index += 1
+
+    def _parse(self, content: str, index: int) -> Iterator[Document]:
+        """Convert given content to documents."""
+        data = self._jq_schema.input(json.loads(content))
+
+        # Perform some validation
+        # This is not a perfect validation, but it should catch most cases
+        # and prevent the user from getting a cryptic error later on.
+        if self._content_key is not None:
+            self._validate_content_key(data)
+        if self._metadata_func is not None:
+            self._validate_metadata_func(data)
+
+        for i, sample in enumerate(data, index + 1):
+            text = self._get_text(sample=sample)
+            metadata = self._get_metadata(
+                sample=sample, source=str(self.file_path), seq_num=i
+            )
+            yield Document(page_content=text, metadata=metadata)
+
+    def _get_text(self, sample: Any) -> str:
+        """Convert sample to string format"""
+        if self._content_key is not None:
+            if self._is_content_key_jq_parsable:
+                compiled_content_key = self.jq.compile(self._content_key)
+                content = compiled_content_key.input(sample).first()
+            else:
+                content = sample[self._content_key]
         else:
-            raise NotImplementedError(f"Unable to convert blob {self}")
-
-    @classmethod
-    def from_path(
-        cls,
-        path: PathLike,
-        *,
-        encoding: str = "utf-8",
-        mime_type: Optional[str] = None,
-        guess_type: bool = True,
-        metadata: Optional[dict] = None,
-    ) -> Blob:
-        """Load the blob from a path like object.
+            content = sample
 
-        Args:
-            path: path like object to file to be read
-            encoding: Encoding to use if decoding the bytes into a string
-            mime_type: if provided, will be set as the mime-type of the data
-            guess_type: If True, the mimetype will be guessed from the file extension,
-                        if a mime-type was not provided
-            metadata: Metadata to associate with the blob
-
-        Returns:
-            Blob instance
-        """
-        if mime_type is None and guess_type:
-            _mimetype = mimetypes.guess_type(path)[0] if guess_type else None
+        if self._text_content and not isinstance(content, str):
+            raise ValueError(
+                f"Expected page_content is string, got {type(content)} instead. \
+                    Set `text_content=False` if the desired input for \
+                    `page_content` is not a string"
+            )
+
+        # In case the text is None, set it to an empty string
+        elif isinstance(content, str):
+            return content
+        elif isinstance(content, dict):
+            return json.dumps(content) if content else ""
         else:
-            _mimetype = mime_type
-        # We do not load the data immediately, instead we treat the blob as a
-        # reference to the underlying data.
-        return cls(
-            data=None,
-            mimetype=_mimetype,
-            encoding=encoding,
-            path=path,
-            metadata=metadata if metadata is not None else {},
-        )
-
-    @classmethod
-    def from_data(
-        cls,
-        data: Union[str, bytes],
-        *,
-        encoding: str = "utf-8",
-        mime_type: Optional[str] = None,
-        path: Optional[str] = None,
-        metadata: Optional[dict] = None,
-    ) -> Blob:
-        """Initialize the blob from in-memory data.
+            return str(content) if content is not None else ""
 
-        Args:
-            data: the in-memory data associated with the blob
-            encoding: Encoding to use if decoding the bytes into a string
-            mime_type: if provided, will be set as the mime-type of the data
-            path: if provided, will be set as the source from which the data came
-            metadata: Metadata to associate with the blob
-
-        Returns:
-            Blob instance
+    def _get_metadata(
+        self, sample: Dict[str, Any], **additional_fields: Any
+    ) -> Dict[str, Any]:
         """
-        return cls(
-            data=data,
-            mimetype=mime_type,
-            encoding=encoding,
-            path=path,
-            metadata=metadata if metadata is not None else {},
-        )
-
-    def __repr__(self) -> str:
-        """Define the blob representation."""
-        str_repr = f"Blob {id(self)}"
-        if self.source:
-            str_repr += f" {self.source}"
-        return str_repr
-
-
-class BlobLoader(ABC):
-    """Abstract interface for blob loaders implementation.
-
-    Implementer should be able to load raw content from a storage system according
-    to some criteria and return the raw content lazily as a stream of blobs.
-    """
+        Return a metadata dictionary base on the existence of metadata_func
+        :param sample: single data payload
+        :param additional_fields: key-word arguments to be added as metadata values
+        :return:
+        """
+        if self._metadata_func is not None:
+            return self._metadata_func(sample, additional_fields)
+        else:
+            return additional_fields
 
-    @abstractmethod
-    def yield_blobs(
-        self,
-    ) -> Iterable[Blob]:
-        """A lazy loader for raw data represented by LangChain's Blob object.
+    def _validate_content_key(self, data: Any) -> None:
+        """Check if a content key is valid"""
 
-        Returns:
-            A generator over blobs
-        """
+        sample = data.first()
+        if not isinstance(sample, dict):
+            raise ValueError(
+                f"Expected the jq schema to result in a list of objects (dict), \
+                    so sample must be a dict but got `{type(sample)}`"
+            )
+
+        if (
+            not self._is_content_key_jq_parsable
+            and sample.get(self._content_key) is None
+        ):
+            raise ValueError(
+                f"Expected the jq schema to result in a list of objects (dict) \
+                    with the key `{self._content_key}`"
+            )
+        if (
+            self._is_content_key_jq_parsable
+            and self.jq.compile(self._content_key).input(sample).text() is None
+        ):
+            raise ValueError(
+                f"Expected the jq schema to result in a list of objects (dict) \
+                    with the key `{self._content_key}` which should be parsable by jq"
+            )
+
+    def _validate_metadata_func(self, data: Any) -> None:
+        """Check if the metadata_func output is valid"""
+
+        sample = data.first()
+        if self._metadata_func is not None:
+            sample_metadata = self._metadata_func(sample, {})
+            if not isinstance(sample_metadata, dict):
+                raise ValueError(
+                    f"Expected the metadata_func to return a dict but got \
+                        `{type(sample_metadata)}`"
+                )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/blob_loaders/youtube_audio.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/blob_loaders/youtube_audio.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from typing import Iterable, List
 
 from langchain_community.document_loaders.blob_loaders import FileSystemBlobLoader
 from langchain_community.document_loaders.blob_loaders.schema import Blob, BlobLoader
 
 
 class YoutubeAudioLoader(BlobLoader):
-
     """Load YouTube urls as audio file(s)."""
 
     def __init__(self, urls: List[str], save_dir: str):
         if not isinstance(urls, list):
             raise TypeError("urls must be a list")
 
         self.urls = urls
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/blockchain.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/blockchain.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/brave_search.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/brave_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/browserless.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/browserless.py`

 * *Files 8% similar despite different names*

```diff
@@ -57,11 +57,7 @@
 
                 yield Document(
                     page_content=response.text,
                     metadata={
                         "source": url,
                     },
                 )
-
-    def load(self) -> List[Document]:
-        """Load Documents from URLs."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/chatgpt.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/chatgpt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/chromium.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/chromium.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,36 +1,34 @@
 import asyncio
 import logging
-from typing import Iterator, List
+from typing import AsyncIterator, Iterator, List
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
 
 class AsyncChromiumLoader(BaseLoader):
     """Scrape HTML pages from URLs using a
     headless instance of the Chromium."""
 
-    def __init__(
-        self,
-        urls: List[str],
-    ):
-        """
-        Initialize the loader with a list of URL paths.
+    def __init__(self, urls: List[str], *, headless: bool = True):
+        """Initialize the loader with a list of URL paths.
 
         Args:
-            urls (List[str]): A list of URLs to scrape content from.
+            urls: A list of URLs to scrape content from.
+            headless: Whether to run browser in headless mode.
 
         Raises:
             ImportError: If the required 'playwright' package is not installed.
         """
         self.urls = urls
+        self.headless = headless
 
         try:
             import playwright  # noqa: F401
         except ImportError:
             raise ImportError(
                 "playwright is required for AsyncChromiumLoader. "
                 "Please install it with `pip install playwright`."
@@ -48,15 +46,15 @@
 
         """
         from playwright.async_api import async_playwright
 
         logger.info("Starting scraping...")
         results = ""
         async with async_playwright() as p:
-            browser = await p.chromium.launch(headless=True)
+            browser = await p.chromium.launch(headless=self.headless)
             try:
                 page = await browser.new_page()
                 await page.goto(url)
                 results = await page.content()  # Simply get the HTML content
                 logger.info("Content scraped")
             except Exception as e:
                 results = f"Error: {e}"
@@ -75,17 +73,25 @@
 
         """
         for url in self.urls:
             html_content = asyncio.run(self.ascrape_playwright(url))
             metadata = {"source": url}
             yield Document(page_content=html_content, metadata=metadata)
 
-    def load(self) -> List[Document]:
+    async def alazy_load(self) -> AsyncIterator[Document]:
         """
-        Load and return all Documents from the provided URLs.
+        Asynchronously load text content from the provided URLs.
 
-        Returns:
-            List[Document]: A list of Document objects
-            containing the scraped content from each URL.
+        This method leverages asyncio to initiate the scraping of all provided URLs
+        simultaneously. It improves performance by utilizing concurrent asynchronous
+        requests. Each Document is yielded as soon as its content is available,
+        encapsulating the scraped content.
 
+        Yields:
+            Document: A Document object containing the scraped content, along with its
+            source URL as metadata.
         """
-        return list(self.lazy_load())
+        tasks = [self.ascrape_playwright(url) for url in self.urls]
+        results = await asyncio.gather(*tasks)
+        for url, content in zip(self.urls, results):
+            metadata = {"source": url}
+            yield Document(page_content=content, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/college_confidential.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/college_confidential.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/concurrent.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/concurrent.py`

 * *Files 10% similar despite different names*

```diff
@@ -19,29 +19,32 @@
 DEFAULT = Literal["default"]
 
 
 class ConcurrentLoader(GenericLoader):
     """Load and pars Documents concurrently."""
 
     def __init__(
-        self, blob_loader: BlobLoader, blob_parser: BaseBlobParser, num_workers: int = 4
+        self,
+        blob_loader: BlobLoader,  # type: ignore[valid-type]
+        blob_parser: BaseBlobParser,
+        num_workers: int = 4,  # type: ignore[valid-type]
     ) -> None:
         super().__init__(blob_loader, blob_parser)
         self.num_workers = num_workers
 
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Load documents lazily with concurrent parsing."""
         with concurrent.futures.ThreadPoolExecutor(
             max_workers=self.num_workers
         ) as executor:
             futures = {
                 executor.submit(self.blob_parser.lazy_parse, blob)
-                for blob in self.blob_loader.yield_blobs()
+                for blob in self.blob_loader.yield_blobs()  # type: ignore[attr-defined]
             }
             for future in concurrent.futures.as_completed(futures):
                 yield from future.result()
 
     @classmethod
     def from_filesystem(
         cls,
@@ -65,15 +68,15 @@
             exclude: A list of patterns to exclude from the loader.
             show_progress: Whether to show a progress bar or not (requires tqdm).
                            Proxies to the file system loader.
             parser: A blob parser which knows how to parse blobs into documents
             num_workers: Max number of concurrent workers to use.
             parser_kwargs: Keyword arguments to pass to the parser.
         """
-        blob_loader = FileSystemBlobLoader(
+        blob_loader = FileSystemBlobLoader(  # type: ignore[attr-defined, misc]
             path,
             glob=glob,
             exclude=exclude,
             suffixes=suffixes,
             show_progress=show_progress,
         )
         if isinstance(parser, str):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/confluence.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/confluence.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import logging
 from enum import Enum
 from io import BytesIO
-from typing import Any, Callable, Dict, List, Optional, Union
+from typing import Any, Callable, Dict, Iterator, List, Optional, Union
 
 import requests
 from langchain_core.documents import Document
 from tenacity import (
     before_sleep_log,
     retry,
     stop_after_attempt,
@@ -45,15 +45,15 @@
     ConfluenceReader will extract the text from the attachments and add it to the
     Document object. Currently supported attachment types are: PDF, PNG, JPEG/JPG,
     SVG, Word and Excel.
 
     Confluence API supports difference format of page content. The storage format is the
     raw XML representation for storage. The view format is the HTML representation for
     viewing with macros are rendered as though it is viewed by users. You can pass
-    a enum `content_format` argument to `load()` to specify the content format, this is
+    a enum `content_format` argument to specify the content format, this is
     set to `ContentFormat.STORAGE` by default, the supported values are:
     `ContentFormat.EDITOR`, `ContentFormat.EXPORT_VIEW`,
     `ContentFormat.ANONYMOUS_EXPORT_VIEW`, `ContentFormat.STORAGE`,
     and `ContentFormat.VIEW`.
 
     Hint: space_key and page_id can both be found in the URL of a page in Confluence
     - https://yoursite.atlassian.com/wiki/spaces/<space_key>/pages/<page_id>
@@ -62,26 +62,30 @@
         .. code-block:: python
 
             from langchain_community.document_loaders import ConfluenceLoader
 
             loader = ConfluenceLoader(
                 url="https://yoursite.atlassian.com/wiki",
                 username="me",
-                api_key="12345"
+                api_key="12345",
+                space_key="SPACE",
+                limit=50,
             )
-            documents = loader.load(space_key="SPACE",limit=50)
+            documents = loader.load()
 
             # Server on perm
             loader = ConfluenceLoader(
                 url="https://confluence.yoursite.com/",
                 username="me",
                 api_key="your_password",
-                cloud=False
+                cloud=False,
+                space_key="SPACE",
+                limit=50,
             )
-            documents = loader.load(space_key="SPACE",limit=50)
+            documents = loader.load()
 
     :param url: _description_
     :type url: str
     :param api_key: _description_, defaults to None
     :type api_key: str, optional
     :param username: _description_, defaults to None
     :type username: str, optional
@@ -95,14 +99,51 @@
     :type number_of_retries: Optional[int], optional
     :param min_retry_seconds: defaults to 2
     :type min_retry_seconds: Optional[int], optional
     :param max_retry_seconds:  defaults to 10
     :type max_retry_seconds: Optional[int], optional
     :param confluence_kwargs: additional kwargs to initialize confluence with
     :type confluence_kwargs: dict, optional
+    :param space_key: Space key retrieved from a confluence URL, defaults to None
+    :type space_key: Optional[str], optional
+    :param page_ids: List of specific page IDs to load, defaults to None
+    :type page_ids: Optional[List[str]], optional
+    :param label: Get all pages with this label, defaults to None
+    :type label: Optional[str], optional
+    :param cql: CQL Expression, defaults to None
+    :type cql: Optional[str], optional
+    :param include_restricted_content: defaults to False
+    :type include_restricted_content: bool, optional
+    :param include_archived_content: Whether to include archived content,
+                                     defaults to False
+    :type include_archived_content: bool, optional
+    :param include_attachments: defaults to False
+    :type include_attachments: bool, optional
+    :param include_comments: defaults to False
+    :type include_comments: bool, optional
+    :param content_format: Specify content format, defaults to
+                            ContentFormat.STORAGE, the supported values are:
+                            `ContentFormat.EDITOR`, `ContentFormat.EXPORT_VIEW`,
+                            `ContentFormat.ANONYMOUS_EXPORT_VIEW`,
+                            `ContentFormat.STORAGE`, and `ContentFormat.VIEW`.
+    :type content_format: ContentFormat
+    :param limit: Maximum number of pages to retrieve per request, defaults to 50
+    :type limit: int, optional
+    :param max_pages: Maximum number of pages to retrieve in total, defaults 1000
+    :type max_pages: int, optional
+    :param ocr_languages: The languages to use for the Tesseract agent. To use a
+                          language, you'll first need to install the appropriate
+                          Tesseract language pack.
+    :type ocr_languages: str, optional
+    :param keep_markdown_format: Whether to keep the markdown format, defaults to
+        False
+    :type keep_markdown_format: bool
+    :param keep_newlines: Whether to keep the newlines format, defaults to
+        False
+    :type keep_newlines: bool
     :raises ValueError: Errors while validating input
     :raises ImportError: Required dependencies not installed.
     """
 
     def __init__(
         self,
         url: str,
@@ -112,28 +153,58 @@
         oauth2: Optional[dict] = None,
         token: Optional[str] = None,
         cloud: Optional[bool] = True,
         number_of_retries: Optional[int] = 3,
         min_retry_seconds: Optional[int] = 2,
         max_retry_seconds: Optional[int] = 10,
         confluence_kwargs: Optional[dict] = None,
+        *,
+        space_key: Optional[str] = None,
+        page_ids: Optional[List[str]] = None,
+        label: Optional[str] = None,
+        cql: Optional[str] = None,
+        include_restricted_content: bool = False,
+        include_archived_content: bool = False,
+        include_attachments: bool = False,
+        include_comments: bool = False,
+        content_format: ContentFormat = ContentFormat.STORAGE,
+        limit: Optional[int] = 50,
+        max_pages: Optional[int] = 1000,
+        ocr_languages: Optional[str] = None,
+        keep_markdown_format: bool = False,
+        keep_newlines: bool = False,
     ):
+        self.space_key = space_key
+        self.page_ids = page_ids
+        self.label = label
+        self.cql = cql
+        self.include_restricted_content = include_restricted_content
+        self.include_archived_content = include_archived_content
+        self.include_attachments = include_attachments
+        self.include_comments = include_comments
+        self.content_format = content_format
+        self.limit = limit
+        self.max_pages = max_pages
+        self.ocr_languages = ocr_languages
+        self.keep_markdown_format = keep_markdown_format
+        self.keep_newlines = keep_newlines
+
         confluence_kwargs = confluence_kwargs or {}
         errors = ConfluenceLoader.validate_init_args(
             url=url,
             api_key=api_key,
             username=username,
             session=session,
             oauth2=oauth2,
             token=token,
         )
         if errors:
             raise ValueError(f"Error(s) while validating input: {errors}")
         try:
-            from atlassian import Confluence  # noqa: F401
+            from atlassian import Confluence
         except ImportError:
             raise ImportError(
                 "`atlassian` package not found, please run "
                 "`pip install atlassian-python-api`"
             )
 
         self.base_url = url
@@ -200,92 +271,58 @@
             errors.append(
                 "You have either omitted require keys or added extra "
                 "keys to the oauth2 dictionary. key values should be "
                 "`['access_token', 'access_token_secret', 'consumer_key', 'key_cert']`"
             )
         return errors or None
 
-    def load(
-        self,
-        space_key: Optional[str] = None,
-        page_ids: Optional[List[str]] = None,
-        label: Optional[str] = None,
-        cql: Optional[str] = None,
-        include_restricted_content: bool = False,
-        include_archived_content: bool = False,
-        include_attachments: bool = False,
-        include_comments: bool = False,
-        content_format: ContentFormat = ContentFormat.STORAGE,
-        limit: Optional[int] = 50,
-        max_pages: Optional[int] = 1000,
-        ocr_languages: Optional[str] = None,
-        keep_markdown_format: bool = False,
-        keep_newlines: bool = False,
-    ) -> List[Document]:
-        """
-        :param space_key: Space key retrieved from a confluence URL, defaults to None
-        :type space_key: Optional[str], optional
-        :param page_ids: List of specific page IDs to load, defaults to None
-        :type page_ids: Optional[List[str]], optional
-        :param label: Get all pages with this label, defaults to None
-        :type label: Optional[str], optional
-        :param cql: CQL Expression, defaults to None
-        :type cql: Optional[str], optional
-        :param include_restricted_content: defaults to False
-        :type include_restricted_content: bool, optional
-        :param include_archived_content: Whether to include archived content,
-                                         defaults to False
-        :type include_archived_content: bool, optional
-        :param include_attachments: defaults to False
-        :type include_attachments: bool, optional
-        :param include_comments: defaults to False
-        :type include_comments: bool, optional
-        :param content_format: Specify content format, defaults to
-                                ContentFormat.STORAGE, the supported values are:
-                                `ContentFormat.EDITOR`, `ContentFormat.EXPORT_VIEW`,
-                                `ContentFormat.ANONYMOUS_EXPORT_VIEW`,
-                                `ContentFormat.STORAGE`, and `ContentFormat.VIEW`.
-        :type content_format: ContentFormat
-        :param limit: Maximum number of pages to retrieve per request, defaults to 50
-        :type limit: int, optional
-        :param max_pages: Maximum number of pages to retrieve in total, defaults 1000
-        :type max_pages: int, optional
-        :param ocr_languages: The languages to use for the Tesseract agent. To use a
-                              language, you'll first need to install the appropriate
-                              Tesseract language pack.
-        :type ocr_languages: str, optional
-        :param keep_markdown_format: Whether to keep the markdown format, defaults to
-            False
-        :type keep_markdown_format: bool
-        :param keep_newlines: Whether to keep the newlines format, defaults to
-            False
-        :type keep_newlines: bool
-        :raises ValueError: _description_
-        :raises ImportError: _description_
-        :return: _description_
-        :rtype: List[Document]
-        """
+    def _resolve_param(self, param_name: str, kwargs: Any) -> Any:
+        return kwargs[param_name] if param_name in kwargs else getattr(self, param_name)
+
+    def _lazy_load(self, **kwargs: Any) -> Iterator[Document]:
+        if kwargs:
+            logger.warning(
+                f"Received runtime arguments {kwargs}. Passing runtime args to `load`"
+                f" is deprecated. Please pass arguments during initialization instead."
+            )
+        space_key = self._resolve_param("space_key", kwargs)
+        page_ids = self._resolve_param("page_ids", kwargs)
+        label = self._resolve_param("label", kwargs)
+        cql = self._resolve_param("cql", kwargs)
+        include_restricted_content = self._resolve_param(
+            "include_restricted_content", kwargs
+        )
+        include_archived_content = self._resolve_param(
+            "include_archived_content", kwargs
+        )
+        include_attachments = self._resolve_param("include_attachments", kwargs)
+        include_comments = self._resolve_param("include_comments", kwargs)
+        content_format = self._resolve_param("content_format", kwargs)
+        limit = self._resolve_param("limit", kwargs)
+        max_pages = self._resolve_param("max_pages", kwargs)
+        ocr_languages = self._resolve_param("ocr_languages", kwargs)
+        keep_markdown_format = self._resolve_param("keep_markdown_format", kwargs)
+        keep_newlines = self._resolve_param("keep_newlines", kwargs)
+
         if not space_key and not page_ids and not label and not cql:
             raise ValueError(
                 "Must specify at least one among `space_key`, `page_ids`, "
                 "`label`, `cql` parameters."
             )
 
-        docs = []
-
         if space_key:
             pages = self.paginate_request(
                 self.confluence.get_all_pages_from_space,
                 space=space_key,
                 limit=limit,
                 max_pages=max_pages,
                 status="any" if include_archived_content else "current",
-                expand=content_format.value,
+                expand=f"{content_format.value},version",
             )
-            docs += self.process_pages(
+            yield from self.process_pages(
                 pages,
                 include_restricted_content,
                 include_attachments,
                 include_comments,
                 content_format,
                 ocr_languages=ocr_languages,
                 keep_markdown_format=keep_markdown_format,
@@ -308,17 +345,17 @@
         if cql:
             pages = self.paginate_request(
                 self._search_content_by_cql,
                 cql=cql,
                 limit=limit,
                 max_pages=max_pages,
                 include_archived_spaces=include_archived_content,
-                expand=content_format.value,
+                expand=f"{content_format.value},version",
             )
-            docs += self.process_pages(
+            yield from self.process_pages(
                 pages,
                 include_restricted_content,
                 include_attachments,
                 include_comments,
                 content_format,
                 ocr_languages,
                 keep_markdown_format,
@@ -339,25 +376,28 @@
                     before_sleep=before_sleep_log(logger, logging.WARNING),
                 )(self.confluence.get_page_by_id)
                 page = get_page(
                     page_id=page_id, expand=f"{content_format.value},version"
                 )
                 if not include_restricted_content and not self.is_public_page(page):
                     continue
-                doc = self.process_page(
+                yield self.process_page(
                     page,
                     include_attachments,
                     include_comments,
                     content_format,
                     ocr_languages,
                     keep_markdown_format,
                 )
-                docs.append(doc)
 
-        return docs
+    def load(self, **kwargs: Any) -> List[Document]:
+        return list(self._lazy_load(**kwargs))
+
+    def lazy_load(self) -> Iterator[Document]:
+        yield from self._lazy_load()
 
     def _search_content_by_cql(
         self, cql: str, include_archived_spaces: Optional[bool] = None, **kwargs: Any
     ) -> List[dict]:
         url = "rest/api/content/search"
 
         params: Dict[str, Any] = {"cql": cql}
@@ -426,32 +466,28 @@
         include_restricted_content: bool,
         include_attachments: bool,
         include_comments: bool,
         content_format: ContentFormat,
         ocr_languages: Optional[str] = None,
         keep_markdown_format: Optional[bool] = False,
         keep_newlines: bool = False,
-    ) -> List[Document]:
+    ) -> Iterator[Document]:
         """Process a list of pages into a list of documents."""
-        docs = []
         for page in pages:
             if not include_restricted_content and not self.is_public_page(page):
                 continue
-            doc = self.process_page(
+            yield self.process_page(
                 page,
                 include_attachments,
                 include_comments,
                 content_format,
                 ocr_languages=ocr_languages,
                 keep_markdown_format=keep_markdown_format,
                 keep_newlines=keep_newlines,
             )
-            docs.append(doc)
-
-        return docs
 
     def process_page(
         self,
         page: dict,
         include_attachments: bool,
         include_comments: bool,
         content_format: ContentFormat,
@@ -465,15 +501,15 @@
             except ImportError:
                 raise ImportError(
                     "`markdownify` package not found, please run "
                     "`pip install markdownify`"
                 )
         if include_comments or not keep_markdown_format:
             try:
-                from bs4 import BeautifulSoup  # type: ignore
+                from bs4 import BeautifulSoup
             except ImportError:
                 raise ImportError(
                     "`beautifulsoup4` package not found, please run "
                     "`pip install beautifulsoup4`"
                 )
         if include_attachments:
             attachment_texts = self.process_attachment(page["id"], ocr_languages)
@@ -560,29 +596,29 @@
                 elif media_type == "image/svg+xml":
                     text = title + self.process_svg(absolute_url, ocr_languages)
                 else:
                     continue
                 texts.append(text)
             except requests.HTTPError as e:
                 if e.response.status_code == 404:
-                    print(f"Attachment not found at {absolute_url}")
+                    print(f"Attachment not found at {absolute_url}")  # noqa: T201
                     continue
                 else:
                     raise
 
         return texts
 
     def process_pdf(
         self,
         link: str,
         ocr_languages: Optional[str] = None,
     ) -> str:
         try:
-            import pytesseract  # noqa: F401
-            from pdf2image import convert_from_bytes  # noqa: F401
+            import pytesseract
+            from pdf2image import convert_from_bytes
         except ImportError:
             raise ImportError(
                 "`pytesseract` or `pdf2image` package not found, "
                 "please run `pip install pytesseract pdf2image`"
             )
 
         response = self.confluence.request(path=link, absolute=True)
@@ -607,16 +643,16 @@
 
     def process_image(
         self,
         link: str,
         ocr_languages: Optional[str] = None,
     ) -> str:
         try:
-            import pytesseract  # noqa: F401
-            from PIL import Image  # noqa: F401
+            import pytesseract
+            from PIL import Image
         except ImportError:
             raise ImportError(
                 "`pytesseract` or `Pillow` package not found, "
                 "please run `pip install pytesseract Pillow`"
             )
 
         response = self.confluence.request(path=link, absolute=True)
@@ -633,15 +669,15 @@
         except OSError:
             return text
 
         return pytesseract.image_to_string(image, lang=ocr_languages)
 
     def process_doc(self, link: str) -> str:
         try:
-            import docx2txt  # noqa: F401
+            import docx2txt
         except ImportError:
             raise ImportError(
                 "`docx2txt` package not found, please run `pip install docx2txt`"
             )
 
         response = self.confluence.request(path=link, absolute=True)
         text = ""
@@ -657,15 +693,15 @@
         return docx2txt.process(file_data)
 
     def process_xls(self, link: str) -> str:
         import io
         import os
 
         try:
-            import xlrd  # noqa: F401
+            import xlrd
 
         except ImportError:
             raise ImportError("`xlrd` package not found, please run `pip install xlrd`")
 
         try:
             import pandas as pd
 
@@ -709,18 +745,18 @@
 
     def process_svg(
         self,
         link: str,
         ocr_languages: Optional[str] = None,
     ) -> str:
         try:
-            import pytesseract  # noqa: F401
-            from PIL import Image  # noqa: F401
-            from reportlab.graphics import renderPM  # noqa: F401
-            from svglib.svglib import svg2rlg  # noqa: F401
+            import pytesseract
+            from PIL import Image
+            from reportlab.graphics import renderPM
+            from svglib.svglib import svg2rlg
         except ImportError:
             raise ImportError(
                 "`pytesseract`, `Pillow`, `reportlab` or `svglib` package not found, "
                 "please run `pip install pytesseract Pillow reportlab svglib`"
             )
 
         response = self.confluence.request(path=link, absolute=True)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/conllu.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/conllu.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 import csv
-from typing import List
+from pathlib import Path
+from typing import List, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class CoNLLULoader(BaseLoader):
     """Load `CoNLL-U` files."""
 
-    def __init__(self, file_path: str):
+    def __init__(self, file_path: Union[str, Path]):
         """Initialize with a file path."""
         self.file_path = file_path
 
     def load(self) -> List[Document]:
         """Load from a file path."""
         with open(self.file_path, encoding="utf8") as f:
             tsv = list(csv.reader(f, delimiter="\t"))
@@ -25,9 +26,9 @@
         for i, line in enumerate(lines):
             # Do not add a space after a punctuation mark or at the end of the sentence
             if line[9] == "SpaceAfter=No" or i == len(lines) - 1:
                 text += line[1]
             else:
                 text += line[1] + " "
 
-        metadata = {"source": self.file_path}
+        metadata = {"source": str(self.file_path)}
         return [Document(page_content=text, metadata=metadata)]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/couchbase.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/couchbase.py`

 * *Files 15% similar despite different names*

```diff
@@ -64,18 +64,14 @@
         )
 
         self.cluster: Cluster = Cluster(connection_string, ClusterOptions(auth))
         self.query = query
         self.page_content_fields = page_content_fields
         self.metadata_fields = metadata_fields
 
-    def load(self) -> List[Document]:
-        """Load Couchbase data into Document objects."""
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Load Couchbase data into Document objects lazily."""
         from datetime import timedelta
 
         # Ensure connection to Couchbase cluster
         self.cluster.wait_until_ready(timedelta(seconds=5))
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/csv_loader.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/csv_loader.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import csv
 from io import TextIOWrapper
-from typing import Any, Dict, List, Optional, Sequence
+from pathlib import Path
+from typing import Any, Dict, Iterator, List, Optional, Sequence, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.helpers import detect_file_encodings
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
@@ -31,15 +32,15 @@
             column1: value1
             column2: value2
             column3: value3
     """
 
     def __init__(
         self,
-        file_path: str,
+        file_path: Union[str, Path],
         source_column: Optional[str] = None,
         metadata_columns: Sequence[str] = (),
         csv_args: Optional[Dict] = None,
         encoding: Optional[str] = None,
         autodetect_encoding: bool = False,
     ):
         """
@@ -57,50 +58,43 @@
         self.file_path = file_path
         self.source_column = source_column
         self.metadata_columns = metadata_columns
         self.encoding = encoding
         self.csv_args = csv_args or {}
         self.autodetect_encoding = autodetect_encoding
 
-    def load(self) -> List[Document]:
-        """Load data into document objects."""
-
-        docs = []
+    def lazy_load(self) -> Iterator[Document]:
         try:
             with open(self.file_path, newline="", encoding=self.encoding) as csvfile:
-                docs = self.__read_file(csvfile)
+                yield from self.__read_file(csvfile)
         except UnicodeDecodeError as e:
             if self.autodetect_encoding:
                 detected_encodings = detect_file_encodings(self.file_path)
                 for encoding in detected_encodings:
                     try:
                         with open(
                             self.file_path, newline="", encoding=encoding.encoding
                         ) as csvfile:
-                            docs = self.__read_file(csvfile)
+                            yield from self.__read_file(csvfile)
                             break
                     except UnicodeDecodeError:
                         continue
             else:
                 raise RuntimeError(f"Error loading {self.file_path}") from e
         except Exception as e:
             raise RuntimeError(f"Error loading {self.file_path}") from e
 
-        return docs
-
-    def __read_file(self, csvfile: TextIOWrapper) -> List[Document]:
-        docs = []
-
-        csv_reader = csv.DictReader(csvfile, **self.csv_args)  # type: ignore
+    def __read_file(self, csvfile: TextIOWrapper) -> Iterator[Document]:
+        csv_reader = csv.DictReader(csvfile, **self.csv_args)
         for i, row in enumerate(csv_reader):
             try:
                 source = (
                     row[self.source_column]
                     if self.source_column is not None
-                    else self.file_path
+                    else str(self.file_path)
                 )
             except KeyError:
                 raise ValueError(
                     f"Source column '{self.source_column}' not found in CSV file."
                 )
             content = "\n".join(
                 f"{k.strip()}: {v.strip() if v is not None else v}"
@@ -109,18 +103,15 @@
             )
             metadata = {"source": source, "row": i}
             for col in self.metadata_columns:
                 try:
                     metadata[col] = row[col]
                 except KeyError:
                     raise ValueError(f"Metadata column '{col}' not found in CSV file.")
-            doc = Document(page_content=content, metadata=metadata)
-            docs.append(doc)
-
-        return docs
+            yield Document(page_content=content, metadata=metadata)
 
 
 class UnstructuredCSVLoader(UnstructuredFileLoader):
     """Load `CSV` files using `Unstructured`.
 
     Like other
     Unstructured loaders, UnstructuredCSVLoader can be used in both
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/cube_semantic.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/cube_semantic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import json
 import logging
 import time
-from typing import List
+from typing import Iterator, List
 
 import requests
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
@@ -95,15 +95,15 @@
                 logger.error("Request failed with status code:", response.status_code)
                 break
 
         if retries == self.dimension_values_max_retries:
             logger.info("Maximum retries reached.")
         return []
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Makes a call to Cube's REST API metadata endpoint.
 
         Returns:
             A list of documents with attributes:
                 - page_content=column_title + column_description
                 - metadata
                     - table_name
@@ -127,16 +127,14 @@
         cube_data_objects = raw_meta_json.get("cubes", [])
 
         logger.info(f"Found {len(cube_data_objects)} cube data objects in metadata.")
 
         if not cube_data_objects:
             raise ValueError("No cubes found in metadata.")
 
-        docs = []
-
         for cube_data_obj in cube_data_objects:
             cube_data_obj_name = cube_data_obj.get("name")
             cube_data_obj_type = cube_data_obj.get("type")
             cube_data_obj_is_public = cube_data_obj.get("public")
             measures = cube_data_obj.get("measures", [])
             dimensions = cube_data_obj.get("dimensions", [])
 
@@ -169,10 +167,8 @@
                     column_values=dimension_values,
                     cube_data_obj_type=cube_data_obj_type,
                 )
 
                 page_content = f"{str(item.get('title'))}, "
                 page_content += f"{str(item.get('description'))}"
 
-                docs.append(Document(page_content=page_content, metadata=metadata))
-
-        return docs
+                yield Document(page_content=page_content, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/datadog_logs.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/datadog_logs.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/dataframe.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/dataframe.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Iterator, List
+from typing import Any, Iterator, Literal
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class BaseDataFrameLoader(BaseLoader):
@@ -22,32 +22,40 @@
 
         for _, row in self.data_frame.iterrows():
             text = row[self.page_content_column]
             metadata = row.to_dict()
             metadata.pop(self.page_content_column)
             yield Document(page_content=text, metadata=metadata)
 
-    def load(self) -> List[Document]:
-        """Load full dataframe."""
-        return list(self.lazy_load())
-
 
 class DataFrameLoader(BaseDataFrameLoader):
     """Load `Pandas` DataFrame."""
 
-    def __init__(self, data_frame: Any, page_content_column: str = "text"):
+    def __init__(
+        self,
+        data_frame: Any,
+        page_content_column: str = "text",
+        engine: Literal["pandas", "modin"] = "pandas",
+    ):
         """Initialize with dataframe object.
 
         Args:
             data_frame: Pandas DataFrame object.
             page_content_column: Name of the column containing the page content.
               Defaults to "text".
         """
         try:
-            import pandas as pd
+            if engine == "pandas":
+                import pandas as pd
+            elif engine == "modin":
+                import modin.pandas as pd
+            else:
+                raise ValueError(
+                    f"Unsupported engine {engine}. Must be one of 'pandas', or 'modin'."
+                )
         except ImportError as e:
             raise ImportError(
                 "Unable to import pandas, please install with `pip install pandas`."
             ) from e
 
         if not isinstance(data_frame, pd.DataFrame):
             raise ValueError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/diffbot.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/diffbot.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/discord.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/discord.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/doc_intelligence.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/doc_intelligence.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,25 +6,27 @@
 from langchain_community.document_loaders.blob_loaders import Blob
 from langchain_community.document_loaders.parsers import (
     AzureAIDocumentIntelligenceParser,
 )
 
 
 class AzureAIDocumentIntelligenceLoader(BaseLoader):
-    """Loads a PDF with Azure Document Intelligence"""
+    """Load a PDF with Azure Document Intelligence."""
 
     def __init__(
         self,
         api_endpoint: str,
         api_key: str,
         file_path: Optional[str] = None,
         url_path: Optional[str] = None,
         api_version: Optional[str] = None,
         api_model: str = "prebuilt-layout",
         mode: str = "markdown",
+        *,
+        analysis_features: Optional[List[str]] = None,
     ) -> None:
         """
         Initialize the object for file processing with Azure Document Intelligence
         (formerly Form Recognizer).
 
         This constructor initializes a AzureAIDocumentIntelligenceParser object to be
         used for parsing files using the Azure Document Intelligence API. The load
@@ -41,49 +43,56 @@
             The path to the file that needs to be loaded.
             Either file_path or url_path must be specified.
         url_path : Optional[str]
             The URL to the file that needs to be loaded.
             Either file_path or url_path must be specified.
         api_version: Optional[str]
             The API version for DocumentIntelligenceClient. Setting None to use
-            the default value from SDK.
+            the default value from `azure-ai-documentintelligence` package.
         api_model: str
-            The model name or ID to be used for form recognition in Azure.
+            Unique document model name. Default value is "prebuilt-layout".
+            Note that overriding this default value may result in unsupported
+            behavior.
+        mode: Optional[str]
+            The type of content representation of the generated Documents.
+            Use either "single", "page", or "markdown". Default value is "markdown".
+        analysis_features: Optional[List[str]]
+            List of optional analysis features, each feature should be passed
+            as a str that conforms to the enum `DocumentAnalysisFeature` in
+            `azure-ai-documentintelligence` package. Default value is None.
 
         Examples:
         ---------
         >>> obj = AzureAIDocumentIntelligenceLoader(
         ...     file_path="path/to/file",
         ...     api_endpoint="https://endpoint.azure.com",
         ...     api_key="APIKEY",
         ...     api_version="2023-10-31-preview",
-        ...     model="prebuilt-document"
+        ...     api_model="prebuilt-layout",
+        ...     mode="markdown"
         ... )
         """
 
         assert (
             file_path is not None or url_path is not None
         ), "file_path or url_path must be provided"
         self.file_path = file_path
         self.url_path = url_path
 
-        self.parser = AzureAIDocumentIntelligenceParser(
+        self.parser = AzureAIDocumentIntelligenceParser(  # type: ignore[misc]
             api_endpoint=api_endpoint,
             api_key=api_key,
             api_version=api_version,
             api_model=api_model,
             mode=mode,
+            analysis_features=analysis_features,
         )
 
-    def load(self) -> List[Document]:
-        """Load given path as pages."""
-        return list(self.lazy_load())
-
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazy load given path as pages."""
         if self.file_path is not None:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
             yield from self.parser.parse(blob)
         else:
-            yield from self.parser.parse_url(self.url_path)
+            yield from self.parser.parse_url(self.url_path)  # type: ignore[arg-type]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/docugami.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/docugami.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import io
 import logging
 import os
 from pathlib import Path
 from typing import Any, Dict, List, Mapping, Optional, Sequence, Union
 
 import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 
 from langchain_community.document_loaders.base import BaseLoader
 
 TABLE_NAME = "{http://www.w3.org/1999/xhtml}table"
 
@@ -22,14 +23,19 @@
 PROJECTS_KEY = "projects"
 
 DEFAULT_API_ENDPOINT = "https://api.docugami.com/v1preview1"
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.24",
+    removal="0.3.0",
+    alternative_import="docugami_langchain.DocugamiLoader",
+)
 class DocugamiLoader(BaseLoader, BaseModel):
     """Load from `Docugami`.
 
     To use, you should have the ``dgml-utils`` python package installed.
     """
 
     api: str = DEFAULT_API_ENDPOINT
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/docusaurus.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/docusaurus.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Load Documents from Docusarus Documentation"""
+
 from typing import Any, List, Optional
 
 from langchain_community.document_loaders.sitemap import SitemapLoader
 
 
 class DocusaurusLoader(SitemapLoader):
     """Load from Docusaurus Documentation.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/dropbox.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/dropbox.py`

 * *Files 4% similar despite different names*

```diff
@@ -117,33 +117,33 @@
 
         try:
             text = response.content.decode("utf-8")
         except UnicodeDecodeError:
             file_extension = os.path.splitext(file_path)[1].lower()
 
             if file_extension == ".pdf":
-                print(f"File {file_path} type detected as .pdf")
+                print(f"File {file_path} type detected as .pdf")  # noqa: T201
                 from langchain_community.document_loaders import UnstructuredPDFLoader
 
                 # Download it to a temporary file.
                 temp_dir = tempfile.TemporaryDirectory()
                 temp_pdf = Path(temp_dir.name) / "tmp.pdf"
                 with open(temp_pdf, mode="wb") as f:
                     f.write(response.content)
 
                 try:
                     loader = UnstructuredPDFLoader(str(temp_pdf))
                     docs = loader.load()
                     if docs:
                         return docs[0]
                 except Exception as pdf_ex:
-                    print(f"Error while trying to parse PDF {file_path}: {pdf_ex}")
+                    print(f"Error while trying to parse PDF {file_path}: {pdf_ex}")  # noqa: T201
                     return None
             else:
-                print(
+                print(  # noqa: T201
                     f"File {file_path} could not be decoded as pdf or text. Skipping."
                 )
 
             return None
 
         metadata = {
             "source": f"dropbox://{file_path}",
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/duckdb_loader.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/duckdb_loader.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/email.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/email.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import os
-from typing import Any, List
+from pathlib import Path
+from typing import Any, Iterator, List, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     satisfies_min_unstructured_version,
@@ -37,15 +38,18 @@
         mode="elements",
         process_attachments=True,
     )
     loader.load()
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         process_attachments = unstructured_kwargs.get("process_attachments")
         attachment_partitioner = unstructured_kwargs.get("attachment_partitioner")
 
         if process_attachments and attachment_partitioner is None:
             from unstructured.partition.auto import partition
 
@@ -75,43 +79,40 @@
 class OutlookMessageLoader(BaseLoader):
     """
     Loads Outlook Message files using extract_msg.
 
     https://github.com/TeamMsgExtractor/msg-extractor
     """
 
-    def __init__(self, file_path: str):
+    def __init__(self, file_path: Union[str, Path]):
         """Initialize with a file path.
 
         Args:
             file_path: The path to the Outlook Message file.
         """
 
-        self.file_path = file_path
+        self.file_path = str(file_path)
 
         if not os.path.isfile(self.file_path):
-            raise ValueError("File path %s is not a valid file" % self.file_path)
+            raise ValueError(f"File path {self.file_path} is not a valid file")
 
         try:
             import extract_msg  # noqa:F401
         except ImportError:
             raise ImportError(
                 "extract_msg is not installed. Please install it with "
                 "`pip install extract_msg`"
             )
 
-    def load(self) -> List[Document]:
-        """Load data into document objects."""
+    def lazy_load(self) -> Iterator[Document]:
         import extract_msg
 
         msg = extract_msg.Message(self.file_path)
-        return [
-            Document(
-                page_content=msg.body,
-                metadata={
-                    "source": self.file_path,
-                    "subject": msg.subject,
-                    "sender": msg.sender,
-                    "date": msg.date,
-                },
-            )
-        ]
+        yield Document(
+            page_content=msg.body,
+            metadata={
+                "source": self.file_path,
+                "subject": msg.subject,
+                "sender": msg.sender,
+                "date": msg.date,
+            },
+        )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/epub.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/epub.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/etherscan.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/etherscan.py`

 * *Files 6% similar despite different names*

```diff
@@ -67,66 +67,62 @@
         elif self.filter == "erc1155_transaction":
             result = self.getERC1155Tx()
         else:
             raise ValueError(f"Invalid filter {filter}")
         for doc in result:
             yield doc
 
-    def load(self) -> List[Document]:
-        """Load transactions from spcifc account by Etherscan."""
-        return list(self.lazy_load())
-
     def getNormTx(self) -> List[Document]:
         url = (
             f"https://api.etherscan.io/api?module=account&action=txlist&address={self.account_address}"
             f"&startblock={self.start_block}&endblock={self.end_block}&page={self.page}"
             f"&offset={self.offset}&sort={self.sort}&apikey={self.api_key}"
         )
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         items = response.json()["result"]
         result = []
         if len(items) == 0:
             return [Document(page_content="")]
         for item in items:
             content = str(item)
             metadata = {"from": item["from"], "tx_hash": item["hash"], "to": item["to"]}
             result.append(Document(page_content=content, metadata=metadata))
-        print(len(result))
+        print(len(result))  # noqa: T201
         return result
 
     def getEthBalance(self) -> List[Document]:
         url = (
             f"https://api.etherscan.io/api?module=account&action=balance"
             f"&address={self.account_address}&tag=latest&apikey={self.api_key}"
         )
 
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         return [Document(page_content=response.json()["result"])]
 
     def getInternalTx(self) -> List[Document]:
         url = (
             f"https://api.etherscan.io/api?module=account&action=txlistinternal"
             f"&address={self.account_address}&startblock={self.start_block}"
             f"&endblock={self.end_block}&page={self.page}&offset={self.offset}"
             f"&sort={self.sort}&apikey={self.api_key}"
         )
 
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         items = response.json()["result"]
         result = []
         if len(items) == 0:
             return [Document(page_content="")]
         for item in items:
             content = str(item)
             metadata = {"from": item["from"], "tx_hash": item["hash"], "to": item["to"]}
@@ -141,15 +137,15 @@
             f"&sort={self.sort}&apikey={self.api_key}"
         )
 
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         items = response.json()["result"]
         result = []
         if len(items) == 0:
             return [Document(page_content="")]
         for item in items:
             content = str(item)
             metadata = {"from": item["from"], "tx_hash": item["hash"], "to": item["to"]}
@@ -164,15 +160,15 @@
             f"&sort={self.sort}&apikey={self.api_key}"
         )
 
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         items = response.json()["result"]
         result = []
         if len(items) == 0:
             return [Document(page_content="")]
         for item in items:
             content = str(item)
             metadata = {"from": item["from"], "tx_hash": item["hash"], "to": item["to"]}
@@ -187,15 +183,15 @@
             f"&sort={self.sort}&apikey={self.api_key}"
         )
 
         try:
             response = requests.get(url)
             response.raise_for_status()
         except requests.exceptions.RequestException as e:
-            print("Error occurred while making the request:", e)
+            print("Error occurred while making the request:", e)  # noqa: T201
         items = response.json()["result"]
         result = []
         if len(items) == 0:
             return [Document(page_content="")]
         for item in items:
             content = str(item)
             metadata = {"from": item["from"], "tx_hash": item["hash"], "to": item["to"]}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/evernote.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/evernote.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 """Load documents from Evernote.
 
 https://gist.github.com/foxmask/7b29c43a161e001ff04afdb2f181e31c
 """
+
 import hashlib
 import logging
 from base64 import b64decode
+from pathlib import Path
 from time import strptime
-from typing import Any, Dict, Iterator, List, Optional
+from typing import Any, Dict, Iterator, List, Optional, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
@@ -31,46 +33,45 @@
         file_path (str): The path to the notebook export with a .enex extension
         load_single_document (bool): Whether or not to concatenate the content of all
             notes into a single long Document.
         If this is set to True (default) then the only metadata on the document will be
             the 'source' which contains the file name of the export.
     """  # noqa: E501
 
-    def __init__(self, file_path: str, load_single_document: bool = True):
+    def __init__(self, file_path: Union[str, Path], load_single_document: bool = True):
         """Initialize with file path."""
-        self.file_path = file_path
+        self.file_path = str(file_path)
         self.load_single_document = load_single_document
 
-    def load(self) -> List[Document]:
-        """Load documents from EverNote export file."""
-        documents = [
-            Document(
-                page_content=note["content"],
-                metadata={
-                    **{
-                        key: value
-                        for key, value in note.items()
-                        if key not in ["content", "content-raw", "resource"]
+    def _lazy_load(self) -> Iterator[Document]:
+        for note in self._parse_note_xml(self.file_path):
+            if note.get("content") is not None:
+                yield Document(
+                    page_content=note["content"],
+                    metadata={
+                        **{
+                            key: value
+                            for key, value in note.items()
+                            if key not in ["content", "content-raw", "resource"]
+                        },
+                        **{"source": self.file_path},
                     },
-                    **{"source": self.file_path},
-                },
-            )
-            for note in self._parse_note_xml(self.file_path)
-            if note.get("content") is not None
-        ]
+                )
 
+    def lazy_load(self) -> Iterator[Document]:
+        """Load documents from EverNote export file."""
         if not self.load_single_document:
-            return documents
-
-        return [
-            Document(
-                page_content="".join([document.page_content for document in documents]),
+            yield from self._lazy_load()
+        else:
+            yield Document(
+                page_content="".join(
+                    [document.page_content for document in self._lazy_load()]
+                ),
                 metadata={"source": self.file_path},
             )
-        ]
 
     @staticmethod
     def _parse_content(content: str) -> str:
         try:
             import html2text
 
             return html2text.html2text(content).strip()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/excel.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/excel.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,42 @@
 """Loads Microsoft Excel files."""
-from typing import Any, List
+
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
 class UnstructuredExcelLoader(UnstructuredFileLoader):
     """Load Microsoft Excel files using `Unstructured`.
 
     Like other
     Unstructured loaders, UnstructuredExcelLoader can be used in both
     "single" and "elements" mode. If you use the loader in "elements"
-    mode, each sheet in the Excel file will be a an Unstructured Table
-    element. If you use the loader in "elements" mode, an
+    mode, each sheet in the Excel file will be an Unstructured Table
+    element. If you use the loader in "single" mode, an
     HTML representation of the table will be available in the
     "text_as_html" key in the document metadata.
 
     Examples
     --------
     from langchain_community.document_loaders.excel import UnstructuredExcelLoader
 
-    loader = UnstructuredExcelLoader("stanley-cups.xlsd", mode="elements")
+    loader = UnstructuredExcelLoader("stanley-cups.xlsx", mode="elements")
     docs = loader.load()
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         """
 
         Args:
             file_path: The path to the Microsoft Excel file.
             mode: The mode to use when partitioning the file. See unstructured docs
               for more info. Optional. Defaults to "single".
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/facebook_chat.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/facebook_chat.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import datetime
 import json
 from pathlib import Path
-from typing import List
+from typing import Iterator, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 def concatenate_rows(row: dict) -> str:
@@ -21,26 +21,25 @@
     )
     return f"{sender} on {date}: {text}\n\n"
 
 
 class FacebookChatLoader(BaseLoader):
     """Load `Facebook Chat` messages directory dump."""
 
-    def __init__(self, path: str):
+    def __init__(self, path: Union[str, Path]):
         """Initialize with a path."""
         self.file_path = path
 
-    def load(self) -> List[Document]:
-        """Load documents."""
+    def lazy_load(self) -> Iterator[Document]:
         p = Path(self.file_path)
 
         with open(p, encoding="utf8") as f:
             d = json.load(f)
 
         text = "".join(
             concatenate_rows(message)
             for message in d["messages"]
             if message.get("content") and isinstance(message["content"], str)
         )
         metadata = {"source": str(p)}
 
-        return [Document(page_content=text, metadata=metadata)]
+        yield Document(page_content=text, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/fauna.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/fauna.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterator, List, Optional, Sequence
+from typing import Iterator, Optional, Sequence
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class FaunaLoader(BaseLoader):
@@ -24,17 +24,14 @@
         metadata_fields: Optional[Sequence[str]] = None,
     ):
         self.query = query
         self.page_content_field = page_content_field
         self.secret = secret
         self.metadata_fields = metadata_fields
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         try:
             from fauna import Page, fql
             from fauna.client import Client
             from fauna.encoding import QuerySuccess
         except ImportError:
             raise ImportError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/figma.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/figma.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/gcs_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/gcs_file.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,24 @@
 import os
 import tempfile
 from typing import Callable, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
 from langchain_community.utilities.vertexai import get_client_info
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GCSFileLoader",
+)
 class GCSFileLoader(BaseLoader):
     """Load from GCS file."""
 
     def __init__(
         self,
         project_name: str,
         bucket: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/generic.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/generic.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 from langchain_community.document_loaders.blob_loaders import (
     BlobLoader,
     FileSystemBlobLoader,
 )
 from langchain_community.document_loaders.parsers.registry import get_parser
 
 if TYPE_CHECKING:
-    from langchain.text_splitter import TextSplitter
+    from langchain_text_splitters import TextSplitter
 
 _PathLike = Union[str, Path]
 
 DEFAULT = Literal["default"]
 
 
 class GenericLoader(BaseLoader):
@@ -92,15 +92,15 @@
             parser=PyPDFParser()
         )
 
     """  # noqa: E501
 
     def __init__(
         self,
-        blob_loader: BlobLoader,
+        blob_loader: BlobLoader,  # type: ignore[valid-type]
         blob_parser: BaseBlobParser,
     ) -> None:
         """A generic document loader.
 
         Args:
             blob_loader: A blob loader which knows how to yield blobs
             blob_parser: A blob parser which knows how to parse blobs into documents
@@ -108,21 +108,17 @@
         self.blob_loader = blob_loader
         self.blob_parser = blob_parser
 
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Load documents lazily. Use this when working at a large scale."""
-        for blob in self.blob_loader.yield_blobs():
+        for blob in self.blob_loader.yield_blobs():  # type: ignore[attr-defined]
             yield from self.blob_parser.lazy_parse(blob)
 
-    def load(self) -> List[Document]:
-        """Load all documents."""
-        return list(self.lazy_load())
-
     def load_and_split(
         self, text_splitter: Optional[TextSplitter] = None
     ) -> List[Document]:
         """Load all documents and split them into sentences."""
         raise NotImplementedError(
             "Loading and splitting is not yet implemented for generic loaders. "
             "When they will be implemented they will be added via the initializer. "
@@ -159,15 +155,15 @@
                     setting the class attribute `blob_parser` (the latter
                     should be used with inheritance).
             parser_kwargs: Keyword arguments to pass to the parser.
 
         Returns:
             A generic document loader.
         """
-        blob_loader = FileSystemBlobLoader(
+        blob_loader = FileSystemBlobLoader(  # type: ignore[attr-defined, misc]
             path,
             glob=glob,
             exclude=exclude,
             suffixes=suffixes,
             show_progress=show_progress,
         )
         if isinstance(parser, str):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/geodataframe.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/geodataframe.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Iterator, List
+from typing import Any, Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class GeoDataFrameLoader(BaseLoader):
@@ -63,11 +63,7 @@
             metadata["xmax"] = xmax
             metadata["ymax"] = ymax
 
             metadata.pop(self.page_content_column)
 
             # using WKT instead of str() to help GIS system interoperability
             yield Document(page_content=geom.wkt, metadata=metadata)
-
-    def load(self) -> List[Document]:
-        """Load full dataframe."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/git.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/git.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import os
-from typing import Callable, List, Optional
+from typing import Callable, Iterator, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class GitLoader(BaseLoader):
@@ -35,17 +35,17 @@
               a boolean indicating whether to load the file. Defaults to None.
         """
         self.repo_path = repo_path
         self.clone_url = clone_url
         self.branch = branch
         self.file_filter = file_filter
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         try:
-            from git import Blob, Repo  # type: ignore
+            from git import Blob, Repo
         except ImportError as ex:
             raise ImportError(
                 "Could not import git python package. "
                 "Please install it with `pip install GitPython`."
             ) from ex
 
         if not os.path.exists(self.repo_path) and self.clone_url is None:
@@ -64,23 +64,21 @@
             else:
                 repo = Repo.clone_from(self.clone_url, self.repo_path)
             repo.git.checkout(self.branch)
         else:
             repo = Repo(self.repo_path)
             repo.git.checkout(self.branch)
 
-        docs: List[Document] = []
-
         for item in repo.tree().traverse():
             if not isinstance(item, Blob):
                 continue
 
             file_path = os.path.join(self.repo_path, item.path)
 
-            ignored_files = repo.ignored([file_path])  # type: ignore
+            ignored_files = repo.ignored([file_path])
             if len(ignored_files):
                 continue
 
             # uses filter to skip files
             if self.file_filter and not self.file_filter(file_path):
                 continue
 
@@ -98,13 +96,10 @@
 
                     metadata = {
                         "source": rel_file_path,
                         "file_path": rel_file_path,
                         "file_name": item.name,
                         "file_type": file_type,
                     }
-                    doc = Document(page_content=text_content, metadata=metadata)
-                    docs.append(doc)
+                    yield Document(page_content=text_content, metadata=metadata)
             except Exception as e:
-                print(f"Error reading file {file_path}: {e}")
-
-        return docs
+                print(f"Error reading file {file_path}: {e}")  # noqa: T201
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/gitbook.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/gitbook.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, List, Optional
+from typing import Any, Iterator, List, Optional
 from urllib.parse import urljoin, urlparse
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.web_base import WebBaseLoader
 
 
@@ -43,31 +43,31 @@
         if load_all_paths:
             # set web_path to the sitemap if we want to crawl all paths
             web_page = f"{self.base_url}/sitemap.xml"
         super().__init__(web_paths=(web_page,), continue_on_failure=continue_on_failure)
         self.load_all_paths = load_all_paths
         self.content_selector = content_selector
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Fetch text from one single GitBook page."""
         if self.load_all_paths:
             soup_info = self.scrape()
             relative_paths = self._get_paths(soup_info)
             urls = [urljoin(self.base_url, path) for path in relative_paths]
             soup_infos = self.scrape_all(urls)
-            _documents = [
-                self._get_document(soup_info, url)
-                for soup_info, url in zip(soup_infos, urls)
-            ]
+            for soup_info, url in zip(soup_infos, urls):
+                doc = self._get_document(soup_info, url)
+                if doc:
+                    yield doc
+
         else:
             soup_info = self.scrape()
-            _documents = [self._get_document(soup_info, self.web_path)]
-        documents = [d for d in _documents if d]
-
-        return documents
+            doc = self._get_document(soup_info, self.web_path)
+            if doc:
+                yield doc
 
     def _get_document(
         self, soup: Any, custom_url: Optional[str] = None
     ) -> Optional[Document]:
         """Fetch content from page and return Document."""
         page_content_raw = soup.find(self.content_selector)
         if not page_content_raw:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/github.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/github.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,11 @@
+import base64
 from abc import ABC
 from datetime import datetime
-from typing import Dict, Iterator, List, Literal, Optional, Union
+from typing import Callable, Dict, Iterator, List, Literal, Optional, Union
 
 import requests
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator, validator
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.document_loaders.base import BaseLoader
@@ -16,15 +17,15 @@
     repo: str
     """Name of repository"""
     access_token: str
     """Personal access token - see https://github.com/settings/tokens?type=beta"""
     github_api_url: str = "https://api.github.com"
     """URL of GitHub API"""
 
-    @root_validator(pre=True)
+    @root_validator(pre=True, allow_reuse=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that access token exists in environment."""
         values["access_token"] = get_from_dict_or_env(
             values, "access_token", "GITHUB_PERSONAL_ACCESS_TOKEN"
         )
         return values
 
@@ -60,16 +61,22 @@
     """What to sort results by. Can be one of: 'created', 'updated', 'comments'.
         Default is 'created'."""
     direction: Optional[Literal["asc", "desc"]] = None
     """The direction to sort the results by. Can be one of: 'asc', 'desc'."""
     since: Optional[str] = None
     """Only show notifications updated after the given time.
         This is a timestamp in ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ."""
+    page: Optional[int] = None
+    """The page number for paginated results. 
+        Defaults to 1 in the GitHub API."""
+    per_page: Optional[int] = None
+    """Number of items per page. 
+        Defaults to 30 in the GitHub API."""
 
-    @validator("since")
+    @validator("since", allow_reuse=True)
     def validate_since(cls, v: Optional[str]) -> Optional[str]:
         if v:
             try:
                 datetime.strptime(v, "%Y-%m-%dT%H:%M:%SZ")
             except ValueError:
                 raise ValueError(
                     "Invalid value for 'since'. Expected a date string in "
@@ -107,45 +114,23 @@
             response.raise_for_status()
             issues = response.json()
             for issue in issues:
                 doc = self.parse_issue(issue)
                 if not self.include_prs and doc.metadata["is_pull_request"]:
                     continue
                 yield doc
-            if response.links and response.links.get("next"):
+            if (
+                response.links
+                and response.links.get("next")
+                and (not self.page and not self.per_page)
+            ):
                 url = response.links["next"]["url"]
             else:
                 url = None
 
-    def load(self) -> List[Document]:
-        """
-        Get issues of a GitHub repository.
-
-        Returns:
-            A list of Documents with attributes:
-                - page_content
-                - metadata
-                    - url
-                    - title
-                    - creator
-                    - created_at
-                    - last_update_time
-                    - closed_time
-                    - number of comments
-                    - state
-                    - labels
-                    - assignee
-                    - assignees
-                    - milestone
-                    - locked
-                    - number
-                    - is_pull_request
-        """
-        return list(self.lazy_load())
-
     def parse_issue(self, issue: dict) -> Document:
         """Create Document objects from a list of GitHub issues."""
         metadata = {
             "url": issue["html_url"],
             "title": issue["title"],
             "creator": issue["user"]["login"],
             "created_at": issue["created_at"],
@@ -171,18 +156,78 @@
             "assignee": self.assignee,
             "creator": self.creator,
             "mentioned": self.mentioned,
             "labels": labels,
             "sort": self.sort,
             "direction": self.direction,
             "since": self.since,
+            "page": self.page,
+            "per_page": self.per_page,
         }
         query_params_list = [
             f"{k}={v}" for k, v in query_params_dict.items() if v is not None
         ]
         query_params = "&".join(query_params_list)
         return query_params
 
     @property
     def url(self) -> str:
         """Create URL for GitHub API."""
         return f"{self.github_api_url}/repos/{self.repo}/issues?{self.query_params}"
+
+
+class GithubFileLoader(BaseGitHubLoader, ABC):
+    """Load GitHub File"""
+
+    file_extension: str = ".md"
+    branch: str = "main"
+
+    file_filter: Optional[Callable[[str], bool]]
+
+    def get_file_paths(self) -> List[Dict]:
+        base_url = (
+            f"{self.github_api_url}/repos/{self.repo}/git/trees/"
+            f"{self.branch}?recursive=1"
+        )
+        response = requests.get(base_url, headers=self.headers)
+        response.raise_for_status()
+        all_files = response.json()["tree"]
+        """ one element in all_files
+        {
+            'path': '.github', 
+            'mode': '040000', 
+            'type': 'tree', 
+            'sha': '5dc46e6b38b22707894ced126270b15e2f22f64e', 
+            'url': 'https://api.github.com/repos/langchain-ai/langchain/git/blobs/5dc46e6b38b22707894ced126270b15e2f22f64e'
+        }
+        """
+        return [
+            f
+            for f in all_files
+            if not (self.file_filter and not self.file_filter(f["path"]))
+        ]
+
+    def get_file_content_by_path(self, path: str) -> str:
+        base_url = f"{self.github_api_url}/repos/{self.repo}/contents/{path}"
+        response = requests.get(base_url, headers=self.headers)
+        response.raise_for_status()
+
+        if isinstance(response.json(), dict):
+            content_encoded = response.json()["content"]
+            return base64.b64decode(content_encoded).decode("utf-8")
+
+        return ""
+
+    def lazy_load(self) -> Iterator[Document]:
+        files = self.get_file_paths()
+        for file in files:
+            content = self.get_file_content_by_path(file["path"])
+            if content == "":
+                continue
+
+            metadata = {
+                "path": file["path"],
+                "sha": file["sha"],
+                "source": f"{self.github_api_url}/{self.repo}/{file['type']}/"
+                f"{self.branch}/{file['path']}",
+            }
+            yield Document(page_content=content, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/google_speech_to_text.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/google_speech_to_text.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,27 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.vertexai import get_client_info
 
 if TYPE_CHECKING:
     from google.cloud.speech_v2 import RecognitionConfig
     from google.protobuf.field_mask_pb2 import FieldMask
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.SpeechToTextLoader",
+)
 class GoogleSpeechToTextLoader(BaseLoader):
     """
     Loader for Google Cloud Speech-to-Text audio transcripts.
 
     It uses the Google Cloud Speech-to-Text API to transcribe audio files
     and loads the transcribed text into one or more Documents,
     depending on the specified format.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/googledrive.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/googledrive.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,22 +7,28 @@
 # 4. For service accounts visit
 #   https://cloud.google.com/iam/docs/service-accounts-create
 
 import os
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Sequence, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator, validator
 
 from langchain_community.document_loaders.base import BaseLoader
 
 SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GoogleDriveLoader",
+)
 class GoogleDriveLoader(BaseLoader, BaseModel):
     """Load Google Docs from `Google Drive`."""
 
     service_account_key: Path = Path.home() / ".credentials" / "keys.json"
     """Path to the service account key file."""
     credentials_path: Path = Path.home() / ".credentials" / "credentials.json"
     """Path to the credentials file."""
@@ -212,17 +218,17 @@
         done = False
         try:
             while done is False:
                 status, done = downloader.next_chunk()
 
         except HttpError as e:
             if e.resp.status == 404:
-                print("File not found: {}".format(id))
+                print("File not found: {}".format(id))  # noqa: T201
             else:
-                print("An error occurred: {}".format(e))
+                print("An error occurred: {}".format(e))  # noqa: T201
 
         text = fh.getvalue().decode("utf-8")
         metadata = {
             "source": f"https://docs.google.com/document/d/{id}/edit",
             "title": f"{file.get('name')}",
             "when": f"{file.get('modifiedTime')}",
         }
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/gutenberg.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/gutenberg.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/helpers.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/helpers.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,36 +1,41 @@
 """Document loader helpers."""
 
 import concurrent.futures
-from typing import List, NamedTuple, Optional, cast
+from pathlib import Path
+from typing import List, NamedTuple, Optional, Union, cast
 
 
 class FileEncoding(NamedTuple):
     """File encoding as the NamedTuple."""
 
     encoding: Optional[str]
     """The encoding of the file."""
     confidence: float
     """The confidence of the encoding."""
     language: Optional[str]
     """The language of the file."""
 
 
-def detect_file_encodings(file_path: str, timeout: int = 5) -> List[FileEncoding]:
+def detect_file_encodings(
+    file_path: Union[str, Path], timeout: int = 5
+) -> List[FileEncoding]:
     """Try to detect the file encoding.
 
     Returns a list of `FileEncoding` tuples with the detected encodings ordered
     by confidence.
 
     Args:
         file_path: The path to the file to detect the encoding for.
         timeout: The timeout in seconds for the encoding detection.
     """
     import chardet
 
+    file_path = str(file_path)
+
     def read_and_detect(file_path: str) -> List[dict]:
         with open(file_path, "rb") as f:
             rawdata = f.read()
         return cast(List[dict], chardet.detect_all(rawdata))
 
     with concurrent.futures.ThreadPoolExecutor() as executor:
         future = executor.submit(read_and_detect, file_path)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/hn.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/hn.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/html.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/html.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/html_bs.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/html_bs.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,24 @@
 import logging
-from typing import Dict, List, Union
+from pathlib import Path
+from typing import Dict, Iterator, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
 
 class BSHTMLLoader(BaseLoader):
     """Load `HTML` files and parse them with `beautiful soup`."""
 
     def __init__(
         self,
-        file_path: str,
+        file_path: Union[str, Path],
         open_encoding: Union[str, None] = None,
         bs_kwargs: Union[dict, None] = None,
         get_text_separator: str = "",
     ) -> None:
         """initialize with path, and optionally, file encoding to use, and any kwargs
         to pass to the BeautifulSoup object.
 
@@ -38,26 +39,26 @@
         self.file_path = file_path
         self.open_encoding = open_encoding
         if bs_kwargs is None:
             bs_kwargs = {"features": "lxml"}
         self.bs_kwargs = bs_kwargs
         self.get_text_separator = get_text_separator
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load HTML document into document objects."""
         from bs4 import BeautifulSoup
 
         with open(self.file_path, "r", encoding=self.open_encoding) as f:
             soup = BeautifulSoup(f, **self.bs_kwargs)
 
         text = soup.get_text(self.get_text_separator)
 
         if soup.title:
             title = str(soup.title.string)
         else:
             title = ""
 
         metadata: Dict[str, Union[str, None]] = {
-            "source": self.file_path,
+            "source": str(self.file_path),
             "title": title,
         }
-        return [Document(page_content=text, metadata=metadata)]
+        yield Document(page_content=text, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/hugging_face_dataset.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/hugging_face_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import json
-from typing import Iterator, List, Mapping, Optional, Sequence, Union
+from typing import Iterator, Mapping, Optional, Sequence, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class HuggingFaceDatasetLoader(BaseLoader):
@@ -80,15 +80,11 @@
                 page_content=self.parse_obj(row.pop(self.page_content_column)),
                 metadata=row,
             )
             for key in dataset.keys()
             for row in dataset[key]
         )
 
-    def load(self) -> List[Document]:
-        """Load documents."""
-        return list(self.lazy_load())
-
     def parse_obj(self, page_content: Union[str, object]) -> str:
         if isinstance(page_content, object):
             return json.dumps(page_content)
         return page_content
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/ifixit.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/ifixit.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/image.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/image.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/image_captions.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/image_captions.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from io import BytesIO
+from pathlib import Path
 from typing import Any, List, Tuple, Union
 
 import requests
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
@@ -13,27 +14,27 @@
     By default, the loader utilizes the pre-trained
     Salesforce BLIP image captioning model.
     https://huggingface.co/Salesforce/blip-image-captioning-base
     """
 
     def __init__(
         self,
-        images: Union[str, bytes, List[Union[str, bytes]]],
+        images: Union[str, Path, bytes, List[Union[str, bytes, Path]]],
         blip_processor: str = "Salesforce/blip-image-captioning-base",
         blip_model: str = "Salesforce/blip-image-captioning-base",
     ):
         """Initialize with a list of image data (bytes) or file paths
 
         Args:
             images: Either a single image or a list of images. Accepts
                     image data (bytes) or file paths to images.
             blip_processor: The name of the pre-trained BLIP processor.
             blip_model: The name of the pre-trained BLIP model.
         """
-        if isinstance(images, (str, bytes)):
+        if isinstance(images, (str, Path, bytes)):
             self.images = [images]
         else:
             self.images = images
 
         self.blip_processor = blip_processor
         self.blip_model = blip_model
 
@@ -57,30 +58,32 @@
             )
             doc = Document(page_content=caption, metadata=metadata)
             results.append(doc)
 
         return results
 
     def _get_captions_and_metadata(
-        self, model: Any, processor: Any, image: Union[str, bytes]
+        self, model: Any, processor: Any, image: Union[str, Path, bytes]
     ) -> Tuple[str, dict]:
         """Helper function for getting the captions and metadata of an image."""
         try:
             from PIL import Image
         except ImportError:
             raise ImportError(
                 "`PIL` package not found, please install with `pip install pillow`"
             )
 
         image_source = image  # Save the original source for later reference
 
         try:
             if isinstance(image, bytes):
                 image = Image.open(BytesIO(image)).convert("RGB")
-            elif image.startswith("http://") or image.startswith("https://"):
+            elif isinstance(image, str) and (
+                image.startswith("http://") or image.startswith("https://")
+            ):
                 image = Image.open(requests.get(image, stream=True).raw).convert("RGB")
             else:
                 image = Image.open(image).convert("RGB")
         except Exception:
             if isinstance(image_source, bytes):
                 msg = "Could not get image data from bytes"
             else:
@@ -90,10 +93,10 @@
         inputs = processor(image, "an image of", return_tensors="pt")
         output = model.generate(**inputs)
 
         caption: str = processor.decode(output[0])
         if isinstance(image_source, bytes):
             metadata: dict = {"image_source": "Image bytes provided"}
         else:
-            metadata = {"image_path": image_source}
+            metadata = {"image_path": str(image_source)}
 
         return caption, metadata
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/iugu.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/iugu.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/joplin.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/joplin.py`

 * *Files 2% similar despite different names*

```diff
@@ -87,10 +87,7 @@
             return [tag["title"] for tag in json_data["items"]]
 
     def _convert_date(self, date: int) -> str:
         return datetime.fromtimestamp(date / 1000).strftime("%Y-%m-%d %H:%M:%S")
 
     def lazy_load(self) -> Iterator[Document]:
         yield from self._get_notes()
-
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/json_loader.py` & `gigachain_community-0.2.0/langchain_community/llms/yuan2.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,151 +1,205 @@
 import json
-from pathlib import Path
-from typing import Any, Callable, Dict, List, Optional, Union
+import logging
+from typing import Any, Dict, List, Mapping, Optional, Set
 
-from langchain_core.documents import Document
+import requests
+from langchain_core.callbacks import CallbackManagerForLLMRun
+from langchain_core.language_models.llms import LLM
+from langchain_core.pydantic_v1 import Field
 
-from langchain_community.document_loaders.base import BaseLoader
+from langchain_community.llms.utils import enforce_stop_tokens
 
+logger = logging.getLogger(__name__)
 
-class JSONLoader(BaseLoader):
-    """Load a `JSON` file using a `jq` schema.
+
+class Yuan2(LLM):
+    """Yuan2.0 language models.
 
     Example:
-        [{"text": ...}, {"text": ...}, {"text": ...}] -> schema = .[].text
-        {"key": [{"text": ...}, {"text": ...}, {"text": ...}]} -> schema = .key[].text
-        ["", "", ""] -> schema = .[]
+        .. code-block:: python
+
+            yuan_llm = Yuan2(
+                infer_api="http://127.0.0.1:8000/yuan",
+                max_tokens=1024,
+                temp=1.0,
+                top_p=0.9,
+                top_k=40,
+            )
+            print(yuan_llm)
+            print(yuan_llm.invoke(""))
+    """
+
+    infer_api: str = "http://127.0.0.1:8000/yuan"
+    """Yuan2.0 inference api"""
+
+    max_tokens: int = Field(1024, alias="max_token")
+    """Token context window."""
+
+    temp: Optional[float] = 0.7
+    """The temperature to use for sampling."""
+
+    top_p: Optional[float] = 0.9
+    """The top-p value to use for sampling."""
+
+    top_k: Optional[int] = 0
+    """The top-k value to use for sampling."""
+
+    do_sample: bool = False
+    """The do_sample is a Boolean value that determines whether 
+    to use the sampling method during text generation.
     """
 
-    def __init__(
+    echo: Optional[bool] = False
+    """Whether to echo the prompt."""
+
+    stop: Optional[List[str]] = []
+    """A list of strings to stop generation when encountered."""
+
+    repeat_last_n: Optional[int] = 64
+    "Last n tokens to penalize"
+
+    repeat_penalty: Optional[float] = 1.18
+    """The penalty to apply to repeated tokens."""
+
+    streaming: bool = False
+    """Whether to stream the results or not."""
+
+    history: List[str] = []
+    """History of the conversation"""
+
+    use_history: bool = False
+    """Whether to use history or not"""
+
+    def __init__(self, **kwargs: Any) -> None:
+        """Initialize the Yuan2 class."""
+        super().__init__(**kwargs)
+
+        if (self.top_p or 0) > 0 and (self.top_k or 0) > 0:
+            logger.warning(
+                "top_p and top_k cannot be set simultaneously. "
+                "set top_k to 0 instead..."
+            )
+            self.top_k = 0
+
+    @property
+    def _llm_type(self) -> str:
+        return "Yuan2.0"
+
+    @staticmethod
+    def _model_param_names() -> Set[str]:
+        return {
+            "max_tokens",
+            "temp",
+            "top_k",
+            "top_p",
+            "do_sample",
+        }
+
+    def _default_params(self) -> Dict[str, Any]:
+        return {
+            "do_sample": self.do_sample,
+            "infer_api": self.infer_api,
+            "max_tokens": self.max_tokens,
+            "repeat_penalty": self.repeat_penalty,
+            "temp": self.temp,
+            "top_k": self.top_k,
+            "top_p": self.top_p,
+            "use_history": self.use_history,
+        }
+
+    @property
+    def _identifying_params(self) -> Mapping[str, Any]:
+        """Get the identifying parameters."""
+        return {
+            "model": self._llm_type,
+            **self._default_params(),
+            **{
+                k: v for k, v in self.__dict__.items() if k in self._model_param_names()
+            },
+        }
+
+    def _call(
         self,
-        file_path: Union[str, Path],
-        jq_schema: str,
-        content_key: Optional[str] = None,
-        metadata_func: Optional[Callable[[Dict, Dict], Dict]] = None,
-        text_content: bool = True,
-        json_lines: bool = False,
-    ):
-        """Initialize the JSONLoader.
+        prompt: str,
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> str:
+        """Call out to a Yuan2.0 LLM inference endpoint.
 
         Args:
-            file_path (Union[str, Path]): The path to the JSON or JSON Lines file.
-            jq_schema (str): The jq schema to use to extract the data or text from
-                the JSON.
-            content_key (str): The key to use to extract the content from the JSON if
-                the jq_schema results to a list of objects (dict).
-            metadata_func (Callable[Dict, Dict]): A function that takes in the JSON
-                object extracted by the jq_schema and the default metadata and returns
-                a dict of the updated metadata.
-            text_content (bool): Boolean flag to indicate whether the content is in
-                string format, default to True.
-            json_lines (bool): Boolean flag to indicate whether the input is in
-                JSON Lines format.
+            prompt: The prompt to pass into the model.
+            stop: Optional list of stop words to use when generating.
+
+        Returns:
+            The string generated by the model.
+
+        Example:
+            .. code-block:: python
+
+                response = yuan_llm.invoke("?")
         """
-        try:
-            import jq  # noqa:F401
-        except ImportError:
-            raise ImportError(
-                "jq package not found, please install it with `pip install jq`"
-            )
 
-        self.file_path = Path(file_path).resolve()
-        self._jq_schema = jq.compile(jq_schema)
-        self._content_key = content_key
-        self._metadata_func = metadata_func
-        self._text_content = text_content
-        self._json_lines = json_lines
-
-    def load(self) -> List[Document]:
-        """Load and return documents from the JSON file."""
-        docs: List[Document] = []
-        if self._json_lines:
-            with self.file_path.open(encoding="utf-8") as f:
-                for line in f:
-                    line = line.strip()
-                    if line:
-                        self._parse(line, docs)
+        if self.use_history:
+            self.history.append(prompt)
+            input = "<n>".join(self.history)
         else:
-            self._parse(self.file_path.read_text(encoding="utf-8"), docs)
-        return docs
+            input = prompt
 
-    def _parse(self, content: str, docs: List[Document]) -> None:
-        """Convert given content to documents."""
-        data = self._jq_schema.input(json.loads(content))
-
-        # Perform some validation
-        # This is not a perfect validation, but it should catch most cases
-        # and prevent the user from getting a cryptic error later on.
-        if self._content_key is not None:
-            self._validate_content_key(data)
-        if self._metadata_func is not None:
-            self._validate_metadata_func(data)
-
-        for i, sample in enumerate(data, len(docs) + 1):
-            text = self._get_text(sample=sample)
-            metadata = self._get_metadata(
-                sample=sample, source=str(self.file_path), seq_num=i
-            )
-            docs.append(Document(page_content=text, metadata=metadata))
+        headers = {"Content-Type": "application/json"}
 
-    def _get_text(self, sample: Any) -> str:
-        """Convert sample to string format"""
-        if self._content_key is not None:
-            content = sample.get(self._content_key)
-        else:
-            content = sample
+        data = json.dumps(
+            {
+                "ques_list": [{"id": "000", "ques": input}],
+                "tokens_to_generate": self.max_tokens,
+                "temperature": self.temp,
+                "top_p": self.top_p,
+                "top_k": self.top_k,
+                "do_sample": self.do_sample,
+            }
+        )
 
-        if self._text_content and not isinstance(content, str):
-            raise ValueError(
-                f"Expected page_content is string, got {type(content)} instead. \
-                    Set `text_content=False` if the desired input for \
-                    `page_content` is not a string"
-            )
+        logger.debug("Yuan2.0 prompt:", input)
 
-        # In case the text is None, set it to an empty string
-        elif isinstance(content, str):
-            return content
-        elif isinstance(content, dict):
-            return json.dumps(content) if content else ""
-        else:
-            return str(content) if content is not None else ""
+        # call api
+        try:
+            response = requests.put(self.infer_api, headers=headers, data=data)
+        except requests.exceptions.RequestException as e:
+            raise ValueError(f"Error raised by inference api: {e}")
 
-    def _get_metadata(
-        self, sample: Dict[str, Any], **additional_fields: Any
-    ) -> Dict[str, Any]:
-        """
-        Return a metadata dictionary base on the existence of metadata_func
-        :param sample: single data payload
-        :param additional_fields: key-word arguments to be added as metadata values
-        :return:
-        """
-        if self._metadata_func is not None:
-            return self._metadata_func(sample, additional_fields)
-        else:
-            return additional_fields
+        logger.debug(f"Yuan2.0 response: {response}")
 
-    def _validate_content_key(self, data: Any) -> None:
-        """Check if a content key is valid"""
-        sample = data.first()
-        if not isinstance(sample, dict):
-            raise ValueError(
-                f"Expected the jq schema to result in a list of objects (dict), \
-                    so sample must be a dict but got `{type(sample)}`"
-            )
+        if response.status_code != 200:
+            raise ValueError(f"Failed with response: {response}")
+        try:
+            resp = response.json()
 
-        if sample.get(self._content_key) is None:
+            if resp["errCode"] != "0":
+                raise ValueError(
+                    f"Failed with error code [{resp['errCode']}], "
+                    f"error message: [{resp['exceptionMsg']}]"
+                )
+
+            if "resData" in resp:
+                if len(resp["resData"]["output"]) >= 0:
+                    generate_text = resp["resData"]["output"][0]["ans"]
+                else:
+                    raise ValueError("No output found in response.")
+            else:
+                raise ValueError("No resData found in response.")
+
+        except requests.exceptions.JSONDecodeError as e:
             raise ValueError(
-                f"Expected the jq schema to result in a list of objects (dict) \
-                    with the key `{self._content_key}`"
+                f"Error raised during decoding response from inference api: {e}."
+                f"\nResponse: {response.text}"
             )
 
-    def _validate_metadata_func(self, data: Any) -> None:
-        """Check if the metadata_func output is valid"""
+        if stop is not None:
+            generate_text = enforce_stop_tokens(generate_text, stop)
 
-        sample = data.first()
-        if self._metadata_func is not None:
-            sample_metadata = self._metadata_func(sample, {})
-            if not isinstance(sample_metadata, dict):
-                raise ValueError(
-                    f"Expected the metadata_func to return a dict but got \
-                        `{type(sample_metadata)}`"
-                )
+        # support multi-turn chat
+        if self.use_history:
+            self.history.append(generate_text)
+
+        logger.debug(f"history: {self.history}")
+        return generate_text
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/lakefs.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/lakefs.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/markdown.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/markdown.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/mastodon.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/mastodon.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,20 @@
 from __future__ import annotations
 
 import os
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Sequence
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterable,
+    Iterator,
+    List,
+    Optional,
+    Sequence,
+)
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 if TYPE_CHECKING:
     import mastodon
@@ -50,30 +59,27 @@
         self.api = mastodon.Mastodon(
             access_token=access_token, api_base_url=api_base_url
         )
         self.mastodon_accounts = mastodon_accounts
         self.number_toots = number_toots
         self.exclude_replies = exclude_replies
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load toots into documents."""
-        results: List[Document] = []
         for account in self.mastodon_accounts:
             user = self.api.account_lookup(account)
             toots = self.api.account_statuses(
                 user.id,
                 only_media=False,
                 pinned=False,
                 exclude_replies=self.exclude_replies,
                 exclude_reblogs=True,
                 limit=self.number_toots,
             )
-            docs = self._format_toots(toots, user)
-            results.extend(docs)
-        return results
+            yield from self._format_toots(toots, user)
 
     def _format_toots(
         self, toots: List[Dict[str, Any]], user_info: dict
     ) -> Iterable[Document]:
         """Format toots into documents.
 
         Adding user info, and selected toot fields into the metadata.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/max_compute.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/max_compute.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from typing import Any, Iterator, List, Optional, Sequence
+from typing import Any, Iterator, Optional, Sequence
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.max_compute import MaxComputeAPIWrapper
 
 
@@ -74,10 +74,7 @@
                 page_content_data = row
             page_content = "\n".join(f"{k}: {v}" for k, v in page_content_data.items())
             if self.metadata_columns:
                 metadata = {k: v for k, v in row.items() if k in self.metadata_columns}
             else:
                 metadata = {k: v for k, v in row.items() if k not in page_content_data}
             yield Document(page_content=page_content, metadata=metadata)
-
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/mediawikidump.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/mediawikidump.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 import logging
 from pathlib import Path
-from typing import List, Optional, Sequence, Union
+from typing import Iterator, Optional, Sequence, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
 
 class MWDumpLoader(BaseLoader):
     """Load `MediaWiki` dump from an `XML` file.
 
     Example:
         .. code-block:: python
 
+            from langchain_text_splitters import RecursiveCharacterTextSplitter
             from langchain_community.document_loaders import MWDumpLoader
 
             loader = MWDumpLoader(
                 file_path="myWiki.xml",
                 encoding="utf8"
             )
             docs = loader.load()
-            from langchain.text_splitter import RecursiveCharacterTextSplitter
             text_splitter = RecursiveCharacterTextSplitter(
                 chunk_size=1000, chunk_overlap=0
             )
             texts = text_splitter.split_documents(docs)
 
 
     :param file_path: XML local file path
@@ -56,41 +56,54 @@
         self.file_path = file_path if isinstance(file_path, str) else str(file_path)
         self.encoding = encoding
         # Namespaces range from -2 to 15, inclusive.
         self.namespaces = namespaces
         self.skip_redirects = skip_redirects
         self.stop_on_error = stop_on_error
 
-    def load(self) -> List[Document]:
-        """Load from a file path."""
+    def _load_dump_file(self):  # type: ignore[no-untyped-def]
         try:
-            import mwparserfromhell
             import mwxml
         except ImportError as e:
             raise ImportError(
-                "Unable to import 'mwparserfromhell' or 'mwxml'. Please install with"
-                " `pip install mwparserfromhell mwxml`."
+                "Unable to import 'mwxml'. Please install with" " `pip install mwxml`."
             ) from e
 
-        dump = mwxml.Dump.from_file(open(self.file_path, encoding=self.encoding))
+        return mwxml.Dump.from_file(open(self.file_path, encoding=self.encoding))
+
+    def _load_single_page_from_dump(self, page) -> Document:  # type: ignore[no-untyped-def, return]
+        """Parse a single page."""
+        try:
+            import mwparserfromhell
+        except ImportError as e:
+            raise ImportError(
+                "Unable to import 'mwparserfromhell'. Please install with"
+                " `pip install mwparserfromhell`."
+            ) from e
+        for revision in page:
+            code = mwparserfromhell.parse(revision.text)
+            text = code.strip_code(
+                normalize=True, collapse=True, keep_template_params=False
+            )
+            metadata = {"source": page.title}
+            return Document(page_content=text, metadata=metadata)
+
+    def lazy_load(
+        self,
+    ) -> Iterator[Document]:
+        """Lazy load from a file path."""
+
+        dump = self._load_dump_file()
 
-        docs = []
         for page in dump.pages:
             if self.skip_redirects and page.redirect:
                 continue
             if self.namespaces and page.namespace not in self.namespaces:
                 continue
             try:
-                for revision in page:
-                    code = mwparserfromhell.parse(revision.text)
-                    text = code.strip_code(
-                        normalize=True, collapse=True, keep_template_params=False
-                    )
-                    metadata = {"source": page.title}
-                    docs.append(Document(page_content=text, metadata=metadata))
+                yield self._load_single_page_from_dump(page)
             except Exception as e:
                 logger.error("Parsing error: {}".format(e))
                 if self.stop_on_error:
                     raise e
                 else:
                     continue
-        return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/merge.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/merge.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Iterator, List
+from typing import AsyncIterator, Iterator, List
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class MergedDataLoader(BaseLoader):
@@ -19,10 +19,12 @@
             try:
                 data = loader.lazy_load()
             except NotImplementedError:
                 data = loader.load()
             for document in data:
                 yield document
 
-    def load(self) -> List[Document]:
-        """Load docs."""
-        return list(self.lazy_load())
+    async def alazy_load(self) -> AsyncIterator[Document]:
+        """Lazy load docs from each individual loader."""
+        for loader in self.loaders:
+            async for document in loader.alazy_load():
+                yield document
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/mhtml.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/mhtml.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 import email
 import logging
-from typing import Dict, List, Union
+from pathlib import Path
+from typing import Dict, Iterator, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
 
 class MHTMLLoader(BaseLoader):
     """Parse `MHTML` files with `BeautifulSoup`."""
 
     def __init__(
         self,
-        file_path: str,
+        file_path: Union[str, Path],
         open_encoding: Union[str, None] = None,
         bs_kwargs: Union[dict, None] = None,
         get_text_separator: str = "",
     ) -> None:
         """initialize with path, and optionally, file encoding to use, and any kwargs
         to pass to the BeautifulSoup object.
 
@@ -40,37 +41,37 @@
         self.file_path = file_path
         self.open_encoding = open_encoding
         if bs_kwargs is None:
             bs_kwargs = {"features": "lxml"}
         self.bs_kwargs = bs_kwargs
         self.get_text_separator = get_text_separator
 
-    def load(self) -> List[Document]:
-        from bs4 import BeautifulSoup
-
+    def lazy_load(self) -> Iterator[Document]:
         """Load MHTML document into document objects."""
 
+        from bs4 import BeautifulSoup
+
         with open(self.file_path, "r", encoding=self.open_encoding) as f:
             message = email.message_from_string(f.read())
             parts = message.get_payload()
 
             if not isinstance(parts, list):
                 parts = [message]
 
             for part in parts:
-                if part.get_content_type() == "text/html":
-                    html = part.get_payload(decode=True).decode()
+                if part.get_content_type() == "text/html":  # type: ignore[union-attr]
+                    html = part.get_payload(decode=True).decode()  # type: ignore[union-attr]
 
                     soup = BeautifulSoup(html, **self.bs_kwargs)
                     text = soup.get_text(self.get_text_separator)
 
                     if soup.title:
                         title = str(soup.title.string)
                     else:
                         title = ""
 
                     metadata: Dict[str, Union[str, None]] = {
-                        "source": self.file_path,
+                        "source": str(self.file_path),
                         "title": title,
                     }
-                    return [Document(page_content=text, metadata=metadata)]
-        return []
+                    yield Document(page_content=text, metadata=metadata)
+                    return
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/modern_treasury.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/modern_treasury.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/mongodb.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/mongodb.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import asyncio
 import logging
-from typing import Dict, List, Optional
+from typing import Dict, List, Optional, Sequence
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
 
@@ -15,14 +15,15 @@
     def __init__(
         self,
         connection_string: str,
         db_name: str,
         collection_name: str,
         *,
         filter_criteria: Optional[Dict] = None,
+        field_names: Optional[Sequence[str]] = None,
     ) -> None:
         try:
             from motor.motor_asyncio import AsyncIOMotorClient
         except ImportError as e:
             raise ImportError(
                 "Cannot import from motor, please install with `pip install motor`."
             ) from e
@@ -34,14 +35,15 @@
 
         if not collection_name:
             raise ValueError("collection_name must be provided.")
 
         self.client = AsyncIOMotorClient(connection_string)
         self.db_name = db_name
         self.collection_name = collection_name
+        self.field_names = field_names
         self.filter_criteria = filter_criteria or {}
 
         self.db = self.client.get_database(db_name)
         self.collection = self.db.get_collection(collection_name)
 
     def load(self) -> List[Document]:
         """Load data into Document objects.
@@ -57,21 +59,36 @@
         """
         return asyncio.run(self.aload())
 
     async def aload(self) -> List[Document]:
         """Load data into Document objects."""
         result = []
         total_docs = await self.collection.count_documents(self.filter_criteria)
-        async for doc in self.collection.find(self.filter_criteria):
+
+        # Construct the projection dictionary if field_names are specified
+        projection = (
+            {field: 1 for field in self.field_names} if self.field_names else None
+        )
+
+        async for doc in self.collection.find(self.filter_criteria, projection):
             metadata = {
                 "database": self.db_name,
                 "collection": self.collection_name,
             }
-            result.append(Document(page_content=str(doc), metadata=metadata))
+
+            # Extract text content from filtered fields or use the entire document
+            if self.field_names is not None:
+                fields = {name: doc[name] for name in self.field_names}
+                texts = [str(value) for value in fields.values()]
+                text = " ".join(texts)
+            else:
+                text = str(doc)
+
+            result.append(Document(page_content=text, metadata=metadata))
 
         if len(result) != total_docs:
             logger.warning(
-                f"Only partial collection of documents returned. Loaded {len(result)} "
-                f"docs, expected {total_docs}."
+                f"Only partial collection of documents returned. "
+                f"Loaded {len(result)} docs, expected {total_docs}."
             )
 
         return result
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/news.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/news.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Loader that uses unstructured to load HTML files."""
+
 import logging
 from typing import Any, Iterator, List
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
@@ -46,15 +47,15 @@
         nlp: bool = False,
         continue_on_failure: bool = True,
         show_progress_bar: bool = False,
         **newspaper_kwargs: Any,
     ) -> None:
         """Initialize with file path."""
         try:
-            import newspaper  # noqa:F401
+            import newspaper
 
             self.__version = newspaper.__version__
         except ImportError:
             raise ImportError(
                 "newspaper package not found, please install it with "
                 "`pip install newspaper3k`"
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/notebook.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/notebook.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """Loads .ipynb notebook files."""
+
 import json
 from pathlib import Path
-from typing import Any, List
+from typing import Any, List, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 def concatenate_cells(
@@ -21,15 +22,19 @@
 
     Returns:
         A string with the cell information.
 
     """
     cell_type = cell["cell_type"]
     source = cell["source"]
-    output = cell["outputs"]
+    if include_outputs:
+        try:
+            output = cell["outputs"]
+        except KeyError:
+            pass
 
     if include_outputs and cell_type == "code" and output:
         if "ename" in output[0].keys():
             error_name = output[0]["ename"]
             error_value = output[0]["evalue"]
             if traceback:
                 traceback = output[0]["traceback"]
@@ -54,32 +59,31 @@
         return f"'{cell_type}' cell: '{source}'\n\n"
 
     return ""
 
 
 def remove_newlines(x: Any) -> Any:
     """Recursively remove newlines, no matter the data structure they are stored in."""
-    import pandas as pd
 
     if isinstance(x, str):
         return x.replace("\n", "")
     elif isinstance(x, list):
         return [remove_newlines(elem) for elem in x]
-    elif isinstance(x, pd.DataFrame):
-        return x.applymap(remove_newlines)
+    elif isinstance(x, dict):
+        return {k: remove_newlines(v) for (k, v) in x.items()}
     else:
         return x
 
 
 class NotebookLoader(BaseLoader):
     """Load `Jupyter notebook` (.ipynb) files."""
 
     def __init__(
         self,
-        path: str,
+        path: Union[str, Path],
         include_outputs: bool = False,
         max_output_length: int = 10,
         remove_newline: bool = False,
         traceback: bool = False,
     ):
         """Initialize with a path.
 
@@ -100,34 +104,34 @@
         self.remove_newline = remove_newline
         self.traceback = traceback
 
     def load(
         self,
     ) -> List[Document]:
         """Load documents."""
-        try:
-            import pandas as pd
-        except ImportError:
-            raise ImportError(
-                "pandas is needed for Notebook Loader, "
-                "please install with `pip install pandas`"
-            )
         p = Path(self.file_path)
 
         with open(p, encoding="utf8") as f:
             d = json.load(f)
 
-        data = pd.json_normalize(d["cells"])
-        filtered_data = data[["cell_type", "source", "outputs"]]
+        filtered_data = [
+            {k: v for (k, v) in cell.items() if k in ["cell_type", "source", "outputs"]}
+            for cell in d["cells"]
+        ]
+
         if self.remove_newline:
-            filtered_data = filtered_data.applymap(remove_newlines)
+            filtered_data = list(map(remove_newlines, filtered_data))
 
-        text = filtered_data.apply(
-            lambda x: concatenate_cells(
-                x, self.include_outputs, self.max_output_length, self.traceback
-            ),
-            axis=1,
-        ).str.cat(sep=" ")
+        text = "".join(
+            list(
+                map(
+                    lambda x: concatenate_cells(
+                        x, self.include_outputs, self.max_output_length, self.traceback
+                    ),
+                    filtered_data,
+                )
+            )
+        )
 
         metadata = {"source": str(p)}
 
         return [Document(page_content=text, metadata=metadata)]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/notion.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/notion.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 from pathlib import Path
-from typing import List
+from typing import List, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class NotionDirectoryLoader(BaseLoader):
     """Load `Notion directory` dump."""
 
-    def __init__(self, path: str, *, encoding: str = "utf-8") -> None:
+    def __init__(self, path: Union[str, Path], *, encoding: str = "utf-8") -> None:
         """Initialize with a file path."""
         self.file_path = path
         self.encoding = encoding
 
     def load(self) -> List[Document]:
         """Load documents."""
         paths = list(Path(self.file_path).glob("**/*.md"))
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/notiondb.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/notiondb.py`

 * *Files 22% similar despite different names*

```diff
@@ -16,21 +16,34 @@
 
     Reads content from pages within a Notion Database.
     Args:
         integration_token (str): Notion integration token.
         database_id (str): Notion database id.
         request_timeout_sec (int): Timeout for Notion requests in seconds.
             Defaults to 10.
+        filter_object (Dict[str, Any]): Filter object used to limit returned
+            entries based on specified criteria.
+            E.g.: {
+                "timestamp": "last_edited_time",
+                "last_edited_time": {
+                    "on_or_after": "2024-02-07"
+                }
+            } -> will only return entries that were last edited
+                on or after 2024-02-07
+            Notion docs: https://developers.notion.com/reference/post-database-query-filter
+            Defaults to None, which will return ALL entries.
     """
 
     def __init__(
         self,
         integration_token: str,
         database_id: str,
         request_timeout_sec: Optional[int] = 10,
+        *,
+        filter_object: Optional[Dict[str, Any]] = None,
     ) -> None:
         """Initialize with parameters."""
         if not integration_token:
             raise ValueError("integration_token must be provided")
         if not database_id:
             raise ValueError("database_id must be provided")
 
@@ -38,35 +51,40 @@
         self.database_id = database_id
         self.headers = {
             "Authorization": "Bearer " + self.token,
             "Content-Type": "application/json",
             "Notion-Version": "2022-06-28",
         }
         self.request_timeout_sec = request_timeout_sec
+        self.filter_object = filter_object or {}
 
     def load(self) -> List[Document]:
         """Load documents from the Notion database.
         Returns:
             List[Document]: List of documents.
         """
         page_summaries = self._retrieve_page_summaries()
 
         return list(self.load_page(page_summary) for page_summary in page_summaries)
 
     def _retrieve_page_summaries(
         self, query_dict: Dict[str, Any] = {"page_size": 100}
     ) -> List[Dict[str, Any]]:
-        """Get all the pages from a Notion database."""
+        """
+        Get all the pages from a Notion database
+        OR filter based on specified criteria.
+        """
         pages: List[Dict[str, Any]] = []
 
         while True:
             data = self._request(
                 DATABASE_URL.format(database_id=self.database_id),
                 method="POST",
                 query_dict=query_dict,
+                filter_object=self.filter_object,
             )
 
             pages.extend(data.get("results"))
 
             if not data.get("has_more"):
                 break
 
@@ -178,18 +196,26 @@
                 result_lines_arr.append("\n".join(cur_result_text_arr))
 
             cur_block_id = data.get("next_cursor")
 
         return "\n".join(result_lines_arr)
 
     def _request(
-        self, url: str, method: str = "GET", query_dict: Dict[str, Any] = {}
+        self,
+        url: str,
+        method: str = "GET",
+        query_dict: Dict[str, Any] = {},
+        *,
+        filter_object: Optional[Dict[str, Any]] = None,
     ) -> Any:
+        json_payload = query_dict.copy()
+        if filter_object:
+            json_payload["filter"] = filter_object
         res = requests.request(
             method,
             url,
             headers=self.headers,
-            json=query_dict,
+            json=json_payload,
             timeout=self.request_timeout_sec,
         )
         res.raise_for_status()
         return res.json()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/nuclia.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/nuclia.py`

 * *Files 11% similar despite different names*

```diff
@@ -21,13 +21,13 @@
         data = self.nua.run(
             {"action": "pull", "id": self.id, "path": None, "text": None}
         )
         if not data:
             return []
         obj = json.loads(data)
         text = obj["extracted_text"][0]["body"]["text"]
-        print(text)
+        print(text)  # noqa: T201
         metadata = {
             "file": obj["file_extracted_data"][0],
             "metadata": obj["field_metadata"][0],
         }
         return [Document(page_content=text, metadata=metadata)]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/obs_directory.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/obs_directory.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/obs_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/obs_file.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/obsidian.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/obsidian.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import functools
 import logging
 import re
 from pathlib import Path
-from typing import Any, Dict, List
+from typing import Any, Dict, Iterator, Union
 
 import yaml
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 logger = logging.getLogger(__name__)
@@ -19,15 +19,18 @@
     TEMPLATE_VARIABLE_REGEX = re.compile(r"{{(.*?)}}", re.DOTALL)
     TAG_REGEX = re.compile(r"[^\S\/]#([a-zA-Z_]+[-_/\w]*)")
     DATAVIEW_LINE_REGEX = re.compile(r"^\s*(\w+)::\s*(.*)$", re.MULTILINE)
     DATAVIEW_INLINE_BRACKET_REGEX = re.compile(r"\[(\w+)::\s*(.*)\]", re.MULTILINE)
     DATAVIEW_INLINE_PAREN_REGEX = re.compile(r"\((\w+)::\s*(.*)\)", re.MULTILINE)
 
     def __init__(
-        self, path: str, encoding: str = "UTF-8", collect_metadata: bool = True
+        self,
+        path: Union[str, Path],
+        encoding: str = "UTF-8",
+        collect_metadata: bool = True,
     ):
         """Initialize with a path.
 
         Args:
             path: Path to the directory containing the Obsidian files.
             encoding: Charset encoding, defaults to "UTF-8"
             collect_metadata: Whether to collect metadata from the front matter.
@@ -132,18 +135,16 @@
 
     def _remove_front_matter(self, content: str) -> str:
         """Remove front matter metadata from the given content."""
         if not self.collect_metadata:
             return content
         return self.FRONT_MATTER_REGEX.sub("", content)
 
-    def load(self) -> List[Document]:
-        """Load documents."""
+    def lazy_load(self) -> Iterator[Document]:
         paths = list(Path(self.file_path).glob("**/*.md"))
-        docs = []
         for path in paths:
             with open(path, encoding=self.encoding) as f:
                 text = f.read()
 
             front_matter = self._parse_front_matter(text)
             tags = self._parse_document_tags(text)
             dataview_fields = self._parse_dataview_fields(text)
@@ -159,10 +160,8 @@
             }
 
             if tags or front_matter.get("tags"):
                 metadata["tags"] = ",".join(
                     tags | set(front_matter.get("tags", []) or [])
                 )
 
-            docs.append(Document(page_content=text, metadata=metadata))
-
-        return docs
+            yield Document(page_content=text, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/odt.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/odt.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import Any, List
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
@@ -27,15 +28,18 @@
 
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition-odt
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         """
 
         Args:
             file_path: The path to the file to load.
             mode: The mode to use when loading the file. Can be one of "single",
                 "multi", or "all". Default is "single".
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/onedrive.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/onedrive.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Loads data from OneDrive"""
+
 from __future__ import annotations
 
 import logging
 from typing import TYPE_CHECKING, Iterator, List, Optional, Sequence, Union
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import Field
@@ -87,11 +88,7 @@
         if self.folder_path:
             folder = self._get_folder_from_path(drive)
             for blob in self._load_from_folder(folder):
                 yield from blob_parser.lazy_parse(blob)
         if self.object_ids:
             for blob in self._load_from_object_ids(drive, self.object_ids):
                 yield from blob_parser.lazy_parse(blob)
-
-    def load(self) -> List[Document]:
-        """Load all documents."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/onedrive_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/onedrive_file.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/onenote.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/onenote.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Loads data from OneNote Notebooks"""
+
 from pathlib import Path
 from typing import Dict, Iterator, List, Optional
 
 import requests
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import (
     BaseModel,
@@ -26,15 +27,15 @@
         case_sentive = False
         env_file = ".env"
 
 
 class OneNoteLoader(BaseLoader, BaseModel):
     """Load pages from OneNote notebooks."""
 
-    settings: _OneNoteGraphSettings = Field(default_factory=_OneNoteGraphSettings)
+    settings: _OneNoteGraphSettings = Field(default_factory=_OneNoteGraphSettings)  # type: ignore[arg-type]
     """Settings for the Microsoft Graph API client."""
     auth_with_token: bool = False
     """Whether to authenticate with a token or not. Defaults to False."""
     access_token: str = ""
     """Personal access token"""
     onenote_api_base_url: str = "https://graph.microsoft.com/v1.0/me/onenote"
     """URL of Microsoft Graph API for OneNote"""
@@ -104,26 +105,14 @@
                     )
 
                 if "@odata.nextLink" in pages:
                     request_url = pages["@odata.nextLink"]
                 else:
                     request_url = ""
 
-    def load(self) -> List[Document]:
-        """
-        Get pages from OneNote notebooks.
-
-        Returns:
-            A list of Documents with attributes:
-                - page_content
-                - metadata
-                    - title
-        """
-        return list(self.lazy_load())
-
     def _get_page_content(self, page_id: str) -> str:
         """Get page content from OneNote API"""
         request_url = self.onenote_api_base_url + f"/pages/{page_id}/content"
         response = requests.get(request_url, headers=self._headers, timeout=10)
         response.raise_for_status()
         return response.text
 
@@ -160,16 +149,16 @@
                 client_credential=self.settings.client_secret.get_secret_value(),
                 authority=self.authority_url,
             )
 
             authorization_request_url = client_instance.get_authorization_request_url(
                 self._scopes
             )
-            print("Visit the following url to give consent:")
-            print(authorization_request_url)
+            print("Visit the following url to give consent:")  # noqa: T201
+            print(authorization_request_url)  # noqa: T201
             authorization_url = input("Paste the authenticated url here:\n")
 
             authorization_code = authorization_url.split("code=")[1].split("&")[0]
             access_token_json = client_instance.acquire_token_by_authorization_code(
                 code=authorization_code, scopes=self._scopes
             )
             self.access_token = access_token_json["access_token"]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/org_mode.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/org_mode.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import Any, List
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
@@ -27,15 +28,18 @@
 
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition-org
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         """
 
         Args:
             file_path: The path to the file to load.
             mode: The mode to load the file from. Default is "single".
             **unstructured_kwargs: Any additional keyword arguments to pass
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/__init__.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,27 @@
-from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser
-from langchain_community.document_loaders.parsers.doc_intelligence import (
-    AzureAIDocumentIntelligenceParser,
-)
-from langchain_community.document_loaders.parsers.docai import DocAIParser
-from langchain_community.document_loaders.parsers.grobid import GrobidParser
-from langchain_community.document_loaders.parsers.html import BS4HTMLParser
-from langchain_community.document_loaders.parsers.language import LanguageParser
-from langchain_community.document_loaders.parsers.pdf import (
-    PDFMinerParser,
-    PDFPlumberParser,
-    PyMuPDFParser,
-    PyPDFium2Parser,
-    PyPDFParser,
-)
+import importlib
+from typing import Any
 
-__all__ = [
-    "AzureAIDocumentIntelligenceParser",
-    "BS4HTMLParser",
-    "DocAIParser",
-    "GrobidParser",
-    "LanguageParser",
-    "OpenAIWhisperParser",
-    "PDFMinerParser",
-    "PDFPlumberParser",
-    "PyMuPDFParser",
-    "PyPDFium2Parser",
-    "PyPDFParser",
-]
+_module_lookup = {
+    "AzureAIDocumentIntelligenceParser": "langchain_community.document_loaders.parsers.doc_intelligence",  # noqa: E501
+    "BS4HTMLParser": "langchain_community.document_loaders.parsers.html",
+    "DocAIParser": "langchain_community.document_loaders.parsers.docai",
+    "GrobidParser": "langchain_community.document_loaders.parsers.grobid",
+    "LanguageParser": "langchain_community.document_loaders.parsers.language",
+    "OpenAIWhisperParser": "langchain_community.document_loaders.parsers.audio",
+    "PDFMinerParser": "langchain_community.document_loaders.parsers.pdf",
+    "PDFPlumberParser": "langchain_community.document_loaders.parsers.pdf",
+    "PyMuPDFParser": "langchain_community.document_loaders.parsers.pdf",
+    "PyPDFParser": "langchain_community.document_loaders.parsers.pdf",
+    "PyPDFium2Parser": "langchain_community.document_loaders.parsers.pdf",
+    "VsdxParser": "langchain_community.document_loaders.parsers.vsdx",
+}
+
+
+def __getattr__(name: str) -> Any:
+    if name in _module_lookup:
+        module = importlib.import_module(_module_lookup[name])
+        return getattr(module, name)
+    raise AttributeError(f"module {__name__} has no attribute {name}")
+
+
+__all__ = list(_module_lookup.keys())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/audio.py` & `gigachain_community-0.2.0/langchain_community/llms/huggingface_pipeline.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,310 +1,303 @@
-import logging
-import time
-from typing import Dict, Iterator, Optional, Tuple
+from __future__ import annotations
 
-from langchain_core.documents import Document
+import importlib.util
+import logging
+from typing import Any, List, Mapping, Optional
 
-from langchain_community.document_loaders.base import BaseBlobParser
-from langchain_community.document_loaders.blob_loaders import Blob
-from langchain_community.utils.openai import is_openai_v1
+from langchain_core._api.deprecation import deprecated
+from langchain_core.callbacks import CallbackManagerForLLMRun
+from langchain_core.language_models.llms import BaseLLM
+from langchain_core.outputs import Generation, LLMResult
+from langchain_core.pydantic_v1 import Extra
+
+DEFAULT_MODEL_ID = "gpt2"
+DEFAULT_TASK = "text-generation"
+VALID_TASKS = (
+    "text2text-generation",
+    "text-generation",
+    "summarization",
+    "translation",
+)
+DEFAULT_BATCH_SIZE = 4
 
 logger = logging.getLogger(__name__)
 
 
-class OpenAIWhisperParser(BaseBlobParser):
-    """Transcribe and parse audio files.
-    Audio transcription is with OpenAI Whisper model."""
-
-    def __init__(self, api_key: Optional[str] = None):
-        self.api_key = api_key
-
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
-        """Lazily parse the blob."""
-
-        import io
+@deprecated(
+    since="0.0.37",
+    removal="0.3",
+    alternative_import="from rom langchain_huggingface.llms import HuggingFacePipeline",
+)
+class HuggingFacePipeline(BaseLLM):
+    """HuggingFace Pipeline API.
+
+    To use, you should have the ``transformers`` python package installed.
+
+    Only supports `text-generation`, `text2text-generation`, `summarization` and
+    `translation`  for now.
+
+    Example using from_model_id:
+        .. code-block:: python
+
+            from langchain_community.llms import HuggingFacePipeline
+            hf = HuggingFacePipeline.from_model_id(
+                model_id="gpt2",
+                task="text-generation",
+                pipeline_kwargs={"max_new_tokens": 10},
+            )
+    Example passing pipeline in directly:
+        .. code-block:: python
+
+            from langchain_community.llms import HuggingFacePipeline
+            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
+
+            model_id = "gpt2"
+            tokenizer = AutoTokenizer.from_pretrained(model_id)
+            model = AutoModelForCausalLM.from_pretrained(model_id)
+            pipe = pipeline(
+                "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
+            )
+            hf = HuggingFacePipeline(pipeline=pipe)
+    """
 
+    pipeline: Any  #: :meta private:
+    model_id: str = DEFAULT_MODEL_ID
+    """Model name to use."""
+    model_kwargs: Optional[dict] = None
+    """Keyword arguments passed to the model."""
+    pipeline_kwargs: Optional[dict] = None
+    """Keyword arguments passed to the pipeline."""
+    batch_size: int = DEFAULT_BATCH_SIZE
+    """Batch size to use when passing multiple documents to generate."""
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        extra = Extra.forbid
+
+    @classmethod
+    def from_model_id(
+        cls,
+        model_id: str,
+        task: str,
+        backend: str = "default",
+        device: Optional[int] = -1,
+        device_map: Optional[str] = None,
+        model_kwargs: Optional[dict] = None,
+        pipeline_kwargs: Optional[dict] = None,
+        batch_size: int = DEFAULT_BATCH_SIZE,
+        **kwargs: Any,
+    ) -> HuggingFacePipeline:
+        """Construct the pipeline object from model_id and task."""
         try:
-            import openai
-        except ImportError:
-            raise ImportError(
-                "openai package not found, please install it with "
-                "`pip install openai`"
+            from transformers import (
+                AutoModelForCausalLM,
+                AutoModelForSeq2SeqLM,
+                AutoTokenizer,
             )
-        try:
-            from pydub import AudioSegment
+            from transformers import pipeline as hf_pipeline
+
         except ImportError:
             raise ImportError(
-                "pydub package not found, please install it with " "`pip install pydub`"
+                "Could not import transformers python package. "
+                "Please install it with `pip install transformers`."
             )
 
-        if is_openai_v1():
-            # api_key optional, defaults to `os.environ['OPENAI_API_KEY']`
-            client = openai.OpenAI(api_key=self.api_key)
-        else:
-            # Set the API key if provided
-            if self.api_key:
-                openai.api_key = self.api_key
-
-        # Audio file from disk
-        audio = AudioSegment.from_file(blob.path)
-
-        # Define the duration of each chunk in minutes
-        # Need to meet 25MB size limit for Whisper API
-        chunk_duration = 20
-        chunk_duration_ms = chunk_duration * 60 * 1000
-
-        # Split the audio into chunk_duration_ms chunks
-        for split_number, i in enumerate(range(0, len(audio), chunk_duration_ms)):
-            # Audio chunk
-            chunk = audio[i : i + chunk_duration_ms]
-            file_obj = io.BytesIO(chunk.export(format="mp3").read())
-            if blob.source is not None:
-                file_obj.name = blob.source + f"_part_{split_number}.mp3"
-            else:
-                file_obj.name = f"part_{split_number}.mp3"
-
-            # Transcribe
-            print(f"Transcribing part {split_number+1}!")
-            attempts = 0
-            while attempts < 3:
-                try:
-                    if is_openai_v1():
-                        transcript = client.audio.transcriptions.create(
-                            model="whisper-1", file=file_obj
-                        )
-                    else:
-                        transcript = openai.Audio.transcribe("whisper-1", file_obj)
-                    break
-                except Exception as e:
-                    attempts += 1
-                    print(f"Attempt {attempts} failed. Exception: {str(e)}")
-                    time.sleep(5)
-            else:
-                print("Failed to transcribe after 3 attempts.")
-                continue
-
-            yield Document(
-                page_content=transcript.text,
-                metadata={"source": blob.source, "chunk": split_number},
-            )
-
-
-class OpenAIWhisperParserLocal(BaseBlobParser):
-    """Transcribe and parse audio files with OpenAI Whisper model.
-
-    Audio transcription with OpenAI Whisper model locally from transformers.
-
-    Parameters:
-    device - device to use
-        NOTE: By default uses the gpu if available,
-        if you want to use cpu, please set device = "cpu"
-    lang_model - whisper model to use, for example "openai/whisper-medium"
-    forced_decoder_ids - id states for decoder in multilanguage model,
-        usage example:
-        from transformers import WhisperProcessor
-        processor = WhisperProcessor.from_pretrained("openai/whisper-medium")
-        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language="french",
-          task="transcribe")
-        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language="french",
-        task="translate")
-
+        _model_kwargs = model_kwargs or {}
+        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)
 
-
-    """
-
-    def __init__(
-        self,
-        device: str = "0",
-        lang_model: Optional[str] = None,
-        forced_decoder_ids: Optional[Tuple[Dict]] = None,
-    ):
-        """Initialize the parser.
-
-        Args:
-            device: device to use.
-            lang_model: whisper model to use, for example "openai/whisper-medium".
-              Defaults to None.
-            forced_decoder_ids: id states for decoder in a multilanguage model.
-              Defaults to None.
-        """
         try:
-            from transformers import pipeline
-        except ImportError:
-            raise ImportError(
-                "transformers package not found, please install it with "
-                "`pip install transformers`"
-            )
-        try:
-            import torch
-        except ImportError:
-            raise ImportError(
-                "torch package not found, please install it with " "`pip install torch`"
-            )
+            if task == "text-generation":
+                if backend == "openvino":
+                    try:
+                        from optimum.intel.openvino import OVModelForCausalLM
+
+                    except ImportError:
+                        raise ImportError(
+                            "Could not import optimum-intel python package. "
+                            "Please install it with: "
+                            "pip install 'optimum[openvino,nncf]' "
+                        )
+                    try:
+                        # use local model
+                        model = OVModelForCausalLM.from_pretrained(
+                            model_id, **_model_kwargs
+                        )
 
-        # set device, cpu by default check if there is a GPU available
-        if device == "cpu":
-            self.device = "cpu"
-            if lang_model is not None:
-                self.lang_model = lang_model
-                print("WARNING! Model override. Using model: ", self.lang_model)
-            else:
-                # unless overridden, use the small base model on cpu
-                self.lang_model = "openai/whisper-base"
-        else:
-            if torch.cuda.is_available():
-                self.device = "cuda:0"
-                # check GPU memory and select automatically the model
-                mem = torch.cuda.get_device_properties(self.device).total_memory / (
-                    1024**2
-                )
-                if mem < 5000:
-                    rec_model = "openai/whisper-base"
-                elif mem < 7000:
-                    rec_model = "openai/whisper-small"
-                elif mem < 12000:
-                    rec_model = "openai/whisper-medium"
+                    except Exception:
+                        # use remote model
+                        model = OVModelForCausalLM.from_pretrained(
+                            model_id, export=True, **_model_kwargs
+                        )
                 else:
-                    rec_model = "openai/whisper-large"
+                    model = AutoModelForCausalLM.from_pretrained(
+                        model_id, **_model_kwargs
+                    )
+            elif task in ("text2text-generation", "summarization", "translation"):
+                if backend == "openvino":
+                    try:
+                        from optimum.intel.openvino import OVModelForSeq2SeqLM
+
+                    except ImportError:
+                        raise ImportError(
+                            "Could not import optimum-intel python package. "
+                            "Please install it with: "
+                            "pip install 'optimum[openvino,nncf]' "
+                        )
+                    try:
+                        # use local model
+                        model = OVModelForSeq2SeqLM.from_pretrained(
+                            model_id, **_model_kwargs
+                        )
 
-                # check if model is overridden
-                if lang_model is not None:
-                    self.lang_model = lang_model
-                    print("WARNING! Model override. Might not fit in your GPU")
+                    except Exception:
+                        # use remote model
+                        model = OVModelForSeq2SeqLM.from_pretrained(
+                            model_id, export=True, **_model_kwargs
+                        )
                 else:
-                    self.lang_model = rec_model
+                    model = AutoModelForSeq2SeqLM.from_pretrained(
+                        model_id, **_model_kwargs
+                    )
             else:
-                "cpu"
-
-        print("Using the following model: ", self.lang_model)
-
-        # load model for inference
-        self.pipe = pipeline(
-            "automatic-speech-recognition",
-            model=self.lang_model,
-            chunk_length_s=30,
-            device=self.device,
-        )
-        if forced_decoder_ids is not None:
-            try:
-                self.pipe.model.config.forced_decoder_ids = forced_decoder_ids
-            except Exception as exception_text:
-                logger.info(
-                    "Unable to set forced_decoder_ids parameter for whisper model"
-                    f"Text of exception: {exception_text}"
-                    "Therefore whisper model will use default mode for decoder"
+                raise ValueError(
+                    f"Got invalid task {task}, "
+                    f"currently only {VALID_TASKS} are supported"
                 )
-
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
-        """Lazily parse the blob."""
-
-        import io
-
-        try:
-            from pydub import AudioSegment
-        except ImportError:
+        except ImportError as e:
             raise ImportError(
-                "pydub package not found, please install it with `pip install pydub`"
-            )
+                f"Could not load the {task} model due to missing dependencies."
+            ) from e
 
-        try:
-            import librosa
-        except ImportError:
-            raise ImportError(
-                "librosa package not found, please install it with "
-                "`pip install librosa`"
-            )
-
-        # Audio file from disk
-        audio = AudioSegment.from_file(blob.path)
-
-        file_obj = io.BytesIO(audio.export(format="mp3").read())
-
-        # Transcribe
-        print(f"Transcribing part {blob.path}!")
-
-        y, sr = librosa.load(file_obj, sr=16000)
+        if tokenizer.pad_token is None:
+            tokenizer.pad_token_id = model.config.eos_token_id
 
-        prediction = self.pipe(y.copy(), batch_size=8)["text"]
+        if (
+            (
+                getattr(model, "is_loaded_in_4bit", False)
+                or getattr(model, "is_loaded_in_8bit", False)
+            )
+            and device is not None
+            and backend == "default"
+        ):
+            logger.warning(
+                f"Setting the `device` argument to None from {device} to avoid "
+                "the error caused by attempting to move the model that was already "
+                "loaded on the GPU using the Accelerate module to the same or "
+                "another device."
+            )
+            device = None
+
+        if (
+            device is not None
+            and importlib.util.find_spec("torch") is not None
+            and backend == "default"
+        ):
+            import torch
 
-        yield Document(
-            page_content=prediction,
-            metadata={"source": blob.source},
+            cuda_device_count = torch.cuda.device_count()
+            if device < -1 or (device >= cuda_device_count):
+                raise ValueError(
+                    f"Got device=={device}, "
+                    f"device is required to be within [-1, {cuda_device_count})"
+                )
+            if device_map is not None and device < 0:
+                device = None
+            if device is not None and device < 0 and cuda_device_count > 0:
+                logger.warning(
+                    "Device has %d GPUs available. "
+                    "Provide device={deviceId} to `from_model_id` to use available"
+                    "GPUs for execution. deviceId is -1 (default) for CPU and "
+                    "can be a positive integer associated with CUDA device id.",
+                    cuda_device_count,
+                )
+        if device is not None and device_map is not None and backend == "openvino":
+            logger.warning("Please set device for OpenVINO through: `model_kwargs`")
+        if "trust_remote_code" in _model_kwargs:
+            _model_kwargs = {
+                k: v for k, v in _model_kwargs.items() if k != "trust_remote_code"
+            }
+        _pipeline_kwargs = pipeline_kwargs or {}
+        pipeline = hf_pipeline(
+            task=task,
+            model=model,
+            tokenizer=tokenizer,
+            device=device,
+            device_map=device_map,
+            batch_size=batch_size,
+            model_kwargs=_model_kwargs,
+            **_pipeline_kwargs,
         )
-
-
-class YandexSTTParser(BaseBlobParser):
-    """Transcribe and parse audio files.
-    Audio transcription is with OpenAI Whisper model."""
-
-    def __init__(
-        self,
-        *,
-        api_key: Optional[str] = None,
-        iam_token: Optional[str] = None,
-        model: str = "general",
-        language: str = "auto",
-    ):
-        """Initialize the parser.
-
-        Args:
-            api_key: API key for a service account
-            with the `ai.speechkit-stt.user` role.
-            iam_token: IAM token for a service account
-            with the `ai.speechkit-stt.user` role.
-            model: Recognition model name.
-              Defaults to general.
-            language: The language in ISO 639-1 format.
-              Defaults to automatic language recognition.
-        Either `api_key` or `iam_token` must be provided, but not both.
-        """
-        if (api_key is None) == (iam_token is None):
+        if pipeline.task not in VALID_TASKS:
             raise ValueError(
-                "Either 'api_key' or 'iam_token' must be provided, but not both."
+                f"Got invalid task {pipeline.task}, "
+                f"currently only {VALID_TASKS} are supported"
             )
-        self.api_key = api_key
-        self.iam_token = iam_token
-        self.model = model
-        self.language = language
-
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
-        """Lazily parse the blob."""
-
-        try:
-            from speechkit import configure_credentials, creds, model_repository
-            from speechkit.stt import AudioProcessingType
-        except ImportError:
-            raise ImportError(
-                "yandex-speechkit package not found, please install it with "
-                "`pip install yandex-speechkit`"
-            )
-        try:
-            from pydub import AudioSegment
-        except ImportError:
-            raise ImportError(
-                "pydub package not found, please install it with " "`pip install pydub`"
-            )
-
-        if self.api_key:
-            configure_credentials(
-                yandex_credentials=creds.YandexCredentials(api_key=self.api_key)
-            )
-        else:
-            configure_credentials(
-                yandex_credentials=creds.YandexCredentials(iam_token=self.iam_token)
-            )
-
-        audio = AudioSegment.from_file(blob.path)
+        return cls(
+            pipeline=pipeline,
+            model_id=model_id,
+            model_kwargs=_model_kwargs,
+            pipeline_kwargs=_pipeline_kwargs,
+            batch_size=batch_size,
+            **kwargs,
+        )
 
-        model = model_repository.recognition_model()
+    @property
+    def _identifying_params(self) -> Mapping[str, Any]:
+        """Get the identifying parameters."""
+        return {
+            "model_id": self.model_id,
+            "model_kwargs": self.model_kwargs,
+            "pipeline_kwargs": self.pipeline_kwargs,
+        }
+
+    @property
+    def _llm_type(self) -> str:
+        return "huggingface_pipeline"
 
-        model.model = self.model
-        model.language = self.language
-        model.audio_processing_type = AudioProcessingType.Full
+    def _generate(
+        self,
+        prompts: List[str],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> LLMResult:
+        # List to hold all results
+        text_generations: List[str] = []
+        pipeline_kwargs = kwargs.get("pipeline_kwargs", {})
+
+        for i in range(0, len(prompts), self.batch_size):
+            batch_prompts = prompts[i : i + self.batch_size]
+
+            # Process batch of prompts
+            responses = self.pipeline(
+                batch_prompts,
+                **pipeline_kwargs,
+            )
+
+            # Process each response in the batch
+            for j, response in enumerate(responses):
+                if isinstance(response, list):
+                    # if model returns multiple generations, pick the top one
+                    response = response[0]
+
+                if self.pipeline.task == "text-generation":
+                    text = response["generated_text"]
+                elif self.pipeline.task == "text2text-generation":
+                    text = response["generated_text"]
+                elif self.pipeline.task == "summarization":
+                    text = response["summary_text"]
+                elif self.pipeline.task in "translation":
+                    text = response["translation_text"]
+                else:
+                    raise ValueError(
+                        f"Got invalid task {self.pipeline.task}, "
+                        f"currently only {VALID_TASKS} are supported"
+                    )
 
-        result = model.transcribe(audio)
+                # Append the processed text to results
+                text_generations.append(text)
 
-        for res in result:
-            yield Document(
-                page_content=res.normalized_text,
-                metadata={"source": blob.source},
-            )
+        return LLMResult(
+            generations=[[Generation(text=text)] for text in text_generations]
+        )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/doc_intelligence.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/doc_intelligence.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,42 +1,66 @@
-from typing import Any, Iterator, Optional
+import logging
+from typing import Any, Iterator, List, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.blob_loaders import Blob
 
+logger = logging.getLogger(__name__)
+
 
 class AzureAIDocumentIntelligenceParser(BaseBlobParser):
     """Loads a PDF with Azure Document Intelligence
     (formerly Forms Recognizer)."""
 
     def __init__(
         self,
         api_endpoint: str,
         api_key: str,
         api_version: Optional[str] = None,
         api_model: str = "prebuilt-layout",
         mode: str = "markdown",
+        analysis_features: Optional[List[str]] = None,
     ):
         from azure.ai.documentintelligence import DocumentIntelligenceClient
+        from azure.ai.documentintelligence.models import DocumentAnalysisFeature
         from azure.core.credentials import AzureKeyCredential
 
         kwargs = {}
         if api_version is not None:
             kwargs["api_version"] = api_version
+
+        if analysis_features is not None:
+            _SUPPORTED_FEATURES = [
+                DocumentAnalysisFeature.OCR_HIGH_RESOLUTION,
+            ]
+
+            analysis_features = [
+                DocumentAnalysisFeature(feature) for feature in analysis_features
+            ]
+            if any(
+                [feature not in _SUPPORTED_FEATURES for feature in analysis_features]
+            ):
+                logger.warning(
+                    f"The current supported features are: "
+                    f"{[f.value for f in _SUPPORTED_FEATURES]}. "
+                    "Using other features may result in unexpected behavior."
+                )
+
         self.client = DocumentIntelligenceClient(
             endpoint=api_endpoint,
             credential=AzureKeyCredential(api_key),
             headers={"x-ms-useragent": "langchain-parser/1.0.0"},
+            features=analysis_features,
             **kwargs,
         )
         self.api_model = api_model
         self.mode = mode
-        assert self.mode in ["single", "page", "object", "markdown"]
+        assert self.mode in ["single", "page", "markdown"]
 
     def _generate_docs_page(self, result: Any) -> Iterator[Document]:
         for p in result.pages:
             content = " ".join([line.content for line in p.lines])
 
             d = Document(
                 page_content=content,
@@ -45,78 +69,43 @@
                 },
             )
             yield d
 
     def _generate_docs_single(self, result: Any) -> Iterator[Document]:
         yield Document(page_content=result.content, metadata={})
 
-    def _generate_docs_object(self, result: Any) -> Iterator[Document]:
-        # record relationship between page id and span offset
-        page_offset = []
-        for page in result.pages:
-            # assume that spans only contain 1 element, to double check
-            page_offset.append(page.spans[0]["offset"])
-
-        # paragraph
-        # warning: paragraph content is overlapping with table content
-        for para in result.paragraphs:
-            yield Document(
-                page_content=para.content,
-                metadata={
-                    "role": para.role,
-                    "page": para.bounding_regions[0].page_number,
-                    "bounding_box": para.bounding_regions[0].polygon,
-                    "type": "paragraph",
-                },
-            )
-
-        # table
-        for table in result.tables:
-            yield Document(
-                page_content=table.cells,  # json object
-                metadata={
-                    "footnote": table.footnotes,
-                    "caption": table.caption,
-                    "page": para.bounding_regions[0].page_number,
-                    "bounding_box": para.bounding_regions[0].polygon,
-                    "row_count": table.row_count,
-                    "column_count": table.column_count,
-                    "type": "table",
-                },
-            )
-
     def lazy_parse(self, blob: Blob) -> Iterator[Document]:
         """Lazily parse the blob."""
 
         with blob.as_bytes_io() as file_obj:
             poller = self.client.begin_analyze_document(
                 self.api_model,
                 file_obj,
                 content_type="application/octet-stream",
                 output_content_format="markdown" if self.mode == "markdown" else "text",
             )
             result = poller.result()
 
             if self.mode in ["single", "markdown"]:
                 yield from self._generate_docs_single(result)
-            elif self.mode == ["page"]:
+            elif self.mode in ["page"]:
                 yield from self._generate_docs_page(result)
             else:
-                yield from self._generate_docs_object(result)
+                raise ValueError(f"Invalid mode: {self.mode}")
 
     def parse_url(self, url: str) -> Iterator[Document]:
         from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
 
         poller = self.client.begin_analyze_document(
             self.api_model,
             AnalyzeDocumentRequest(url_source=url),
             # content_type="application/octet-stream",
             output_content_format="markdown" if self.mode == "markdown" else "text",
         )
         result = poller.result()
 
         if self.mode in ["single", "markdown"]:
             yield from self._generate_docs_single(result)
-        elif self.mode == ["page"]:
+        elif self.mode in ["page"]:
             yield from self._generate_docs_page(result)
         else:
-            yield from self._generate_docs_object(result)
+            raise ValueError(f"Invalid mode: {self.mode}")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/docai.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/docai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 """Module contains a PDF parser based on Document AI from Google Cloud.
 
 You need to install two libraries to use this parser:
 pip install google-cloud-documentai
 pip install google-cloud-documentai-toolbox
 """
+
 import logging
 import re
 import time
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Iterator, List, Optional, Sequence
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.utils.iter import batch_iterate
 
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.blob_loaders import Blob
 from langchain_community.utilities.vertexai import get_client_info
 
@@ -23,20 +25,25 @@
 
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class DocAIParsingResults:
-    """A dataclass to store Document AI parsing results."""
+    """Dataclass to store Document AI parsing results."""
 
     source_path: str
     parsed_path: str
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.DocAIParser",
+)
 class DocAIParser(BaseBlobParser):
     """`Google Cloud Document AI` parser.
 
     For a detailed explanation of Document AI, refer to the product documentation.
     https://cloud.google.com/document-ai/docs/overview
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/generic.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/generic.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """Code for generic / auxiliary parsers.
 
 This module contains some logic to help assemble more sophisticated parsers.
 """
+
 from typing import Iterator, Mapping, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.blob_loaders.schema import Blob
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/grobid.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/grobid.py`

 * *Files 3% similar despite different names*

```diff
@@ -55,27 +55,28 @@
             if sect is not None:
                 for i, paragraph in enumerate(section.find_all("p")):
                     chunk_bboxes = []
                     paragraph_text = []
                     for i, sentence in enumerate(paragraph.find_all("s")):
                         paragraph_text.append(sentence.text)
                         sbboxes = []
-                        for bbox in sentence.get("coords").split(";"):
-                            box = bbox.split(",")
-                            sbboxes.append(
-                                {
-                                    "page": box[0],
-                                    "x": box[1],
-                                    "y": box[2],
-                                    "h": box[3],
-                                    "w": box[4],
-                                }
-                            )
-                        chunk_bboxes.append(sbboxes)
-                        if segment_sentences is True:
+                        if sentence.get("coords") is not None:
+                            for bbox in sentence.get("coords").split(";"):
+                                box = bbox.split(",")
+                                sbboxes.append(
+                                    {
+                                        "page": box[0],
+                                        "x": box[1],
+                                        "y": box[2],
+                                        "h": box[3],
+                                        "w": box[4],
+                                    }
+                                )
+                            chunk_bboxes.append(sbboxes)
+                        if (segment_sentences is True) and (len(sbboxes) > 0):
                             fpage, lpage = sbboxes[0]["page"], sbboxes[-1]["page"]
                             sentence_dict = {
                                 "text": sentence.text,
                                 "para": str(i),
                                 "bboxes": [sbboxes],
                                 "section_title": sect.text,
                                 "section_number": sect.get("n"),
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/html/bs4.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/html/bs4.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.blob_loaders import Blob
 
 logger = logging.getLogger(__name__)
 
 
 class BS4HTMLParser(BaseBlobParser):
-    """Pparse HTML files using `Beautiful Soup`."""
+    """Parse HTML files using `Beautiful Soup`."""
 
     def __init__(
         self,
         *,
         features: str = "lxml",
         get_text_separator: str = "",
         **kwargs: Any,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/cobol.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/cobol.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/javascript.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/javascript.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, List
+from typing import Any, List, Tuple
 
 from langchain_community.document_loaders.parsers.language.code_segmenter import (
     CodeSegmenter,
 )
 
 
 class JavaScriptSegmenter(CodeSegmenter):
@@ -51,19 +51,22 @@
 
     def simplify_code(self) -> str:
         import esprima
 
         tree = esprima.parseScript(self.code, loc=True)
         simplified_lines = self.source_lines[:]
 
+        indices_to_del: List[Tuple[int, int]] = []
         for node in tree.body:
             if isinstance(
                 node,
                 (esprima.nodes.FunctionDeclaration, esprima.nodes.ClassDeclaration),
             ):
-                start = node.loc.start.line - 1
+                start, end = node.loc.start.line - 1, node.loc.end.line
                 simplified_lines[start] = f"// Code for: {simplified_lines[start]}"
 
-                for line_num in range(start + 1, node.loc.end.line):
-                    simplified_lines[line_num] = None  # type: ignore
+                indices_to_del.append((start + 1, end))
 
-        return "\n".join(line for line in simplified_lines if line is not None)
+        for start, end in reversed(indices_to_del):
+            del simplified_lines[start + 0 : end]
+
+        return "\n".join(line for line in simplified_lines)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/language/python.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/language/python.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import ast
-from typing import Any, List
+from typing import Any, List, Tuple
 
 from langchain_community.document_loaders.parsers.language.code_segmenter import (
     CodeSegmenter,
 )
 
 
 class PythonSegmenter(CodeSegmenter):
@@ -35,17 +35,19 @@
 
         return functions_classes
 
     def simplify_code(self) -> str:
         tree = ast.parse(self.code)
         simplified_lines = self.source_lines[:]
 
+        indices_to_del: List[Tuple[int, int]] = []
         for node in ast.iter_child_nodes(tree):
             if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
-                start = node.lineno - 1
+                start, end = node.lineno - 1, node.end_lineno
                 simplified_lines[start] = f"# Code for: {simplified_lines[start]}"
+                assert isinstance(end, int)
+                indices_to_del.append((start + 1, end))
 
-                assert isinstance(node.end_lineno, int)
-                for line_num in range(start + 1, node.end_lineno):
-                    simplified_lines[line_num] = None  # type: ignore
+        for start, end in reversed(indices_to_del):
+            del simplified_lines[start + 0 : end]
 
-        return "\n".join(line for line in simplified_lines if line is not None)
+        return "\n".join(simplified_lines)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/msword.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/msword.py`

 * *Files 18% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.blob_loaders import Blob
 
 
 class MsWordParser(BaseBlobParser):
     """Parse the Microsoft Word documents from a blob."""
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Parse a Microsoft Word document into the Document iterator.
 
         Args:
             blob: The blob to parse.
 
         Returns: An iterator of Documents.
 
@@ -29,17 +29,17 @@
 
         mime_type_parser = {
             "application/msword": partition_doc,
             "application/vnd.openxmlformats-officedocument.wordprocessingml.document": (
                 partition_docx
             ),
         }
-        if blob.mimetype not in (
+        if blob.mimetype not in (  # type: ignore[attr-defined]
             "application/msword",
             "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
         ):
             raise ValueError("This blob type is not supported for this parser.")
-        with blob.as_bytes_io() as word_document:
-            elements = mime_type_parser[blob.mimetype](file=word_document)
+        with blob.as_bytes_io() as word_document:  # type: ignore[attr-defined]
+            elements = mime_type_parser[blob.mimetype](file=word_document)  # type: ignore[attr-defined]
             text = "\n\n".join([str(el) for el in elements])
-            metadata = {"source": blob.source}
+            metadata = {"source": blob.source}  # type: ignore[attr-defined]
             yield Document(page_content=text, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/pdf.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/pdf.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Module contains common parsers for PDFs."""
+
 from __future__ import annotations
 
 import warnings
 from typing import (
     TYPE_CHECKING,
     Any,
     Iterable,
@@ -22,14 +23,15 @@
 
 if TYPE_CHECKING:
     import fitz.fitz
     import pdfminer.layout
     import pdfplumber.page
     import pypdf._page
     import pypdfium2._helpers.page
+    from textractor.data.text_linearization_config import TextLinearizationConfig
 
 
 _PDF_FILTER_WITH_LOSS = ["DCTDecode", "DCT", "JPXDecode"]
 _PDF_FILTER_WITHOUT_LOSS = [
     "LZWDecode",
     "LZW",
     "FlateDecode",
@@ -82,25 +84,25 @@
 
     def __init__(
         self, password: Optional[Union[str, bytes]] = None, extract_images: bool = False
     ):
         self.password = password
         self.extract_images = extract_images
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
         import pypdf
 
-        with blob.as_bytes_io() as pdf_file_obj:
+        with blob.as_bytes_io() as pdf_file_obj:  # type: ignore[attr-defined]
             pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)
             yield from [
                 Document(
                     page_content=page.extract_text()
                     + self._extract_images_from_page(page),
-                    metadata={"source": blob.source, "page": page_number},
+                    metadata={"source": blob.source, "page": page_number},  # type: ignore[attr-defined]
                 )
                 for page_number, page in enumerate(pdf_reader.pages)
             ]
 
     def _extract_images_from_page(self, page: pypdf._page.PageObject) -> str:
         """Extract images from page and get the text with RapidOCR."""
         if not self.extract_images or "/XObject" not in page["/Resources"].keys():
@@ -135,58 +137,58 @@
             extract_images: Whether to extract images from PDF.
             concatenate_pages: If True, concatenate all PDF pages into one a single
                                document. Otherwise, return one document per page.
         """
         self.extract_images = extract_images
         self.concatenate_pages = concatenate_pages
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
 
         if not self.extract_images:
             from pdfminer.high_level import extract_text
 
-            with blob.as_bytes_io() as pdf_file_obj:
+            with blob.as_bytes_io() as pdf_file_obj:  # type: ignore[attr-defined]
                 if self.concatenate_pages:
                     text = extract_text(pdf_file_obj)
-                    metadata = {"source": blob.source}
+                    metadata = {"source": blob.source}  # type: ignore[attr-defined]
                     yield Document(page_content=text, metadata=metadata)
                 else:
                     from pdfminer.pdfpage import PDFPage
 
                     pages = PDFPage.get_pages(pdf_file_obj)
                     for i, _ in enumerate(pages):
                         text = extract_text(pdf_file_obj, page_numbers=[i])
-                        metadata = {"source": blob.source, "page": str(i)}
+                        metadata = {"source": blob.source, "page": str(i)}  # type: ignore[attr-defined]
                         yield Document(page_content=text, metadata=metadata)
         else:
             import io
 
             from pdfminer.converter import PDFPageAggregator, TextConverter
             from pdfminer.layout import LAParams
             from pdfminer.pdfinterp import PDFPageInterpreter, PDFResourceManager
             from pdfminer.pdfpage import PDFPage
 
             text_io = io.StringIO()
-            with blob.as_bytes_io() as pdf_file_obj:
+            with blob.as_bytes_io() as pdf_file_obj:  # type: ignore[attr-defined]
                 pages = PDFPage.get_pages(pdf_file_obj)
                 rsrcmgr = PDFResourceManager()
                 device_for_text = TextConverter(rsrcmgr, text_io, laparams=LAParams())
                 device_for_image = PDFPageAggregator(rsrcmgr, laparams=LAParams())
                 interpreter_for_text = PDFPageInterpreter(rsrcmgr, device_for_text)
                 interpreter_for_image = PDFPageInterpreter(rsrcmgr, device_for_image)
                 for i, page in enumerate(pages):
                     interpreter_for_text.process_page(page)
                     interpreter_for_image.process_page(page)
                     content = text_io.getvalue() + self._extract_images_from_page(
                         device_for_image.get_result()
                     )
                     text_io.truncate(0)
                     text_io.seek(0)
-                    metadata = {"source": blob.source, "page": str(i)}
+                    metadata = {"source": blob.source, "page": str(i)}  # type: ignore[attr-defined]
                     yield Document(page_content=content, metadata=metadata)
 
     def _extract_images_from_page(self, page: pdfminer.layout.LTPage) -> str:
         """Extract images from page and get the text with RapidOCR."""
         import pdfminer
 
         def get_image(layout_object: Any) -> Any:
@@ -226,32 +228,32 @@
 
         Args:
             text_kwargs: Keyword arguments to pass to ``fitz.Page.get_text()``.
         """
         self.text_kwargs = text_kwargs or {}
         self.extract_images = extract_images
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
         import fitz
 
-        with blob.as_bytes_io() as file_path:
-            if blob.data is None:
+        with blob.as_bytes_io() as file_path:  # type: ignore[attr-defined]
+            if blob.data is None:  # type: ignore[attr-defined]
                 doc = fitz.open(file_path)
             else:
                 doc = fitz.open(stream=file_path, filetype="pdf")
 
             yield from [
                 Document(
                     page_content=page.get_text(**self.text_kwargs)
                     + self._extract_images_from_page(doc, page),
                     metadata=dict(
                         {
-                            "source": blob.source,
-                            "file_path": blob.source,
+                            "source": blob.source,  # type: ignore[attr-defined]
+                            "file_path": blob.source,  # type: ignore[attr-defined]
                             "page": page.number,
                             "total_pages": len(doc),
                         },
                         **{
                             k: doc.metadata[k]
                             for k in doc.metadata
                             if type(doc.metadata[k]) in [str, int]
@@ -292,30 +294,30 @@
         except ImportError:
             raise ImportError(
                 "pypdfium2 package not found, please install it with"
                 " `pip install pypdfium2`"
             )
         self.extract_images = extract_images
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
         import pypdfium2
 
         # pypdfium2 is really finicky with respect to closing things,
         # if done incorrectly creates seg faults.
-        with blob.as_bytes_io() as file_path:
+        with blob.as_bytes_io() as file_path:  # type: ignore[attr-defined]
             pdf_reader = pypdfium2.PdfDocument(file_path, autoclose=True)
             try:
                 for page_number, page in enumerate(pdf_reader):
                     text_page = page.get_textpage()
                     content = text_page.get_text_range()
                     text_page.close()
                     content += "\n" + self._extract_images_from_page(page)
                     page.close()
-                    metadata = {"source": blob.source, "page": page_number}
+                    metadata = {"source": blob.source, "page": page_number}  # type: ignore[attr-defined]
                     yield Document(page_content=content, metadata=metadata)
             finally:
                 pdf_reader.close()
 
     def _extract_images_from_page(self, page: pypdfium2._helpers.page.PdfPage) -> str:
         """Extract images from page and get the text with RapidOCR."""
         if not self.extract_images:
@@ -344,30 +346,30 @@
             text_kwargs: Keyword arguments to pass to ``pdfplumber.Page.extract_text()``
             dedupe: Avoiding the error of duplicate characters if `dedupe=True`.
         """
         self.text_kwargs = text_kwargs or {}
         self.dedupe = dedupe
         self.extract_images = extract_images
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
         import pdfplumber
 
-        with blob.as_bytes_io() as file_path:
+        with blob.as_bytes_io() as file_path:  # type: ignore[attr-defined]
             doc = pdfplumber.open(file_path)  # open document
 
             yield from [
                 Document(
                     page_content=self._process_page_content(page)
                     + "\n"
                     + self._extract_images_from_page(page),
                     metadata=dict(
                         {
-                            "source": blob.source,
-                            "file_path": blob.source,
+                            "source": blob.source,  # type: ignore[attr-defined]
+                            "file_path": blob.source,  # type: ignore[attr-defined]
                             "page": page.page_number - 1,
                             "total_pages": len(doc.pages),
                         },
                         **{
                             k: doc.metadata[k]
                             for k in doc.metadata
                             if type(doc.metadata[k]) in [str, int]
@@ -450,22 +452,27 @@
 
     """
 
     def __init__(
         self,
         textract_features: Optional[Sequence[int]] = None,
         client: Optional[Any] = None,
+        *,
+        linearization_config: Optional["TextLinearizationConfig"] = None,
     ) -> None:
         """Initializes the parser.
 
         Args:
             textract_features: Features to be used for extraction, each feature
                                should be passed as an int that conforms to the enum
                                `Textract_Features`, see `amazon-textract-caller` pkg
             client: boto3 textract client
+            linearization_config: Config to be used for linearization of the output
+                                  should be an instance of TextLinearizationConfig from
+                                  the `textractor` pkg
         """
 
         try:
             import textractcaller as tc
             import textractor.entities.document as textractor
 
             self.tc = tc
@@ -473,14 +480,24 @@
 
             if textract_features is not None:
                 self.textract_features = [
                     tc.Textract_Features(f) for f in textract_features
                 ]
             else:
                 self.textract_features = []
+
+            if linearization_config is not None:
+                self.linearization_config = linearization_config
+            else:
+                self.linearization_config = self.textractor.TextLinearizationConfig(
+                    hide_figure_layout=True,
+                    title_prefix="# ",
+                    section_header_prefix="## ",
+                    list_element_prefix="*",
+                )
         except ImportError:
             raise ImportError(
                 "Could not import amazon-textract-caller or "
                 "amazon-textract-textractor python package. Please install it "
                 "with `pip install amazon-textract-caller` & "
                 "`pip install amazon-textract-textractor`."
             )
@@ -494,88 +511,82 @@
                 raise ImportError(
                     "Could not import boto3 python package. "
                     "Please install it with `pip install boto3`."
                 )
         else:
             self.boto3_textract_client = client
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Iterates over the Blob pages and returns an Iterator with a Document
         for each page, like the other parsers If multi-page document, blob.path
         has to be set to the S3 URI and for single page docs
         the blob.data is taken
         """
 
-        url_parse_result = urlparse(str(blob.path)) if blob.path else None
+        url_parse_result = urlparse(str(blob.path)) if blob.path else None  # type: ignore[attr-defined]
         # Either call with S3 path (multi-page) or with bytes (single-page)
         if (
             url_parse_result
             and url_parse_result.scheme == "s3"
             and url_parse_result.netloc
         ):
             textract_response_json = self.tc.call_textract(
-                input_document=str(blob.path),
+                input_document=str(blob.path),  # type: ignore[attr-defined]
                 features=self.textract_features,
                 boto3_textract_client=self.boto3_textract_client,
             )
         else:
             textract_response_json = self.tc.call_textract(
-                input_document=blob.as_bytes(),
+                input_document=blob.as_bytes(),  # type: ignore[attr-defined]
                 features=self.textract_features,
                 call_mode=self.tc.Textract_Call_Mode.FORCE_SYNC,
                 boto3_textract_client=self.boto3_textract_client,
             )
 
         document = self.textractor.Document.open(textract_response_json)
 
-        linearizer_config = self.textractor.TextLinearizationConfig(
-            hide_figure_layout=True,
-            title_prefix="# ",
-            section_header_prefix="## ",
-            list_element_prefix="*",
-        )
         for idx, page in enumerate(document.pages):
             yield Document(
-                page_content=page.get_text(config=linearizer_config),
-                metadata={"source": blob.source, "page": idx + 1},
+                page_content=page.get_text(config=self.linearization_config),
+                metadata={"source": blob.source, "page": idx + 1},  # type: ignore[attr-defined]
             )
 
 
 class DocumentIntelligenceParser(BaseBlobParser):
     """Loads a PDF with Azure Document Intelligence
     (formerly Form Recognizer) and chunks at character level."""
 
     def __init__(self, client: Any, model: str):
         warnings.warn(
-            "langchain.document_loaders.parsers.pdf.DocumentIntelligenceParser"
-            "and langchain.document_loaders.pdf.DocumentIntelligenceLoader"
+            "langchain_community.document_loaders.parsers.pdf.DocumentIntelligenceParser"
+            "and langchain_community.document_loaders.pdf.DocumentIntelligenceLoader"
             " are deprecated. Please upgrade to "
-            "langchain.document_loaders.DocumentIntelligenceLoader "
+            "langchain_community.document_loaders.DocumentIntelligenceLoader "
             "for any file parsing purpose using Azure Document Intelligence "
             "service."
         )
         self.client = client
         self.model = model
 
-    def _generate_docs(self, blob: Blob, result: Any) -> Iterator[Document]:
+    def _generate_docs(self, blob: Blob, result: Any) -> Iterator[Document]:  # type: ignore[valid-type]
         for p in result.pages:
             content = " ".join([line.content for line in p.lines])
 
             d = Document(
                 page_content=content,
                 metadata={
-                    "source": blob.source,
+                    "source": blob.source,  # type: ignore[attr-defined]
                     "page": p.page_number,
                 },
             )
             yield d
 
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
+    def lazy_parse(self, blob: Blob) -> Iterator[Document]:  # type: ignore[valid-type]
         """Lazily parse the blob."""
 
-        with blob.as_bytes_io() as file_obj:
+        with blob.as_bytes_io() as file_obj:  # type: ignore[attr-defined]
             poller = self.client.begin_analyze_document(self.model, file_obj)
             result = poller.result()
 
             docs = self._generate_docs(blob, result)
 
             yield from docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/parsers/registry.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/parsers/registry.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Module includes a registry of default parser configurations."""
+
 from langchain_community.document_loaders.base import BaseBlobParser
 from langchain_community.document_loaders.parsers.generic import MimeTypeBasedParser
 from langchain_community.document_loaders.parsers.msword import MsWordParser
 from langchain_community.document_loaders.parsers.pdf import PyMuPDFParser
 from langchain_community.document_loaders.parsers.txt import TextParser
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/pdf.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/pdf.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,16 +1,27 @@
 import json
 import logging
 import os
+import re
 import tempfile
 import time
 from abc import ABC
 from io import StringIO
 from pathlib import Path
-from typing import Any, Dict, Iterator, List, Mapping, Optional, Sequence, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Iterator,
+    List,
+    Mapping,
+    Optional,
+    Sequence,
+    Union,
+)
 from urllib.parse import urlparse
 
 import requests
 from langchain_core.documents import Document
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.document_loaders.base import BaseLoader
@@ -22,14 +33,17 @@
     PDFPlumberParser,
     PyMuPDFParser,
     PyPDFium2Parser,
     PyPDFParser,
 )
 from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
 
+if TYPE_CHECKING:
+    from textractor.data.text_linearization_config import TextLinearizationConfig
+
 logger = logging.getLogger(__file__)
 
 
 class UnstructuredPDFLoader(UnstructuredFileLoader):
     """Load `PDF` files using `Unstructured`.
 
     You can run the loader in one of two modes: "single" and "elements".
@@ -62,31 +76,33 @@
 class BasePDFLoader(BaseLoader, ABC):
     """Base Loader class for `PDF` files.
 
     If the file is a web path, it will download it to a temporary file, use it, then
         clean up the temporary file after completion.
     """
 
-    def __init__(self, file_path: str, *, headers: Optional[Dict] = None):
+    def __init__(self, file_path: Union[str, Path], *, headers: Optional[Dict] = None):
         """Initialize with a file path.
 
         Args:
             file_path: Either a local, S3 or web path to a PDF file.
             headers: Headers to use for GET request to download a file from a web path.
         """
-        self.file_path = file_path
+        self.file_path = str(file_path)
         self.web_path = None
         self.headers = headers
         if "~" in self.file_path:
             self.file_path = os.path.expanduser(self.file_path)
 
         # If the file is a web path or S3, download it to a temporary file, and use that
         if not os.path.isfile(self.file_path) and self._is_valid_url(self.file_path):
             self.temp_dir = tempfile.TemporaryDirectory()
             _, suffix = os.path.splitext(self.file_path)
+            if self._is_s3_presigned_url(self.file_path):
+                suffix = urlparse(self.file_path).path.split("/")[-1]
             temp_pdf = os.path.join(self.temp_dir.name, f"tmp{suffix}")
             self.web_path = self.file_path
             if not self._is_s3_url(self.file_path):
                 r = requests.get(self.file_path, headers=self.headers)
                 if r.status_code != 200:
                     raise ValueError(
                         "Check the url of your file; returned status code %s"
@@ -116,14 +132,23 @@
             result = urlparse(url)
             if result.scheme == "s3" and result.netloc:
                 return True
             return False
         except ValueError:
             return False
 
+    @staticmethod
+    def _is_s3_presigned_url(url: str) -> bool:
+        """Check if the url is a presigned S3 url."""
+        try:
+            result = urlparse(url)
+            return bool(re.search(r"\.s3\.amazonaws\.com$", result.netloc))
+        except ValueError:
+            return False
+
     @property
     def source(self) -> str:
         return self.web_path if self.web_path is not None else self.file_path
 
 
 class OnlinePDFLoader(BasePDFLoader):
     """Load online `PDF`."""
@@ -153,26 +178,22 @@
         except ImportError:
             raise ImportError(
                 "pypdf package not found, please install it with " "`pip install pypdf`"
             )
         super().__init__(file_path, headers=headers)
         self.parser = PyPDFParser(password=password, extract_images=extract_images)
 
-    def load(self) -> List[Document]:
-        """Load given path as pages."""
-        return list(self.lazy_load())
-
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazy load given path as pages."""
         if self.web_path:
-            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)
+            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)  # type: ignore[attr-defined]
         else:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
         yield from self.parser.parse(blob)
 
 
 class PyPDFium2Loader(BasePDFLoader):
     """Load `PDF` using `pypdfium2` and chunks at character level."""
 
     def __init__(
@@ -182,38 +203,34 @@
         headers: Optional[Dict] = None,
         extract_images: bool = False,
     ):
         """Initialize with a file path."""
         super().__init__(file_path, headers=headers)
         self.parser = PyPDFium2Parser(extract_images=extract_images)
 
-    def load(self) -> List[Document]:
-        """Load given path as pages."""
-        return list(self.lazy_load())
-
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazy load given path as pages."""
         if self.web_path:
-            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)
+            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)  # type: ignore[attr-defined]
         else:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
         yield from self.parser.parse(blob)
 
 
 class PyPDFDirectoryLoader(BaseLoader):
     """Load a directory with `PDF` files using `pypdf` and chunks at character level.
 
     Loader also stores page numbers in metadata.
     """
 
     def __init__(
         self,
-        path: str,
+        path: Union[str, Path],
         glob: str = "**/[!.]*.pdf",
         silent_errors: bool = False,
         load_hidden: bool = False,
         recursive: bool = False,
         extract_images: bool = False,
     ):
         self.path = path
@@ -275,26 +292,22 @@
             )
 
         super().__init__(file_path, headers=headers)
         self.parser = PDFMinerParser(
             extract_images=extract_images, concatenate_pages=concatenate_pages
         )
 
-    def load(self) -> List[Document]:
-        """Eagerly load the content."""
-        return list(self.lazy_load())
-
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazily load documents."""
         if self.web_path:
-            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)
+            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)  # type: ignore[attr-defined]
         else:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
         yield from self.parser.parse(blob)
 
 
 class PDFMinerPDFasHTMLLoader(BasePDFLoader):
     """Load `PDF` files as HTML content using `PDFMiner`."""
 
     def __init__(self, file_path: str, *, headers: Optional[Dict] = None):
@@ -305,33 +318,33 @@
             raise ImportError(
                 "`pdfminer` package not found, please install it with "
                 "`pip install pdfminer.six`"
             )
 
         super().__init__(file_path, headers=headers)
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load file."""
         from pdfminer.high_level import extract_text_to_fp
         from pdfminer.layout import LAParams
         from pdfminer.utils import open_filename
 
         output_string = StringIO()
         with open_filename(self.file_path, "rb") as fp:
             extract_text_to_fp(
-                fp,  # type: ignore[arg-type]
+                fp,
                 output_string,
                 codec="",
                 laparams=LAParams(),
                 output_type="html",
             )
         metadata = {
             "source": self.file_path if self.web_path is None else self.web_path
         }
-        return [Document(page_content=output_string.getvalue(), metadata=metadata)]
+        yield Document(page_content=output_string.getvalue(), metadata=metadata)
 
 
 class PyMuPDFLoader(BasePDFLoader):
     """Load `PDF` files using `PyMuPDF`."""
 
     def __init__(
         self,
@@ -349,31 +362,36 @@
                 "`PyMuPDF` package not found, please install it with "
                 "`pip install pymupdf`"
             )
         super().__init__(file_path, headers=headers)
         self.extract_images = extract_images
         self.text_kwargs = kwargs
 
-    def load(self, **kwargs: Any) -> List[Document]:
-        """Load file."""
+    def _lazy_load(self, **kwargs: Any) -> Iterator[Document]:
         if kwargs:
             logger.warning(
                 f"Received runtime arguments {kwargs}. Passing runtime args to `load`"
                 f" is deprecated. Please pass arguments during initialization instead."
             )
 
         text_kwargs = {**self.text_kwargs, **kwargs}
         parser = PyMuPDFParser(
             text_kwargs=text_kwargs, extract_images=self.extract_images
         )
         if self.web_path:
-            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)
+            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)  # type: ignore[attr-defined]
         else:
-            blob = Blob.from_path(self.file_path)
-        return parser.parse(blob)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
+        yield from parser.lazy_parse(blob)
+
+    def load(self, **kwargs: Any) -> List[Document]:
+        return list(self._lazy_load(**kwargs))
+
+    def lazy_load(self) -> Iterator[Document]:
+        yield from self._lazy_load()
 
 
 # MathpixPDFLoader implementation taken largely from Daniel Gross's:
 # https://gist.github.com/danielgross/3ab4104e14faccc12b49200843adab21
 class MathpixPDFLoader(BasePDFLoader):
     """Load `PDF` files using `Mathpix` service."""
 
@@ -458,27 +476,33 @@
         url = self.url + "/" + pdf_id
         for _ in range(0, self.max_wait_time_seconds, 5):
             response = requests.get(url, headers=self._mathpix_headers)
             response_data = response.json()
 
             # This indicates an error with the request (e.g. auth problems)
             error = response_data.get("error", None)
+            error_info = response_data.get("error_info", None)
 
             if error is not None:
-                raise ValueError(f"Unable to retrieve PDF from Mathpix: {error}")
+                error_msg = f"Unable to retrieve PDF from Mathpix: {error}"
+
+                if error_info is not None:
+                    error_msg += f" ({error_info['id']})"
+
+                raise ValueError(error_msg)
 
             status = response_data.get("status", None)
 
             if status == "completed":
                 return
             elif status == "error":
                 # This indicates an error with the PDF processing
                 raise ValueError("Unable to retrieve PDF from Mathpix")
             else:
-                print(f"Status: {status}, waiting for processing to complete")
+                print(f"Status: {status}, waiting for processing to complete")  # noqa: T201
                 time.sleep(5)
         raise TimeoutError
 
     def get_processed_pdf(self, pdf_id: str) -> str:
         self.wait_for_processing(pdf_id)
         url = f"{self.url}/{pdf_id}.{self.processed_file_format}"
         response = requests.get(url, headers=self._mathpix_headers)
@@ -508,15 +532,15 @@
         return contents
 
     def load(self) -> List[Document]:
         pdf_id = self.send_pdf()
         contents = self.get_processed_pdf(pdf_id)
         if self.should_clean_pdf:
             contents = self.clean_pdf(contents)
-        metadata = {"source": self.source, "file_path": self.source}
+        metadata = {"source": self.source, "file_path": self.source, "pdf_id": pdf_id}
         return [Document(page_content=contents, metadata=metadata)]
 
 
 class PDFPlumberLoader(BasePDFLoader):
     """Load `PDF` files using `pdfplumber`."""
 
     def __init__(
@@ -546,17 +570,17 @@
 
         parser = PDFPlumberParser(
             text_kwargs=self.text_kwargs,
             dedupe=self.dedupe,
             extract_images=self.extract_images,
         )
         if self.web_path:
-            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)
+            blob = Blob.from_data(open(self.file_path, "rb").read(), path=self.web_path)  # type: ignore[attr-defined]
         else:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
         return parser.parse(blob)
 
 
 class AmazonTextractPDFLoader(BasePDFLoader):
     """Load `PDF` files from a local file system, HTTP or S3.
 
     To authenticate, the AWS client uses the following methods to
@@ -583,34 +607,38 @@
         file_path: str,
         textract_features: Optional[Sequence[str]] = None,
         client: Optional[Any] = None,
         credentials_profile_name: Optional[str] = None,
         region_name: Optional[str] = None,
         endpoint_url: Optional[str] = None,
         headers: Optional[Dict] = None,
+        *,
+        linearization_config: Optional["TextLinearizationConfig"] = None,
     ) -> None:
         """Initialize the loader.
 
         Args:
             file_path: A file, url or s3 path for input file
             textract_features: Features to be used for extraction, each feature
                                should be passed as a str that conforms to the enum
                                `Textract_Features`, see `amazon-textract-caller` pkg
             client: boto3 textract client (Optional)
             credentials_profile_name: AWS profile name, if not default (Optional)
             region_name: AWS region, eg us-east-1 (Optional)
             endpoint_url: endpoint url for the textract service (Optional)
-
+            linearization_config: Config to be used for linearization of the output
+                                  should be an instance of TextLinearizationConfig from
+                                  the `textractor` pkg
         """
         super().__init__(file_path, headers=headers)
 
         try:
-            import textractcaller as tc  # noqa: F401
+            import textractcaller as tc
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import amazon-textract-caller python package. "
                 "Please install it with `pip install amazon-textract-caller`."
             )
         if textract_features:
             features = [tc.Textract_Features[x] for x in textract_features]
         else:
             features = []
@@ -630,80 +658,84 @@
                     client_params["region_name"] = region_name
                 if endpoint_url:
                     client_params["endpoint_url"] = endpoint_url
 
                 client = session.client("textract", **client_params)
 
             except ImportError:
-                raise ModuleNotFoundError(
+                raise ImportError(
                     "Could not import boto3 python package. "
                     "Please install it with `pip install boto3`."
                 )
             except Exception as e:
                 raise ValueError(
                     "Could not load credentials to authenticate with AWS client. "
                     "Please check that credentials in the specified "
-                    "profile name are valid."
+                    f"profile name are valid. {e}"
                 ) from e
-        self.parser = AmazonTextractPDFParser(textract_features=features, client=client)
+        self.parser = AmazonTextractPDFParser(
+            textract_features=features,
+            client=client,
+            linearization_config=linearization_config,
+        )
 
     def load(self) -> List[Document]:
         """Load given path as pages."""
         return list(self.lazy_load())
 
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazy load documents"""
         # the self.file_path is local, but the blob has to include
         # the S3 location if the file originated from S3 for multi-page documents
         # raises ValueError when multi-page and not on S3"""
 
         if self.web_path and self._is_s3_url(self.web_path):
-            blob = Blob(path=self.web_path)
+            blob = Blob(path=self.web_path)  # type: ignore[call-arg] # type: ignore[misc]
         else:
-            blob = Blob.from_path(self.file_path)
+            blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
             if AmazonTextractPDFLoader._get_number_of_pages(blob) > 1:
                 raise ValueError(
                     f"the file {blob.path} is a multi-page document, \
                     but not stored on S3. \
                     Textract requires multi-page documents to be on S3."
                 )
 
         yield from self.parser.parse(blob)
 
     @staticmethod
-    def _get_number_of_pages(blob: Blob) -> int:
+    def _get_number_of_pages(blob: Blob) -> int:  # type: ignore[valid-type]
         try:
             import pypdf
             from PIL import Image, ImageSequence
 
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import pypdf or Pilloe python package. "
                 "Please install it with `pip install pypdf Pillow`."
             )
-        if blob.mimetype == "application/pdf":
-            with blob.as_bytes_io() as input_pdf_file:
+        if blob.mimetype == "application/pdf":  # type: ignore[attr-defined]
+            with blob.as_bytes_io() as input_pdf_file:  # type: ignore[attr-defined]
                 pdf_reader = pypdf.PdfReader(input_pdf_file)
                 return len(pdf_reader.pages)
-        elif blob.mimetype == "image/tiff":
+        elif blob.mimetype == "image/tiff":  # type: ignore[attr-defined]
             num_pages = 0
-            img = Image.open(blob.as_bytes())
+            img = Image.open(blob.as_bytes())  # type: ignore[attr-defined]
             for _, _ in enumerate(ImageSequence.Iterator(img)):
                 num_pages += 1
             return num_pages
-        elif blob.mimetype in ["image/png", "image/jpeg"]:
+        elif blob.mimetype in ["image/png", "image/jpeg"]:  # type: ignore[attr-defined]
             return 1
         else:
-            raise ValueError(f"unsupported mime type: {blob.mimetype}")
+            raise ValueError(f"unsupported mime type: {blob.mimetype}")  # type: ignore[attr-defined]
 
 
 class DocumentIntelligenceLoader(BasePDFLoader):
-    """Loads a PDF with Azure Document Intelligence"""
+    """Load a PDF with Azure Document Intelligence"""
 
     def __init__(
         self,
         file_path: str,
         client: Any,
         model: str = "prebuilt-document",
         headers: Optional[Dict] = None,
@@ -742,9 +774,13 @@
         """Load given path as pages."""
         return list(self.lazy_load())
 
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazy load given path as pages."""
-        blob = Blob.from_path(self.file_path)
+        blob = Blob.from_path(self.file_path)  # type: ignore[attr-defined]
         yield from self.parser.parse(blob)
+
+
+# Legacy: only for backwards compatibility. Use PyPDFLoader instead
+PagedPDFSplitter = PyPDFLoader
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/polars_dataframe.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/polars_dataframe.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/powerpoint.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/powerpoint.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/psychic.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/psychic.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import List, Optional
+from typing import Iterator, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class PsychicLoader(BaseLoader):
@@ -16,29 +16,25 @@
         Args:
             api_key: The Psychic API key.
             account_id: The Psychic account id.
             connector_id: The Psychic connector id.
         """
 
         try:
-            from psychicapi import ConnectorId, Psychic  # noqa: F401
+            from psychicapi import ConnectorId, Psychic
         except ImportError:
             raise ImportError(
                 "`psychicapi` package not found, please run `pip install psychicapi`"
             )
         self.psychic = Psychic(secret_key=api_key)
         self.connector_id = ConnectorId(connector_id)
         self.account_id = account_id
 
-    def load(self) -> List[Document]:
-        """Load documents."""
-
+    def lazy_load(self) -> Iterator[Document]:
         psychic_docs = self.psychic.get_documents(
             connector_id=self.connector_id, account_id=self.account_id
         )
-        return [
-            Document(
+        for doc in psychic_docs.documents:
+            yield Document(
                 page_content=doc["content"],
                 metadata={"title": doc["title"], "source": doc["uri"]},
             )
-            for doc in psychic_docs.documents
-        ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/pyspark_dataframe.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/pyspark_dataframe.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/quip.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/quip.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/readthedocs.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/readthedocs.py`

 * *Files 2% similar despite different names*

```diff
@@ -80,18 +80,14 @@
             for p in self.file_path.rglob(file_pattern):
                 if p.is_dir():
                     continue
                 with open(p, encoding=self.encoding, errors=self.errors) as f:
                     text = self._clean_data(f.read())
                 yield Document(page_content=text, metadata={"source": str(p)})
 
-    def load(self) -> List[Document]:
-        """Load documents."""
-        return list(self.lazy_load())
-
     def _clean_data(self, data: str) -> str:
         from bs4 import BeautifulSoup
 
         soup = BeautifulSoup(data, "html.parser", **self.bs_kwargs)
 
         # default tags
         html_tags = [
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/recursive_url_loader.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/recursive_url_loader.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,38 +1,40 @@
 from __future__ import annotations
 
 import asyncio
+import inspect
 import logging
 import re
 from typing import (
-    TYPE_CHECKING,
     Callable,
     Iterator,
     List,
     Optional,
     Sequence,
     Set,
     Union,
+    cast,
 )
 
+import aiohttp
 import requests
 from langchain_core.documents import Document
 from langchain_core.utils.html import extract_sub_links
 
 from langchain_community.document_loaders.base import BaseLoader
 
-if TYPE_CHECKING:
-    import aiohttp
-
 logger = logging.getLogger(__name__)
 
 
-def _metadata_extractor(raw_html: str, url: str) -> dict:
+def _metadata_extractor(
+    raw_html: str, url: str, response: Union[requests.Response, aiohttp.ClientResponse]
+) -> dict:
     """Extract metadata from raw html using BeautifulSoup."""
-    metadata = {"source": url}
+    content_type = getattr(response, "headers").get("Content-Type", "")
+    metadata = {"source": url, "content_type": content_type}
 
     try:
         from bs4 import BeautifulSoup
     except ImportError:
         logger.warning(
             "The bs4 package is required for default metadata extraction. "
             "Please install it with `pip install bs4`."
@@ -82,70 +84,100 @@
 
     def __init__(
         self,
         url: str,
         max_depth: Optional[int] = 2,
         use_async: Optional[bool] = None,
         extractor: Optional[Callable[[str], str]] = None,
-        metadata_extractor: Optional[Callable[[str, str], str]] = None,
+        metadata_extractor: Optional[_MetadataExtractorType] = None,
         exclude_dirs: Optional[Sequence[str]] = (),
         timeout: Optional[int] = 10,
         prevent_outside: bool = True,
         link_regex: Union[str, re.Pattern, None] = None,
         headers: Optional[dict] = None,
         check_response_status: bool = False,
+        continue_on_failure: bool = True,
+        *,
+        base_url: Optional[str] = None,
+        autoset_encoding: bool = True,
+        encoding: Optional[str] = None,
     ) -> None:
         """Initialize with URL to crawl and any subdirectories to exclude.
 
         Args:
             url: The URL to crawl.
             max_depth: The max depth of the recursive loading.
             use_async: Whether to use asynchronous loading.
                 If True, this function will not be lazy, but it will still work in the
                 expected way, just not lazy.
             extractor: A function to extract document contents from raw html.
                 When extract function returns an empty string, the document is
                 ignored.
-            metadata_extractor: A function to extract metadata from raw html and the
-                source url (args in that order). Default extractor will attempt
-                to use BeautifulSoup4 to extract the title, description and language
-                of the page.
+            metadata_extractor: A function to extract metadata from args: raw html, the
+                source url, and the requests.Response/aiohttp.ClientResponse object
+                (args in that order).
+                Default extractor will attempt to use BeautifulSoup4 to extract the
+                title, description and language of the page.
+                ..code-block:: python
+
+                    import requests
+                    import aiohttp
+
+                    def simple_metadata_extractor(
+                        raw_html: str, url: str, response: Union[requests.Response, aiohttp.ClientResponse]
+                    ) -> dict:
+                        content_type = getattr(response, "headers").get("Content-Type", "")
+                        return {"source": url, "content_type": content_type}
+
             exclude_dirs: A list of subdirectories to exclude.
             timeout: The timeout for the requests, in the unit of seconds. If None then
                 connection will not timeout.
             prevent_outside: If True, prevent loading from urls which are not children
                 of the root url.
             link_regex: Regex for extracting sub-links from the raw html of a web page.
             check_response_status: If True, check HTTP response status and skip
                 URLs with error responses (400-599).
-        """
+            continue_on_failure: If True, continue if getting or parsing a link raises
+                an exception. Otherwise, raise the exception.
+            base_url: The base url to check for outside links against.
+            autoset_encoding: Whether to automatically set the encoding of the response.
+                If True, the encoding of the response will be set to the apparent
+                encoding, unless the `encoding` argument has already been explicitly set.
+            encoding: The encoding of the response. If manually set, the encoding will be
+                set to given value, regardless of the `autoset_encoding` argument.
+        """  # noqa: E501
 
         self.url = url
         self.max_depth = max_depth if max_depth is not None else 2
         self.use_async = use_async if use_async is not None else False
         self.extractor = extractor if extractor is not None else lambda x: x
-        self.metadata_extractor = (
+        metadata_extractor = (
             metadata_extractor
             if metadata_extractor is not None
             else _metadata_extractor
         )
+        self.autoset_encoding = autoset_encoding
+        self.encoding = encoding
+        self.metadata_extractor = _wrap_metadata_extractor(metadata_extractor)
         self.exclude_dirs = exclude_dirs if exclude_dirs is not None else ()
 
         if any(url.startswith(exclude_dir) for exclude_dir in self.exclude_dirs):
             raise ValueError(
                 f"Base url is included in exclude_dirs. Received base_url: {url} and "
                 f"exclude_dirs: {self.exclude_dirs}"
             )
 
         self.timeout = timeout
         self.prevent_outside = prevent_outside if prevent_outside is not None else True
         self.link_regex = link_regex
         self._lock = asyncio.Lock() if self.use_async else None
         self.headers = headers
         self.check_response_status = check_response_status
+        self.continue_on_failure = continue_on_failure
+        self.base_url = base_url if base_url is not None else url
 
     def _get_child_links_recursive(
         self, url: str, visited: Set[str], *, depth: int = 0
     ) -> Iterator[Document]:
         """Recursively get all child links starting with the path of the input URL.
 
         Args:
@@ -157,37 +189,47 @@
         if depth >= self.max_depth:
             return
 
         # Get all links that can be accessed from the current URL
         visited.add(url)
         try:
             response = requests.get(url, timeout=self.timeout, headers=self.headers)
+
+            if self.encoding is not None:
+                response.encoding = self.encoding
+            elif self.autoset_encoding:
+                response.encoding = response.apparent_encoding
+
             if self.check_response_status and 400 <= response.status_code <= 599:
                 raise ValueError(f"Received HTTP status {response.status_code}")
         except Exception as e:
-            logger.warning(
-                f"Unable to load from {url}. Received error {e} of type "
-                f"{e.__class__.__name__}"
-            )
-            return
+            if self.continue_on_failure:
+                logger.warning(
+                    f"Unable to load from {url}. Received error {e} of type "
+                    f"{e.__class__.__name__}"
+                )
+                return
+            else:
+                raise e
         content = self.extractor(response.text)
         if content:
             yield Document(
                 page_content=content,
-                metadata=self.metadata_extractor(response.text, url),
+                metadata=self.metadata_extractor(response.text, url, response),
             )
 
         # Store the visited links and recursively visit the children
         sub_links = extract_sub_links(
             response.text,
             url,
-            base_url=self.url,
+            base_url=self.base_url,
             pattern=self.link_regex,
             prevent_outside=self.prevent_outside,
             exclude_prefixes=self.exclude_dirs,
+            continue_on_failure=self.continue_on_failure,
         )
         for link in sub_links:
             # Check all unvisited links
             if link not in visited:
                 yield from self._get_child_links_recursive(
                     link, visited, depth=depth + 1
                 )
@@ -203,14 +245,19 @@
         """Recursively get all child links starting with the path of the input URL.
 
         Args:
             url: The URL to crawl.
             visited: A set of visited URLs.
             depth: To reach the current url, how many pages have been visited.
         """
+        if not self.use_async or not self._lock:
+            raise ValueError(
+                "Async functions forbidden when not initialized with `use_async`"
+            )
+
         try:
             import aiohttp
         except ImportError:
             raise ImportError(
                 "The aiohttp package is required for the RecursiveUrlLoader. "
                 "Please install it with `pip install aiohttp`."
             )
@@ -225,51 +272,55 @@
             if session is not None
             else aiohttp.ClientSession(
                 connector=aiohttp.TCPConnector(ssl=False),
                 timeout=aiohttp.ClientTimeout(total=self.timeout),
                 headers=self.headers,
             )
         )
-        async with self._lock:  # type: ignore
+        async with self._lock:
             visited.add(url)
         try:
             async with session.get(url) as response:
                 text = await response.text()
                 if self.check_response_status and 400 <= response.status <= 599:
                     raise ValueError(f"Received HTTP status {response.status}")
         except (aiohttp.client_exceptions.InvalidURL, Exception) as e:
-            logger.warning(
-                f"Unable to load {url}. Received error {e} of type "
-                f"{e.__class__.__name__}"
-            )
             if close_session:
                 await session.close()
-            return []
+            if self.continue_on_failure:
+                logger.warning(
+                    f"Unable to load {url}. Received error {e} of type "
+                    f"{e.__class__.__name__}"
+                )
+                return []
+            else:
+                raise e
         results = []
         content = self.extractor(text)
         if content:
             results.append(
                 Document(
                     page_content=content,
-                    metadata=self.metadata_extractor(text, url),
+                    metadata=self.metadata_extractor(text, url, response),
                 )
             )
         if depth < self.max_depth - 1:
             sub_links = extract_sub_links(
                 text,
                 url,
-                base_url=self.url,
+                base_url=self.base_url,
                 pattern=self.link_regex,
                 prevent_outside=self.prevent_outside,
                 exclude_prefixes=self.exclude_dirs,
+                continue_on_failure=self.continue_on_failure,
             )
 
             # Recursively call the function to get the children of the children
             sub_tasks = []
-            async with self._lock:  # type: ignore
+            async with self._lock:
                 to_visit = set(sub_links).difference(visited)
                 for link in to_visit:
                     sub_tasks.append(
                         self._async_get_child_links_recursive(
                             link, visited, session=session, depth=depth + 1
                         )
                     )
@@ -294,10 +345,30 @@
             results = asyncio.run(
                 self._async_get_child_links_recursive(self.url, visited)
             )
             return iter(results or [])
         else:
             return self._get_child_links_recursive(self.url, visited)
 
-    def load(self) -> List[Document]:
-        """Load web pages."""
-        return list(self.lazy_load())
+
+_MetadataExtractorType1 = Callable[[str, str], dict]
+_MetadataExtractorType2 = Callable[
+    [str, str, Union[requests.Response, aiohttp.ClientResponse]], dict
+]
+_MetadataExtractorType = Union[_MetadataExtractorType1, _MetadataExtractorType2]
+
+
+def _wrap_metadata_extractor(
+    metadata_extractor: _MetadataExtractorType,
+) -> _MetadataExtractorType2:
+    if len(inspect.signature(metadata_extractor).parameters) == 3:
+        return cast(_MetadataExtractorType2, metadata_extractor)
+    else:
+
+        def _metadata_extractor_wrapper(
+            raw_html: str,
+            url: str,
+            response: Union[requests.Response, aiohttp.ClientResponse],
+        ) -> dict:
+            return cast(_MetadataExtractorType1, metadata_extractor)(raw_html, url)
+
+        return _metadata_extractor_wrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/reddit.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/reddit.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/roam.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/srt.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,29 @@
 from pathlib import Path
-from typing import List
+from typing import List, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
-class RoamLoader(BaseLoader):
-    """Load `Roam` files from a directory."""
+class SRTLoader(BaseLoader):
+    """Load `.srt` (subtitle) files."""
 
-    def __init__(self, path: str):
-        """Initialize with a path."""
-        self.file_path = path
+    def __init__(self, file_path: Union[str, Path]):
+        """Initialize with a file path."""
+        try:
+            import pysrt  # noqa:F401
+        except ImportError:
+            raise ImportError(
+                "package `pysrt` not found, please install it with `pip install pysrt`"
+            )
+        self.file_path = str(file_path)
 
     def load(self) -> List[Document]:
-        """Load documents."""
-        ps = list(Path(self.file_path).glob("**/*.md"))
-        docs = []
-        for p in ps:
-            with open(p) as f:
-                text = f.read()
-            metadata = {"source": str(p)}
-            docs.append(Document(page_content=text, metadata=metadata))
-        return docs
+        """Load using pysrt file."""
+        import pysrt
+
+        parsed_info = pysrt.open(self.file_path)
+        text = " ".join([t.text for t in parsed_info])
+        metadata = {"source": self.file_path}
+        return [Document(page_content=text, metadata=metadata)]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/rocksetdb.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/rocksetdb.py`

 * *Files 6% similar despite different names*

```diff
@@ -96,17 +96,14 @@
 
         try:
             self.client.set_application("langchain")
         except AttributeError:
             # ignore
             pass
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         query_results = self.client.Queries.query(
             sql=self.query
         ).results  # execute the SQL query
         for doc in query_results:  # for each doc in the response
             try:
                 yield Document(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/rspace.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/rspace.py`

 * *Files 2% similar despite different names*

```diff
@@ -120,10 +120,7 @@
             for d in self._load_structured_doc():
                 yield d
         elif self.global_id and self.global_id[0:2] in ["FL", "NB"]:
             for d in self._load_folder_tree():
                 yield d
         else:
             raise ValueError("Unknown global ID type")
-
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/rss.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/rss.py`

 * *Files 1% similar despite different names*

```diff
@@ -92,15 +92,15 @@
                 "urls arg instead."
             ) from e
         rss = listparser.parse(self.opml)
         return [feed.url for feed in rss.feeds]
 
     def lazy_load(self) -> Iterator[Document]:
         try:
-            import feedparser  # noqa:F401
+            import feedparser
         except ImportError:
             raise ImportError(
                 "feedparser package not found, please install it with "
                 "`pip install feedparser`"
             )
 
         for url in self._get_urls:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/rst.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/rst.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 """Loads RST files."""
-from typing import Any, List
+
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
@@ -28,15 +30,18 @@
 
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition-rst
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         """
         Initialize with a file path.
 
         Args:
             file_path: The path to the file to load.
             mode: The mode to use for partitioning. See unstructured for details.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/rtf.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/rtf.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 """Loads rich text files."""
-from typing import Any, List
+
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     satisfies_min_unstructured_version,
 )
 
 
@@ -28,15 +30,18 @@
 
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition-rtf
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         """
         Initialize with a file path.
 
         Args:
             file_path: The path to the file to load.
             mode: The mode to use for partitioning. See unstructured for details.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/s3_directory.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/s3_directory.py`

 * *Files 2% similar despite different names*

```diff
@@ -116,14 +116,17 @@
             aws_secret_access_key=self.aws_secret_access_key,
             aws_session_token=self.aws_session_token,
             config=self.boto_config,
         )
         bucket = s3.Bucket(self.bucket)
         docs = []
         for obj in bucket.objects.filter(Prefix=self.prefix):
+            # Skip directories
+            if obj.size == 0 and obj.key.endswith("/"):
+                continue
             loader = S3FileLoader(
                 self.bucket,
                 obj.key,
                 region_name=self.region_name,
                 api_version=self.api_version,
                 use_ssl=self.use_ssl,
                 verify=self.verify,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/s3_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/s3_file.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from __future__ import annotations
 
 import os
 import tempfile
-from typing import TYPE_CHECKING, List, Optional, Union
+from typing import TYPE_CHECKING, Any, Callable, List, Optional, Union
 
 from langchain_community.document_loaders.unstructured import UnstructuredBaseLoader
 
 if TYPE_CHECKING:
     import botocore
 
 
@@ -23,14 +23,17 @@
         use_ssl: Optional[bool] = True,
         verify: Union[str, bool, None] = None,
         endpoint_url: Optional[str] = None,
         aws_access_key_id: Optional[str] = None,
         aws_secret_access_key: Optional[str] = None,
         aws_session_token: Optional[str] = None,
         boto_config: Optional[botocore.client.Config] = None,
+        mode: str = "single",
+        post_processors: Optional[List[Callable]] = None,
+        **unstructured_kwargs: Any,
     ):
         """Initialize with bucket and key name.
 
         :param bucket: The name of the S3 bucket.
         :param key: The key of the S3 object.
 
         :param region_name: The name of the region associated with the client.
@@ -78,16 +81,22 @@
         :param boto_config: Advanced boto3 client configuration options. If a value
             is specified in the client config, its value will take precedence
             over environment variables and configuration values, but not over
             a value passed explicitly to the method. If a default config
             object is set on the session, the config object used when creating
             the client will be the result of calling ``merge()`` on the
             default config with the config provided to this call.
+        :param mode: Mode in which to read the file. Valid options are: single,
+            paged and elements.
+        :param post_processors: Post processing functions to be applied to
+            extracted elements.
+        :param **unstructured_kwargs: Arbitrary additional kwargs to pass in when
+            calling `partition`
         """
-        super().__init__()
+        super().__init__(mode, post_processors, **unstructured_kwargs)
         self.bucket = bucket
         self.key = key
         self.region_name = region_name
         self.api_version = api_version
         self.use_ssl = use_ssl
         self.verify = verify
         self.endpoint_url = endpoint_url
@@ -119,11 +128,11 @@
             aws_session_token=self.aws_session_token,
             config=self.boto_config,
         )
         with tempfile.TemporaryDirectory() as temp_dir:
             file_path = f"{temp_dir}/{self.key}"
             os.makedirs(os.path.dirname(file_path), exist_ok=True)
             s3.download_file(self.bucket, self.key, file_path)
-            return partition(filename=file_path)
+            return partition(filename=file_path, **self.unstructured_kwargs)
 
     def _get_metadata(self) -> dict:
         return {"source": f"s3://{self.bucket}/{self.key}"}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/sharepoint.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/sharepoint.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Loader that loads data from Sharepoint Document Library"""
+
 from __future__ import annotations
 
 from typing import Iterator, List, Optional, Sequence
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import Field
 
@@ -18,14 +19,16 @@
 
     document_library_id: str = Field(...)
     """ The ID of the SharePoint document library to load data from."""
     folder_path: Optional[str] = None
     """ The path to the folder to load data from."""
     object_ids: Optional[List[str]] = None
     """ The IDs of the objects to load data from."""
+    folder_id: Optional[str] = None
+    """ The ID of the folder to load data from."""
 
     @property
     def _file_types(self) -> Sequence[_FileType]:
         """Return supported file types."""
         return _FileType.DOC, _FileType.DOCX, _FileType.PDF
 
     @property
@@ -47,14 +50,22 @@
         blob_parser = get_parser("default")
         if self.folder_path:
             target_folder = drive.get_item_by_path(self.folder_path)
             if not isinstance(target_folder, Folder):
                 raise ValueError(f"There isn't a folder with path {self.folder_path}.")
             for blob in self._load_from_folder(target_folder):
                 yield from blob_parser.lazy_parse(blob)
+        if self.folder_id:
+            target_folder = drive.get_item(self.folder_id)
+            if not isinstance(target_folder, Folder):
+                raise ValueError(f"There isn't a folder with path {self.folder_path}.")
+            for blob in self._load_from_folder(target_folder):
+                yield from blob_parser.lazy_parse(blob)
         if self.object_ids:
             for blob in self._load_from_object_ids(drive, self.object_ids):
                 yield from blob_parser.lazy_parse(blob)
-
-    def load(self) -> List[Document]:
-        """Load all documents."""
-        return list(self.lazy_load())
+        if not (self.folder_path or self.folder_id or self.object_ids):
+            target_folder = drive.get_root_folder()
+            if not isinstance(target_folder, Folder):
+                raise ValueError("Unable to fetch root folder")
+            for blob in self._load_from_folder(target_folder):
+                yield from blob_parser.lazy_parse(blob)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/sitemap.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/sitemap.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import itertools
 import re
-from typing import Any, Callable, Generator, Iterable, List, Optional, Tuple
+from typing import Any, Callable, Generator, Iterable, Iterator, List, Optional, Tuple
 from urllib.parse import urlparse
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.web_base import WebBaseLoader
 
 
@@ -178,15 +178,15 @@
             if not loc:
                 continue
             soup_child = self.scrape_all([loc.text], "xml")[0]
 
             els.extend(self.parse_sitemap(soup_child))
         return els
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load sitemap."""
         if self.is_local:
             try:
                 import bs4
             except ImportError:
                 raise ImportError(
                     "beautifulsoup4 package not found, please install it"
@@ -207,14 +207,12 @@
                     "Selected sitemap does not contain enough blocks for given blocknum"
                 )
             else:
                 els = elblocks[self.blocknum]
 
         results = self.scrape_all([el["loc"].strip() for el in els if "loc" in el])
 
-        return [
-            Document(
-                page_content=self.parsing_function(results[i]),
-                metadata=self.meta_function(els[i], results[i]),
+        for i, result in enumerate(results):
+            yield Document(
+                page_content=self.parsing_function(result),
+                metadata=self.meta_function(els[i], result),
             )
-            for i in range(len(results))
-        ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/slack_directory.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/slack_directory.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 import json
 import zipfile
 from pathlib import Path
-from typing import Dict, List, Optional
+from typing import Dict, Iterator, List, Optional, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class SlackDirectoryLoader(BaseLoader):
     """Load from a `Slack` directory dump."""
 
-    def __init__(self, zip_path: str, workspace_url: Optional[str] = None):
+    def __init__(self, zip_path: Union[str, Path], workspace_url: Optional[str] = None):
         """Initialize the SlackDirectoryLoader.
 
         Args:
             zip_path (str): The path to the Slack directory dump zip file.
             workspace_url (Optional[str]): The Slack workspace URL.
               Including the URL will turn
               sources into links. Defaults to None.
@@ -31,30 +31,25 @@
             try:
                 with zip_file.open("channels.json", "r") as f:
                     channels = json.load(f)
                 return {channel["name"]: channel["id"] for channel in channels}
             except KeyError:
                 return {}
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load and return documents from the Slack directory dump."""
-        docs = []
         with zipfile.ZipFile(self.zip_path, "r") as zip_file:
             for channel_path in zip_file.namelist():
                 channel_name = Path(channel_path).parent.name
                 if not channel_name:
                     continue
                 if channel_path.endswith(".json"):
                     messages = self._read_json(zip_file, channel_path)
                     for message in messages:
-                        document = self._convert_message_to_document(
-                            message, channel_name
-                        )
-                        docs.append(document)
-        return docs
+                        yield self._convert_message_to_document(message, channel_name)
 
     def _read_json(self, zip_file: zipfile.ZipFile, file_path: str) -> List[dict]:
         """Read JSON data from a zip subfile."""
         with zip_file.open(file_path, "r") as f:
             data = json.load(f)
         return data
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/snowflake_loader.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/snowflake_loader.py`

 * *Files 6% similar despite different names*

```diff
@@ -84,15 +84,15 @@
             cur.execute("USE DATABASE " + self.database)
             cur.execute("USE SCHEMA " + self.schema)
             cur.execute(self.query, self.parameters)
             query_result = cur.fetchall()
             column_names = [column[0] for column in cur.description]
             query_result = [dict(zip(column_names, row)) for row in query_result]
         except Exception as e:
-            print(f"An error occurred: {e}")
+            print(f"An error occurred: {e}")  # noqa: T201
             query_result = []
         finally:
             cur.close()
         return query_result
 
     def _get_columns(
         self, query_result: List[Dict[str, Any]]
@@ -106,23 +106,19 @@
         if metadata_columns is None:
             metadata_columns = []
         return page_content_columns or [], metadata_columns
 
     def lazy_load(self) -> Iterator[Document]:
         query_result = self._execute_query()
         if isinstance(query_result, Exception):
-            print(f"An error occurred during the query: {query_result}")
-            return []
+            print(f"An error occurred during the query: {query_result}")  # noqa: T201
+            return []  # type: ignore[return-value]
         page_content_columns, metadata_columns = self._get_columns(query_result)
         if "*" in page_content_columns:
             page_content_columns = list(query_result[0].keys())
         for row in query_result:
             page_content = "\n".join(
                 f"{k}: {v}" for k, v in row.items() if k in page_content_columns
             )
             metadata = {k: v for k, v in row.items() if k in metadata_columns}
             doc = Document(page_content=page_content, metadata=metadata)
             yield doc
-
-    def load(self) -> List[Document]:
-        """Load data into document objects."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/spreedly.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/spreedly.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/stripe.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/stripe.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/telegram.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/telegram.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,15 +21,15 @@
     text = row["text"]
     return f"{sender} on {date}: {text}\n\n"
 
 
 class TelegramChatFileLoader(BaseLoader):
     """Load from `Telegram chat` dump."""
 
-    def __init__(self, path: str):
+    def __init__(self, path: Union[str, Path]):
         """Initialize with a path."""
         self.file_path = path
 
     def load(self) -> List[Document]:
         """Load documents."""
         p = Path(self.file_path)
 
@@ -44,34 +44,35 @@
         metadata = {"source": str(p)}
 
         return [Document(page_content=text, metadata=metadata)]
 
 
 def text_to_docs(text: Union[str, List[str]]) -> List[Document]:
     """Convert a string or list of strings to a list of Documents with metadata."""
-    from langchain.text_splitter import RecursiveCharacterTextSplitter
+    from langchain_text_splitters import RecursiveCharacterTextSplitter
+
+    text_splitter = RecursiveCharacterTextSplitter(
+        chunk_size=800,
+        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
+        chunk_overlap=20,
+    )
 
     if isinstance(text, str):
         # Take a single string as one page
         text = [text]
     page_docs = [Document(page_content=page) for page in text]
 
     # Add page numbers as metadata
     for i, doc in enumerate(page_docs):
         doc.metadata["page"] = i + 1
 
     # Split pages into chunks
     doc_chunks = []
 
     for doc in page_docs:
-        text_splitter = RecursiveCharacterTextSplitter(
-            chunk_size=800,
-            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
-            chunk_overlap=20,
-        )
         chunks = text_splitter.split_text(doc.page_content)
         for i, chunk in enumerate(chunks):
             doc = Document(
                 page_content=chunk, metadata={"page": doc.metadata["page"], "chunk": i}
             )
             # Add sources a metadata
             doc.metadata["source"] = f"{doc.metadata['page']}-{doc.metadata['chunk']}"
@@ -257,7 +258,11 @@
         normalized_messages = pd.json_normalize(d)
         df = pd.DataFrame(normalized_messages)
 
         message_threads = self._get_message_threads(df)
         combined_texts = self._combine_message_texts(message_threads, df)
 
         return text_to_docs(combined_texts)
+
+
+# For backwards compatibility
+TelegramChatLoader = TelegramChatFileLoader
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/tencent_cos_directory.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/tencent_cos_directory.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Iterator, List
+from typing import Any, Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.tencent_cos_file import TencentCOSFileLoader
 
 
@@ -15,17 +15,14 @@
         :param bucket(str): COS bucket.
         :param prefix(str): prefix.
         """
         self.conf = conf
         self.bucket = bucket
         self.prefix = prefix
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Load documents."""
         try:
             from qcloud_cos import CosS3Client
         except ImportError:
             raise ImportError(
                 "Could not import cos-python-sdk-v5 python package. "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/tencent_cos_file.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/tencent_cos_file.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import os
 import tempfile
-from typing import Any, Iterator, List
+from typing import Any, Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
 
 
@@ -17,17 +17,14 @@
         :param bucket(str): COS bucket.
         :param key(str): COS file key.
         """
         self.conf = conf
         self.bucket = bucket
         self.key = key
 
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Load documents."""
         try:
             from qcloud_cos import CosS3Client
         except ImportError:
             raise ImportError(
                 "Could not import cos-python-sdk-v5 python package. "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/tensorflow_datasets.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/tensorflow_datasets.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Callable, Dict, Iterator, List, Optional
+from typing import Callable, Dict, Iterator, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.tensorflow_datasets import TensorflowDatasets
 
 
@@ -62,19 +62,16 @@
         self.load_max_docs = load_max_docs
         """The maximum number of documents to load."""
         self.sample_to_document_function: Optional[
             Callable[[Dict], Document]
         ] = sample_to_document_function
         """Custom function that transform a dataset sample into a Document."""
 
-        self._tfds_client = TensorflowDatasets(
+        self._tfds_client = TensorflowDatasets(  # type: ignore[call-arg]
             dataset_name=self.dataset_name,
             split_name=self.split_name,
-            load_max_docs=self.load_max_docs,
+            load_max_docs=self.load_max_docs,  # type: ignore[arg-type]
             sample_to_document_function=self.sample_to_document_function,
         )
 
     def lazy_load(self) -> Iterator[Document]:
         yield from self._tfds_client.lazy_load()
-
-    def load(self) -> List[Document]:
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/text.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/text.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import logging
-from typing import List, Optional
+from pathlib import Path
+from typing import Iterator, Optional, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.helpers import detect_file_encodings
 
 logger = logging.getLogger(__name__)
@@ -21,24 +22,24 @@
 
         autodetect_encoding: Whether to try to autodetect the file encoding
             if the specified encoding fails.
     """
 
     def __init__(
         self,
-        file_path: str,
+        file_path: Union[str, Path],
         encoding: Optional[str] = None,
         autodetect_encoding: bool = False,
     ):
         """Initialize with file path."""
         self.file_path = file_path
         self.encoding = encoding
         self.autodetect_encoding = autodetect_encoding
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load from file path."""
         text = ""
         try:
             with open(self.file_path, encoding=self.encoding) as f:
                 text = f.read()
         except UnicodeDecodeError as e:
             if self.autodetect_encoding:
@@ -52,9 +53,9 @@
                     except UnicodeDecodeError:
                         continue
             else:
                 raise RuntimeError(f"Error loading {self.file_path}") from e
         except Exception as e:
             raise RuntimeError(f"Error loading {self.file_path}") from e
 
-        metadata = {"source": self.file_path}
-        return [Document(page_content=text, metadata=metadata)]
+        metadata = {"source": str(self.file_path)}
+        yield Document(page_content=text, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/tomarkdown.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/tomarkdown.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from typing import Iterator, List
+from typing import Iterator
 
 import requests
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
@@ -24,11 +24,7 @@
             "https://2markdown.com/api/2md",
             headers={"X-Api-Key": self.api_key},
             json={"url": self.url},
         )
         text = response.json()["article"]
         metadata = {"source": self.url}
         yield Document(page_content=text, metadata=metadata)
-
-    def load(self) -> List[Document]:
-        """Load file."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/toml.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/toml.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import json
 from pathlib import Path
-from typing import Iterator, List, Union
+from typing import Iterator, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 class TomlLoader(BaseLoader):
@@ -14,18 +14,14 @@
     directory.
     """
 
     def __init__(self, source: Union[str, Path]):
         """Initialize the TomlLoader with a source file or directory."""
         self.source = Path(source)
 
-    def load(self) -> List[Document]:
-        """Load and return all documents."""
-        return list(self.lazy_load())
-
     def lazy_load(self) -> Iterator[Document]:
         """Lazily load the TOML documents from the source file or directory."""
         import tomli
 
         if self.source.is_file() and self.source.suffix == ".toml":
             files = [self.source]
         elif self.source.is_dir():
@@ -40,8 +36,8 @@
                     data = tomli.loads(content)
                     doc = Document(
                         page_content=json.dumps(data),
                         metadata={"source": str(file_path)},
                     )
                     yield doc
                 except tomli.TOMLDecodeError as e:
-                    print(f"Error parsing TOML file {file_path}: {e}")
+                    print(f"Error parsing TOML file {file_path}: {e}")  # noqa: T201
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/trello.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/trello.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, List, Literal, Optional, Tuple
+from typing import TYPE_CHECKING, Any, Iterator, Literal, Optional, Tuple
 
 from langchain_core.documents import Document
 from langchain_core.utils import get_from_env
 
 from langchain_community.document_loaders.base import BaseLoader
 
 if TYPE_CHECKING:
@@ -85,15 +85,15 @@
                 "Please install it with `pip install py-trello`."
             ) from ex
         api_key = api_key or get_from_env("api_key", "TRELLO_API_KEY")
         token = token or get_from_env("token", "TRELLO_TOKEN")
         client = TrelloClient(api_key=api_key, token=token)
         return cls(client, board_name, **kwargs)
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Loads all cards from the specified Trello board.
 
         You can filter the cards, metadata and text included by using the optional
             parameters.
 
          Returns:
             A list of documents, one for each card in the board.
@@ -107,15 +107,16 @@
             ) from ex
 
         board = self._get_board()
         # Create a dictionary with the list IDs as keys and the list names as values
         list_dict = {list_item.id: list_item.name for list_item in board.list_lists()}
         # Get Cards on the board
         cards = board.get_cards(card_filter=self.card_filter)
-        return [self._card_to_doc(card, list_dict) for card in cards]
+        for card in cards:
+            yield self._card_to_doc(card, list_dict)
 
     def _get_board(self) -> Board:
         # Find the first board with a matching name
         board = next(
             (b for b in self.client.list_boards() if b.name == self.board_name), None
         )
         if not board:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/tsv.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/tsv.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import Any, List
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
@@ -22,15 +23,18 @@
     from langchain_community.document_loaders.tsv import UnstructuredTSVLoader
 
     loader = UnstructuredTSVLoader("stanley-cups.tsv", mode="elements")
     docs = loader.load()
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
         validate_unstructured_version(min_unstructured_version="0.7.6")
         super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)
 
     def _get_elements(self) -> List:
         from unstructured.partition.tsv import partition_tsv
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/twitter.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/twitter.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/unstructured.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/unstructured.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """Loader that uses unstructured to load files."""
+
 import collections
 from abc import ABC, abstractmethod
-from typing import IO, Any, Callable, Dict, List, Optional, Sequence, Union
+from pathlib import Path
+from typing import IO, Any, Callable, Dict, Iterator, List, Optional, Sequence, Union
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 def satisfies_min_unstructured_version(min_version: str) -> bool:
@@ -43,15 +45,15 @@
         post_processors: Optional[List[Callable]] = None,
         **unstructured_kwargs: Any,
     ):
         """Initialize with file path."""
         try:
             import unstructured  # noqa:F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "unstructured package not found, please install it with "
                 "`pip install unstructured`"
             )
         _valid_modes = {"single", "elements", "paged"}
         if mode not in _valid_modes:
             raise ValueError(
                 f"Got {mode} for `mode`, but should be one of `{_valid_modes}`"
@@ -78,29 +80,28 @@
         Post processing functions are str -> str callables are passed
         in using the post_processors kwarg when the loader is instantiated."""
         for element in elements:
             for post_processor in self.post_processors:
                 element.apply(post_processor)
         return elements
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load file."""
         elements = self._get_elements()
         self._post_process_elements(elements)
         if self.mode == "elements":
-            docs: List[Document] = list()
             for element in elements:
                 metadata = self._get_metadata()
                 # NOTE(MthwRobinson) - the attribute check is for backward compatibility
                 # with unstructured<0.4.9. The metadata attributed was added in 0.4.9.
                 if hasattr(element, "metadata"):
                     metadata.update(element.metadata.to_dict())
                 if hasattr(element, "category"):
                     metadata["category"] = element.category
-                docs.append(Document(page_content=str(element), metadata=metadata))
+                yield Document(page_content=str(element), metadata=metadata)
         elif self.mode == "paged":
             text_dict: Dict[int, str] = {}
             meta_dict: Dict[int, Dict] = {}
 
             for idx, element in enumerate(elements):
                 metadata = self._get_metadata()
                 if hasattr(element, "metadata"):
@@ -114,25 +115,22 @@
                     meta_dict[page_number] = metadata
                 else:
                     # If exists, append to text and update the metadata
                     text_dict[page_number] += str(element) + "\n\n"
                     meta_dict[page_number].update(metadata)
 
             # Convert the dict to a list of Document objects
-            docs = [
-                Document(page_content=text_dict[key], metadata=meta_dict[key])
-                for key in text_dict.keys()
-            ]
+            for key in text_dict.keys():
+                yield Document(page_content=text_dict[key], metadata=meta_dict[key])
         elif self.mode == "single":
             metadata = self._get_metadata()
             text = "\n\n".join([str(el) for el in elements])
-            docs = [Document(page_content=text, metadata=metadata)]
+            yield Document(page_content=text, metadata=metadata)
         else:
             raise ValueError(f"mode of {self.mode} not supported.")
-        return docs
 
 
 class UnstructuredFileLoader(UnstructuredBaseLoader):
     """Load files using `Unstructured`.
 
     The file loader uses the
     unstructured partition function and will automatically detect the file
@@ -155,40 +153,52 @@
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition
     """
 
     def __init__(
         self,
-        file_path: Union[str, List[str]],
+        file_path: Union[str, List[str], Path, List[Path], None],
         mode: str = "single",
         **unstructured_kwargs: Any,
     ):
         """Initialize with file path."""
         self.file_path = file_path
         super().__init__(mode=mode, **unstructured_kwargs)
 
     def _get_elements(self) -> List:
         from unstructured.partition.auto import partition
 
-        return partition(filename=self.file_path, **self.unstructured_kwargs)
+        if isinstance(self.file_path, list):
+            elements = []
+            for file in self.file_path:
+                if isinstance(file, Path):
+                    file = str(file)
+                elements.extend(partition(filename=file, **self.unstructured_kwargs))
+            return elements
+        else:
+            if isinstance(self.file_path, Path):
+                self.file_path = str(self.file_path)
+            return partition(filename=self.file_path, **self.unstructured_kwargs)
 
     def _get_metadata(self) -> dict:
         return {"source": self.file_path}
 
 
 def get_elements_from_api(
-    file_path: Union[str, List[str], None] = None,
+    file_path: Union[str, List[str], Path, List[Path], None] = None,
     file: Union[IO, Sequence[IO], None] = None,
     api_url: str = "https://api.unstructured.io/general/v0/general",
     api_key: str = "",
     **unstructured_kwargs: Any,
 ) -> List:
     """Retrieve a list of elements from the `Unstructured API`."""
-    if isinstance(file, collections.abc.Sequence) or isinstance(file_path, list):
+    if is_list := isinstance(file_path, list):
+        file_path = [str(path) for path in file_path]
+    if isinstance(file, collections.abc.Sequence) or is_list:
         from unstructured.partition.api import partition_multiple_via_api
 
         _doc_elements = partition_multiple_via_api(
             filenames=file_path,
             files=file,
             api_key=api_key,
             api_url=api_url,
@@ -200,15 +210,15 @@
             elements.extend(_elements)
 
         return elements
     else:
         from unstructured.partition.api import partition_via_api
 
         return partition_via_api(
-            filename=file_path,
+            filename=str(file_path) if file_path is not None else None,
             file=file,
             api_key=api_key,
             api_url=api_url,
             **unstructured_kwargs,
         )
 
 
@@ -242,15 +252,15 @@
     https://unstructured-io.github.io/unstructured/bricks.html#partition
     https://www.unstructured.io/api-key/
     https://github.com/Unstructured-IO/unstructured-api
     """
 
     def __init__(
         self,
-        file_path: Union[str, List[str]] = "",
+        file_path: Union[str, List[str], None] = "",
         mode: str = "single",
         url: str = "https://api.unstructured.io/general/v0/general",
         api_key: str = "",
         **unstructured_kwargs: Any,
     ):
         """Initialize with file path."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/url.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/url.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Loader that uses unstructured to load HTML files."""
+
 import logging
 from typing import Any, List
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/url_playwright.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/url_playwright.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,13 @@
-"""Loader that uses Playwright to load a page, then uses unstructured to load the html.
-"""
+"""Loader that uses Playwright to load a page,
+then uses unstructured to load the html."""
+
 import logging
 from abc import ABC, abstractmethod
-from typing import TYPE_CHECKING, List, Optional
+from typing import TYPE_CHECKING, AsyncIterator, Dict, Iterator, List, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 if TYPE_CHECKING:
     from playwright.async_api import Browser as AsyncBrowser
@@ -53,15 +54,15 @@
         Returns:
             text: The text content of the page.
         """
         pass
 
 
 class UnstructuredHtmlEvaluator(PlaywrightEvaluator):
-    """Evaluates the page HTML content using the `unstructured` library."""
+    """Evaluate the page HTML content using the `unstructured` library."""
 
     def __init__(self, remove_selectors: Optional[List[str]] = None):
         """Initialize UnstructuredHtmlEvaluator."""
         try:
             import unstructured  # noqa:F401
         except ImportError:
             raise ImportError(
@@ -107,102 +108,123 @@
 
     This is useful for loading pages that require javascript to render.
 
     Attributes:
         urls (List[str]): List of URLs to load.
         continue_on_failure (bool): If True, continue loading other URLs on failure.
         headless (bool): If True, the browser will run in headless mode.
+        proxy (Optional[Dict[str, str]]): If set, the browser will access URLs
+            through the specified proxy.
+
+    Example:
+        .. code-block:: python
+
+            from langchain_community.document_loaders import PlaywrightURLLoader
+
+            urls = ["https://api.ipify.org/?format=json",]
+            proxy={
+                "server": "https://xx.xx.xx:15818", # https://<host>:<port>
+                "username": "username",
+                "password": "password"
+            }
+            loader = PlaywrightURLLoader(urls, proxy=proxy)
+            data = loader.load()
     """
 
     def __init__(
         self,
         urls: List[str],
         continue_on_failure: bool = True,
         headless: bool = True,
         remove_selectors: Optional[List[str]] = None,
         evaluator: Optional[PlaywrightEvaluator] = None,
+        proxy: Optional[Dict[str, str]] = None,
     ):
         """Load a list of URLs using Playwright."""
         try:
             import playwright  # noqa:F401
         except ImportError:
             raise ImportError(
                 "playwright package not found, please install it with "
                 "`pip install playwright`"
             )
 
         self.urls = urls
         self.continue_on_failure = continue_on_failure
         self.headless = headless
+        self.proxy = proxy
 
         if remove_selectors and evaluator:
             raise ValueError(
                 "`remove_selectors` and `evaluator` cannot be both not None"
             )
 
         # Use the provided evaluator, if any, otherwise, use the default.
         self.evaluator = evaluator or UnstructuredHtmlEvaluator(remove_selectors)
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """Load the specified URLs using Playwright and create Document instances.
 
         Returns:
-            List[Document]: A list of Document instances with loaded content.
+            A list of Document instances with loaded content.
         """
         from playwright.sync_api import sync_playwright
 
-        docs: List[Document] = list()
-
         with sync_playwright() as p:
-            browser = p.chromium.launch(headless=self.headless)
+            browser = p.chromium.launch(headless=self.headless, proxy=self.proxy)
             for url in self.urls:
                 try:
                     page = browser.new_page()
                     response = page.goto(url)
                     if response is None:
                         raise ValueError(f"page.goto() returned None for url {url}")
 
                     text = self.evaluator.evaluate(page, browser, response)
                     metadata = {"source": url}
-                    docs.append(Document(page_content=text, metadata=metadata))
+                    yield Document(page_content=text, metadata=metadata)
                 except Exception as e:
                     if self.continue_on_failure:
                         logger.error(
                             f"Error fetching or processing {url}, exception: {e}"
                         )
                     else:
                         raise e
             browser.close()
-        return docs
 
     async def aload(self) -> List[Document]:
         """Load the specified URLs with Playwright and create Documents asynchronously.
         Use this function when in a jupyter notebook environment.
 
         Returns:
-            List[Document]: A list of Document instances with loaded content.
+            A list of Document instances with loaded content.
         """
-        from playwright.async_api import async_playwright
+        return [doc async for doc in self.alazy_load()]
 
-        docs: List[Document] = list()
+    async def alazy_load(self) -> AsyncIterator[Document]:
+        """Load the specified URLs with Playwright and create Documents asynchronously.
+        Use this function when in a jupyter notebook environment.
+
+        Returns:
+            A list of Document instances with loaded content.
+        """
+        from playwright.async_api import async_playwright
 
         async with async_playwright() as p:
-            browser = await p.chromium.launch(headless=self.headless)
+            browser = await p.chromium.launch(headless=self.headless, proxy=self.proxy)
             for url in self.urls:
                 try:
                     page = await browser.new_page()
                     response = await page.goto(url)
                     if response is None:
                         raise ValueError(f"page.goto() returned None for url {url}")
 
                     text = await self.evaluator.evaluate_async(page, browser, response)
                     metadata = {"source": url}
-                    docs.append(Document(page_content=text, metadata=metadata))
+                    yield Document(page_content=text, metadata=metadata)
                 except Exception as e:
                     if self.continue_on_failure:
                         logger.error(
                             f"Error fetching or processing {url}, exception: {e}"
                         )
                     else:
                         raise e
             await browser.close()
-        return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/url_selenium.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/url_selenium.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-"""Loader that uses Selenium to load a page, then uses unstructured to load the html.
-"""
+"""Loader that uses Selenium to load a page, then uses unstructured to load the html."""
+
 import logging
 from typing import TYPE_CHECKING, List, Literal, Optional, Union
 
 if TYPE_CHECKING:
     from selenium.webdriver import Chrome, Firefox
 
 from langchain_core.documents import Document
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/weather.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/weather.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """Simple reader that reads weather data from OpenWeatherMap API"""
+
 from __future__ import annotations
 
 from datetime import datetime
-from typing import Iterator, List, Optional, Sequence
+from typing import Iterator, Optional, Sequence
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper
 
 
@@ -28,24 +29,18 @@
         self.client = client
         self.places = places
 
     @classmethod
     def from_params(
         cls, places: Sequence[str], *, openweathermap_api_key: Optional[str] = None
     ) -> WeatherDataLoader:
-        client = OpenWeatherMapAPIWrapper(openweathermap_api_key=openweathermap_api_key)
+        client = OpenWeatherMapAPIWrapper(openweathermap_api_key=openweathermap_api_key)  # type: ignore[call-arg]
         return cls(client, places)
 
     def lazy_load(
         self,
     ) -> Iterator[Document]:
         """Lazily load weather data for the given locations."""
         for place in self.places:
             metadata = {"queried_at": datetime.now()}
             content = self.client.run(place)
             yield Document(page_content=content, metadata=metadata)
-
-    def load(
-        self,
-    ) -> List[Document]:
-        """Load weather data for the given locations."""
-        return list(self.lazy_load())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/web_base.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/web_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Web base loader class."""
+
 import asyncio
 import logging
 import warnings
 from typing import Any, Dict, Iterator, List, Optional, Sequence, Union
 
 import aiohttp
 import requests
@@ -128,14 +129,15 @@
         async with aiohttp.ClientSession() as session:
             for i in range(retries):
                 try:
                     async with session.get(
                         url,
                         headers=self.session.headers,
                         ssl=None if self.session.verify else False,
+                        cookies=self.session.cookies.get_dict(),
                     ) as response:
                         return await response.text()
                 except aiohttp.ClientConnectionError as e:
                     if i == retries - 1:
                         raise
                     else:
                         logger.warning(
@@ -246,19 +248,15 @@
         """Lazy load text from the url(s) in web_path."""
         for path in self.web_paths:
             soup = self._scrape(path, bs_kwargs=self.bs_kwargs)
             text = soup.get_text(**self.bs_get_text_kwargs)
             metadata = _build_metadata(soup, path)
             yield Document(page_content=text, metadata=metadata)
 
-    def load(self) -> List[Document]:
-        """Load text from the url(s) in web_path."""
-        return list(self.lazy_load())
-
-    def aload(self) -> List[Document]:
+    def aload(self) -> List[Document]:  # type: ignore
         """Load text from the urls in web_path async into Documents."""
 
         results = self.scrape_all(self.web_paths)
         docs = []
         for path, soup in zip(self.web_paths, results):
             text = soup.get_text(**self.bs_get_text_kwargs)
             metadata = _build_metadata(soup, path)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/whatsapp_chat.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/whatsapp_chat.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import re
 from pathlib import Path
-from typing import List
+from typing import Iterator
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 
 
 def concatenate_rows(date: str, sender: str, text: str) -> str:
@@ -15,16 +15,15 @@
 class WhatsAppChatLoader(BaseLoader):
     """Load `WhatsApp` messages text file."""
 
     def __init__(self, path: str):
         """Initialize with path."""
         self.file_path = path
 
-    def load(self) -> List[Document]:
-        """Load documents."""
+    def lazy_load(self) -> Iterator[Document]:
         p = Path(self.file_path)
         text_content = ""
 
         with open(p, encoding="utf8") as f:
             lines = f.readlines()
 
         message_line_regex = r"""
@@ -58,8 +57,8 @@
             if result:
                 date, sender, text = result.groups()
                 if text not in ignore_lines:
                     text_content += concatenate_rows(date, sender, text)
 
         metadata = {"source": str(p)}
 
-        return [Document(page_content=text_content, metadata=metadata)]
+        yield Document(page_content=text_content, metadata=metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/wikipedia.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/wikipedia.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import List, Optional
+from typing import Iterator, Optional
 
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.utilities.wikipedia import WikipediaAPIWrapper
 
 
@@ -38,23 +38,22 @@
         """
         self.query = query
         self.lang = lang
         self.load_max_docs = load_max_docs
         self.load_all_available_meta = load_all_available_meta
         self.doc_content_chars_max = doc_content_chars_max
 
-    def load(self) -> List[Document]:
+    def lazy_load(self) -> Iterator[Document]:
         """
         Loads the query result from Wikipedia into a list of Documents.
 
         Returns:
-            List[Document]: A list of Document objects representing the loaded
+            A list of Document objects representing the loaded
                 Wikipedia pages.
         """
-        client = WikipediaAPIWrapper(
+        client = WikipediaAPIWrapper(  # type: ignore[call-arg]
             lang=self.lang,
-            top_k_results=self.load_max_docs,
-            load_all_available_meta=self.load_all_available_meta,
-            doc_content_chars_max=self.doc_content_chars_max,
+            top_k_results=self.load_max_docs,  # type: ignore[arg-type]
+            load_all_available_meta=self.load_all_available_meta,  # type: ignore[arg-type]
+            doc_content_chars_max=self.doc_content_chars_max,  # type: ignore[arg-type]
         )
-        docs = client.load(self.query)
-        return docs
+        yield from client.load(self.query)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/word_document.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/word_document.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """Loads word documents."""
+
 import os
 import tempfile
 from abc import ABC
-from typing import List
+from pathlib import Path
+from typing import List, Union
 from urllib.parse import urlparse
 
 import requests
 from langchain_core.documents import Document
 
 from langchain_community.document_loaders.base import BaseLoader
 from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
@@ -15,17 +17,17 @@
 class Docx2txtLoader(BaseLoader, ABC):
     """Load `DOCX` file using `docx2txt` and chunks at character level.
 
     Defaults to check for local file, but if the file is a web path, it will download it
     to a temporary file, and use that, then clean up the temporary file after completion
     """
 
-    def __init__(self, file_path: str):
+    def __init__(self, file_path: Union[str, Path]):
         """Initialize with file path."""
-        self.file_path = file_path
+        self.file_path = str(file_path)
         if "~" in self.file_path:
             self.file_path = os.path.expanduser(self.file_path)
 
         # If the file is a web path, download it to a temporary file, and use that
         if not os.path.isfile(self.file_path) and self._is_valid_url(self.file_path):
             r = requests.get(self.file_path)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/xml.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/xml.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,11 @@
 """Loads Microsoft Excel files."""
-from typing import Any, List
+
+from pathlib import Path
+from typing import Any, List, Union
 
 from langchain_community.document_loaders.unstructured import (
     UnstructuredFileLoader,
     validate_unstructured_version,
 )
 
 
@@ -28,16 +30,20 @@
 
     References
     ----------
     https://unstructured-io.github.io/unstructured/bricks.html#partition-xml
     """
 
     def __init__(
-        self, file_path: str, mode: str = "single", **unstructured_kwargs: Any
+        self,
+        file_path: Union[str, Path],
+        mode: str = "single",
+        **unstructured_kwargs: Any,
     ):
+        file_path = str(file_path)
         validate_unstructured_version(min_unstructured_version="0.6.7")
         super().__init__(file_path=file_path, mode=mode, **unstructured_kwargs)
 
     def _get_elements(self) -> List:
         from unstructured.partition.xml import partition_xml
 
         return partition_xml(filename=self.file_path, **self.unstructured_kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/xorbits.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/xorbits.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_loaders/youtube.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/youtube.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 """Loads YouTube transcript."""
+
 from __future__ import annotations
 
 import logging
+from enum import Enum
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Sequence, Union
 from urllib.parse import parse_qs, urlparse
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import root_validator
 from langchain_core.pydantic_v1.dataclasses import dataclass
@@ -135,34 +137,43 @@
 
     if len(video_id) != 11:  # Video IDs are 11 characters long
         return None
 
     return video_id
 
 
+class TranscriptFormat(Enum):
+    """Transcript format."""
+
+    TEXT = "text"
+    LINES = "lines"
+
+
 class YoutubeLoader(BaseLoader):
     """Load `YouTube` transcripts."""
 
     def __init__(
         self,
         video_id: str,
         add_video_info: bool = False,
         language: Union[str, Sequence[str]] = "en",
         translation: Optional[str] = None,
+        transcript_format: TranscriptFormat = TranscriptFormat.TEXT,
         continue_on_failure: bool = False,
     ):
         """Initialize with YouTube video ID."""
         self.video_id = video_id
         self.add_video_info = add_video_info
         self.language = language
         if isinstance(language, str):
             self.language = [language]
         else:
             self.language = language
         self.translation = translation
+        self.transcript_format = transcript_format
         self.continue_on_failure = continue_on_failure
 
     @staticmethod
     def extract_video_id(youtube_url: str) -> str:
         """Extract video id from common YT urls."""
         video_id = _parse_video_id(youtube_url)
         if not video_id:
@@ -210,17 +221,27 @@
             transcript = transcript_list.find_transcript(["en"])
 
         if self.translation is not None:
             transcript = transcript.translate(self.translation)
 
         transcript_pieces = transcript.fetch()
 
-        transcript = " ".join([t["text"].strip(" ") for t in transcript_pieces])
-
-        return [Document(page_content=transcript, metadata=metadata)]
+        if self.transcript_format == TranscriptFormat.TEXT:
+            transcript = " ".join([t["text"].strip(" ") for t in transcript_pieces])
+            return [Document(page_content=transcript, metadata=metadata)]
+        elif self.transcript_format == TranscriptFormat.LINES:
+            return [
+                Document(
+                    page_content=t["text"].strip(" "),
+                    metadata=dict((key, t[key]) for key in t if key != "text"),
+                )
+                for t in transcript_pieces
+            ]
+        else:
+            raise ValueError("Unknown transcript format.")
 
     def _get_video_info(self) -> dict:
         """Get important video information.
 
         Components are:
             - title
             - description
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_extract.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_extract.py`

 * *Files 16% similar despite different names*

```diff
@@ -59,20 +59,40 @@
         self.openai_api_key = openai_api_key or get_from_env(
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
 
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
-        raise NotImplementedError
+        """Extracts properties from text documents using doctran."""
+        try:
+            from doctran import Doctran, ExtractProperty
 
-    async def atransform_documents(
+            doctran = Doctran(
+                openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
+            )
+        except ImportError:
+            raise ImportError(
+                "Install doctran to use this parser. (pip install doctran)"
+            )
+        properties = [ExtractProperty(**property) for property in self.properties]
+        for d in documents:
+            doctran_doc = (
+                doctran.parse(content=d.page_content)
+                .extract(properties=properties)
+                .execute()
+            )
+
+            d.metadata["extracted_properties"] = doctran_doc.extracted_properties
+        return documents
+
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Extracts properties from text documents using doctran."""
         try:
             from doctran import Doctran, ExtractProperty
 
             doctran = Doctran(
@@ -81,14 +101,14 @@
         except ImportError:
             raise ImportError(
                 "Install doctran to use this parser. (pip install doctran)"
             )
         properties = [ExtractProperty(**property) for property in self.properties]
         for d in documents:
             doctran_doc = (
-                await doctran.parse(content=d.page_content)
+                doctran.parse(content=d.page_content)
                 .extract(properties=properties)
                 .execute()
             )
 
             d.metadata["extracted_properties"] = doctran_doc.extracted_properties
         return documents
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_qa.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_qa.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,35 +29,33 @@
         self.openai_api_key = openai_api_key or get_from_env(
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
 
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         raise NotImplementedError
 
-    async def atransform_documents(
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Extracts QA from text documents using doctran."""
         try:
             from doctran import Doctran
 
             doctran = Doctran(
                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
             )
         except ImportError:
             raise ImportError(
                 "Install doctran to use this parser. (pip install doctran)"
             )
         for d in documents:
-            doctran_doc = (
-                await doctran.parse(content=d.page_content).interrogate().execute()
-            )
+            doctran_doc = doctran.parse(content=d.page_content).interrogate().execute()
             questions_and_answers = doctran_doc.extracted_properties.get(
                 "questions_and_answers"
             )
             d.metadata["questions_and_answers"] = questions_and_answers
         return documents
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/doctran_text_translate.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/doctran_text_translate.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,20 +32,20 @@
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
         self.language = language
 
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         raise NotImplementedError
 
-    async def atransform_documents(
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Translates text documents using doctran."""
         try:
             from doctran import Doctran
 
             doctran = Doctran(
@@ -56,12 +56,12 @@
                 "Install doctran to use this parser. (pip install doctran)"
             )
         doctran_docs = [
             doctran.parse(content=doc.page_content, metadata=doc.metadata)
             for doc in documents
         ]
         for i, doc in enumerate(doctran_docs):
-            doctran_docs[i] = await doc.translate(language=self.language).execute()
+            doctran_docs[i] = doc.translate(language=self.language).execute()
         return [
             Document(page_content=doc.transformed_content, metadata=doc.metadata)
             for doc in doctran_docs
         ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/embeddings_redundant_filter.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/embeddings_redundant_filter.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Transform documents"""
+
 from typing import Any, Callable, List, Sequence
 
 import numpy as np
 from langchain_core.documents import BaseDocumentTransformer, Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Field
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/google_translate.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/google_translate.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,20 @@
 from typing import Any, Optional, Sequence
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import BaseDocumentTransformer, Document
 
 from langchain_community.utilities.vertexai import get_client_info
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.DocAIParser",
+)
 class GoogleTranslateTransformer(BaseDocumentTransformer):
     """Translate text documents using Google Cloud Translation."""
 
     def __init__(
         self,
         project_id: str,
         *,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/html2text.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/html2text.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/long_context_reorder.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/long_context_reorder.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Reorder documents"""
+
 from typing import Any, List, Sequence
 
 from langchain_core.documents import BaseDocumentTransformer, Document
 from langchain_core.pydantic_v1 import BaseModel
 
 
 def _litm_reordering(documents: List[Document]) -> List[Document]:
@@ -17,15 +18,17 @@
             reordered_result.append(value)
         else:
             reordered_result.insert(0, value)
     return reordered_result
 
 
 class LongContextReorder(BaseDocumentTransformer, BaseModel):
-    """Lost in the middle:
+    """Reorder long context.
+
+    Lost in the middle:
     Performance degrades when models must access relevant information
     in the middle of long contexts.
     See: https://arxiv.org/abs//2307.03172"""
 
     class Config:
         """Configuration for this pydantic object."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/nuclia_text_transform.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/nuclia_text_transform.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,16 @@
 
 from langchain_core.documents import BaseDocumentTransformer, Document
 
 from langchain_community.tools.nuclia.tool import NucliaUnderstandingAPI
 
 
 class NucliaTextTransformer(BaseDocumentTransformer):
-    """
+    """Nuclia Text Transformer.
+
     The Nuclia Understanding API splits into paragraphs and sentences,
     identifies entities, provides a summary of the text and generates
     embeddings for all sentences.
     """
 
     def __init__(self, nua: NucliaUnderstandingAPI):
         self.nua = nua
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/openai_functions.py` & `gigachain_community-0.2.0/langchain_community/document_transformers/openai_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Document transformers that use OpenAI Functions models"""
+
 from typing import Any, Dict, Optional, Sequence, Type, Union
 
 from langchain_core.documents import BaseDocumentTransformer, Document
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.prompts import ChatPromptTemplate
 from langchain_core.pydantic_v1 import BaseModel
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/document_transformers/xsl/html_chunks_with_headers.xslt` & `gigachain_community-0.2.0/langchain_community/document_transformers/xsl/html_chunks_with_headers.xslt`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/aleph_alpha.py` & `gigachain_community-0.2.0/langchain_community/embeddings/aleph_alpha.py`

 * *Files 3% similar despite different names*

```diff
@@ -94,15 +94,15 @@
                 host=values["host"],
                 hosting=values["hosting"],
                 request_timeout_seconds=values["request_timeout_seconds"],
                 total_retries=values["total_retries"],
                 nice=values["nice"],
             )
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import aleph_alpha_client python package. "
                 "Please install it with `pip install aleph_alpha_client`."
             )
 
         return values
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
@@ -117,15 +117,15 @@
         try:
             from aleph_alpha_client import (
                 Prompt,
                 SemanticEmbeddingRequest,
                 SemanticRepresentation,
             )
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import aleph_alpha_client python package. "
                 "Please install it with `pip install aleph_alpha_client`."
             )
         document_embeddings = []
 
         for text in texts:
             document_params = {
@@ -157,15 +157,15 @@
         try:
             from aleph_alpha_client import (
                 Prompt,
                 SemanticEmbeddingRequest,
                 SemanticRepresentation,
             )
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import aleph_alpha_client python package. "
                 "Please install it with `pip install aleph_alpha_client`."
             )
         symmetric_params = {
             "prompt": Prompt.from_text(text),
             "representation": SemanticRepresentation.Query,
             "compress_to_size": self.compress_to_size,
@@ -179,15 +179,15 @@
             request=symmetric_request, model=self.model
         )
 
         return symmetric_response.embedding
 
 
 class AlephAlphaSymmetricSemanticEmbedding(AlephAlphaAsymmetricSemanticEmbedding):
-    """The symmetric version of the Aleph Alpha's semantic embeddings.
+    """Symmetric version of the Aleph Alpha's semantic embeddings.
 
     The main difference is that here, both the documents and
     queries are embedded with a SemanticRepresentation.Symmetric
     Example:
         .. code-block:: python
 
             from aleph_alpha import AlephAlphaSymmetricSemanticEmbedding
@@ -205,15 +205,15 @@
         try:
             from aleph_alpha_client import (
                 Prompt,
                 SemanticEmbeddingRequest,
                 SemanticRepresentation,
             )
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import aleph_alpha_client python package. "
                 "Please install it with `pip install aleph_alpha_client`."
             )
         query_params = {
             "prompt": Prompt.from_text(text),
             "representation": SemanticRepresentation.Symmetric,
             "compress_to_size": self.compress_to_size,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/awa.py` & `gigachain_community-0.2.0/langchain_community/embeddings/awa.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/azure_openai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/azure_openai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,28 @@
 """Azure OpenAI embeddings wrapper."""
+
 from __future__ import annotations
 
 import os
 import warnings
 from typing import Callable, Dict, Optional, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.pydantic_v1 import Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.embeddings.openai import OpenAIEmbeddings
 from langchain_community.utils.openai import is_openai_v1
 
 
+@deprecated(
+    since="0.0.9",
+    removal="0.3.0",
+    alternative_import="langchain_openai.AzureOpenAIEmbeddings",
+)
 class AzureOpenAIEmbeddings(OpenAIEmbeddings):
     """`Azure OpenAI` Embeddings API."""
 
     azure_endpoint: Union[str, None] = None
     """Your Azure endpoint, including the resource.
 
         Automatically inferred from env var `AZURE_OPENAI_ENDPOINT` if not provided.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/baidu_qianfan_endpoint.py` & `gigachain_community-0.2.0/langchain_community/embeddings/baidu_qianfan_endpoint.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 from __future__ import annotations
 
 import logging
 from typing import Any, Dict, List, Optional
 
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, root_validator
+from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 logger = logging.getLogger(__name__)
 
 
 class QianfanEmbeddingsEndpoint(BaseModel, Embeddings):
     """`Baidu Qianfan Embeddings` embedding models."""
@@ -37,16 +37,20 @@
 
     endpoint: str = ""
     """Endpoint of the Qianfan Embedding, required if custom model used."""
 
     client: Any
     """Qianfan client"""
 
-    max_retries: int = 5
-    """Max reties times"""
+    init_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """init kwargs for qianfan client init, such as `query_per_second` which is 
+        associated with qianfan resource object to limit QPS"""
+
+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """extra params for model invoke using with `do`."""
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """
         Validate whether qianfan_ak and qianfan_sk in the environment variables or
         configuration file are available or not.
 
@@ -84,14 +88,15 @@
             )
         )
 
         try:
             import qianfan
 
             params = {
+                **values.get("init_kwargs", {}),
                 "model": values["model"],
             }
             if values["qianfan_ak"].get_secret_value() != "":
                 params["ak"] = values["qianfan_ak"].get_secret_value()
             if values["qianfan_sk"].get_secret_value() != "":
                 params["sk"] = values["qianfan_sk"].get_secret_value()
             if values["endpoint"] is not None and values["endpoint"] != "":
@@ -121,26 +126,26 @@
         """
         text_in_chunks = [
             texts[i : i + self.chunk_size]
             for i in range(0, len(texts), self.chunk_size)
         ]
         lst = []
         for chunk in text_in_chunks:
-            resp = self.client.do(texts=chunk)
+            resp = self.client.do(texts=chunk, **self.model_kwargs)
             lst.extend([res["embedding"] for res in resp["data"]])
         return lst
 
     async def aembed_query(self, text: str) -> List[float]:
         embeddings = await self.aembed_documents([text])
         return embeddings[0]
 
     async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
         text_in_chunks = [
             texts[i : i + self.chunk_size]
             for i in range(0, len(texts), self.chunk_size)
         ]
         lst = []
         for chunk in text_in_chunks:
-            resp = await self.client.ado(texts=chunk)
+            resp = await self.client.ado(texts=chunk, **self.model_kwargs)
             for res in resp["data"]:
                 lst.extend([res["embedding"]])
         return lst
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/bedrock.py` & `gigachain_community-0.2.0/langchain_community/embeddings/bedrock.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 import asyncio
 import json
 import os
-from functools import partial
 from typing import Any, Dict, List, Optional
 
+import numpy as np
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
+from langchain_core.runnables.config import run_in_executor
 
 
 class BedrockEmbeddings(BaseModel, Embeddings):
     """Bedrock embedding models.
 
     To authenticate, the AWS client uses the following methods to
     automatically load credentials:
@@ -60,14 +61,17 @@
 
     model_kwargs: Optional[Dict] = None
     """Keyword arguments to pass to the model."""
 
     endpoint_url: Optional[str] = None
     """Needed if you don't want to default to us-east-1 endpoint"""
 
+    normalize: bool = False
+    """Whether the embeddings should be normalized to unit vectors"""
+
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
@@ -91,23 +95,23 @@
 
             if values["endpoint_url"]:
                 client_params["endpoint_url"] = values["endpoint_url"]
 
             values["client"] = session.client("bedrock-runtime", **client_params)
 
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import boto3 python package. "
                 "Please install it with `pip install boto3`."
             )
         except Exception as e:
             raise ValueError(
                 "Could not load credentials to authenticate with AWS client. "
                 "Please check that credentials in the specified "
-                "profile name are valid."
+                f"profile name are valid. Bedrock error: {e}"
             ) from e
 
         return values
 
     def _embedding_func(self, text: str) -> List[float]:
         """Call out to Bedrock embedding endpoint."""
         # replace newlines, which can negatively affect performance.
@@ -141,53 +145,67 @@
                 return response_body.get("embeddings")[0]
             else:
                 # includes common provider == "amazon"
                 return response_body.get("embedding")
         except Exception as e:
             raise ValueError(f"Error raised by inference endpoint: {e}")
 
+    def _normalize_vector(self, embeddings: List[float]) -> List[float]:
+        """Normalize the embedding to a unit vector."""
+        emb = np.array(embeddings)
+        norm_emb = emb / np.linalg.norm(emb)
+        return norm_emb.tolist()
+
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Compute doc embeddings using a Bedrock model.
 
         Args:
             texts: The list of texts to embed
 
         Returns:
             List of embeddings, one for each text.
         """
         results = []
         for text in texts:
             response = self._embedding_func(text)
+
+            if self.normalize:
+                response = self._normalize_vector(response)
+
             results.append(response)
+
         return results
 
     def embed_query(self, text: str) -> List[float]:
         """Compute query embeddings using a Bedrock model.
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
-        return self._embedding_func(text)
+        embedding = self._embedding_func(text)
+
+        if self.normalize:
+            return self._normalize_vector(embedding)
+
+        return embedding
 
     async def aembed_query(self, text: str) -> List[float]:
         """Asynchronous compute query embeddings using a Bedrock model.
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
 
-        return await asyncio.get_running_loop().run_in_executor(
-            None, partial(self.embed_query, text)
-        )
+        return await run_in_executor(None, self.embed_query, text)
 
     async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
         """Asynchronous compute doc embeddings using a Bedrock model.
 
         Args:
             texts: The list of texts to embed
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/bookend.py` & `gigachain_community-0.2.0/langchain_community/embeddings/bookend.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/clarifai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/clarifai.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import logging
-from typing import Dict, List, Optional
+from typing import Any, Dict, List, Optional
 
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 
 logger = logging.getLogger(__name__)
 
 
 class ClarifaiEmbeddings(BaseModel, Embeddings):
     """Clarifai embedding models.
 
@@ -19,100 +18,97 @@
         .. code-block:: python
 
             from langchain_community.embeddings import ClarifaiEmbeddings
             clarifai = ClarifaiEmbeddings(user_id=USER_ID,
                                           app_id=APP_ID,
                                           model_id=MODEL_ID)
                              (or)
-            clarifai_llm = Clarifai(model_url=EXAMPLE_URL)
+            Example_URL = "https://clarifai.com/clarifai/main/models/BAAI-bge-base-en-v15"
+            clarifai = ClarifaiEmbeddings(model_url=EXAMPLE_URL)
     """
 
     model_url: Optional[str] = None
     """Model url to use."""
     model_id: Optional[str] = None
     """Model id to use."""
     model_version_id: Optional[str] = None
     """Model version id to use."""
     app_id: Optional[str] = None
     """Clarifai application id to use."""
     user_id: Optional[str] = None
     """Clarifai user id to use."""
-    pat: Optional[str] = None
+    pat: Optional[str] = Field(default=None, exclude=True)
     """Clarifai personal access token to use."""
+    token: Optional[str] = Field(default=None, exclude=True)
+    """Clarifai session token to use."""
+    model: Any = Field(default=None, exclude=True)  #: :meta private:
     api_base: str = "https://api.clarifai.com"
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that we have all required info to access Clarifai
         platform and python package exists in environment."""
 
-        values["pat"] = get_from_dict_or_env(values, "pat", "CLARIFAI_PAT")
+        try:
+            from clarifai.client.model import Model
+        except ImportError:
+            raise ImportError(
+                "Could not import clarifai python package. "
+                "Please install it with `pip install clarifai`."
+            )
         user_id = values.get("user_id")
         app_id = values.get("app_id")
         model_id = values.get("model_id")
+        model_version_id = values.get("model_version_id")
         model_url = values.get("model_url")
-
-        if model_url is not None and model_id is not None:
-            raise ValueError("Please provide either model_url or model_id, not both.")
-
-        if model_url is None and model_id is None:
-            raise ValueError("Please provide one of model_url or model_id.")
-
-        if model_url is None and model_id is not None:
-            if user_id is None or app_id is None:
-                raise ValueError("Please provide a user_id and app_id.")
+        api_base = values.get("api_base")
+        pat = values.get("pat")
+        token = values.get("token")
+
+        values["model"] = Model(
+            url=model_url,
+            app_id=app_id,
+            user_id=user_id,
+            model_version=dict(id=model_version_id),
+            pat=pat,
+            token=token,
+            model_id=model_id,
+            base_url=api_base,
+        )
 
         return values
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Call out to Clarifai's embedding models.
 
         Args:
             texts: The list of texts to embed.
 
         Returns:
             List of embeddings, one for each text.
         """
-        try:
-            from clarifai.client.input import Inputs
-            from clarifai.client.model import Model
-        except ImportError:
-            raise ImportError(
-                "Could not import clarifai python package. "
-                "Please install it with `pip install clarifai`."
-            )
-        if self.pat is not None:
-            pat = self.pat
-        if self.model_url is not None:
-            _model_init = Model(url=self.model_url, pat=pat)
-        else:
-            _model_init = Model(
-                model_id=self.model_id,
-                user_id=self.user_id,
-                app_id=self.app_id,
-                pat=pat,
-            )
+        from clarifai.client.input import Inputs
 
-        input_obj = Inputs(pat=pat)
+        input_obj = Inputs.from_auth_helper(self.model.auth_helper)
         batch_size = 32
         embeddings = []
 
         try:
             for i in range(0, len(texts), batch_size):
                 batch = texts[i : i + batch_size]
                 input_batch = [
                     input_obj.get_text_input(input_id=str(id), raw_text=inp)
                     for id, inp in enumerate(batch)
                 ]
-                predict_response = _model_init.predict(input_batch)
+                predict_response = self.model.predict(input_batch)
                 embeddings.extend(
                     [
                         list(output.data.embeddings[0].vector)
                         for output in predict_response.outputs
                     ]
                 )
 
@@ -126,35 +122,17 @@
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
-        try:
-            from clarifai.client.model import Model
-        except ImportError:
-            raise ImportError(
-                "Could not import clarifai python package. "
-                "Please install it with `pip install clarifai`."
-            )
-        if self.pat is not None:
-            pat = self.pat
-        if self.model_url is not None:
-            _model_init = Model(url=self.model_url, pat=pat)
-        else:
-            _model_init = Model(
-                model_id=self.model_id,
-                user_id=self.user_id,
-                app_id=self.app_id,
-                pat=pat,
-            )
 
         try:
-            predict_response = _model_init.predict_by_bytes(
+            predict_response = self.model.predict_by_bytes(
                 bytes(text, "utf-8"), input_type="text"
             )
             embeddings = [
                 list(op.data.embeddings[0].vector) for op in predict_response.outputs
             ]
 
         except Exception as e:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/cloudflare_workersai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/cloudflare_workersai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/cohere.py` & `gigachain_community-0.2.0/langchain_community/embeddings/cohere.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,22 @@
 from typing import Any, Dict, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
+from langchain_community.llms.cohere import _create_retry_decorator
 
+
+@deprecated(
+    since="0.0.30",
+    removal="0.3.0",
+    alternative_import="langchain_cohere.CohereEmbeddings",
+)
 class CohereEmbeddings(BaseModel, Embeddings):
     """Cohere embedding models.
 
     To use, you should have the ``cohere`` python package installed, and the
     environment variable ``COHERE_API_KEY`` set with your API key or pass it
     as a named parameter to the constructor.
 
@@ -30,15 +38,15 @@
     """Model name to use."""
 
     truncate: Optional[str] = None
     """Truncate embeddings that are too long from start or end ("NONE"|"START"|"END")"""
 
     cohere_api_key: Optional[str] = None
 
-    max_retries: Optional[int] = None
+    max_retries: int = 3
     """Maximum number of retries to make when generating."""
     request_timeout: Optional[float] = None
     """Timeout in seconds for the Cohere API request."""
     user_agent: str = "langchain"
     """Identifier for the application making the request."""
 
     class Config:
@@ -48,59 +56,78 @@
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
         cohere_api_key = get_from_dict_or_env(
             values, "cohere_api_key", "COHERE_API_KEY"
         )
-        max_retries = values.get("max_retries")
         request_timeout = values.get("request_timeout")
 
         try:
             import cohere
 
             client_name = values["user_agent"]
             values["client"] = cohere.Client(
                 cohere_api_key,
-                max_retries=max_retries,
                 timeout=request_timeout,
                 client_name=client_name,
             )
             values["async_client"] = cohere.AsyncClient(
                 cohere_api_key,
-                max_retries=max_retries,
                 timeout=request_timeout,
                 client_name=client_name,
             )
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import cohere python package. "
                 "Please install it with `pip install cohere`."
             )
         return values
 
+    def embed_with_retry(self, **kwargs: Any) -> Any:
+        """Use tenacity to retry the embed call."""
+        retry_decorator = _create_retry_decorator(self.max_retries)
+
+        @retry_decorator
+        def _embed_with_retry(**kwargs: Any) -> Any:
+            return self.client.embed(**kwargs)
+
+        return _embed_with_retry(**kwargs)
+
+    def aembed_with_retry(self, **kwargs: Any) -> Any:
+        """Use tenacity to retry the embed call."""
+        retry_decorator = _create_retry_decorator(self.max_retries)
+
+        @retry_decorator
+        async def _embed_with_retry(**kwargs: Any) -> Any:
+            return await self.async_client.embed(**kwargs)
+
+        return _embed_with_retry(**kwargs)
+
     def embed(
         self, texts: List[str], *, input_type: Optional[str] = None
     ) -> List[List[float]]:
-        embeddings = self.client.embed(
+        embeddings = self.embed_with_retry(
             model=self.model,
             texts=texts,
             input_type=input_type,
             truncate=self.truncate,
         ).embeddings
         return [list(map(float, e)) for e in embeddings]
 
     async def aembed(
         self, texts: List[str], *, input_type: Optional[str] = None
     ) -> List[List[float]]:
-        embeddings = await self.async_client.embed(
-            model=self.model,
-            texts=texts,
-            input_type=input_type,
-            truncate=self.truncate,
+        embeddings = (
+            await self.aembed_with_retry(
+                model=self.model,
+                texts=texts,
+                input_type=input_type,
+                truncate=self.truncate,
+            )
         ).embeddings
         return [list(map(float, e)) for e in embeddings]
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Embed a list of document texts.
 
         Args:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/dashscope.py` & `gigachain_community-0.2.0/langchain_community/embeddings/dashscope.py`

 * *Files 4% similar despite different names*

```diff
@@ -41,28 +41,35 @@
 
 def embed_with_retry(embeddings: DashScopeEmbeddings, **kwargs: Any) -> Any:
     """Use tenacity to retry the embedding call."""
     retry_decorator = _create_retry_decorator(embeddings)
 
     @retry_decorator
     def _embed_with_retry(**kwargs: Any) -> Any:
-        resp = embeddings.client.call(**kwargs)
-        if resp.status_code == 200:
-            return resp.output["embeddings"]
-        elif resp.status_code in [400, 401]:
-            raise ValueError(
-                f"status_code: {resp.status_code} \n "
-                f"code: {resp.code} \n message: {resp.message}"
-            )
-        else:
-            raise HTTPError(
-                f"HTTP error occurred: status_code: {resp.status_code} \n "
-                f"code: {resp.code} \n message: {resp.message}",
-                response=resp,
-            )
+        result = []
+        i = 0
+        input_data = kwargs["input"]
+        while i < len(input_data):
+            kwargs["input"] = input_data[i : i + 25]
+            resp = embeddings.client.call(**kwargs)
+            if resp.status_code == 200:
+                result += resp.output["embeddings"]
+            elif resp.status_code in [400, 401]:
+                raise ValueError(
+                    f"status_code: {resp.status_code} \n "
+                    f"code: {resp.code} \n message: {resp.message}"
+                )
+            else:
+                raise HTTPError(
+                    f"HTTP error occurred: status_code: {resp.status_code} \n "
+                    f"code: {resp.code} \n message: {resp.message}",
+                    response=resp,
+                )
+            i += 25
+        return result
 
     return _embed_with_retry(**kwargs)
 
 
 class DashScopeEmbeddings(BaseModel, Embeddings):
     """DashScope embedding models.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/databricks.py` & `gigachain_community-0.2.0/langchain_community/embeddings/databricks.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 
 def _chunk(texts: List[str], size: int) -> Iterator[List[str]]:
     for i in range(0, len(texts), size):
         yield texts[i : i + size]
 
 
 class DatabricksEmbeddings(MlflowEmbeddings):
-    """Wrapper around embeddings LLMs in Databricks.
+    """Databricks embeddings.
 
     To use, you should have the ``mlflow`` python package installed.
-    For more information, see https://mlflow.org/docs/latest/llms/deployments/databricks.html.
+    For more information, see https://mlflow.org/docs/latest/llms/deployments.
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import DatabricksEmbeddings
 
             embeddings = DatabricksEmbeddings(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/deepinfra.py` & `gigachain_community-0.2.0/langchain_community/embeddings/deepinfra.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 import requests
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 DEFAULT_MODEL_ID = "sentence-transformers/clip-ViT-B-32"
+MAX_BATCH_SIZE = 1024
 
 
 class DeepInfraEmbeddings(BaseModel, Embeddings):
     """Deep Infra's embedding inference service.
 
     To use, you should have the
     environment variable ``DEEPINFRA_API_TOKEN`` set with your API token, or pass
@@ -43,16 +44,19 @@
     """whether to normalize the computed embeddings"""
     embed_instruction: str = "passage: "
     """Instruction used to embed documents."""
     query_instruction: str = "query: "
     """Instruction used to embed the query."""
     model_kwargs: Optional[dict] = None
     """Other model keyword args"""
-
     deepinfra_api_token: Optional[str] = None
+    """API token for Deep Infra. If not provided, the token is 
+    fetched from the environment variable 'DEEPINFRA_API_TOKEN'."""
+    batch_size: int = MAX_BATCH_SIZE
+    """Batch size for embedding requests."""
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
@@ -99,23 +103,34 @@
                 f"Error raised by inference API: {e}.\nResponse: {res.text}"
             )
 
         return embeddings
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Embed documents using a Deep Infra deployed embedding model.
+        For larger batches, the input list of texts is chunked into smaller
+        batches to avoid exceeding the maximum request size.
 
         Args:
             texts: The list of texts to embed.
 
         Returns:
             List of embeddings, one for each text.
         """
+
+        embeddings = []
         instruction_pairs = [f"{self.embed_instruction}{text}" for text in texts]
-        embeddings = self._embed(instruction_pairs)
+
+        chunks = [
+            instruction_pairs[i : i + self.batch_size]
+            for i in range(0, len(instruction_pairs), self.batch_size)
+        ]
+        for chunk in chunks:
+            embeddings += self._embed(chunk)
+
         return embeddings
 
     def embed_query(self, text: str) -> List[float]:
         """Embed a query using a Deep Infra deployed embedding model.
 
         Args:
             text: The text to embed.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/edenai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/edenai.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,23 +1,29 @@
 from typing import Any, Dict, List, Optional
 
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import (
+    BaseModel,
+    Extra,
+    Field,
+    SecretStr,
+    root_validator,
+)
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.utilities.requests import Requests
 
 
 class EdenAiEmbeddings(BaseModel, Embeddings):
     """EdenAI embedding.
     environment variable ``EDENAI_API_KEY`` set with your API key, or pass
     it as a named parameter.
     """
 
-    edenai_api_key: Optional[str] = Field(None, description="EdenAI API Token")
+    edenai_api_key: Optional[SecretStr] = Field(None, description="EdenAI API Token")
 
     provider: str = "openai"
     """embedding provider to use (eg: openai,google etc.)"""
 
     model: Optional[str] = None
     """
     model name for above provider (eg: 'gpt-3.5-turbo-instruct' for openai)
@@ -28,16 +34,16 @@
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key exists in environment."""
-        values["edenai_api_key"] = get_from_dict_or_env(
-            values, "edenai_api_key", "EDENAI_API_KEY"
+        values["edenai_api_key"] = convert_to_secret_str(
+            get_from_dict_or_env(values, "edenai_api_key", "EDENAI_API_KEY")
         )
         return values
 
     @staticmethod
     def get_user_agent() -> str:
         from langchain_community import __version__
 
@@ -46,15 +52,15 @@
     def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
         """Compute embeddings using EdenAi api."""
         url = "https://api.edenai.run/v2/text/embeddings"
 
         headers = {
             "accept": "application/json",
             "content-type": "application/json",
-            "authorization": f"Bearer {self.edenai_api_key}",
+            "authorization": f"Bearer {self.edenai_api_key.get_secret_value()}",  # type: ignore[union-attr]
             "User-Agent": self.get_user_agent(),
         }
 
         payload: Dict[str, Any] = {"texts": texts, "providers": self.provider}
 
         if self.model is not None:
             payload["settings"] = {self.provider: self.model}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/elasticsearch.py` & `gigachain_community-0.2.0/langchain_community/embeddings/elasticsearch.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,24 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, List, Optional
 
+from langchain_core._api import deprecated
 from langchain_core.utils import get_from_env
 
 if TYPE_CHECKING:
     from elasticsearch import Elasticsearch
     from elasticsearch.client import MlClient
 
 from langchain_core.embeddings import Embeddings
 
 
+@deprecated(
+    "0.1.11", alternative="Use class in langchain-elasticsearch package", pending=True
+)
 class ElasticsearchEmbeddings(Embeddings):
     """Elasticsearch embedding models.
 
     This class provides an interface to generate embeddings using a model deployed
     in an Elasticsearch cluster. It requires an Elasticsearch connection object
     and the model_id of the model deployed in the cluster.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/embaas.py` & `gigachain_community-0.2.0/langchain_community/embeddings/embaas.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Any, Dict, List, Mapping, Optional
 
 import requests
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from requests.adapters import HTTPAdapter, Retry
 from typing_extensions import NotRequired, TypedDict
 
 # Currently supported maximum batch size for embedding requests
 MAX_BATCH_SIZE = 256
 EMBAAS_API_URL = "https://api.embaas.io/v1/embeddings/"
 
@@ -46,30 +46,30 @@
 
     model: str = "e5-large-v2"
     """The model used for embeddings."""
     instruction: Optional[str] = None
     """Instruction used for domain-specific embeddings."""
     api_url: str = EMBAAS_API_URL
     """The URL for the embaas embeddings API."""
-    embaas_api_key: Optional[str] = None
+    embaas_api_key: Optional[SecretStr] = None
     """max number of retries for requests"""
     max_retries: Optional[int] = 3
     """request timeout in seconds"""
     timeout: Optional[int] = 30
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        embaas_api_key = get_from_dict_or_env(
-            values, "embaas_api_key", "EMBAAS_API_KEY"
+        embaas_api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "embaas_api_key", "EMBAAS_API_KEY")
         )
         values["embaas_api_key"] = embaas_api_key
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying params."""
@@ -81,15 +81,15 @@
         if self.instruction:
             payload["instruction"] = self.instruction
         return payload
 
     def _handle_request(self, payload: EmbaasEmbeddingsPayload) -> List[List[float]]:
         """Sends a request to the Embaas API and handles the response."""
         headers = {
-            "Authorization": f"Bearer {self.embaas_api_key}",
+            "Authorization": f"Bearer {self.embaas_api_key.get_secret_value()}",  # type: ignore[union-attr]
             "Content-Type": "application/json",
         }
 
         session = requests.Session()
         retries = Retry(
             total=self.max_retries,
             backoff_factor=0.5,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/ernie.py` & `gigachain_community-0.2.0/langchain_community/embeddings/ernie.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 import asyncio
 import logging
 import threading
-from functools import partial
 from typing import Dict, List, Optional
 
 import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, root_validator
+from langchain_core.runnables.config import run_in_executor
 from langchain_core.utils import get_from_dict_or_env
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.13",
+    alternative="langchain_community.embeddings.QianfanEmbeddingsEndpoint",
+)
 class ErnieEmbeddings(BaseModel, Embeddings):
     """`Ernie Embeddings V1` embedding models."""
 
     ernie_api_base: Optional[str] = None
     ernie_client_id: Optional[str] = None
     ernie_client_secret: Optional[str] = None
     access_token: Optional[str] = None
@@ -130,17 +135,15 @@
         Args:
             text: The text to embed.
 
         Returns:
             List[float]: Embeddings for the text.
         """
 
-        return await asyncio.get_running_loop().run_in_executor(
-            None, partial(self.embed_query, text)
-        )
+        return await run_in_executor(None, self.embed_query, text)
 
     async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
         """Asynchronous Embed search docs.
 
         Args:
             texts: The list of texts to embed
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/fake.py` & `gigachain_community-0.2.0/langchain_community/embeddings/fake.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/fastembed.py` & `gigachain_community-0.2.0/langchain_community/embeddings/fastembed.py`

 * *Files 5% similar despite different names*

```diff
@@ -40,46 +40,58 @@
     threads: Optional[int]
     """The number of threads single onnxruntime session can use.
     Defaults to None
     """
 
     doc_embed_type: Literal["default", "passage"] = "default"
     """Type of embedding to use for documents
-    "default": Uses FastEmbed's default embedding method
-    "passage": Prefixes the text with "passage" before embedding.
+    The available options are: "default" and "passage"
     """
 
     _model: Any  # : :meta private:
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that FastEmbed has been installed."""
+        model_name = values.get("model_name")
+        max_length = values.get("max_length")
+        cache_dir = values.get("cache_dir")
+        threads = values.get("threads")
+
         try:
-            from fastembed.embedding import FlagEmbedding
+            # >= v0.2.0
+            from fastembed import TextEmbedding
 
-            model_name = values.get("model_name")
-            max_length = values.get("max_length")
-            cache_dir = values.get("cache_dir")
-            threads = values.get("threads")
-            values["_model"] = FlagEmbedding(
+            values["_model"] = TextEmbedding(
                 model_name=model_name,
                 max_length=max_length,
                 cache_dir=cache_dir,
                 threads=threads,
             )
         except ImportError as ie:
-            raise ImportError(
-                "Could not import 'fastembed' Python package. "
-                "Please install it with `pip install fastembed`."
-            ) from ie
+            try:
+                # < v0.2.0
+                from fastembed.embedding import FlagEmbedding
+
+                values["_model"] = FlagEmbedding(
+                    model_name=model_name,
+                    max_length=max_length,
+                    cache_dir=cache_dir,
+                    threads=threads,
+                )
+            except ImportError:
+                raise ImportError(
+                    "Could not import 'fastembed' Python package. "
+                    "Please install it with `pip install fastembed`."
+                ) from ie
         return values
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Generate embeddings for documents using FastEmbed.
 
         Args:
             texts: The list of texts to embed.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/gigachat.py` & `gigachain_community-0.2.0/langchain_community/embeddings/premai.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,149 +1,130 @@
-"""Wrapper around YandexGPT embedding models."""
 from __future__ import annotations
 
 import logging
-from functools import cached_property
-from typing import Any, Dict, List, Optional
+from typing import Any, Callable, Dict, List, Optional, Union
 
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, root_validator
-
-from tqdm import tqdm
-import time
+from langchain_core.language_models.llms import create_base_retry_decorator
+from langchain_core.pydantic_v1 import BaseModel, SecretStr, root_validator
+from langchain_core.utils import get_from_dict_or_env
 
 logger = logging.getLogger(__name__)
 
 
-class GigaChatEmbeddings(BaseModel, Embeddings):
-    """GigaChat Embeddings models.
-    To use, you should initialize GigaChat from chat_models
-
-    Example:
-        .. code-block:: python
-            from langchain_community.embeddings.gigachat import GigaChatEmbeddings
+class PremAIEmbeddings(BaseModel, Embeddings):
+    """Prem's Embedding APIs"""
 
-            embeddings = GigaChatEmbeddings(credentials=..., verify_ssl_certs=False)
-    """
+    project_id: int
+    """The project ID in which the experiments or deployments are carried out. 
+    You can find all your projects here: https://app.premai.io/projects/"""
 
-    one_by_one_mode: bool = True
-    """ Send texts one-by-one to server (to increse token limit)"""
+    premai_api_key: Optional[SecretStr] = None
+    """Prem AI API Key. Get it here: https://app.premai.io/api_keys/"""
 
-    base_url: Optional[str] = None
-    """ Base API URL """
-    auth_url: Optional[str] = None
-    """ Auth URL """
-    credentials: Optional[str] = None
-    """ Auth Token """
-    scope: Optional[str] = None
-    """ Permission scope for access token """
-
-    access_token: Optional[str] = None
-    """ Access token for GigaChat """
-
-    model: Optional[str] = None
-    """Model name to use."""
-    user: Optional[str] = None
-    """ Username for authenticate """
-    password: Optional[str] = None
-    """ Password for authenticate """
-
-    timeout: Optional[float] = None
-    """ Timeout for request """
-    verify_ssl_certs: Optional[bool] = None
-    """ Check certificates for all requests """
-
-    _debug_delay: float = 0
-    """ Debug timeout for limit rps to server"""
-
-    ca_bundle_file: Optional[str] = None
-    cert_file: Optional[str] = None
-    key_file: Optional[str] = None
-    key_file_password: Optional[str] = None
-    # Support for connection to GigaChat through SSL certificates
-
-    @cached_property
-    def _client(self) -> Any:
-        """Returns GigaChat API client"""
-        import gigachat
-
-        return gigachat.GigaChat(
-            base_url=self.base_url,
-            auth_url=self.auth_url,
-            credentials=self.credentials,
-            scope=self.scope,
-            access_token=self.access_token,
-            model=self.model,
-            user=self.user,
-            password=self.password,
-            timeout=self.timeout,
-            verify_ssl_certs=self.verify_ssl_certs,
-            ca_bundle_file=self.ca_bundle_file,
-            cert_file=self.cert_file,
-            key_file=self.key_file,
-            key_file_password=self.key_file_password,
-        )
+    model: str
+    """The Embedding model to choose from"""
+
+    show_progress_bar: bool = False
+    """Whether to show a tqdm progress bar. Must have `tqdm` installed."""
+
+    max_retries: int = 1
+    """Max number of retries for tenacity"""
+
+    client: Any
 
     @root_validator()
-    def validate_environment(cls, values: Dict) -> Dict:
-        """Validate authenticate data in environment and python package is installed."""
+    def validate_environments(cls, values: Dict) -> Dict:
+        """Validate that the package is installed and that the API token is valid"""
         try:
-            import gigachat  # noqa: F401
-        except ImportError:
+            from premai import Prem
+        except ImportError as error:
             raise ImportError(
-                "Could not import gigachat python package. "
-                "Please install it with `pip install gigachat`."
-            )
-        fields = set(cls.__fields__.keys())
-        fields.add("profanity_check")
-        diff = set(values.keys()) - fields
-        if diff:
-            logger.warning(f"Extra fields {diff} in GigaChat class")
-        if "profanity" in fields:
-            logger.warning(
-                "Profanity field is deprecated. Use 'profanity_check' instead."
+                "Could not import Prem Python package."
+                "Please install it with: `pip install premai`"
+            ) from error
+
+        try:
+            premai_api_key = get_from_dict_or_env(
+                values, "premai_api_key", "PREMAI_API_KEY"
             )
-            if values.get("profanity_check") is None:
-                values["profanity_check"] = values.get("profanity")
+            values["client"] = Prem(api_key=premai_api_key)
+        except Exception as error:
+            raise ValueError("Your API Key is incorrect. Please try again.") from error
         return values
 
-    def embed_documents(
-        self, texts: List[str], model="Embeddings"
-    ) -> List[List[float]]:
-        """Embed documents using a GigaChat embeddings models.
-
-        Args:
-            texts: The list of texts to embed.
-
-        Returns:
-            List of embeddings, one for each text.
-        """
-        if self.one_by_one_mode:
-            result: List[List[float]] = []
-            if self._debug_delay == 0:
-                for text in texts:
-                    for embedding in self._client.embeddings(texts=[text], model=model).data:
-                        result.append(embedding.embedding)
-            else:
-                for text in texts:
-                    time.sleep(self._debug_delay)
-                    for embedding in tqdm(self._client.embeddings(texts=[text], model=model).data):
-                        result.append(embedding.embedding)
-            return result
-        else:
-            return [
-                embedding.embedding
-                for embedding in self._client.embeddings(texts=texts, model=model).data
-            ]
-
-        # return _embed_with_retry(self, texts=texts)
-
-    def embed_query(self, text: str, model="Embeddings") -> List[float]:
-        """Embed a query using a GigaChat embeddings models.
-
-        Args:
-            text: The text to embed.
-
-        Returns:
-            Embeddings for the text.
-        """
-        return self.embed_documents(texts=[text], model=model)[0]
+    def embed_query(self, text: str) -> List[float]:
+        """Embed query text"""
+        embeddings = embed_with_retry(
+            self, model=self.model, project_id=self.project_id, input=text
+        )
+        return embeddings.data[0].embedding
+
+    def embed_documents(self, texts: List[str]) -> List[List[float]]:
+        embeddings = embed_with_retry(
+            self, model=self.model, project_id=self.project_id, input=texts
+        ).data
+
+        return [embedding.embedding for embedding in embeddings]
+
+
+def create_prem_retry_decorator(
+    embedder: PremAIEmbeddings,
+    *,
+    max_retries: int = 1,
+) -> Callable[[Any], Any]:
+    """Create a retry decorator for PremAIEmbeddings.
+
+    Args:
+        embedder (PremAIEmbeddings): The PremAIEmbeddings instance
+        max_retries (int): The maximum number of retries
+
+    Returns:
+        Callable[[Any], Any]: The retry decorator
+    """
+    import premai.models
+
+    errors = [
+        premai.models.api_response_validation_error.APIResponseValidationError,
+        premai.models.conflict_error.ConflictError,
+        premai.models.model_not_found_error.ModelNotFoundError,
+        premai.models.permission_denied_error.PermissionDeniedError,
+        premai.models.provider_api_connection_error.ProviderAPIConnectionError,
+        premai.models.provider_api_status_error.ProviderAPIStatusError,
+        premai.models.provider_api_timeout_error.ProviderAPITimeoutError,
+        premai.models.provider_internal_server_error.ProviderInternalServerError,
+        premai.models.provider_not_found_error.ProviderNotFoundError,
+        premai.models.rate_limit_error.RateLimitError,
+        premai.models.unprocessable_entity_error.UnprocessableEntityError,
+        premai.models.validation_error.ValidationError,
+    ]
+
+    decorator = create_base_retry_decorator(
+        error_types=errors, max_retries=max_retries, run_manager=None
+    )
+    return decorator
+
+
+def embed_with_retry(
+    embedder: PremAIEmbeddings,
+    model: str,
+    project_id: int,
+    input: Union[str, List[str]],
+) -> Any:
+    """Using tenacity for retry in embedding calls"""
+    retry_decorator = create_prem_retry_decorator(
+        embedder, max_retries=embedder.max_retries
+    )
+
+    @retry_decorator
+    def _embed_with_retry(
+        embedder: PremAIEmbeddings,
+        project_id: int,
+        model: str,
+        input: Union[str, List[str]],
+    ) -> Any:
+        embedding_response = embedder.client.embeddings.create(
+            project_id=project_id, model=model, input=input
+        )
+        return embedding_response
+
+    return _embed_with_retry(embedder, project_id=project_id, model=model, input=input)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/google_palm.py` & `gigachain_community-0.2.0/langchain_community/embeddings/google_palm.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/gpt4all.py` & `gigachain_community-0.2.0/langchain_community/embeddings/gpt4all.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 
 
 class GPT4AllEmbeddings(BaseModel, Embeddings):
     """GPT4All embedding models.
@@ -10,27 +10,41 @@
     To use, you should have the gpt4all python package installed
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import GPT4AllEmbeddings
 
-            embeddings = GPT4AllEmbeddings()
+            model_name = "all-MiniLM-L6-v2.gguf2.f16.gguf"
+            gpt4all_kwargs = {'allow_download': 'True'}
+            embeddings = GPT4AllEmbeddings(
+                model_name=model_name,
+                gpt4all_kwargs=gpt4all_kwargs
+            )
     """
 
+    model_name: str
+    n_threads: Optional[int] = None
+    device: Optional[str] = "cpu"
+    gpt4all_kwargs: Optional[dict] = {}
     client: Any  #: :meta private:
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that GPT4All library is installed."""
 
         try:
             from gpt4all import Embed4All
 
-            values["client"] = Embed4All()
+            values["client"] = Embed4All(
+                model_name=values["model_name"],
+                n_threads=values.get("n_threads"),
+                device=values.get("device"),
+                **values.get("gpt4all_kwargs"),
+            )
         except ImportError:
             raise ImportError(
                 "Could not import gpt4all library. "
                 "Please install the gpt4all library to "
                 "use this embedding model: pip install gpt4all"
             )
         return values
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/gradient_ai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/gradient_ai.py`

 * *Files 1% similar despite different names*

```diff
@@ -158,9 +158,9 @@
     """Deprecated, TinyAsyncGradientEmbeddingClient was removed.
 
     This class is just for backwards compatibility with older versions
     of langchain_community.
     It might be entirely removed in the future.
     """
 
-    def __init__(self, *args, **kwargs) -> None:
+    def __init__(self, *args, **kwargs) -> None:  # type: ignore[no-untyped-def]
         raise ValueError("Deprecated,TinyAsyncGradientEmbeddingClient was removed.")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/huggingface.py` & `gigachain_community-0.2.0/langchain_community/embeddings/huggingface.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Any, Dict, List, Optional
 
 import requests
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, Extra, Field
+from langchain_core.pydantic_v1 import BaseModel, Extra, Field, SecretStr
 
 DEFAULT_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
 DEFAULT_INSTRUCT_MODEL = "hkunlp/instructor-large"
 DEFAULT_BGE_MODEL = "BAAI/bge-large-en"
 DEFAULT_EMBED_INSTRUCTION = "Represent the document for retrieval: "
 DEFAULT_QUERY_INSTRUCTION = (
     "Represent the question for retrieving supporting documents: "
@@ -40,19 +40,26 @@
     client: Any  #: :meta private:
     model_name: str = DEFAULT_MODEL_NAME
     """Model name to use."""
     cache_folder: Optional[str] = None
     """Path to store models. 
     Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
-    """Keyword arguments to pass to the model."""
+    """Keyword arguments to pass to the Sentence Transformer model, such as `device`,
+    `prompts`, `default_prompt_name`, `revision`, `trust_remote_code`, or `token`.
+    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer"""
     encode_kwargs: Dict[str, Any] = Field(default_factory=dict)
-    """Keyword arguments to pass when calling the `encode` method of the model."""
+    """Keyword arguments to pass when calling the `encode` method of the Sentence
+    Transformer model, such as `prompt_name`, `prompt`, `batch_size`, `precision`,
+    `normalize_embeddings`, and more.
+    See also the Sentence Transformer documentation: https://sbert.net/docs/package_reference/SentenceTransformer.html#sentence_transformers.SentenceTransformer.encode"""
     multi_process: bool = False
     """Run encode() on multiple GPUs."""
+    show_progress: bool = False
+    """Whether to show a progress bar."""
 
     def __init__(self, **kwargs: Any):
         """Initialize the sentence_transformer."""
         super().__init__(**kwargs)
         try:
             import sentence_transformers
 
@@ -84,15 +91,17 @@
 
         texts = list(map(lambda x: x.replace("\n", " "), texts))
         if self.multi_process:
             pool = self.client.start_multi_process_pool()
             embeddings = self.client.encode_multi_process(texts, pool)
             sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)
         else:
-            embeddings = self.client.encode(texts, **self.encode_kwargs)
+            embeddings = self.client.encode(
+                texts, show_progress_bar=self.show_progress, **self.encode_kwargs
+            )
 
         return embeddings.tolist()
 
     def embed_query(self, text: str) -> List[float]:
         """Compute query embeddings using a HuggingFace transformer model.
 
         Args:
@@ -181,45 +190,66 @@
         """
         instruction_pair = [self.query_instruction, text]
         embedding = self.client.encode([instruction_pair], **self.encode_kwargs)[0]
         return embedding.tolist()
 
 
 class HuggingFaceBgeEmbeddings(BaseModel, Embeddings):
-    """HuggingFace BGE sentence_transformers embedding models.
+    """HuggingFace sentence_transformers embedding models.
 
     To use, you should have the ``sentence_transformers`` python package installed.
+    To use Nomic, make sure the version of ``sentence_transformers`` >= 2.3.0.
 
-    Example:
+    Bge Example:
         .. code-block:: python
 
             from langchain_community.embeddings import HuggingFaceBgeEmbeddings
 
             model_name = "BAAI/bge-large-en"
             model_kwargs = {'device': 'cpu'}
             encode_kwargs = {'normalize_embeddings': True}
             hf = HuggingFaceBgeEmbeddings(
                 model_name=model_name,
                 model_kwargs=model_kwargs,
                 encode_kwargs=encode_kwargs
             )
+     Nomic Example:
+        .. code-block:: python
+
+            from langchain_community.embeddings import HuggingFaceBgeEmbeddings
+
+            model_name = "nomic-ai/nomic-embed-text-v1"
+            model_kwargs = {
+                'device': 'cpu',
+                'trust_remote_code':True
+                }
+            encode_kwargs = {'normalize_embeddings': True}
+            hf = HuggingFaceBgeEmbeddings(
+                model_name=model_name,
+                model_kwargs=model_kwargs,
+                encode_kwargs=encode_kwargs,
+                query_instruction = "search_query:",
+                embed_instruction = "search_document:"
+            )
     """
 
     client: Any  #: :meta private:
     model_name: str = DEFAULT_BGE_MODEL
     """Model name to use."""
     cache_folder: Optional[str] = None
     """Path to store models.
     Can be also set by SENTENCE_TRANSFORMERS_HOME environment variable."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Keyword arguments to pass to the model."""
     encode_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Keyword arguments to pass when calling the `encode` method of the model."""
     query_instruction: str = DEFAULT_QUERY_BGE_INSTRUCTION_EN
     """Instruction to use for embedding query."""
+    embed_instruction: str = ""
+    """Instruction to use for embedding document."""
 
     def __init__(self, **kwargs: Any):
         """Initialize the sentence_transformer."""
         super().__init__(**kwargs)
         try:
             import sentence_transformers
 
@@ -245,15 +275,15 @@
 
         Args:
             texts: The list of texts to embed.
 
         Returns:
             List of embeddings, one for each text.
         """
-        texts = [t.replace("\n", " ") for t in texts]
+        texts = [self.embed_instruction + t.replace("\n", " ") for t in texts]
         embeddings = self.client.encode(texts, **self.encode_kwargs)
         return embeddings.tolist()
 
     def embed_query(self, text: str) -> List[float]:
         """Compute query embeddings using a HuggingFace transformer model.
 
         Args:
@@ -271,20 +301,22 @@
 
 class HuggingFaceInferenceAPIEmbeddings(BaseModel, Embeddings):
     """Embed texts using the HuggingFace API.
 
     Requires a HuggingFace Inference API key and a model name.
     """
 
-    api_key: str
+    api_key: SecretStr
     """Your API key for the HuggingFace Inference API."""
     model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
     """The name of the model to use for text embeddings."""
     api_url: Optional[str] = None
     """Custom inference endpoint url. None for using default public url."""
+    additional_headers: Dict[str, str] = {}
+    """Pass additional headers to the requests library if needed."""
 
     @property
     def _api_url(self) -> str:
         return self.api_url or self._default_api_url
 
     @property
     def _default_api_url(self) -> str:
@@ -293,15 +325,18 @@
             "/pipeline"
             "/feature-extraction"
             f"/{self.model_name}"
         )
 
     @property
     def _headers(self) -> dict:
-        return {"Authorization": f"Bearer {self.api_key}"}
+        return {
+            "Authorization": f"Bearer {self.api_key.get_secret_value()}",
+            **self.additional_headers,
+        }
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Get the embeddings for a list of texts.
 
         Args:
             texts (Documents): A list of texts to get embeddings for.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/huggingface_hub.py` & `gigachain_community-0.2.0/langchain_community/embeddings/huggingface_hub.py`

 * *Files 17% similar despite different names*

```diff
@@ -25,14 +25,15 @@
                 model=model,
                 task="feature-extraction",
                 huggingfacehub_api_token="my-api-key",
             )
     """
 
     client: Any  #: :meta private:
+    async_client: Any  #: :meta private:
     model: Optional[str] = None
     """Model name to use."""
     repo_id: Optional[str] = None
     """Huggingfacehub repository id, for backward compatibility."""
     task: Optional[str] = "feature-extraction"
     """Task to call the model with."""
     model_kwargs: Optional[dict] = None
@@ -49,34 +50,42 @@
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
         huggingfacehub_api_token = values["huggingfacehub_api_token"] or os.getenv(
             "HUGGINGFACEHUB_API_TOKEN"
         )
 
         try:
-            from huggingface_hub import InferenceClient
+            from huggingface_hub import AsyncInferenceClient, InferenceClient
 
             if values["model"]:
                 values["repo_id"] = values["model"]
             elif values["repo_id"]:
                 values["model"] = values["repo_id"]
             else:
                 values["model"] = DEFAULT_MODEL
                 values["repo_id"] = DEFAULT_MODEL
 
             client = InferenceClient(
                 model=values["model"],
                 token=huggingfacehub_api_token,
             )
+
+            async_client = AsyncInferenceClient(
+                model=values["model"],
+                token=huggingfacehub_api_token,
+            )
+
             if values["task"] not in VALID_TASKS:
                 raise ValueError(
                     f"Got invalid task {values['task']}, "
                     f"currently only {VALID_TASKS} are supported"
                 )
             values["client"] = client
+            values["async_client"] = async_client
+
         except ImportError:
             raise ImportError(
                 "Could not import huggingface_hub python package. "
                 "Please install it with `pip install huggingface_hub`."
             )
         return values
 
@@ -93,18 +102,47 @@
         texts = [text.replace("\n", " ") for text in texts]
         _model_kwargs = self.model_kwargs or {}
         responses = self.client.post(
             json={"inputs": texts, "parameters": _model_kwargs}, task=self.task
         )
         return json.loads(responses.decode())
 
+    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
+        """Async Call to HuggingFaceHub's embedding endpoint for embedding search docs.
+
+        Args:
+            texts: The list of texts to embed.
+
+        Returns:
+            List of embeddings, one for each text.
+        """
+        # replace newlines, which can negatively affect performance.
+        texts = [text.replace("\n", " ") for text in texts]
+        _model_kwargs = self.model_kwargs or {}
+        responses = await self.async_client.post(
+            json={"inputs": texts, "parameters": _model_kwargs}, task=self.task
+        )
+        return json.loads(responses.decode())
+
     def embed_query(self, text: str) -> List[float]:
         """Call out to HuggingFaceHub's embedding endpoint for embedding query text.
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
         response = self.embed_documents([text])[0]
         return response
+
+    async def aembed_query(self, text: str) -> List[float]:
+        """Async Call to HuggingFaceHub's embedding endpoint for embedding query text.
+
+        Args:
+            text: The text to embed.
+
+        Returns:
+            Embeddings for the text.
+        """
+        response = (await self.aembed_documents([text]))[0]
+        return response
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/infinity.py` & `gigachain_community-0.2.0/langchain_community/embeddings/infinity.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,35 +11,37 @@
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 __all__ = ["InfinityEmbeddings"]
 
 
 class InfinityEmbeddings(BaseModel, Embeddings):
-    """Embedding models for self-hosted https://github.com/michaelfeil/infinity
-    This should also work for text-embeddings-inference and other
+    """Self-hosted embedding models for `infinity` package.
+
+    See https://github.com/michaelfeil/infinity
+    This also works for text-embeddings-inference and other
     self-hosted openai-compatible servers.
 
-    Infinity is a class to interact with Embedding Models on https://github.com/michaelfeil/infinity
+    Infinity is a package to interact with Embedding Models on https://github.com/michaelfeil/infinity
 
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import InfinityEmbeddings
             InfinityEmbeddings(
                 model="BAAI/bge-small",
-                infinity_api_url="http://localhost:7797/v1",
+                infinity_api_url="http://localhost:7997",
             )
     """
 
     model: str
     "Underlying Infinity model id."
 
-    infinity_api_url: str = "http://localhost:7797/v1"
+    infinity_api_url: str = "http://localhost:7997"
     """Endpoint URL to use."""
 
     client: Any = None  #: :meta private:
     """Infinity client."""
 
     # LLM call kwargs
     class Config:
@@ -111,15 +113,17 @@
             Embeddings for the text.
         """
         embeddings = await self.aembed_documents([text])
         return embeddings[0]
 
 
 class TinyAsyncOpenAIInfinityEmbeddingClient:  #: :meta private:
-    """A helper tool to embed Infinity. Not part of Langchain's stable API,
+    """Helper tool to embed Infinity.
+
+    It is not a part of Langchain's stable API,
     direct use discouraged.
 
     Example:
         .. code-block:: python
 
 
             mini_client = TinyAsyncInfinityEmbeddingClient(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/javelin_ai_gateway.py` & `gigachain_community-0.2.0/langchain_community/embeddings/javelin_ai_gateway.py`

 * *Files 12% similar despite different names*

```diff
@@ -8,16 +8,15 @@
 
 def _chunk(texts: List[str], size: int) -> Iterator[List[str]]:
     for i in range(0, len(texts), size):
         yield texts[i : i + size]
 
 
 class JavelinAIGatewayEmbeddings(Embeddings, BaseModel):
-    """
-    Wrapper around embeddings LLMs in the Javelin AI Gateway.
+    """Javelin AI Gateway embeddings.
 
     To use, you should have the ``javelin_sdk`` python package installed.
     For more information, see https://docs.getjavelin.io
 
     Example:
         .. code-block:: python
 
@@ -70,15 +69,15 @@
                 resp_dict = resp.dict()
 
                 embeddings_chunk = resp_dict.get("llm_response", {}).get("data", [])
                 for item in embeddings_chunk:
                     if "embedding" in item:
                         embeddings.append(item["embedding"])
             except ValueError as e:
-                print("Failed to query route: " + str(e))
+                print("Failed to query route: " + str(e))  # noqa: T201
 
         return embeddings
 
     async def _aquery(self, texts: List[str]) -> List[List[float]]:
         embeddings = []
         for txt in _chunk(texts, 20):
             try:
@@ -88,15 +87,15 @@
                 resp_dict = resp.dict()
 
                 embeddings_chunk = resp_dict.get("llm_response", {}).get("data", [])
                 for item in embeddings_chunk:
                     if "embedding" in item:
                         embeddings.append(item["embedding"])
             except ValueError as e:
-                print("Failed to query route: " + str(e))
+                print("Failed to query route: " + str(e))  # noqa: T201
 
         return embeddings
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         return self._query(texts)
 
     def embed_query(self, text: str) -> List[float]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/jina.py` & `gigachain_community-0.2.0/langchain_community/embeddings/jina.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/johnsnowlabs.py` & `gigachain_community-0.2.0/langchain_community/embeddings/johnsnowlabs.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/llamacpp.py` & `gigachain_community-0.2.0/langchain_community/embeddings/llamacpp.py`

 * *Files 0% similar despite different names*

```diff
@@ -43,15 +43,15 @@
     use_mlock: bool = Field(False, alias="use_mlock")
     """Force system to keep model in RAM."""
 
     n_threads: Optional[int] = Field(None, alias="n_threads")
     """Number of threads to use. If None, the number 
     of threads is automatically determined."""
 
-    n_batch: Optional[int] = Field(8, alias="n_batch")
+    n_batch: Optional[int] = Field(512, alias="n_batch")
     """Number of tokens to process in parallel.
     Should be a number between 1 and n_ctx."""
 
     n_gpu_layers: Optional[int] = Field(None, alias="n_gpu_layers")
     """Number of layers to be loaded into gpu memory. Default None."""
 
     verbose: bool = Field(True, alias="verbose")
@@ -84,15 +84,15 @@
             model_params["n_gpu_layers"] = values["n_gpu_layers"]
 
         try:
             from llama_cpp import Llama
 
             values["client"] = Llama(model_path, embedding=True, **model_params)
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import llama-cpp-python library. "
                 "Please install the llama-cpp-python library to "
                 "use this embedding model: pip install llama-cpp-python"
             )
         except Exception as e:
             raise ValueError(
                 f"Could not load Llama model from path: {model_path}. "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/llm_rails.py` & `gigachain_community-0.2.0/langchain_community/embeddings/modelscope_hub.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,71 +1,73 @@
-""" This file is for LLMRails Embedding """
-import logging
-import os
-from typing import List, Optional
+from typing import Any, List, Optional
 
-import requests
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra
 
 
-class LLMRailsEmbeddings(BaseModel, Embeddings):
-    """LLMRails embedding models.
+class ModelScopeEmbeddings(BaseModel, Embeddings):
+    """ModelScopeHub embedding models.
 
-    To use, you should have the  environment
-    variable ``LLM_RAILS_API_KEY`` set with your API key or pass it
-    as a named parameter to the constructor.
-
-    Model can be one of ["embedding-english-v1","embedding-multi-v1"]
+    To use, you should have the ``modelscope`` python package installed.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.embeddings import LLMRailsEmbeddings
-            cohere = LLMRailsEmbeddings(
-                model="embedding-english-v1", api_key="my-api-key"
-            )
+            from langchain_community.embeddings import ModelScopeEmbeddings
+            model_id = "damo/nlp_corom_sentence-embedding_english-base"
+            embed = ModelScopeEmbeddings(model_id=model_id, model_revision="v1.0.0")
     """
 
-    model: str = "embedding-english-v1"
+    embed: Any
+    model_id: str = "damo/nlp_corom_sentence-embedding_english-base"
     """Model name to use."""
+    model_revision: Optional[str] = None
 
-    api_key: Optional[str] = None
-    """LLMRails API key."""
+    def __init__(self, **kwargs: Any):
+        """Initialize the modelscope"""
+        super().__init__(**kwargs)
+        try:
+            from modelscope.pipelines import pipeline
+            from modelscope.utils.constant import Tasks
+        except ImportError as e:
+            raise ImportError(
+                "Could not import some python packages."
+                "Please install it with `pip install modelscope`."
+            ) from e
+        self.embed = pipeline(
+            Tasks.sentence_embedding,
+            model=self.model_id,
+            model_revision=self.model_revision,
+        )
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
-        """Call out to Cohere's embedding endpoint.
+        """Compute doc embeddings using a modelscope embedding model.
 
         Args:
             texts: The list of texts to embed.
 
         Returns:
             List of embeddings, one for each text.
         """
-        api_key = self.api_key or os.environ.get("LLM_RAILS_API_KEY")
-        if api_key is None:
-            logging.warning("Can't find LLMRails credentials in environment.")
-            raise ValueError("LLM_RAILS_API_KEY is not set")
-
-        response = requests.post(
-            "https://api.llmrails.com/v1/embeddings",
-            headers={"X-API-KEY": api_key},
-            json={"input": texts, "model": self.model},
-            timeout=60,
-        )
-        return [item["embedding"] for item in response.json()["data"]]
+        texts = list(map(lambda x: x.replace("\n", " "), texts))
+        inputs = {"source_sentence": texts}
+        embeddings = self.embed(input=inputs)["text_embedding"]
+        return embeddings.tolist()
 
     def embed_query(self, text: str) -> List[float]:
-        """Call out to Cohere's embedding endpoint.
+        """Compute query embeddings using a modelscope embedding model.
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
-        return self.embed_documents([text])[0]
+        text = text.replace("\n", " ")
+        inputs = {"source_sentence": [text]}
+        embedding = self.embed(input=inputs)["text_embedding"][0]
+        return embedding.tolist()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/localai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/localai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/minimax.py` & `gigachain_community-0.2.0/langchain_community/embeddings/minimax.py`

 * *Files 2% similar despite different names*

```diff
@@ -106,15 +106,15 @@
             "model": self.model,
             "type": embed_type,
             "texts": texts,
         }
 
         # HTTP headers for authorization
         headers = {
-            "Authorization": f"Bearer {self.minimax_api_key.get_secret_value()}",
+            "Authorization": f"Bearer {self.minimax_api_key.get_secret_value()}",  # type: ignore[union-attr]
             "Content-Type": "application/json",
         }
 
         params = {
             "GroupId": self.minimax_group_id,
         }
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/mlflow.py` & `gigachain_community-0.2.0/langchain_community/embeddings/mlflow.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 from __future__ import annotations
 
-from typing import Any, Iterator, List
+from typing import Any, Dict, Iterator, List
 from urllib.parse import urlparse
 
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, PrivateAttr
 
 
 def _chunk(texts: List[str], size: int) -> Iterator[List[str]]:
     for i in range(0, len(texts), size):
         yield texts[i : i + size]
 
 
 class MlflowEmbeddings(Embeddings, BaseModel):
-    """Wrapper around embeddings LLMs in MLflow.
+    """Embedding LLMs in MLflow.
 
     To use, you should have the `mlflow[genai]` python package installed.
-    For more information, see https://mlflow.org/docs/latest/llms/deployments/server.html.
+    For more information, see https://mlflow.org/docs/latest/llms/deployments.
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import MlflowEmbeddings
 
             embeddings = MlflowEmbeddings(
@@ -30,14 +30,18 @@
     """
 
     endpoint: str
     """The endpoint to use."""
     target_uri: str
     """The target URI to use."""
     _client: Any = PrivateAttr()
+    """The parameters to use for queries."""
+    query_params: Dict[str, str] = {}
+    """The parameters to use for documents."""
+    documents_params: Dict[str, str] = {}
 
     def __init__(self, **kwargs: Any):
         super().__init__(**kwargs)
         self._validate_uri()
         try:
             from mlflow.deployments import get_deploy_client
 
@@ -59,16 +63,29 @@
         allowed = ["http", "https", "databricks"]
         if urlparse(self.target_uri).scheme not in allowed:
             raise ValueError(
                 f"Invalid target URI: {self.target_uri}. "
                 f"The scheme must be one of {allowed}."
             )
 
-    def embed_documents(self, texts: List[str]) -> List[List[float]]:
+    def embed(self, texts: List[str], params: Dict[str, str]) -> List[List[float]]:
         embeddings: List[List[float]] = []
         for txt in _chunk(texts, 20):
-            resp = self._client.predict(endpoint=self.endpoint, inputs={"input": txt})
+            resp = self._client.predict(
+                endpoint=self.endpoint,
+                inputs={"input": txt, **params},  # type: ignore[arg-type]
+            )
             embeddings.extend(r["embedding"] for r in resp["data"])
         return embeddings
 
+    def embed_documents(self, texts: List[str]) -> List[List[float]]:
+        return self.embed(texts, params=self.documents_params)
+
     def embed_query(self, text: str) -> List[float]:
-        return self.embed_documents([text])[0]
+        return self.embed([text], params=self.query_params)[0]
+
+
+class MlflowCohereEmbeddings(MlflowEmbeddings):
+    """Cohere embedding LLMs in MLflow."""
+
+    query_params: Dict[str, str] = {"input_type": "search_query"}
+    documents_params: Dict[str, str] = {"input_type": "search_document"}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/mlflow_gateway.py` & `gigachain_community-0.2.0/langchain_community/embeddings/mlflow_gateway.py`

 * *Files 9% similar despite different names*

```diff
@@ -9,16 +9,15 @@
 
 def _chunk(texts: List[str], size: int) -> Iterator[List[str]]:
     for i in range(0, len(texts), size):
         yield texts[i : i + size]
 
 
 class MlflowAIGatewayEmbeddings(Embeddings, BaseModel):
-    """
-    Wrapper around embeddings LLMs in the MLflow AI Gateway.
+    """MLflow AI Gateway embeddings.
 
     To use, you should have the ``mlflow[gateway]`` python package installed.
     For more information, see https://mlflow.org/docs/latest/gateway/index.html.
 
     Example:
         .. code-block:: python
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/mosaicml.py` & `gigachain_community-0.2.0/langchain_community/embeddings/mosaicml.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/nlpcloud.py` & `gigachain_community-0.2.0/langchain_community/embeddings/nlpcloud.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/octoai_embeddings.py` & `gigachain_community-0.2.0/langchain_community/embeddings/octoai_embeddings.py`

 * *Files 17% similar despite different names*

```diff
@@ -37,15 +37,15 @@
     @root_validator(allow_reuse=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Ensure that the API key and python package exist in environment."""
         values["octoai_api_token"] = get_from_dict_or_env(
             values, "octoai_api_token", "OCTOAI_API_TOKEN"
         )
         values["endpoint_url"] = get_from_dict_or_env(
-            values, "endpoint_url", "ENDPOINT_URL"
+            values, "endpoint_url", "https://text.octoai.run/v1/embeddings"
         )
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Return the identifying parameters."""
         return {
@@ -55,27 +55,37 @@
 
     def _compute_embeddings(
         self, texts: List[str], instruction: str
     ) -> List[List[float]]:
         """Compute embeddings using an OctoAI instruct model."""
         from octoai import client
 
+        embedding = []
         embeddings = []
         octoai_client = client.Client(token=self.octoai_api_token)
 
         for text in texts:
             parameter_payload = {
-                "sentence": str([text]),  # for item in text]),
-                "instruction": str([instruction]),  # for item in text]),
+                "sentence": str([text]),
+                "input": str([text]),
+                "instruction": str([instruction]),
+                "model": "thenlper/gte-large",
                 "parameters": self.model_kwargs or {},
             }
 
             try:
                 resp_json = octoai_client.infer(self.endpoint_url, parameter_payload)
-                embedding = resp_json["embeddings"]
+                if "embeddings" in resp_json:
+                    embedding = resp_json["embeddings"]
+                elif "data" in resp_json:
+                    json_data = resp_json["data"]
+                    for item in json_data:
+                        if "embedding" in item:
+                            embedding = item["embedding"]
+
             except Exception as e:
                 raise ValueError(f"Error raised by the inference endpoint: {e}") from e
 
             embeddings.append(embedding)
 
         return embeddings
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/ollama.py` & `gigachain_community-0.2.0/langchain_community/embeddings/ollama.py`

 * *Files 4% similar despite different names*

```diff
@@ -93,22 +93,28 @@
     impact more, while a value of 1.0 disables this setting. (default: 1)"""
 
     top_k: Optional[int] = None
     """Reduces the probability of generating nonsense. A higher value (e.g. 100)
     will give more diverse answers, while a lower value (e.g. 10)
     will be more conservative. (Default: 40)"""
 
-    top_p: Optional[int] = None
+    top_p: Optional[float] = None
     """Works together with top-k. A higher value (e.g., 0.95) will lead
     to more diverse text, while a lower value (e.g., 0.5) will
     generate more focused and conservative text. (Default: 0.9)"""
 
     show_progress: bool = False
     """Whether to show a tqdm progress bar. Must have `tqdm` installed."""
 
+    headers: Optional[dict] = None
+    """Additional headers to pass to endpoint (e.g. Authorization, Referer).
+    This is useful when Ollama is hosted on cloud services that require
+    tokens for authentication.
+    """
+
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Ollama."""
         return {
             "model": self.model,
             "options": {
                 "mirostat": self.mirostat,
@@ -147,14 +153,15 @@
             response: The response from the API.
 
         Returns:
             The response as a dictionary.
         """
         headers = {
             "Content-Type": "application/json",
+            **(self.headers or {}),
         }
 
         try:
             res = requests.post(
                 f"{self.base_url}/api/embeddings",
                 headers=headers,
                 json={"model": self.model, "prompt": input, **self._default_params},
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/openai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/openai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from __future__ import annotations
 
 import logging
 import os
 import warnings
-from importlib.metadata import version
 from typing import (
     Any,
     Callable,
     Dict,
     List,
     Literal,
     Mapping,
@@ -16,27 +15,29 @@
     Set,
     Tuple,
     Union,
     cast,
 )
 
 import numpy as np
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env, get_pydantic_field_names
-from packaging.version import Version, parse
 from tenacity import (
     AsyncRetrying,
     before_sleep_log,
     retry,
     retry_if_exception_type,
     stop_after_attempt,
     wait_exponential,
 )
 
+from langchain_community.utils.openai import is_openai_v1
+
 logger = logging.getLogger(__name__)
 
 
 def _create_retry_decorator(embeddings: OpenAIEmbeddings) -> Callable[[Any], Any]:
     import openai
 
     # Wait 2^x * 1 second between each retry starting with
@@ -107,45 +108,45 @@
 
         raise openai.error.APIError("OpenAI API returned an empty embedding")
     return response
 
 
 def embed_with_retry(embeddings: OpenAIEmbeddings, **kwargs: Any) -> Any:
     """Use tenacity to retry the embedding call."""
-    if _is_openai_v1():
+    if is_openai_v1():
         return embeddings.client.create(**kwargs)
     retry_decorator = _create_retry_decorator(embeddings)
 
     @retry_decorator
     def _embed_with_retry(**kwargs: Any) -> Any:
         response = embeddings.client.create(**kwargs)
         return _check_response(response, skip_empty=embeddings.skip_empty)
 
     return _embed_with_retry(**kwargs)
 
 
 async def async_embed_with_retry(embeddings: OpenAIEmbeddings, **kwargs: Any) -> Any:
     """Use tenacity to retry the embedding call."""
 
-    if _is_openai_v1():
+    if is_openai_v1():
         return await embeddings.async_client.create(**kwargs)
 
     @_async_retry_decorator(embeddings)
     async def _async_embed_with_retry(**kwargs: Any) -> Any:
         response = await embeddings.client.acreate(**kwargs)
         return _check_response(response, skip_empty=embeddings.skip_empty)
 
     return await _async_embed_with_retry(**kwargs)
 
 
-def _is_openai_v1() -> bool:
-    _version = parse(version("openai"))
-    return _version >= Version("1.0.0")
-
-
+@deprecated(
+    since="0.0.9",
+    removal="0.3.0",
+    alternative_import="langchain_openai.OpenAIEmbeddings",
+)
 class OpenAIEmbeddings(BaseModel, Embeddings):
     """OpenAI embedding models.
 
     To use, you should have the ``openai`` python package installed, and the
     environment variable ``OPENAI_API_KEY`` set with your API key or pass it
     as a named parameter to the constructor.
 
@@ -326,15 +327,15 @@
             import openai
         except ImportError:
             raise ImportError(
                 "Could not import openai python package. "
                 "Please install it with `pip install openai`."
             )
         else:
-            if _is_openai_v1():
+            if is_openai_v1():
                 if values["openai_api_type"] in ("azure", "azure_ad", "azuread"):
                     warnings.warn(
                         "If you have openai>=1.0.0 installed and are using Azure, "
                         "please use the `AzureOpenAIEmbeddings` class."
                     )
                 client_params = {
                     "api_key": values["openai_api_key"],
@@ -356,15 +357,15 @@
                 values["client"] = openai.Embedding
             else:
                 pass
         return values
 
     @property
     def _invocation_params(self) -> Dict[str, Any]:
-        if _is_openai_v1():
+        if is_openai_v1():
             openai_args: Dict = {"model": self.model, **self.model_kwargs}
         else:
             openai_args = {
                 "model": self.model,
                 "request_timeout": self.request_timeout,
                 "headers": self.headers,
                 "api_key": self.openai_api_key,
@@ -419,15 +420,15 @@
         _chunk_size = chunk_size or self.chunk_size
 
         # If tiktoken flag set to False
         if not self.tiktoken_enabled:
             try:
                 from transformers import AutoTokenizer
             except ImportError:
-                raise ValueError(
+                raise ImportError(
                     "Could not import transformers python package. "
                     "This is needed in order to for OpenAIEmbeddings without "
                     "`tiktoken`. Please install it with `pip install transformers`. "
                 )
 
             tokenizer = AutoTokenizer.from_pretrained(
                 pretrained_model_name_or_path=model_name
@@ -552,15 +553,15 @@
         _chunk_size = chunk_size or self.chunk_size
 
         # If tiktoken flag set to False
         if not self.tiktoken_enabled:
             try:
                 from transformers import AutoTokenizer
             except ImportError:
-                raise ValueError(
+                raise ImportError(
                     "Could not import transformers python package. "
                     "This is needed in order to for OpenAIEmbeddings without "
                     " `tiktoken`. Please install it with `pip install transformers`."
                 )
 
             tokenizer = AutoTokenizer.from_pretrained(
                 pretrained_model_name_or_path=model_name
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/sagemaker_endpoint.py` & `gigachain_community-0.2.0/langchain_community/embeddings/sagemaker_endpoint.py`

 * *Files 2% similar despite different names*

```diff
@@ -138,15 +138,15 @@
                     "sagemaker-runtime", region_name=values["region_name"]
                 )
 
             except Exception as e:
                 raise ValueError(
                     "Could not load credentials to authenticate with AWS client. "
                     "Please check that credentials in the specified "
-                    "profile name are valid."
+                    f"profile name are valid. {e}"
                 ) from e
 
         except ImportError:
             raise ImportError(
                 "Could not import boto3 python package. "
                 "Please install it with `pip install boto3`."
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/self_hosted.py` & `gigachain_community-0.2.0/langchain_community/embeddings/self_hosted.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/self_hosted_hugging_face.py` & `gigachain_community-0.2.0/langchain_community/embeddings/self_hosted_hugging_face.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,17 +67,17 @@
     To use, you should have the ``runhouse`` python package installed.
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import SelfHostedHuggingFaceEmbeddings
             import runhouse as rh
-            model_name = "sentence-transformers/all-mpnet-base-v2"
+            model_id = "sentence-transformers/all-mpnet-base-v2"
             gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
-            hf = SelfHostedHuggingFaceEmbeddings(model_name=model_name, hardware=gpu)
+            hf = SelfHostedHuggingFaceEmbeddings(model_id=model_id, hardware=gpu)
     """
 
     client: Any  #: :meta private:
     model_id: str = DEFAULT_MODEL_NAME
     """Model name to use."""
     model_reqs: List[str] = ["./", "sentence_transformers", "torch"]
     """Requirements to install on hardware to inference the model."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/spacy_embeddings.py` & `gigachain_community-0.2.0/langchain_community/embeddings/spacy_embeddings.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,94 +1,100 @@
 import importlib.util
-from typing import Any, Dict, List
+from typing import Any, Dict, List, Optional
 
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 
 
 class SpacyEmbeddings(BaseModel, Embeddings):
-    """Embeddings by SpaCy models.
-
-    It only supports the 'en_core_web_sm' model.
+    """Embeddings by spaCy models.
 
     Attributes:
-        nlp (Any): The Spacy model loaded into memory.
+        model_name (str): Name of a spaCy model.
+        nlp (Any): The spaCy model loaded into memory.
 
     Methods:
         embed_documents(texts: List[str]) -> List[List[float]]:
             Generates embeddings for a list of documents.
         embed_query(text: str) -> List[float]:
             Generates an embedding for a single piece of text.
     """
 
-    nlp: Any  # The Spacy model loaded into memory
+    model_name: str = "en_core_web_sm"
+    nlp: Optional[Any] = None
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid  # Forbid extra attributes during model initialization
 
     @root_validator(pre=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """
-        Validates that the Spacy package and the 'en_core_web_sm' model are installed.
+        Validates that the spaCy package and the model are installed.
 
         Args:
             values (Dict): The values provided to the class constructor.
 
         Returns:
             The validated values.
 
         Raises:
-            ValueError: If the Spacy package or the 'en_core_web_sm'
+            ValueError: If the spaCy package or the
             model are not installed.
         """
-        # Check if the Spacy package is installed
+        if values.get("model_name") is None:
+            values["model_name"] = "en_core_web_sm"
+
+        model_name = values.get("model_name")
+
+        # Check if the spaCy package is installed
         if importlib.util.find_spec("spacy") is None:
             raise ValueError(
-                "Spacy package not found. "
+                "SpaCy package not found. "
                 "Please install it with `pip install spacy`."
             )
         try:
-            # Try to load the 'en_core_web_sm' Spacy model
+            # Try to load the spaCy model
             import spacy
 
-            values["nlp"] = spacy.load("en_core_web_sm")
+            values["nlp"] = spacy.load(model_name)
         except OSError:
             # If the model is not found, raise a ValueError
             raise ValueError(
-                "Spacy model 'en_core_web_sm' not found. "
-                "Please install it with"
-                " `python -m spacy download en_core_web_sm`."
+                f"SpaCy model '{model_name}' not found. "
+                f"Please install it with"
+                f" `python -m spacy download {model_name}`"
+                "or provide a valid spaCy model name."
             )
         return values  # Return the validated values
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """
         Generates embeddings for a list of documents.
 
         Args:
             texts (List[str]): The documents to generate embeddings for.
 
         Returns:
             A list of embeddings, one for each document.
         """
-        return [self.nlp(text).vector.tolist() for text in texts]
+        return [self.nlp(text).vector.tolist() for text in texts]  # type: ignore[misc]
 
     def embed_query(self, text: str) -> List[float]:
         """
         Generates an embedding for a single piece of text.
 
         Args:
             text (str): The text to generate an embedding for.
 
         Returns:
             The embedding for the text.
         """
-        return self.nlp(text).vector.tolist()
+        return self.nlp(text).vector.tolist()  # type: ignore[misc]
 
     async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
         """
         Asynchronously generates embeddings for a list of documents.
         This method is not implemented and raises a NotImplementedError.
 
         Args:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/tensorflow_hub.py` & `gigachain_community-0.2.0/langchain_community/embeddings/tensorflow_hub.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/vertexai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/vertexai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,33 +1,41 @@
 import logging
 import re
 import string
 import threading
 from concurrent.futures import ThreadPoolExecutor, wait
 from typing import Any, Dict, List, Literal, Optional, Tuple
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
 from langchain_core.language_models.llms import create_base_retry_decorator
 from langchain_core.pydantic_v1 import root_validator
 
 from langchain_community.llms.vertexai import _VertexAICommon
 from langchain_community.utilities.vertexai import raise_vertex_import_error
 
 logger = logging.getLogger(__name__)
 
 _MAX_TOKENS_PER_BATCH = 20000
 _MAX_BATCH_SIZE = 250
 _MIN_BATCH_SIZE = 5
 
 
+@deprecated(
+    since="0.0.12",
+    removal="0.3.0",
+    alternative_import="langchain_google_vertexai.VertexAIEmbeddings",
+)
 class VertexAIEmbeddings(_VertexAICommon, Embeddings):
     """Google Cloud VertexAI embedding models."""
 
     # Instance context
     instance: Dict[str, Any] = {}  #: :meta private:
+    show_progress_bar: bool = False
+    """Whether to show a tqdm progress bar. Must have `tqdm` installed."""
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validates that the python package exists in environment."""
         cls._try_init_vertexai(values)
         if values["model_name"] == "textembedding-gecko-default":
             logger.warning(
@@ -292,15 +300,28 @@
             first_batch_result, batches = self._prepare_and_validate_batches(
                 texts, embeddings_task_type
             )
         # First batch result may have some embeddings already.
         # In such case, batches have texts that were not processed yet.
         embeddings.extend(first_batch_result)
         tasks = []
-        for batch in batches:
+        if self.show_progress_bar:
+            try:
+                from tqdm import tqdm
+
+                iter_ = tqdm(batches, desc="VertexAIEmbeddings")
+            except ImportError:
+                logger.warning(
+                    "Unable to show progress bar because tqdm could not be imported. "
+                    "Please install with `pip install tqdm`."
+                )
+                iter_ = batches
+        else:
+            iter_ = batches
+        for batch in iter_:
             tasks.append(
                 self.instance["task_executor"].submit(
                     self._get_embeddings_with_retry,
                     texts=batch,
                     embeddings_type=embeddings_task_type,
                 )
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/voyageai.py` & `gigachain_community-0.2.0/langchain_community/embeddings/voyageai.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,14 +10,15 @@
     Optional,
     Tuple,
     Union,
     cast,
 )
 
 import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     stop_after_attempt,
@@ -54,66 +55,100 @@
     def _embed_with_retry(**kwargs: Any) -> Any:
         response = requests.post(**kwargs)
         return _check_response(response.json())
 
     return _embed_with_retry(**kwargs)
 
 
+@deprecated(
+    since="0.0.29",
+    removal="0.3",
+    alternative_import="langchain_voyageai.VoyageAIEmbeddings",
+)
 class VoyageEmbeddings(BaseModel, Embeddings):
     """Voyage embedding models.
 
     To use, you should have the environment variable ``VOYAGE_API_KEY`` set with
     your API key or pass it as a named parameter to the constructor.
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import VoyageEmbeddings
 
-            voyage = VoyageEmbeddings(voyage_api_key="your-api-key")
+            voyage = VoyageEmbeddings(voyage_api_key="your-api-key", model="voyage-2")
             text = "This is a test query."
             query_result = voyage.embed_query(text)
     """
 
-    model: str = "voyage-01"
+    model: str
     voyage_api_base: str = "https://api.voyageai.com/v1/embeddings"
     voyage_api_key: Optional[SecretStr] = None
-    batch_size: int = 8
+    batch_size: int
     """Maximum number of texts to embed in each API request."""
     max_retries: int = 6
     """Maximum number of retries to make when generating."""
     request_timeout: Optional[Union[float, Tuple[float, float]]] = None
     """Timeout in seconds for the API request."""
     show_progress_bar: bool = False
     """Whether to show a progress bar when embedding. Must have tqdm installed if set 
         to True."""
+    truncation: bool = True
+    """Whether to truncate the input texts to fit within the context length.
+    
+        If True, over-length input texts will be truncated to fit within the context 
+        length, before vectorized by the embedding model. If False, an error will be 
+        raised if any given text exceeds the context length."""
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator(pre=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
         values["voyage_api_key"] = convert_to_secret_str(
             get_from_dict_or_env(values, "voyage_api_key", "VOYAGE_API_KEY")
         )
+
+        if "model" not in values:
+            values["model"] = "voyage-01"
+            logger.warning(
+                "model will become a required arg for VoyageAIEmbeddings, "
+                "we recommend to specify it when using this class. "
+                "Currently the default is set to voyage-01."
+            )
+
+        if "batch_size" not in values:
+            values["batch_size"] = (
+                72
+                if "model" in values and (values["model"] in ["voyage-2", "voyage-02"])
+                else 7
+            )
+
         return values
 
     def _invocation_params(
         self, input: List[str], input_type: Optional[str] = None
     ) -> Dict:
         api_key = cast(SecretStr, self.voyage_api_key).get_secret_value()
-        params = {
+        params: Dict = {
             "url": self.voyage_api_base,
             "headers": {"Authorization": f"Bearer {api_key}"},
-            "json": {"model": self.model, "input": input, "input_type": input_type},
+            "json": {
+                "model": self.model,
+                "input": input,
+                "input_type": input_type,
+                "truncation": self.truncation,
+            },
             "timeout": self.request_timeout,
         }
+        if self.truncation is not None:
+            params["json"]["truncation"] = self.truncation
         return params
 
     def _get_embeddings(
         self,
         texts: List[str],
         batch_size: Optional[int] = None,
         input_type: Optional[str] = None,
@@ -171,15 +206,17 @@
 
         Args:
             text: The text to embed.
 
         Returns:
             Embedding for the text.
         """
-        return self._get_embeddings([text], input_type="query")[0]
+        return self._get_embeddings(
+            [text], batch_size=self.batch_size, input_type="query"
+        )[0]
 
     def embed_general_texts(
         self, texts: List[str], *, input_type: Optional[str] = None
     ) -> List[List[float]]:
         """Call out to Voyage Embedding endpoint for embedding general text.
 
         Args:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/xinference.py` & `gigachain_community-0.2.0/langchain_community/embeddings/xinference.py`

 * *Ordering differences only*

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Wrapper around Xinference embedding models."""
+
 from typing import Any, List, Optional
 
 from langchain_core.embeddings import Embeddings
 
 
 class XinferenceEmbeddings(Embeddings):
-
     """Xinference embedding models.
 
     To use, you should have the xinference library installed:
 
     .. code-block:: bash
 
         pip install xinference
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/embeddings/yandex.py` & `gigachain_community-0.2.0/langchain_community/embeddings/yandex.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 """Wrapper around YandexGPT embedding models."""
+
 from __future__ import annotations
 
 import logging
-from typing import Any, Callable, Dict, List
+import time
+from typing import Any, Callable, Dict, List, Sequence
 
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     retry_if_exception_type,
     stop_after_attempt,
     wait_exponential,
 )
@@ -28,72 +30,102 @@
         - You can specify the token in a constructor parameter `iam_token`
         or in an environment variable `YC_IAM_TOKEN`.
         - You can specify the key in a constructor parameter `api_key`
         or in an environment variable `YC_API_KEY`.
 
     To use the default model specify the folder ID in a parameter `folder_id`
     or in an environment variable `YC_FOLDER_ID`.
-    Or specify the model URI in a constructor parameter `model_uri`
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings.yandex import YandexGPTEmbeddings
-            embeddings = YandexGPTEmbeddings(iam_token="t1.9eu...", model_uri="emb://<folder-id>/text-search-query/latest")
-    """
+            embeddings = YandexGPTEmbeddings(iam_token="t1.9eu...", folder_id=<folder-id>)
+    """  # noqa: E501
 
-    iam_token: str = ""
+    iam_token: SecretStr = ""  # type: ignore[assignment]
     """Yandex Cloud IAM token for service account
     with the `ai.languageModels.user` role"""
-    api_key: str = ""
+    api_key: SecretStr = ""  # type: ignore[assignment]
     """Yandex Cloud Api Key for service account
     with the `ai.languageModels.user` role"""
-    model_uri: str = ""
-    """Model uri to use."""
+    model_uri: str = Field(default="", alias="query_model_uri")
+    """Query model uri to use."""
+    doc_model_uri: str = ""
+    """Doc model uri to use."""
     folder_id: str = ""
     """Yandex Cloud folder ID"""
-    model_uri: str = ""
-    """Model uri to use."""
-    model_name: str = "text-search-query"
-    """Model name to use."""
+    doc_model_name: str = "text-search-doc"
+    """Doc model name to use."""
+    model_name: str = Field(default="text-search-query", alias="query_model_name")
+    """Query model name to use."""
     model_version: str = "latest"
     """Model version to use."""
     url: str = "llm.api.cloud.yandex.net:443"
     """The url of the API."""
     max_retries: int = 6
     """Maximum number of retries to make when generating."""
+    sleep_interval: float = 0.0
+    """Delay between API requests"""
+    disable_request_logging: bool = False
+    """YandexGPT API logs all request data by default. 
+    If you provide personal data, confidential information, disable logging."""
+    _grpc_metadata: Sequence
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        allow_population_by_field_name = True
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that iam token exists in environment."""
 
-        iam_token = get_from_dict_or_env(values, "iam_token", "YC_IAM_TOKEN", "")
+        iam_token = convert_to_secret_str(
+            get_from_dict_or_env(values, "iam_token", "YC_IAM_TOKEN", "")
+        )
         values["iam_token"] = iam_token
-        api_key = get_from_dict_or_env(values, "api_key", "YC_API_KEY", "")
+        api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "api_key", "YC_API_KEY", "")
+        )
         values["api_key"] = api_key
         folder_id = get_from_dict_or_env(values, "folder_id", "YC_FOLDER_ID", "")
         values["folder_id"] = folder_id
-        if api_key == "" and iam_token == "":
+        if api_key.get_secret_value() == "" and iam_token.get_secret_value() == "":
             raise ValueError("Either 'YC_API_KEY' or 'YC_IAM_TOKEN' must be provided.")
         if values["iam_token"]:
             values["_grpc_metadata"] = [
-                ("authorization", f"Bearer {values['iam_token']}")
+                ("authorization", f"Bearer {values['iam_token'].get_secret_value()}")
             ]
             if values["folder_id"]:
                 values["_grpc_metadata"].append(("x-folder-id", values["folder_id"]))
         else:
             values["_grpc_metadata"] = (
-                ("authorization", f"Api-Key {values['api_key']}"),
+                ("authorization", f"Api-Key {values['api_key'].get_secret_value()}"),
             )
-        if values["model_uri"] == "" and values["folder_id"] == "":
-            raise ValueError("Either 'model_uri' or 'folder_id' must be provided.")
-        if not values["model_uri"]:
+
+        if not values.get("doc_model_uri"):
+            if values["folder_id"] == "":
+                raise ValueError("'doc_model_uri' or 'folder_id' must be provided.")
+            values[
+                "doc_model_uri"
+            ] = f"emb://{values['folder_id']}/{values['doc_model_name']}/{values['model_version']}"  # noqa: E501
+        if not values.get("model_uri"):
+            if values["folder_id"] == "":
+                raise ValueError("'model_uri' or 'folder_id' must be provided.")
             values[
                 "model_uri"
-            ] = f"emb://{values['folder_id']}/{values['model_name']}/{values['model_version']}"
+            ] = f"emb://{values['folder_id']}/{values['model_name']}/{values['model_version']}"  # noqa: E501
+        if values["disable_request_logging"]:
+            values["_grpc_metadata"].append(
+                (
+                    "x-data-logging-enabled",
+                    "false",
+                )
+            )
         return values
 
     def embed_documents(self, texts: List[str]) -> List[List[float]]:
         """Embed documents using a YandexGPT embeddings models.
 
         Args:
             texts: The list of texts to embed.
@@ -109,15 +141,15 @@
 
         Args:
             text: The text to embed.
 
         Returns:
             Embeddings for the text.
         """
-        return _embed_with_retry(self, texts=[text])[0]
+        return _embed_with_retry(self, texts=[text], embed_query=True)[0]
 
 
 def _create_retry_decorator(llm: YandexGPTEmbeddings) -> Callable[[Any], Any]:
     from grpc import RpcError
 
     min_seconds = 1
     max_seconds = 60
@@ -137,31 +169,47 @@
     @retry_decorator
     def _completion_with_retry(**_kwargs: Any) -> Any:
         return _make_request(llm, **_kwargs)
 
     return _completion_with_retry(**kwargs)
 
 
-def _make_request(self: YandexGPTEmbeddings, texts: List[str]):
+def _make_request(self: YandexGPTEmbeddings, texts: List[str], **kwargs):  # type: ignore[no-untyped-def]
     try:
         import grpc
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
-            TextEmbeddingRequest,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
-            EmbeddingsServiceStub,
-        )
+
+        try:
+            from yandex.cloud.ai.foundation_models.v1.embedding.embedding_service_pb2 import (  # noqa: E501
+                TextEmbeddingRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.embedding.embedding_service_pb2_grpc import (  # noqa: E501
+                EmbeddingsServiceStub,
+            )
+        except ModuleNotFoundError:
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
+                TextEmbeddingRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
+                EmbeddingsServiceStub,
+            )
     except ImportError as e:
         raise ImportError(
-            "Please install YandexCloud SDK" " with `pip install yandexcloud`."
+            "Please install YandexCloud SDK  with `pip install yandexcloud` \
+            or upgrade it to recent version."
         ) from e
     result = []
     channel_credentials = grpc.ssl_channel_credentials()
     channel = grpc.secure_channel(self.url, channel_credentials)
+    # Use the query model if embed_query is True
+    if kwargs.get("embed_query"):
+        model_uri = self.model_uri
+    else:
+        model_uri = self.doc_model_uri
 
     for text in texts:
-        request = TextEmbeddingRequest(model_uri=self.model_uri, text=text)
+        request = TextEmbeddingRequest(model_uri=model_uri, text=text)
         stub = EmbeddingsServiceStub(channel)
-        res = stub.TextEmbedding(request, metadata=self._grpc_metadata)
-        result.append(res.embedding)
+        res = stub.TextEmbedding(request, metadata=self._grpc_metadata)  # type: ignore[attr-defined]
+        result.append(list(res.embedding))
+        time.sleep(self.sleep_interval)
 
     return result
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/arangodb_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/arangodb_graph.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/falkordb_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/falkordb_graph.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,12 @@
+import warnings
 from typing import Any, Dict, List, Optional
 
+from langchain_core._api import deprecated
+
 from langchain_community.graphs.graph_document import GraphDocument
 from langchain_community.graphs.graph_store import GraphStore
 
 node_properties_query = """
 MATCH (n)
 WITH keys(n) as keys, labels(n) AS labels
 WITH CASE WHEN keys = [] THEN [NULL] ELSE keys END AS keys, labels
@@ -54,34 +57,85 @@
         port: int = 6379,
         username: Optional[str] = None,
         password: Optional[str] = None,
         ssl: bool = False,
     ) -> None:
         """Create a new FalkorDB graph wrapper instance."""
         try:
-            import redis
-            from redis.commands.graph import Graph
-        except ImportError:
-            raise ImportError(
-                "Could not import redis python package. "
-                "Please install it with `pip install redis`."
+            self.__init_falkordb_connection(
+                database, host, port, username, password, ssl
             )
 
-        self._driver = redis.Redis(
-            host=host, port=port, username=username, password=password, ssl=ssl
-        )
-        self._graph = Graph(self._driver, database)
+        except ImportError:
+            try:
+                # Falls back to using the redis package just for backwards compatibility
+                self.__init_redis_connection(
+                    database, host, port, username, password, ssl
+                )
+            except ImportError:
+                raise ImportError(
+                    "Could not import falkordb python package. "
+                    "Please install it with `pip install falkordb`."
+                )
+
         self.schema: str = ""
         self.structured_schema: Dict[str, Any] = {}
 
         try:
             self.refresh_schema()
         except Exception as e:
             raise ValueError(f"Could not refresh schema. Error: {e}")
 
+    def __init_falkordb_connection(
+        self,
+        database: str,
+        host: str = "localhost",
+        port: int = 6379,
+        username: Optional[str] = None,
+        password: Optional[str] = None,
+        ssl: bool = False,
+    ) -> None:
+        from falkordb import FalkorDB
+
+        try:
+            self._driver = FalkorDB(
+                host=host, port=port, username=username, password=password, ssl=ssl
+            )
+        except Exception as e:
+            raise ConnectionError(f"Failed to connect to FalkorDB: {e}")
+
+        self._graph = self._driver.select_graph(database)
+
+    @deprecated("0.0.31", alternative="__init_falkordb_connection")
+    def __init_redis_connection(
+        self,
+        database: str,
+        host: str = "localhost",
+        port: int = 6379,
+        username: Optional[str] = None,
+        password: Optional[str] = None,
+        ssl: bool = False,
+    ) -> None:
+        import redis
+        from redis.commands.graph import Graph
+
+        # show deprecation warning
+        warnings.warn(
+            "Using the redis package is deprecated. "
+            "Please use the falkordb package instead, "
+            "install it with `pip install falkordb`.",
+            DeprecationWarning,
+        )
+
+        self._driver = redis.Redis(
+            host=host, port=port, username=username, password=password, ssl=ssl
+        )
+
+        self._graph = Graph(self._driver, database)
+
     @property
     def get_schema(self) -> str:
         """Returns the schema of the FalkorDB database"""
         return self.schema
 
     @property
     def get_structured_schema(self) -> Dict[str, Any]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/graph_document.py` & `gigachain_community-0.2.0/langchain_community/graphs/graph_document.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/graph_store.py` & `gigachain_community-0.2.0/langchain_community/graphs/graph_store.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 from abc import abstractmethod
 from typing import Any, Dict, List
 
 from langchain_community.graphs.graph_document import GraphDocument
 
 
 class GraphStore:
-    """An abstract class wrapper for graph operations."""
+    """Abstract class for graph operations."""
 
     @property
     @abstractmethod
     def get_schema(self) -> str:
-        """Returns the schema of the Graph database"""
+        """Return the schema of the Graph database"""
         pass
 
     @property
     @abstractmethod
     def get_structured_schema(self) -> Dict[str, Any]:
-        """Returns the schema of the Graph database"""
+        """Return the schema of the Graph database"""
         pass
 
     @abstractmethod
     def query(self, query: str, params: dict = {}) -> List[Dict[str, Any]]:
         """Query the graph."""
         pass
 
     @abstractmethod
     def refresh_schema(self) -> None:
-        """Refreshes the graph schema information."""
+        """Refresh the graph schema information."""
         pass
 
     @abstractmethod
     def add_graph_documents(
         self, graph_documents: List[GraphDocument], include_source: bool = False
     ) -> None:
         """Take GraphDocument as input as uses it to construct a graph."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/hugegraph.py` & `gigachain_community-0.2.0/langchain_community/graphs/hugegraph.py`

 * *Files 5% similar despite different names*

```diff
@@ -24,15 +24,15 @@
         port: int = 8081,
         graph: str = "hugegraph",
     ) -> None:
         """Create a new HugeGraph wrapper instance."""
         try:
             from hugegraph.connection import PyHugeGraph
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Please install HugeGraph Python client first: "
                 "`pip3 install hugegraph-python`"
             )
 
         self.username = username
         self.password = password
         self.address = address
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/kuzu_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/kuzu_graph.py`

 * *Files 8% similar despite different names*

```diff
@@ -32,18 +32,15 @@
     @property
     def get_schema(self) -> str:
         """Returns the schema of the Kzu database"""
         return self.schema
 
     def query(self, query: str, params: dict = {}) -> List[Dict[str, Any]]:
         """Query Kzu database"""
-        params_list = []
-        for param_name in params:
-            params_list.append([param_name, params[param_name]])
-        result = self.conn.execute(query, params_list)
+        result = self.conn.execute(query, params)
         column_names = result.get_column_names()
         return_list = []
         while result.has_next():
             row = result.get_next()
             return_list.append(dict(zip(column_names, row)))
         return return_list
 
@@ -75,28 +72,24 @@
         for table in rel_tables:
             relationships.append(
                 "(:%s)-[:%s]->(:%s)" % (table["src"], table["name"], table["dst"])
             )
 
         rel_properties = []
         for table in rel_tables:
-            current_table_schema = {"properties": [], "label": table["name"]}
-            properties_text = self.conn._connection.get_rel_property_names(
-                table["name"]
-            ).split("\n")
-            for i, line in enumerate(properties_text):
-                # The first 3 lines defines src, dst and name, so we skip them
-                if i < 3:
-                    continue
-                if not line:
-                    continue
-                property_name, property_type = line.strip().split(" ")
-                current_table_schema["properties"].append(
-                    (property_name, property_type)
-                )
+            table_name = table["name"]
+            current_table_schema = {"properties": [], "label": table_name}
+            query_result = self.conn.execute(
+                f"CALL table_info('{table_name}') RETURN *;"
+            )
+            while query_result.has_next():
+                row = query_result.get_next()
+                prop_name = row[1]
+                prop_type = row[2]
+                current_table_schema["properties"].append((prop_name, prop_type))
             rel_properties.append(current_table_schema)
 
         self.schema = (
             f"Node properties: {node_properties}\n"
             f"Relationships properties: {rel_properties}\n"
             f"Relationships: {relationships}\n"
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/nebula_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/nebula_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
         session_pool_size: int = 30,
     ) -> None:
         """Create a new NebulaGraph wrapper instance."""
         try:
             import nebula3  # noqa: F401
             import pandas  # noqa: F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Please install NebulaGraph Python client and pandas first: "
                 "`pip install nebula3-python pandas`"
             )
 
         self.username = username
         self.password = password
         self.address = address
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/neptune_graph.py` & `gigachain_community-0.2.0/langchain_community/utilities/redis.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,270 +1,217 @@
-from typing import Any, Dict, List, Optional, Tuple, Union
+from __future__ import annotations
 
+import logging
+import re
+from typing import TYPE_CHECKING, Any, List, Optional, Pattern
+from urllib.parse import urlparse
 
-class NeptuneQueryException(Exception):
-    """A class to handle queries that fail to execute"""
+import numpy as np
 
-    def __init__(self, exception: Union[str, Dict]):
-        if isinstance(exception, dict):
-            self.message = exception["message"] if "message" in exception else "unknown"
-            self.details = exception["details"] if "details" in exception else "unknown"
+logger = logging.getLogger(__name__)
+
+if TYPE_CHECKING:
+    from redis.client import Redis as RedisType
+
+
+def _array_to_buffer(array: List[float], dtype: Any = np.float32) -> bytes:
+    return np.array(array).astype(dtype).tobytes()
+
+
+def _buffer_to_array(buffer: bytes, dtype: Any = np.float32) -> List[float]:
+    return np.frombuffer(buffer, dtype=dtype).tolist()
+
+
+class TokenEscaper:
+    """
+    Escape punctuation within an input string.
+    """
+
+    # Characters that RediSearch requires us to escape during queries.
+    # Source: https://redis.io/docs/stack/search/reference/escaping/#the-rules-of-text-field-tokenization
+    DEFAULT_ESCAPED_CHARS = r"[,.<>{}\[\]\\\"\':;!@#$%^&*()\-+=~\/ ]"
+
+    def __init__(self, escape_chars_re: Optional[Pattern] = None):
+        if escape_chars_re:
+            self.escaped_chars_re = escape_chars_re
         else:
-            self.message = exception
-            self.details = "unknown"
+            self.escaped_chars_re = re.compile(self.DEFAULT_ESCAPED_CHARS)
+
+    def escape(self, value: str) -> str:
+        if not isinstance(value, str):
+            raise TypeError(
+                "Value must be a string object for token escaping."
+                f"Got type {type(value)}"
+            )
 
-    def get_message(self) -> str:
-        return self.message
+        def escape_symbol(match: re.Match) -> str:
+            value = match.group(0)
+            return f"\\{value}"
+
+        return self.escaped_chars_re.sub(escape_symbol, value)
+
+
+def check_redis_module_exist(client: RedisType, required_modules: List[dict]) -> None:
+    """Check if the correct Redis modules are installed."""
+    installed_modules = client.module_list()
+    installed_modules = {
+        module[b"name"].decode("utf-8"): module for module in installed_modules
+    }
+    for module in required_modules:
+        if module["name"] in installed_modules and int(
+            installed_modules[module["name"]][b"ver"]
+        ) >= int(module["ver"]):
+            return
+    # otherwise raise error
+    error_message = (
+        "Redis cannot be used as a vector database without RediSearch >=2.4"
+        "Please head to https://redis.io/docs/stack/search/quick_start/"
+        "to know more about installing the RediSearch module within Redis Stack."
+    )
+    logger.error(error_message)
+    raise ValueError(error_message)
+
+
+def get_client(redis_url: str, **kwargs: Any) -> RedisType:
+    """Get a redis client from the connection url given. This helper accepts
+    urls for Redis server (TCP with/without TLS or UnixSocket) as well as
+    Redis Sentinel connections.
 
-    def get_details(self) -> Any:
-        return self.details
+    Redis Cluster is not supported.
 
+    Before creating a connection the existence of the database driver is checked
+    an and ValueError raised otherwise
 
-class NeptuneGraph:
-    """Neptune wrapper for graph operations.
-
-    Args:
-        host: endpoint for the database instance
-        port: port number for the database instance, default is 8182
-        use_https: whether to use secure connection, default is True
-        client: optional boto3 Neptune client
-        credentials_profile_name: optional AWS profile name
-        region_name: optional AWS region, e.g., us-west-2
-        service: optional service name, default is neptunedata
-        sign: optional, whether to sign the request payload, default is True
+    To use, you should have the ``redis`` python package installed.
 
     Example:
         .. code-block:: python
 
-        graph = NeptuneGraph(
-            host='<my-cluster>',
-            port=8182
-        )
+            from langchain_community.utilities.redis import get_client
+            redis_client = get_client(
+                redis_url="redis://username:password@localhost:6379"
+                index_name="my-index",
+                embedding_function=embeddings.embed_query,
+            )
 
-    *Security note*: Make sure that the database connection uses credentials
-        that are narrowly-scoped to only include necessary permissions.
-        Failure to do so may result in data corruption or loss, since the calling
-        code may attempt commands that would result in deletion, mutation
-        of data if appropriately prompted or reading sensitive data if such
-        data is present in the database.
-        The best way to guard against such negative outcomes is to (as appropriate)
-        limit the permissions granted to the credentials used with this tool.
+    To use a redis replication setup with multiple redis server and redis sentinels
+    set "redis_url" to "redis+sentinel://" scheme. With this url format a path is
+    needed holding the name of the redis service within the sentinels to get the
+    correct redis server connection. The default service name is "mymaster". The
+    optional second part of the path is the redis db number to connect to.
+
+    An optional username or password is used for booth connections to the rediserver
+    and the sentinel, different passwords for server and sentinel are not supported.
+    And as another constraint only one sentinel instance can be given:
 
-        See https://python.langchain.com/docs/security for more information.
-    """
+    Example:
+        .. code-block:: python
 
-    def __init__(
-        self,
-        host: str,
-        port: int = 8182,
-        use_https: bool = True,
-        client: Any = None,
-        credentials_profile_name: Optional[str] = None,
-        region_name: Optional[str] = None,
-        service: str = "neptunedata",
-        sign: bool = True,
-    ) -> None:
-        """Create a new Neptune graph wrapper instance."""
-
-        try:
-            if client is not None:
-                self.client = client
-            else:
-                import boto3
-
-                if credentials_profile_name is not None:
-                    session = boto3.Session(profile_name=credentials_profile_name)
-                else:
-                    # use default credentials
-                    session = boto3.Session()
-
-                client_params = {}
-                if region_name:
-                    client_params["region_name"] = region_name
-
-                protocol = "https" if use_https else "http"
-
-                client_params["endpoint_url"] = f"{protocol}://{host}:{port}"
-
-                if sign:
-                    self.client = session.client(service, **client_params)
-                else:
-                    from botocore import UNSIGNED
-                    from botocore.config import Config
-
-                    self.client = session.client(
-                        service,
-                        **client_params,
-                        config=Config(signature_version=UNSIGNED),
-                    )
-
-        except ImportError:
-            raise ModuleNotFoundError(
-                "Could not import boto3 python package. "
-                "Please install it with `pip install boto3`."
-            )
-        except Exception as e:
-            if type(e).__name__ == "UnknownServiceError":
-                raise ModuleNotFoundError(
-                    "NeptuneGraph requires a boto3 version 1.28.38 or greater."
-                    "Please install it with `pip install -U boto3`."
-                ) from e
-            else:
-                raise ValueError(
-                    "Could not load credentials to authenticate with AWS client. "
-                    "Please check that credentials in the specified "
-                    "profile name are valid."
-                ) from e
-
-        try:
-            self._refresh_schema()
-        except Exception as e:
-            raise NeptuneQueryException(
-                {
-                    "message": "Could not get schema for Neptune database",
-                    "detail": str(e),
-                }
+            from langchain_community.utilities.redis import get_client
+            redis_client = get_client(
+                redis_url="redis+sentinel://username:password@sentinelhost:26379/mymaster/0"
+                index_name="my-index",
+                embedding_function=embeddings.embed_query,
             )
+    """
 
-    @property
-    def get_schema(self) -> str:
-        """Returns the schema of the Neptune database"""
-        return self.schema
-
-    def query(self, query: str, params: dict = {}) -> Dict[str, Any]:
-        """Query Neptune database."""
-        try:
-            return self.client.execute_open_cypher_query(openCypherQuery=query)
-        except Exception as e:
-            raise NeptuneQueryException(
-                {
-                    "message": "An error occurred while executing the query.",
-                    "details": str(e),
-                }
-            )
+    # Initialize with necessary components.
+    try:
+        import redis
+    except ImportError:
+        raise ImportError(
+            "Could not import redis python package. "
+            "Please install it with `pip install redis>=4.1.0`."
+        )
 
-    def _get_summary(self) -> Dict:
-        try:
-            response = self.client.get_propertygraph_summary()
-        except Exception as e:
-            raise NeptuneQueryException(
-                {
-                    "message": (
-                        "Summary API is not available for this instance of Neptune,"
-                        "ensure the engine version is >=1.2.1.0"
-                    ),
-                    "details": str(e),
-                }
-            )
+    # check if normal redis:// or redis+sentinel:// url
+    if redis_url.startswith("redis+sentinel"):
+        redis_client = _redis_sentinel_client(redis_url, **kwargs)
+    elif redis_url.startswith("rediss+sentinel"):  # sentinel with TLS support enables
+        kwargs["ssl"] = True
+        if "ssl_cert_reqs" not in kwargs:
+            kwargs["ssl_cert_reqs"] = "none"
+        redis_client = _redis_sentinel_client(redis_url, **kwargs)
+    else:
+        # connect to redis server from url, reconnect with cluster client if needed
+        redis_client = redis.from_url(redis_url, **kwargs)
+        if _check_for_cluster(redis_client):
+            redis_client.close()
+            redis_client = _redis_cluster_client(redis_url, **kwargs)
+    return redis_client
+
+
+def _redis_sentinel_client(redis_url: str, **kwargs: Any) -> RedisType:
+    """helper method to parse an (un-official) redis+sentinel url
+    and create a Sentinel connection to fetch the final redis client
+    connection to a replica-master for read-write operations.
+
+    If username and/or password for authentication is given the
+    same credentials are used for the Redis Sentinel as well as Redis Server.
+    With this implementation using a redis url only it is not possible
+    to use different data for authentication on booth systems.
+    """
+    import redis
 
-        try:
-            summary = response["payload"]["graphSummary"]
-        except Exception:
-            raise NeptuneQueryException(
-                {
-                    "message": "Summary API did not return a valid response.",
-                    "details": response.content.decode(),
-                }
+    parsed_url = urlparse(redis_url)
+    # sentinel needs list with (host, port) tuple, use default port if none available
+    sentinel_list = [(parsed_url.hostname or "localhost", parsed_url.port or 26379)]
+    if parsed_url.path:
+        # "/mymaster/0" first part is service name, optional second part is db number
+        path_parts = parsed_url.path.split("/")
+        service_name = path_parts[1] or "mymaster"
+        if len(path_parts) > 2:
+            kwargs["db"] = path_parts[2]
+    else:
+        service_name = "mymaster"
+
+    sentinel_args = {}
+    if parsed_url.password:
+        sentinel_args["password"] = parsed_url.password
+        kwargs["password"] = parsed_url.password
+    if parsed_url.username:
+        sentinel_args["username"] = parsed_url.username
+        kwargs["username"] = parsed_url.username
+
+    # check for all SSL related properties and copy them into sentinel_kwargs too,
+    # add client_name also
+    for arg in kwargs:
+        if arg.startswith("ssl") or arg == "client_name":
+            sentinel_args[arg] = kwargs[arg]
+
+    # sentinel user/pass is part of sentinel_kwargs, user/pass for redis server
+    # connection as direct parameter in kwargs
+    sentinel_client = redis.sentinel.Sentinel(
+        sentinel_list, sentinel_kwargs=sentinel_args, **kwargs
+    )
+
+    # redis server might have password but not sentinel - fetch this error and try
+    # again without pass, everything else cannot be handled here -> user needed
+    try:
+        sentinel_client.execute_command("ping")
+    except redis.exceptions.AuthenticationError as ae:
+        if "no password is set" in ae.args[0]:
+            logger.warning(
+                "Redis sentinel connection configured with password but Sentinel \
+answered NO PASSWORD NEEDED - Please check Sentinel configuration"
             )
+            sentinel_client = redis.sentinel.Sentinel(sentinel_list, **kwargs)
         else:
-            return summary
+            raise ae
+
+    return sentinel_client.master_for(service_name)
+
+
+def _check_for_cluster(redis_client: RedisType) -> bool:
+    import redis
+
+    try:
+        cluster_info = redis_client.info("cluster")
+        return cluster_info["cluster_enabled"] == 1
+    except redis.exceptions.RedisError:
+        return False
+
+
+def _redis_cluster_client(redis_url: str, **kwargs: Any) -> RedisType:
+    from redis.cluster import RedisCluster
 
-    def _get_labels(self) -> Tuple[List[str], List[str]]:
-        """Get node and edge labels from the Neptune statistics summary"""
-        summary = self._get_summary()
-        n_labels = summary["nodeLabels"]
-        e_labels = summary["edgeLabels"]
-        return n_labels, e_labels
-
-    def _get_triples(self, e_labels: List[str]) -> List[str]:
-        triple_query = """
-        MATCH (a)-[e:`{e_label}`]->(b)
-        WITH a,e,b LIMIT 3000
-        RETURN DISTINCT labels(a) AS from, type(e) AS edge, labels(b) AS to
-        LIMIT 10
-        """
-
-        triple_template = "(:`{a}`)-[:`{e}`]->(:`{b}`)"
-        triple_schema = []
-        for label in e_labels:
-            q = triple_query.format(e_label=label)
-            data = self.query(q)
-            for d in data["results"]:
-                triple = triple_template.format(
-                    a=d["from"][0], e=d["edge"], b=d["to"][0]
-                )
-                triple_schema.append(triple)
-
-        return triple_schema
-
-    def _get_node_properties(self, n_labels: List[str], types: Dict) -> List:
-        node_properties_query = """
-        MATCH (a:`{n_label}`)
-        RETURN properties(a) AS props
-        LIMIT 100
-        """
-        node_properties = []
-        for label in n_labels:
-            q = node_properties_query.format(n_label=label)
-            data = {"label": label, "properties": self.query(q)["results"]}
-            s = set({})
-            for p in data["properties"]:
-                for k, v in p["props"].items():
-                    s.add((k, types[type(v).__name__]))
-
-            np = {
-                "properties": [{"property": k, "type": v} for k, v in s],
-                "labels": label,
-            }
-            node_properties.append(np)
-
-        return node_properties
-
-    def _get_edge_properties(self, e_labels: List[str], types: Dict[str, Any]) -> List:
-        edge_properties_query = """
-        MATCH ()-[e:`{e_label}`]->()
-        RETURN properties(e) AS props
-        LIMIT 100
-        """
-        edge_properties = []
-        for label in e_labels:
-            q = edge_properties_query.format(e_label=label)
-            data = {"label": label, "properties": self.query(q)["results"]}
-            s = set({})
-            for p in data["properties"]:
-                for k, v in p["props"].items():
-                    s.add((k, types[type(v).__name__]))
-
-            ep = {
-                "type": label,
-                "properties": [{"property": k, "type": v} for k, v in s],
-            }
-            edge_properties.append(ep)
-
-        return edge_properties
-
-    def _refresh_schema(self) -> None:
-        """
-        Refreshes the Neptune graph schema information.
-        """
-
-        types = {
-            "str": "STRING",
-            "float": "DOUBLE",
-            "int": "INTEGER",
-            "list": "LIST",
-            "dict": "MAP",
-            "bool": "BOOLEAN",
-        }
-        n_labels, e_labels = self._get_labels()
-        triple_schema = self._get_triples(e_labels)
-        node_properties = self._get_node_properties(n_labels, types)
-        edge_properties = self._get_edge_properties(e_labels, types)
-
-        self.schema = f"""
-        Node properties are the following:
-        {node_properties}
-        Relationship properties are the following:
-        {edge_properties}
-        The relationships are the following:
-        {triple_schema}
-        """
+    return RedisCluster.from_url(redis_url, **kwargs)  # type: ignore[return-value]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/networkx_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/networkx_graph.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 """Networkx wrapper for graph operations."""
+
 from __future__ import annotations
 
 from typing import Any, List, NamedTuple, Optional, Tuple
 
 KG_TRIPLE_DELIMITER = "<|>"
 
 
 class KnowledgeTriple(NamedTuple):
-    """A triple in the graph."""
+    """Knowledge triple in the graph."""
 
     subject: str
     predicate: str
     object_: str
 
     @classmethod
     def from_string(cls, triple_string: str) -> "KnowledgeTriple":
@@ -131,14 +132,50 @@
 
         nx.write_gml(self._graph, path)
 
     def clear(self) -> None:
         """Clear the graph."""
         self._graph.clear()
 
+    def clear_edges(self) -> None:
+        """Clear the graph edges."""
+        self._graph.clear_edges()
+
+    def add_node(self, node: str) -> None:
+        """Add node in the graph."""
+        self._graph.add_node(node)
+
+    def remove_node(self, node: str) -> None:
+        """Remove node from the graph."""
+        if self._graph.has_node(node):
+            self._graph.remove_node(node)
+
+    def has_node(self, node: str) -> bool:
+        """Return if graph has the given node."""
+        return self._graph.has_node(node)
+
+    def remove_edge(self, source_node: str, destination_node: str) -> None:
+        """Remove edge from the graph."""
+        self._graph.remove_edge(source_node, destination_node)
+
+    def has_edge(self, source_node: str, destination_node: str) -> bool:
+        """Return if graph has an edge between the given nodes."""
+        if self._graph.has_node(source_node) and self._graph.has_node(destination_node):
+            return self._graph.has_edge(source_node, destination_node)
+        else:
+            return False
+
+    def get_neighbors(self, node: str) -> List[str]:
+        """Return the neighbor nodes of the given node."""
+        return self._graph.neighbors(node)
+
+    def get_number_of_nodes(self) -> int:
+        """Get number of nodes in the graph."""
+        return self._graph.number_of_nodes()
+
     def get_topological_sort(self) -> List[str]:
         """Get a list of entity names in the graph sorted by causal dependence."""
         import networkx as nx
 
         return list(nx.topological_sort(self._graph))
 
     def draw_graphviz(self, **kwargs: Any) -> None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/graphs/rdf_graph.py` & `gigachain_community-0.2.0/langchain_community/graphs/rdf_graph.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from typing import (
     TYPE_CHECKING,
+    Dict,
     List,
     Optional,
 )
 
 if TYPE_CHECKING:
     import rdflib
 
@@ -111,38 +112,45 @@
         self,
         source_file: Optional[str] = None,
         serialization: Optional[str] = "ttl",
         query_endpoint: Optional[str] = None,
         update_endpoint: Optional[str] = None,
         standard: Optional[str] = "rdf",
         local_copy: Optional[str] = None,
+        graph_kwargs: Optional[Dict] = None,
+        store_kwargs: Optional[Dict] = None,
     ) -> None:
         """
         Set up the RDFlib graph
 
         :param source_file: either a path for a local file or a URL
         :param serialization: serialization of the input
         :param query_endpoint: SPARQL endpoint for queries, read access
         :param update_endpoint: SPARQL endpoint for UPDATE queries, write access
         :param standard: RDF, RDFS, or OWL
         :param local_copy: new local copy for storing changes
+        :param graph_kwargs: Additional rdflib.Graph specific kwargs
+        that will be used to initialize it,
+        if query_endpoint is provided.
+        :param store_kwargs: Additional sparqlstore.SPARQLStore specific kwargs
+        that will be used to initialize it,
+        if query_endpoint is provided.
         """
         self.source_file = source_file
         self.serialization = serialization
         self.query_endpoint = query_endpoint
         self.update_endpoint = update_endpoint
         self.standard = standard
         self.local_copy = local_copy
 
         try:
             import rdflib
-            from rdflib.graph import DATASET_DEFAULT_GRAPH_ID as default
             from rdflib.plugins.stores import sparqlstore
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import rdflib python package. "
                 "Please install it with `pip install rdflib`."
             )
         if self.standard not in (supported_standards := ("rdf", "rdfs", "owl")):
             raise ValueError(
                 f"Invalid standard. Supported standards are: {supported_standards}."
             )
@@ -166,22 +174,24 @@
                 self.mode = "local"
                 if self.local_copy is None:
                     self.local_copy = self.source_file
             self.graph = rdflib.Graph()
             self.graph.parse(source_file, format=self.serialization)
 
         if query_endpoint:
+            store_kwargs = store_kwargs or {}
             self.mode = "store"
             if not update_endpoint:
-                self._store = sparqlstore.SPARQLStore()
+                self._store = sparqlstore.SPARQLStore(**store_kwargs)
                 self._store.open(query_endpoint)
             else:
-                self._store = sparqlstore.SPARQLUpdateStore()
+                self._store = sparqlstore.SPARQLUpdateStore(**store_kwargs)
                 self._store.open((query_endpoint, update_endpoint))
-            self.graph = rdflib.Graph(self._store, identifier=default)
+            graph_kwargs = graph_kwargs or {}
+            self.graph = rdflib.Graph(self._store, **graph_kwargs)
 
         # Verify that the graph was loaded
         if not len(self.graph):
             raise AssertionError("The graph is empty.")
 
         # Set schema
         self.schema = ""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/indexes/_sql_record_manager.py` & `gigachain_community-0.2.0/langchain_community/indexes/_sql_record_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,18 +9,29 @@
 allow it to work with a variety of SQL as a backend.
 
 * Each key is associated with an updated_at field.
 * This filed is updated whenever the key is updated.
 * Keys can be listed based on the updated at field.
 * Keys can be deleted.
 """
+
 import contextlib
 import decimal
 import uuid
-from typing import Any, AsyncGenerator, Dict, Generator, List, Optional, Sequence, Union
+from typing import (
+    Any,
+    AsyncGenerator,
+    Dict,
+    Generator,
+    List,
+    Optional,
+    Sequence,
+    Union,
+    cast,
+)
 
 from sqlalchemy import (
     URL,
     Column,
     Engine,
     Float,
     Index,
@@ -171,18 +182,18 @@
         finally:
             session.close()
 
     @contextlib.asynccontextmanager
     async def _amake_session(self) -> AsyncGenerator[AsyncSession, None]:
         """Create a session and close it after use."""
 
-        if not isinstance(self.session_factory, async_sessionmaker):
+        if not isinstance(self.engine, AsyncEngine):
             raise AssertionError("This method is not supported for sync engines.")
 
-        async with self.session_factory() as session:
+        async with cast(AsyncSession, self.session_factory()) as session:
             yield session
 
     def get_time(self) -> float:
         """Get the current server time as a timestamp.
 
         Please note it's critical that time is obtained from the server since
         we want a monotonic clock.
@@ -298,15 +309,15 @@
                     ),
                 )
             elif self.dialect == "postgresql":
                 from sqlalchemy.dialects.postgresql import insert as pg_insert
 
                 # Note: uses SQLite insert to make on_conflict_do_update work.
                 # This code needs to be generalized a bit to work with more dialects.
-                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)
+                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)  # type: ignore[assignment]
                 stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]
                     "uix_key_namespace",  # Name of constraint
                     set_=dict(
                         # attr-defined type ignore
                         updated_at=insert_stmt.excluded.updated_at,  # type: ignore
                         group_id=insert_stmt.excluded.group_id,  # type: ignore
                     ),
@@ -373,15 +384,15 @@
                     ),
                 )
             elif self.dialect == "postgresql":
                 from sqlalchemy.dialects.postgresql import insert as pg_insert
 
                 # Note: uses SQLite insert to make on_conflict_do_update work.
                 # This code needs to be generalized a bit to work with more dialects.
-                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)
+                insert_stmt = pg_insert(UpsertionRecord).values(records_to_upsert)  # type: ignore[assignment]
                 stmt = insert_stmt.on_conflict_do_update(  # type: ignore[attr-defined]
                     "uix_key_namespace",  # Name of constraint
                     set_=dict(
                         # attr-defined type ignore
                         updated_at=insert_stmt.excluded.updated_at,  # type: ignore
                         group_id=insert_stmt.excluded.group_id,  # type: ignore
                     ),
@@ -456,15 +467,15 @@
                 query = query.filter(  # type: ignore[attr-defined]
                     UpsertionRecord.group_id.in_(group_ids)
                 )
 
             if limit:
                 query = query.limit(limit)  # type: ignore[attr-defined]
             records = query.all()  # type: ignore[attr-defined]
-        return [r.key for r in records]
+        return [r.key for r in records]  # type: ignore[misc]
 
     async def alist_keys(
         self,
         *,
         before: Optional[float] = None,
         after: Optional[float] = None,
         group_ids: Optional[Sequence[str]] = None,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/indexes/base.py` & `gigachain_community-0.2.0/langchain_community/indexes/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from abc import ABC, abstractmethod
 from typing import List, Optional, Sequence
 
 NAMESPACE_UUID = uuid.UUID(int=1984)
 
 
 class RecordManager(ABC):
-    """An abstract base class representing the interface for a record manager."""
+    """Abstract base class for a record manager."""
 
     def __init__(
         self,
         namespace: str,
     ) -> None:
         """Initialize the record manager.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/__init__.py` & `gigachain_community-0.2.0/langchain_community/llms/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -13,555 +13,649 @@
 .. code-block::
 
     LLMResult, PromptValue,
     CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,
     CallbackManager, AsyncCallbackManager,
     AIMessage, BaseMessage
 """  # noqa: E501
+
 from typing import Any, Callable, Dict, Type
 
+from langchain_core._api.deprecation import warn_deprecated
 from langchain_core.language_models.llms import BaseLLM
 
 
-def _import_ai21() -> Any:
+def _import_ai21() -> Type[BaseLLM]:
     from langchain_community.llms.ai21 import AI21
 
     return AI21
 
 
-def _import_aleph_alpha() -> Any:
+def _import_aleph_alpha() -> Type[BaseLLM]:
     from langchain_community.llms.aleph_alpha import AlephAlpha
 
     return AlephAlpha
 
 
-def _import_amazon_api_gateway() -> Any:
+def _import_amazon_api_gateway() -> Type[BaseLLM]:
     from langchain_community.llms.amazon_api_gateway import AmazonAPIGateway
 
     return AmazonAPIGateway
 
 
-def _import_anthropic() -> Any:
+def _import_anthropic() -> Type[BaseLLM]:
     from langchain_community.llms.anthropic import Anthropic
 
     return Anthropic
 
 
-def _import_anyscale() -> Any:
+def _import_anyscale() -> Type[BaseLLM]:
     from langchain_community.llms.anyscale import Anyscale
 
     return Anyscale
 
 
-def _import_aphrodite() -> Any:
+def _import_aphrodite() -> Type[BaseLLM]:
     from langchain_community.llms.aphrodite import Aphrodite
 
     return Aphrodite
 
 
-def _import_arcee() -> Any:
+def _import_arcee() -> Type[BaseLLM]:
     from langchain_community.llms.arcee import Arcee
 
     return Arcee
 
 
-def _import_aviary() -> Any:
+def _import_aviary() -> Type[BaseLLM]:
     from langchain_community.llms.aviary import Aviary
 
     return Aviary
 
 
-def _import_azureml_endpoint() -> Any:
+def _import_azureml_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint
 
     return AzureMLOnlineEndpoint
 
 
-def _import_baidu_qianfan_endpoint() -> Any:
+def _import_baichuan() -> Type[BaseLLM]:
+    from langchain_community.llms.baichuan import BaichuanLLM
+
+    return BaichuanLLM
+
+
+def _import_baidu_qianfan_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.baidu_qianfan_endpoint import QianfanLLMEndpoint
 
     return QianfanLLMEndpoint
 
 
-def _import_bananadev() -> Any:
+def _import_bananadev() -> Type[BaseLLM]:
     from langchain_community.llms.bananadev import Banana
 
     return Banana
 
 
-def _import_baseten() -> Any:
+def _import_baseten() -> Type[BaseLLM]:
     from langchain_community.llms.baseten import Baseten
 
     return Baseten
 
 
-def _import_beam() -> Any:
+def _import_beam() -> Type[BaseLLM]:
     from langchain_community.llms.beam import Beam
 
     return Beam
 
 
-def _import_bedrock() -> Any:
+def _import_bedrock() -> Type[BaseLLM]:
     from langchain_community.llms.bedrock import Bedrock
 
     return Bedrock
 
 
-def _import_bittensor() -> Any:
+def _import_bigdlllm() -> Type[BaseLLM]:
+    from langchain_community.llms.bigdl_llm import BigdlLLM
+
+    return BigdlLLM
+
+
+def _import_bittensor() -> Type[BaseLLM]:
     from langchain_community.llms.bittensor import NIBittensorLLM
 
     return NIBittensorLLM
 
 
-def _import_cerebriumai() -> Any:
+def _import_cerebriumai() -> Type[BaseLLM]:
     from langchain_community.llms.cerebriumai import CerebriumAI
 
     return CerebriumAI
 
 
-def _import_chatglm() -> Any:
+def _import_chatglm() -> Type[BaseLLM]:
     from langchain_community.llms.chatglm import ChatGLM
 
     return ChatGLM
 
 
-def _import_clarifai() -> Any:
+def _import_clarifai() -> Type[BaseLLM]:
     from langchain_community.llms.clarifai import Clarifai
 
     return Clarifai
 
 
-def _import_cohere() -> Any:
+def _import_cohere() -> Type[BaseLLM]:
     from langchain_community.llms.cohere import Cohere
 
     return Cohere
 
 
-def _import_ctransformers() -> Any:
+def _import_ctransformers() -> Type[BaseLLM]:
     from langchain_community.llms.ctransformers import CTransformers
 
     return CTransformers
 
 
-def _import_ctranslate2() -> Any:
+def _import_ctranslate2() -> Type[BaseLLM]:
     from langchain_community.llms.ctranslate2 import CTranslate2
 
     return CTranslate2
 
 
-def _import_databricks() -> Any:
+def _import_databricks() -> Type[BaseLLM]:
     from langchain_community.llms.databricks import Databricks
 
     return Databricks
 
 
+# deprecated / only for back compat - do not add to __all__
 def _import_databricks_chat() -> Any:
+    warn_deprecated(
+        since="0.0.22",
+        removal="0.3",
+        alternative_import="langchain_community.chat_models.ChatDatabricks",
+    )
     from langchain_community.chat_models.databricks import ChatDatabricks
 
     return ChatDatabricks
 
 
-def _import_deepinfra() -> Any:
+def _import_deepinfra() -> Type[BaseLLM]:
     from langchain_community.llms.deepinfra import DeepInfra
 
     return DeepInfra
 
 
-def _import_deepsparse() -> Any:
+def _import_deepsparse() -> Type[BaseLLM]:
     from langchain_community.llms.deepsparse import DeepSparse
 
     return DeepSparse
 
 
-def _import_edenai() -> Any:
+def _import_edenai() -> Type[BaseLLM]:
     from langchain_community.llms.edenai import EdenAI
 
     return EdenAI
 
 
-def _import_fake() -> Any:
+def _import_fake() -> Type[BaseLLM]:
     from langchain_community.llms.fake import FakeListLLM
 
     return FakeListLLM
 
 
-def _import_fireworks() -> Any:
+def _import_fireworks() -> Type[BaseLLM]:
     from langchain_community.llms.fireworks import Fireworks
 
     return Fireworks
 
 
-def _import_forefrontai() -> Any:
+def _import_forefrontai() -> Type[BaseLLM]:
     from langchain_community.llms.forefrontai import ForefrontAI
 
     return ForefrontAI
 
 
-def _import_gigachat() -> Any:
+def _import_friendli() -> Type[BaseLLM]:
+    from langchain_community.llms.friendli import Friendli
+
+    return Friendli
+
+
+def _import_gigachat() -> Type[BaseLLM]:
     from langchain_community.llms.gigachat import GigaChat
 
     return GigaChat
 
 
-def _import_google_palm() -> Any:
+def _import_google_palm() -> Type[BaseLLM]:
     from langchain_community.llms.google_palm import GooglePalm
 
     return GooglePalm
 
 
-def _import_gooseai() -> Any:
+def _import_gooseai() -> Type[BaseLLM]:
     from langchain_community.llms.gooseai import GooseAI
 
     return GooseAI
 
 
-def _import_gpt4all() -> Any:
+def _import_gpt4all() -> Type[BaseLLM]:
     from langchain_community.llms.gpt4all import GPT4All
 
     return GPT4All
 
 
-def _import_gradient_ai() -> Any:
+def _import_gradient_ai() -> Type[BaseLLM]:
     from langchain_community.llms.gradient_ai import GradientLLM
 
     return GradientLLM
 
 
-def _import_huggingface_endpoint() -> Any:
+def _import_huggingface_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint
 
     return HuggingFaceEndpoint
 
 
-def _import_huggingface_hub() -> Any:
+def _import_huggingface_hub() -> Type[BaseLLM]:
     from langchain_community.llms.huggingface_hub import HuggingFaceHub
 
     return HuggingFaceHub
 
 
-def _import_huggingface_pipeline() -> Any:
+def _import_huggingface_pipeline() -> Type[BaseLLM]:
     from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
 
     return HuggingFacePipeline
 
 
-def _import_huggingface_text_gen_inference() -> Any:
+def _import_huggingface_text_gen_inference() -> Type[BaseLLM]:
     from langchain_community.llms.huggingface_text_gen_inference import (
         HuggingFaceTextGenInference,
     )
 
     return HuggingFaceTextGenInference
 
 
-def _import_human() -> Any:
+def _import_human() -> Type[BaseLLM]:
     from langchain_community.llms.human import HumanInputLLM
 
     return HumanInputLLM
 
 
-def _import_javelin_ai_gateway() -> Any:
+def _import_ipex_llm() -> Type[BaseLLM]:
+    from langchain_community.llms.ipex_llm import IpexLLM
+
+    return IpexLLM
+
+
+def _import_javelin_ai_gateway() -> Type[BaseLLM]:
     from langchain_community.llms.javelin_ai_gateway import JavelinAIGateway
 
     return JavelinAIGateway
 
 
-def _import_koboldai() -> Any:
+def _import_koboldai() -> Type[BaseLLM]:
     from langchain_community.llms.koboldai import KoboldApiLLM
 
     return KoboldApiLLM
 
 
-def _import_llamacpp() -> Any:
+def _import_konko() -> Type[BaseLLM]:
+    from langchain_community.llms.konko import Konko
+
+    return Konko
+
+
+def _import_llamacpp() -> Type[BaseLLM]:
     from langchain_community.llms.llamacpp import LlamaCpp
 
     return LlamaCpp
 
 
-def _import_manifest() -> Any:
+def _import_llamafile() -> Type[BaseLLM]:
+    from langchain_community.llms.llamafile import Llamafile
+
+    return Llamafile
+
+
+def _import_manifest() -> Type[BaseLLM]:
     from langchain_community.llms.manifest import ManifestWrapper
 
     return ManifestWrapper
 
 
-def _import_minimax() -> Any:
+def _import_minimax() -> Type[BaseLLM]:
     from langchain_community.llms.minimax import Minimax
 
     return Minimax
 
 
-def _import_mlflow() -> Any:
+def _import_mlflow() -> Type[BaseLLM]:
     from langchain_community.llms.mlflow import Mlflow
 
     return Mlflow
 
 
+# deprecated / only for back compat - do not add to __all__
 def _import_mlflow_chat() -> Any:
+    warn_deprecated(
+        since="0.0.22",
+        removal="0.3",
+        alternative_import="langchain_community.chat_models.ChatMlflow",
+    )
     from langchain_community.chat_models.mlflow import ChatMlflow
 
     return ChatMlflow
 
 
-def _import_mlflow_ai_gateway() -> Any:
+def _import_mlflow_ai_gateway() -> Type[BaseLLM]:
     from langchain_community.llms.mlflow_ai_gateway import MlflowAIGateway
 
     return MlflowAIGateway
 
 
-def _import_modal() -> Any:
+def _import_mlx_pipeline() -> Type[BaseLLM]:
+    from langchain_community.llms.mlx_pipeline import MLXPipeline
+
+    return MLXPipeline
+
+
+def _import_modal() -> Type[BaseLLM]:
     from langchain_community.llms.modal import Modal
 
     return Modal
 
 
-def _import_mosaicml() -> Any:
+def _import_mosaicml() -> Type[BaseLLM]:
     from langchain_community.llms.mosaicml import MosaicML
 
     return MosaicML
 
 
-def _import_nlpcloud() -> Any:
+def _import_nlpcloud() -> Type[BaseLLM]:
     from langchain_community.llms.nlpcloud import NLPCloud
 
     return NLPCloud
 
 
-def _import_oci_md_tgi() -> Any:
+def _import_oci_md_tgi() -> Type[BaseLLM]:
     from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
         OCIModelDeploymentTGI,
     )
 
     return OCIModelDeploymentTGI
 
 
-def _import_oci_md_vllm() -> Any:
+def _import_oci_md_vllm() -> Type[BaseLLM]:
     from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
         OCIModelDeploymentVLLM,
     )
 
     return OCIModelDeploymentVLLM
 
 
-def _import_octoai_endpoint() -> Any:
+def _import_oci_gen_ai() -> Type[BaseLLM]:
+    from langchain_community.llms.oci_generative_ai import OCIGenAI
+
+    return OCIGenAI
+
+
+def _import_octoai_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
 
     return OctoAIEndpoint
 
 
-def _import_ollama() -> Any:
+def _import_ollama() -> Type[BaseLLM]:
     from langchain_community.llms.ollama import Ollama
 
     return Ollama
 
 
-def _import_opaqueprompts() -> Any:
+def _import_opaqueprompts() -> Type[BaseLLM]:
     from langchain_community.llms.opaqueprompts import OpaquePrompts
 
     return OpaquePrompts
 
 
-def _import_azure_openai() -> Any:
+def _import_azure_openai() -> Type[BaseLLM]:
     from langchain_community.llms.openai import AzureOpenAI
 
     return AzureOpenAI
 
 
-def _import_openai() -> Any:
+def _import_openai() -> Type[BaseLLM]:
     from langchain_community.llms.openai import OpenAI
 
     return OpenAI
 
 
-def _import_openai_chat() -> Any:
+def _import_openai_chat() -> Type[BaseLLM]:
     from langchain_community.llms.openai import OpenAIChat
 
     return OpenAIChat
 
 
-def _import_openllm() -> Any:
+def _import_openllm() -> Type[BaseLLM]:
     from langchain_community.llms.openllm import OpenLLM
 
     return OpenLLM
 
 
-def _import_openlm() -> Any:
+def _import_openlm() -> Type[BaseLLM]:
     from langchain_community.llms.openlm import OpenLM
 
     return OpenLM
 
 
-def _import_pai_eas_endpoint() -> Any:
+def _import_pai_eas_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.pai_eas_endpoint import PaiEasEndpoint
 
     return PaiEasEndpoint
 
 
-def _import_petals() -> Any:
+def _import_petals() -> Type[BaseLLM]:
     from langchain_community.llms.petals import Petals
 
     return Petals
 
 
-def _import_pipelineai() -> Any:
+def _import_pipelineai() -> Type[BaseLLM]:
     from langchain_community.llms.pipelineai import PipelineAI
 
     return PipelineAI
 
 
-def _import_predibase() -> Any:
+def _import_predibase() -> Type[BaseLLM]:
     from langchain_community.llms.predibase import Predibase
 
     return Predibase
 
 
-def _import_predictionguard() -> Any:
+def _import_predictionguard() -> Type[BaseLLM]:
     from langchain_community.llms.predictionguard import PredictionGuard
 
     return PredictionGuard
 
 
-def _import_promptlayer() -> Any:
+def _import_promptlayer() -> Type[BaseLLM]:
     from langchain_community.llms.promptlayer_openai import PromptLayerOpenAI
 
     return PromptLayerOpenAI
 
 
-def _import_promptlayer_chat() -> Any:
+def _import_promptlayer_chat() -> Type[BaseLLM]:
     from langchain_community.llms.promptlayer_openai import PromptLayerOpenAIChat
 
     return PromptLayerOpenAIChat
 
 
-def _import_replicate() -> Any:
+def _import_replicate() -> Type[BaseLLM]:
     from langchain_community.llms.replicate import Replicate
 
     return Replicate
 
 
-def _import_rwkv() -> Any:
+def _import_rwkv() -> Type[BaseLLM]:
     from langchain_community.llms.rwkv import RWKV
 
     return RWKV
 
 
-def _import_sagemaker_endpoint() -> Any:
+def _import_sagemaker_endpoint() -> Type[BaseLLM]:
     from langchain_community.llms.sagemaker_endpoint import SagemakerEndpoint
 
     return SagemakerEndpoint
 
 
-def _import_self_hosted() -> Any:
+def _import_sambaverse() -> Type[BaseLLM]:
+    from langchain_community.llms.sambanova import Sambaverse
+
+    return Sambaverse
+
+
+def _import_sambastudio() -> Type[BaseLLM]:
+    from langchain_community.llms.sambanova import SambaStudio
+
+    return SambaStudio
+
+
+def _import_self_hosted() -> Type[BaseLLM]:
     from langchain_community.llms.self_hosted import SelfHostedPipeline
 
     return SelfHostedPipeline
 
 
-def _import_self_hosted_hugging_face() -> Any:
+def _import_self_hosted_hugging_face() -> Type[BaseLLM]:
     from langchain_community.llms.self_hosted_hugging_face import (
         SelfHostedHuggingFaceLLM,
     )
 
     return SelfHostedHuggingFaceLLM
 
 
-def _import_stochasticai() -> Any:
+def _import_stochasticai() -> Type[BaseLLM]:
     from langchain_community.llms.stochasticai import StochasticAI
 
     return StochasticAI
 
 
-def _import_symblai_nebula() -> Any:
+def _import_symblai_nebula() -> Type[BaseLLM]:
     from langchain_community.llms.symblai_nebula import Nebula
 
     return Nebula
 
 
-def _import_textgen() -> Any:
+def _import_textgen() -> Type[BaseLLM]:
     from langchain_community.llms.textgen import TextGen
 
     return TextGen
 
 
-def _import_titan_takeoff() -> Any:
+def _import_titan_takeoff() -> Type[BaseLLM]:
     from langchain_community.llms.titan_takeoff import TitanTakeoff
 
     return TitanTakeoff
 
 
-def _import_titan_takeoff_pro() -> Any:
-    from langchain_community.llms.titan_takeoff_pro import TitanTakeoffPro
+def _import_titan_takeoff_pro() -> Type[BaseLLM]:
+    from langchain_community.llms.titan_takeoff import TitanTakeoff
 
-    return TitanTakeoffPro
+    return TitanTakeoff
 
 
-def _import_together() -> Any:
+def _import_together() -> Type[BaseLLM]:
     from langchain_community.llms.together import Together
 
     return Together
 
 
-def _import_tongyi() -> Any:
+def _import_tongyi() -> Type[BaseLLM]:
     from langchain_community.llms.tongyi import Tongyi
 
     return Tongyi
 
 
-def _import_vertex() -> Any:
+def _import_vertex() -> Type[BaseLLM]:
     from langchain_community.llms.vertexai import VertexAI
 
     return VertexAI
 
 
-def _import_vertex_model_garden() -> Any:
+def _import_vertex_model_garden() -> Type[BaseLLM]:
     from langchain_community.llms.vertexai import VertexAIModelGarden
 
     return VertexAIModelGarden
 
 
-def _import_vllm() -> Any:
+def _import_vllm() -> Type[BaseLLM]:
     from langchain_community.llms.vllm import VLLM
 
     return VLLM
 
 
-def _import_vllm_openai() -> Any:
+def _import_vllm_openai() -> Type[BaseLLM]:
     from langchain_community.llms.vllm import VLLMOpenAI
 
     return VLLMOpenAI
 
 
-def _import_watsonxllm() -> Any:
+def _import_watsonxllm() -> Type[BaseLLM]:
     from langchain_community.llms.watsonxllm import WatsonxLLM
 
     return WatsonxLLM
 
 
-def _import_writer() -> Any:
+def _import_weight_only_quantization() -> Any:
+    from langchain_community.llms.weight_only_quantization import (
+        WeightOnlyQuantPipeline,
+    )
+
+    return WeightOnlyQuantPipeline
+
+
+def _import_writer() -> Type[BaseLLM]:
     from langchain_community.llms.writer import Writer
 
     return Writer
 
 
-def _import_xinference() -> Any:
+def _import_xinference() -> Type[BaseLLM]:
     from langchain_community.llms.xinference import Xinference
 
     return Xinference
 
 
-def _import_yandex_gpt() -> Any:
+def _import_yandex_gpt() -> Type[BaseLLM]:
     from langchain_community.llms.yandex import YandexGPT
 
     return YandexGPT
 
 
-def _import_volcengine_maas() -> Any:
+def _import_yuan2() -> Type[BaseLLM]:
+    from langchain_community.llms.yuan2 import Yuan2
+
+    return Yuan2
+
+
+def _import_volcengine_maas() -> Type[BaseLLM]:
     from langchain_community.llms.volcengine_maas import VolcEngineMaasLLM
 
     return VolcEngineMaasLLM
 
 
+def _import_sparkllm() -> Type[BaseLLM]:
+    from langchain_community.llms.sparkllm import SparkLLM
+
+    return SparkLLM
+
+
 def __getattr__(name: str) -> Any:
     if name == "AI21":
         return _import_ai21()
     elif name == "AlephAlpha":
         return _import_aleph_alpha()
     elif name == "AmazonAPIGateway":
         return _import_amazon_api_gateway()
@@ -573,24 +667,28 @@
         return _import_aphrodite()
     elif name == "Arcee":
         return _import_arcee()
     elif name == "Aviary":
         return _import_aviary()
     elif name == "AzureMLOnlineEndpoint":
         return _import_azureml_endpoint()
+    elif name == "BaichuanLLM" or name == "Baichuan":
+        return _import_baichuan()
     elif name == "QianfanLLMEndpoint":
         return _import_baidu_qianfan_endpoint()
     elif name == "Banana":
         return _import_bananadev()
     elif name == "Baseten":
         return _import_baseten()
     elif name == "Beam":
         return _import_beam()
     elif name == "Bedrock":
         return _import_bedrock()
+    elif name == "BigdlLLM":
+        return _import_bigdlllm()
     elif name == "NIBittensorLLM":
         return _import_bittensor()
     elif name == "CerebriumAI":
         return _import_cerebriumai()
     elif name == "ChatGLM":
         return _import_chatglm()
     elif name == "Clarifai":
@@ -611,14 +709,16 @@
         return _import_edenai()
     elif name == "FakeListLLM":
         return _import_fake()
     elif name == "Fireworks":
         return _import_fireworks()
     elif name == "ForefrontAI":
         return _import_forefrontai()
+    elif name == "Friendli":
+        return _import_friendli()
     elif name == "GigaChat":
         return _import_gigachat()
     elif name == "GooglePalm":
         return _import_google_palm()
     elif name == "GooseAI":
         return _import_gooseai()
     elif name == "GPT4All":
@@ -631,38 +731,48 @@
         return _import_huggingface_hub()
     elif name == "HuggingFacePipeline":
         return _import_huggingface_pipeline()
     elif name == "HuggingFaceTextGenInference":
         return _import_huggingface_text_gen_inference()
     elif name == "HumanInputLLM":
         return _import_human()
+    elif name == "IpexLLM":
+        return _import_ipex_llm()
     elif name == "JavelinAIGateway":
         return _import_javelin_ai_gateway()
     elif name == "KoboldApiLLM":
         return _import_koboldai()
+    elif name == "Konko":
+        return _import_konko()
     elif name == "LlamaCpp":
         return _import_llamacpp()
+    elif name == "Llamafile":
+        return _import_llamafile()
     elif name == "ManifestWrapper":
         return _import_manifest()
     elif name == "Minimax":
         return _import_minimax()
     elif name == "Mlflow":
         return _import_mlflow()
     elif name == "MlflowAIGateway":
         return _import_mlflow_ai_gateway()
+    elif name == "MLXPipeline":
+        return _import_mlx_pipeline()
     elif name == "Modal":
         return _import_modal()
     elif name == "MosaicML":
         return _import_mosaicml()
     elif name == "NLPCloud":
         return _import_nlpcloud()
     elif name == "OCIModelDeploymentTGI":
         return _import_oci_md_tgi()
     elif name == "OCIModelDeploymentVLLM":
         return _import_oci_md_vllm()
+    elif name == "OCIGenAI":
+        return _import_oci_gen_ai()
     elif name == "OctoAIEndpoint":
         return _import_octoai_endpoint()
     elif name == "Ollama":
         return _import_ollama()
     elif name == "OpaquePrompts":
         return _import_opaqueprompts()
     elif name == "AzureOpenAI":
@@ -691,14 +801,18 @@
         return _import_promptlayer_chat()
     elif name == "Replicate":
         return _import_replicate()
     elif name == "RWKV":
         return _import_rwkv()
     elif name == "SagemakerEndpoint":
         return _import_sagemaker_endpoint()
+    elif name == "Sambaverse":
+        return _import_sambaverse()
+    elif name == "SambaStudio":
+        return _import_sambastudio()
     elif name == "SelfHostedPipeline":
         return _import_self_hosted()
     elif name == "SelfHostedHuggingFaceLLM":
         return _import_self_hosted_hugging_face()
     elif name == "StochasticAI":
         return _import_stochasticai()
     elif name == "Nebula":
@@ -719,28 +833,34 @@
         return _import_vertex_model_garden()
     elif name == "VLLM":
         return _import_vllm()
     elif name == "VLLMOpenAI":
         return _import_vllm_openai()
     elif name == "WatsonxLLM":
         return _import_watsonxllm()
+    elif name == "WeightOnlyQuantPipeline":
+        return _import_weight_only_quantization()
     elif name == "Writer":
         return _import_writer()
     elif name == "Xinference":
         return _import_xinference()
     elif name == "YandexGPT":
         return _import_yandex_gpt()
+    elif name == "Yuan2":
+        return _import_yuan2()
     elif name == "VolcEngineMaasLLM":
         return _import_volcengine_maas()
     elif name == "type_to_cls_dict":
         # for backwards compatibility
         type_to_cls_dict: Dict[str, Type[BaseLLM]] = {
             k: v() for k, v in get_type_to_cls_dict().items()
         }
         return type_to_cls_dict
+    elif name == "SparkLLM":
+        return _import_sparkllm()
     else:
         raise AttributeError(f"Could not find: {name}")
 
 
 __all__ = [
     "AI21",
     "AlephAlpha",
@@ -748,14 +868,15 @@
     "Anthropic",
     "Anyscale",
     "Aphrodite",
     "Arcee",
     "Aviary",
     "AzureMLOnlineEndpoint",
     "AzureOpenAI",
+    "BaichuanLLM",
     "Banana",
     "Baseten",
     "Beam",
     "Bedrock",
     "CTransformers",
     "CTranslate2",
     "CerebriumAI",
@@ -765,71 +886,84 @@
     "Databricks",
     "DeepInfra",
     "DeepSparse",
     "EdenAI",
     "FakeListLLM",
     "Fireworks",
     "ForefrontAI",
-    "GigaChat",
+    "Friendli",
     "GPT4All",
+    "GigaChat",
     "GooglePalm",
     "GooseAI",
     "GradientLLM",
     "HuggingFaceEndpoint",
     "HuggingFaceHub",
     "HuggingFacePipeline",
     "HuggingFaceTextGenInference",
     "HumanInputLLM",
+    "IpexLLM",
+    "JavelinAIGateway",
     "KoboldApiLLM",
+    "Konko",
     "LlamaCpp",
-    "TextGen",
+    "Llamafile",
     "ManifestWrapper",
     "Minimax",
+    "Mlflow",
     "MlflowAIGateway",
+    "MLXPipeline",
     "Modal",
     "MosaicML",
-    "Nebula",
     "NIBittensorLLM",
     "NLPCloud",
+    "Nebula",
+    "OCIGenAI",
     "OCIModelDeploymentTGI",
     "OCIModelDeploymentVLLM",
+    "OctoAIEndpoint",
     "Ollama",
+    "OpaquePrompts",
     "OpenAI",
     "OpenAIChat",
     "OpenLLM",
     "OpenLM",
     "PaiEasEndpoint",
     "Petals",
     "PipelineAI",
     "Predibase",
     "PredictionGuard",
     "PromptLayerOpenAI",
     "PromptLayerOpenAIChat",
-    "OpaquePrompts",
+    "QianfanLLMEndpoint",
     "RWKV",
     "Replicate",
     "SagemakerEndpoint",
+    "Sambaverse",
+    "SambaStudio",
     "SelfHostedHuggingFaceLLM",
     "SelfHostedPipeline",
+    "SparkLLM",
     "StochasticAI",
+    "TextGen",
     "TitanTakeoff",
     "TitanTakeoffPro",
+    "Together",
     "Tongyi",
-    "VertexAI",
-    "VertexAIModelGarden",
     "VLLM",
     "VLLMOpenAI",
+    "VertexAI",
+    "VertexAIModelGarden",
+    "VolcEngineMaasLLM",
     "WatsonxLLM",
+    "WeightOnlyQuantPipeline",
     "Writer",
-    "OctoAIEndpoint",
     "Xinference",
-    "JavelinAIGateway",
-    "QianfanLLMEndpoint",
     "YandexGPT",
-    "VolcEngineMaasLLM",
+    "Yuan2",
 ]
 
 
 def get_type_to_cls_dict() -> Dict[str, Callable[[], Type[BaseLLM]]]:
     return {
         "ai21": _import_ai21,
         "aleph_alpha": _import_aleph_alpha,
@@ -837,79 +971,91 @@
         "amazon_bedrock": _import_bedrock,
         "anthropic": _import_anthropic,
         "anyscale": _import_anyscale,
         "arcee": _import_arcee,
         "aviary": _import_aviary,
         "azure": _import_azure_openai,
         "azureml_endpoint": _import_azureml_endpoint,
+        "baichuan": _import_baichuan,
         "bananadev": _import_bananadev,
         "baseten": _import_baseten,
         "beam": _import_beam,
         "cerebriumai": _import_cerebriumai,
         "chat_glm": _import_chatglm,
         "clarifai": _import_clarifai,
         "cohere": _import_cohere,
         "ctransformers": _import_ctransformers,
         "ctranslate2": _import_ctranslate2,
         "databricks": _import_databricks,
-        "databricks-chat": _import_databricks_chat,
+        "databricks-chat": _import_databricks_chat,  # deprecated / only for back compat
         "deepinfra": _import_deepinfra,
         "deepsparse": _import_deepsparse,
         "edenai": _import_edenai,
         "fake-list": _import_fake,
         "forefrontai": _import_forefrontai,
+        "friendli": _import_friendli,
         "giga-chat-model": _import_gigachat,
         "google_palm": _import_google_palm,
         "gooseai": _import_gooseai,
         "gradient": _import_gradient_ai,
         "gpt4all": _import_gpt4all,
         "huggingface_endpoint": _import_huggingface_endpoint,
         "huggingface_hub": _import_huggingface_hub,
         "huggingface_pipeline": _import_huggingface_pipeline,
         "huggingface_textgen_inference": _import_huggingface_text_gen_inference,
         "human-input": _import_human,
         "koboldai": _import_koboldai,
+        "konko": _import_konko,
         "llamacpp": _import_llamacpp,
+        "llamafile": _import_llamafile,
         "textgen": _import_textgen,
         "minimax": _import_minimax,
         "mlflow": _import_mlflow,
-        "mlflow-chat": _import_mlflow_chat,
+        "mlflow-chat": _import_mlflow_chat,  # deprecated / only for back compat
         "mlflow-ai-gateway": _import_mlflow_ai_gateway,
+        "mlx_pipeline": _import_mlx_pipeline,
         "modal": _import_modal,
         "mosaic": _import_mosaicml,
         "nebula": _import_symblai_nebula,
         "nibittensor": _import_bittensor,
         "nlpcloud": _import_nlpcloud,
         "oci_model_deployment_tgi_endpoint": _import_oci_md_tgi,
         "oci_model_deployment_vllm_endpoint": _import_oci_md_vllm,
+        "oci_generative_ai": _import_oci_gen_ai,
+        "octoai_endpoint": _import_octoai_endpoint,
         "ollama": _import_ollama,
         "openai": _import_openai,
         "openlm": _import_openlm,
         "pai_eas_endpoint": _import_pai_eas_endpoint,
         "petals": _import_petals,
         "pipelineai": _import_pipelineai,
         "predibase": _import_predibase,
         "opaqueprompts": _import_opaqueprompts,
         "replicate": _import_replicate,
         "rwkv": _import_rwkv,
         "sagemaker_endpoint": _import_sagemaker_endpoint,
+        "sambaverse": _import_sambaverse,
+        "sambastudio": _import_sambastudio,
         "self_hosted": _import_self_hosted,
         "self_hosted_hugging_face": _import_self_hosted_hugging_face,
         "stochasticai": _import_stochasticai,
         "together": _import_together,
         "tongyi": _import_tongyi,
         "titan_takeoff": _import_titan_takeoff,
         "titan_takeoff_pro": _import_titan_takeoff_pro,
         "vertexai": _import_vertex,
         "vertexai_model_garden": _import_vertex_model_garden,
         "openllm": _import_openllm,
         "openllm_client": _import_openllm,
         "vllm": _import_vllm,
         "vllm_openai": _import_vllm_openai,
         "watsonxllm": _import_watsonxllm,
+        "weight_only_quantization": _import_weight_only_quantization,
         "writer": _import_writer,
         "xinference": _import_xinference,
         "javelin-ai-gateway": _import_javelin_ai_gateway,
         "qianfan_endpoint": _import_baidu_qianfan_endpoint,
         "yandex_gpt": _import_yandex_gpt,
+        "yuan2": _import_yuan2,
         "VolcEngineMaasLLM": _import_volcengine_maas,
+        "SparkLLM": _import_sparkllm,
     }
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/ai21.py` & `gigachain_community-0.2.0/langchain_community/llms/ai21.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/aleph_alpha.py` & `gigachain_community-0.2.0/langchain_community/llms/aleph_alpha.py`

 * *Files 1% similar despite different names*

```diff
@@ -278,10 +278,10 @@
         # In order to make this consistent with other endpoints, we strip them.
         if stop is not None or self.stop_sequences is not None:
             text = enforce_stop_tokens(text, params["stop_sequences"])
         return text
 
 
 if __name__ == "__main__":
-    aa = AlephAlpha()
+    aa = AlephAlpha()  # type: ignore[call-arg]
 
-    print(aa("How are you?"))
+    print(aa.invoke("How are you?"))  # noqa: T201
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/amazon_api_gateway.py` & `gigachain_community-0.2.0/langchain_community/llms/amazon_api_gateway.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/anthropic.py` & `gigachain_community-0.2.0/langchain_community/llms/anthropic.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
     Dict,
     Iterator,
     List,
     Mapping,
     Optional,
 )
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
@@ -48,14 +49,17 @@
 
     streaming: bool = False
     """Whether to stream the results."""
 
     default_request_timeout: Optional[float] = None
     """Timeout for requests to Anthropic Completion API. Default is 600 seconds."""
 
+    max_retries: int = 2
+    """Number of retries allowed for requests sent to the Anthropic Completion API."""
+
     anthropic_api_url: Optional[str] = None
 
     anthropic_api_key: Optional[SecretStr] = None
 
     HUMAN_PROMPT: Optional[str] = None
     AI_PROMPT: Optional[str] = None
     count_tokens: Optional[Callable[[str], int]] = None
@@ -88,19 +92,21 @@
             import anthropic
 
             check_package_version("anthropic", gte_version="0.3")
             values["client"] = anthropic.Anthropic(
                 base_url=values["anthropic_api_url"],
                 api_key=values["anthropic_api_key"].get_secret_value(),
                 timeout=values["default_request_timeout"],
+                max_retries=values["max_retries"],
             )
             values["async_client"] = anthropic.AsyncAnthropic(
                 base_url=values["anthropic_api_url"],
                 api_key=values["anthropic_api_key"].get_secret_value(),
                 timeout=values["default_request_timeout"],
+                max_retries=values["max_retries"],
             )
             values["HUMAN_PROMPT"] = anthropic.HUMAN_PROMPT
             values["AI_PROMPT"] = anthropic.AI_PROMPT
             values["count_tokens"] = values["client"].count_tokens
 
         except ImportError:
             raise ImportError(
@@ -138,14 +144,19 @@
 
         # Never want model to invent new turns of Human / Assistant dialog.
         stop.extend([self.HUMAN_PROMPT])
 
         return stop
 
 
+@deprecated(
+    since="0.0.28",
+    removal="0.3",
+    alternative_import="langchain_anthropic.AnthropicLLM",
+)
 class Anthropic(LLM, _AnthropicCommon):
     """Anthropic large language models.
 
     To use, you should have the ``anthropic`` python package installed, and the
     environment variable ``ANTHROPIC_API_KEY`` set with your API key, or pass
     it as a named parameter to the constructor.
 
@@ -155,21 +166,21 @@
             import anthropic
             from langchain_community.llms import Anthropic
 
             model = Anthropic(model="<model_name>", anthropic_api_key="my-api-key")
 
             # Simplest invocation, automatically wrapped with HUMAN_PROMPT
             # and AI_PROMPT.
-            response = model("What are the biggest risks facing humanity?")
+            response = model.invoke("What are the biggest risks facing humanity?")
 
             # Or if you want to use the chat mode, build a few-shot-prompt, or
             # put words in the Assistant's mouth, use HUMAN_PROMPT and AI_PROMPT:
             raw_prompt = "What are the biggest risks facing humanity?"
             prompt = f"{anthropic.HUMAN_PROMPT} {prompt}{anthropic.AI_PROMPT}"
-            response = model(prompt)
+            response = model.invoke(prompt)
     """
 
     class Config:
         """Configuration for this pydantic object."""
 
         allow_population_by_field_name = True
         arbitrary_types_allowed = True
@@ -221,15 +232,15 @@
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
                 prompt = "What are the biggest risks facing humanity?"
                 prompt = f"\n\nHuman: {prompt}\n\nAssistant:"
-                response = model(prompt)
+                response = model.invoke(prompt)
 
         """
         if self.streaming:
             completion = ""
             for chunk in self._stream(
                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
             ):
@@ -300,17 +311,17 @@
         stop = self._get_anthropic_stop(stop)
         params = {**self._default_params, **kwargs}
 
         for token in self.client.completions.create(
             prompt=self._wrap_prompt(prompt), stop_sequences=stop, stream=True, **params
         ):
             chunk = GenerationChunk(text=token.completion)
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+            yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -336,16 +347,16 @@
         async for token in await self.async_client.completions.create(
             prompt=self._wrap_prompt(prompt),
             stop_sequences=stop,
             stream=True,
             **params,
         ):
             chunk = GenerationChunk(text=token.completion)
-            yield chunk
             if run_manager:
                 await run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+            yield chunk
 
     def get_num_tokens(self, text: str) -> int:
         """Calculate number of tokens."""
         if not self.count_tokens:
             raise NameError("Please ensure the anthropic package is loaded")
         return self.count_tokens(text)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/anyscale.py` & `gigachain_community-0.2.0/langchain_community/chat_models/perplexity.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,279 +1,274 @@
-"""Wrapper around Anyscale Endpoint"""
+"""Wrapper around Perplexity APIs."""
+
+from __future__ import annotations
+
+import logging
 from typing import (
     Any,
-    AsyncIterator,
     Dict,
     Iterator,
     List,
     Mapping,
     Optional,
-    Set,
     Tuple,
-    cast,
+    Type,
+    Union,
 )
 
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
+from langchain_core.callbacks import CallbackManagerForLLMRun
+from langchain_core.language_models.chat_models import (
+    BaseChatModel,
+    generate_from_stream,
 )
-from langchain_core.outputs import Generation, GenerationChunk, LLMResult
-from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
-
-from langchain_community.llms.openai import (
-    BaseOpenAI,
-    acompletion_with_retry,
-    completion_with_retry,
+from langchain_core.messages import (
+    AIMessage,
+    AIMessageChunk,
+    BaseMessage,
+    BaseMessageChunk,
+    ChatMessage,
+    ChatMessageChunk,
+    FunctionMessageChunk,
+    HumanMessage,
+    HumanMessageChunk,
+    SystemMessage,
+    SystemMessageChunk,
+    ToolMessageChunk,
 )
+from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
+from langchain_core.pydantic_v1 import Field, root_validator
+from langchain_core.utils import get_from_dict_or_env, get_pydantic_field_names
 
-
-def update_token_usage(
-    keys: Set[str], response: Dict[str, Any], token_usage: Dict[str, Any]
-) -> None:
-    """Update token usage."""
-    _keys_to_use = keys.intersection(response["usage"])
-    for _key in _keys_to_use:
-        if _key not in token_usage:
-            token_usage[_key] = response["usage"][_key]
-        else:
-            token_usage[_key] += response["usage"][_key]
-
-
-def create_llm_result(
-    choices: Any, prompts: List[str], token_usage: Dict[str, int], model_name: str
-) -> LLMResult:
-    """Create the LLMResult from the choices and prompts."""
-    generations = []
-    for i, _ in enumerate(prompts):
-        choice = choices[i]
-        generations.append(
-            [
-                Generation(
-                    text=choice["message"]["content"],
-                    generation_info=dict(
-                        finish_reason=choice.get("finish_reason"),
-                        logprobs=choice.get("logprobs"),
-                    ),
-                )
-            ]
-        )
-    llm_output = {"token_usage": token_usage, "model_name": model_name}
-    return LLMResult(generations=generations, llm_output=llm_output)
+logger = logging.getLogger(__name__)
 
 
-class Anyscale(BaseOpenAI):
-    """Anyscale large language models.
+class ChatPerplexity(BaseChatModel):
+    """`Perplexity AI` Chat models API.
 
-    To use, you should have the environment variable ``ANYSCALE_API_BASE`` and
-    ``ANYSCALE_API_KEY``set with your Anyscale Endpoint, or pass it as a named
-    parameter to the constructor.
+    To use, you should have the ``openai`` python package installed, and the
+    environment variable ``PPLX_API_KEY`` set to your API key.
+    Any parameters that are valid to be passed to the openai.create call can be passed
+    in, even if not explicitly saved on this class.
 
     Example:
         .. code-block:: python
-            from langchain_community.llms import Anyscale
-            anyscalellm = Anyscale(anyscale_api_base="ANYSCALE_API_BASE",
-                                   anyscale_api_key="ANYSCALE_API_KEY",
-                                   model_name="meta-llama/Llama-2-7b-chat-hf")
-            # To leverage Ray for parallel processing
-            @ray.remote(num_cpus=1)
-            def send_query(llm, text):
-                resp = llm(text)
-                return resp
-            futures = [send_query.remote(anyscalellm, text) for text in texts]
-            results = ray.get(futures)
+
+            from langchain_community.chat_models import ChatPerplexity
+
+            chat = ChatPerplexity(model="pplx-70b-online", temperature=0.7)
     """
 
-    """Key word arguments to pass to the model."""
-    anyscale_api_base: Optional[str] = None
-    anyscale_api_key: Optional[SecretStr] = None
+    client: Any  #: :meta private:
+    model: str = "pplx-70b-online"
+    """Model name."""
+    temperature: float = 0.7
+    """What sampling temperature to use."""
+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """Holds any model parameters valid for `create` call not explicitly specified."""
+    pplx_api_key: Optional[str] = Field(None, alias="api_key")
+    """Base URL path for API requests, 
+    leave blank if not using a proxy or service emulator."""
+    request_timeout: Optional[Union[float, Tuple[float, float]]] = Field(
+        None, alias="timeout"
+    )
+    """Timeout for requests to PerplexityChat completion API. Default is 600 seconds."""
+    max_retries: int = 6
+    """Maximum number of retries to make when generating."""
+    streaming: bool = False
+    """Whether to stream the results or not."""
+    max_tokens: Optional[int] = None
+    """Maximum number of tokens to generate."""
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        allow_population_by_field_name = True
 
-    prefix_messages: List = Field(default_factory=list)
+    @property
+    def lc_secrets(self) -> Dict[str, str]:
+        return {"pplx_api_key": "PPLX_API_KEY"}
 
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return False
+    @root_validator(pre=True, allow_reuse=True)
+    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        """Build extra kwargs from additional params that were passed in."""
+        all_required_field_names = get_pydantic_field_names(cls)
+        extra = values.get("model_kwargs", {})
+        for field_name in list(values):
+            if field_name in extra:
+                raise ValueError(f"Found {field_name} supplied twice.")
+            if field_name not in all_required_field_names:
+                logger.warning(
+                    f"""WARNING! {field_name} is not a default parameter.
+                    {field_name} was transferred to model_kwargs.
+                    Please confirm that {field_name} is what you intended."""
+                )
+                extra[field_name] = values.pop(field_name)
 
-    @root_validator()
+        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())
+        if invalid_model_kwargs:
+            raise ValueError(
+                f"Parameters {invalid_model_kwargs} should be specified explicitly. "
+                f"Instead they were passed in as part of `model_kwargs` parameter."
+            )
+
+        values["model_kwargs"] = extra
+        return values
+
+    @root_validator(allow_reuse=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        values["anyscale_api_base"] = get_from_dict_or_env(
-            values, "anyscale_api_base", "ANYSCALE_API_BASE"
-        )
-        values["anyscale_api_key"] = convert_to_secret_str(
-            get_from_dict_or_env(values, "anyscale_api_key", "ANYSCALE_API_KEY")
+        values["pplx_api_key"] = get_from_dict_or_env(
+            values, "pplx_api_key", "PPLX_API_KEY"
         )
-
         try:
             import openai
-
-            ## Always create ChatComplete client, replacing the legacy Complete client
-            values["client"] = openai.ChatCompletion
         except ImportError:
             raise ImportError(
                 "Could not import openai python package. "
                 "Please install it with `pip install openai`."
             )
-        if values["streaming"] and values["n"] > 1:
-            raise ValueError("Cannot stream results when n > 1.")
-        if values["streaming"] and values["best_of"] > 1:
-            raise ValueError("Cannot stream results when best_of > 1.")
-
+        try:
+            values["client"] = openai.OpenAI(
+                api_key=values["pplx_api_key"], base_url="https://api.perplexity.ai"
+            )
+        except AttributeError:
+            raise ValueError(
+                "`openai` has no `ChatCompletion` attribute, this is likely "
+                "due to an old version of the openai package. Try upgrading it "
+                "with `pip install --upgrade openai`."
+            )
         return values
 
     @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
+    def _default_params(self) -> Dict[str, Any]:
+        """Get the default parameters for calling PerplexityChat API."""
         return {
-            **{"model_name": self.model_name},
-            **super()._identifying_params,
-        }
-
-    @property
-    def _invocation_params(self) -> Dict[str, Any]:
-        """Get the parameters used to invoke the model."""
-        openai_creds: Dict[str, Any] = {
-            "api_key": cast(SecretStr, self.anyscale_api_key).get_secret_value(),
-            "api_base": self.anyscale_api_base,
+            "request_timeout": self.request_timeout,
+            "max_tokens": self.max_tokens,
+            "stream": self.streaming,
+            "temperature": self.temperature,
+            **self.model_kwargs,
         }
-        return {**openai_creds, **{"model": self.model_name}, **super()._default_params}
 
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "Anyscale LLM"
+    def _convert_message_to_dict(self, message: BaseMessage) -> Dict[str, Any]:
+        if isinstance(message, ChatMessage):
+            message_dict = {"role": message.role, "content": message.content}
+        elif isinstance(message, SystemMessage):
+            message_dict = {"role": "system", "content": message.content}
+        elif isinstance(message, HumanMessage):
+            message_dict = {"role": "user", "content": message.content}
+        elif isinstance(message, AIMessage):
+            message_dict = {"role": "assistant", "content": message.content}
+        else:
+            raise TypeError(f"Got unknown type {message}")
+        return message_dict
 
-    def _get_chat_messages(
-        self, prompts: List[str], stop: Optional[List[str]] = None
-    ) -> Tuple:
-        if len(prompts) > 1:
-            raise ValueError(
-                f"Anyscale currently only supports single prompt, got {prompts}"
-            )
-        messages = self.prefix_messages + [{"role": "user", "content": prompts[0]}]
-        params: Dict[str, Any] = self._invocation_params
+    def _create_message_dicts(
+        self, messages: List[BaseMessage], stop: Optional[List[str]]
+    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
+        params = dict(self._invocation_params)
         if stop is not None:
             if "stop" in params:
                 raise ValueError("`stop` found in both the input and default params.")
             params["stop"] = stop
-        if params.get("max_tokens") == -1:
-            # for Chat api, omitting max_tokens is equivalent to having no limit
-            del params["max_tokens"]
-        return messages, params
+        message_dicts = [self._convert_message_to_dict(m) for m in messages]
+        return message_dicts, params
+
+    def _convert_delta_to_message_chunk(
+        self, _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]
+    ) -> BaseMessageChunk:
+        role = _dict.get("role")
+        content = _dict.get("content") or ""
+        additional_kwargs: Dict = {}
+        if _dict.get("function_call"):
+            function_call = dict(_dict["function_call"])
+            if "name" in function_call and function_call["name"] is None:
+                function_call["name"] = ""
+            additional_kwargs["function_call"] = function_call
+        if _dict.get("tool_calls"):
+            additional_kwargs["tool_calls"] = _dict["tool_calls"]
+
+        if role == "user" or default_class == HumanMessageChunk:
+            return HumanMessageChunk(content=content)
+        elif role == "assistant" or default_class == AIMessageChunk:
+            return AIMessageChunk(content=content, additional_kwargs=additional_kwargs)
+        elif role == "system" or default_class == SystemMessageChunk:
+            return SystemMessageChunk(content=content)
+        elif role == "function" or default_class == FunctionMessageChunk:
+            return FunctionMessageChunk(content=content, name=_dict["name"])
+        elif role == "tool" or default_class == ToolMessageChunk:
+            return ToolMessageChunk(content=content, tool_call_id=_dict["tool_call_id"])
+        elif role or default_class == ChatMessageChunk:
+            return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
+        else:
+            return default_class(content=content)  # type: ignore[call-arg]
 
     def _stream(
         self,
-        prompt: str,
+        messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Iterator[GenerationChunk]:
-        messages, params = self._get_chat_messages([prompt], stop)
-        params = {**params, **kwargs, "stream": True}
-        for stream_resp in completion_with_retry(
-            self, messages=messages, run_manager=run_manager, **params
-        ):
-            token = stream_resp["choices"][0]["delta"].get("content", "")
-            chunk = GenerationChunk(text=token)
-            yield chunk
+    ) -> Iterator[ChatGenerationChunk]:
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs}
+        default_chunk_class = AIMessageChunk
+
+        if stop:
+            params["stop_sequences"] = stop
+        stream_resp = self.client.chat.completions.create(
+            model=params["model"], messages=message_dicts, stream=True
+        )
+        for chunk in stream_resp:
+            if not isinstance(chunk, dict):
+                chunk = chunk.dict()
+            if len(chunk["choices"]) == 0:
+                continue
+            choice = chunk["choices"][0]
+            chunk = self._convert_delta_to_message_chunk(
+                choice["delta"], default_chunk_class
+            )
+            finish_reason = choice.get("finish_reason")
+            generation_info = (
+                dict(finish_reason=finish_reason) if finish_reason is not None else None
+            )
+            default_chunk_class = chunk.__class__
+            chunk = ChatGenerationChunk(message=chunk, generation_info=generation_info)
             if run_manager:
-                run_manager.on_llm_new_token(token, chunk=chunk)
-
-    async def _astream(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[GenerationChunk]:
-        messages, params = self._get_chat_messages([prompt], stop)
-        params = {**params, **kwargs, "stream": True}
-        async for stream_resp in await acompletion_with_retry(
-            self, messages=messages, run_manager=run_manager, **params
-        ):
-            token = stream_resp["choices"][0]["delta"].get("content", "")
-            chunk = GenerationChunk(text=token)
+                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
             yield chunk
-            if run_manager:
-                await run_manager.on_llm_new_token(token, chunk=chunk)
 
     def _generate(
         self,
-        prompts: List[str],
+        messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> LLMResult:
-        choices = []
-        token_usage: Dict[str, int] = {}
-        _keys = {"completion_tokens", "prompt_tokens", "total_tokens"}
-        for prompt in prompts:
-            if self.streaming:
-                generation: Optional[GenerationChunk] = None
-                for chunk in self._stream(prompt, stop, run_manager, **kwargs):
-                    if generation is None:
-                        generation = chunk
-                    else:
-                        generation += chunk
-                assert generation is not None
-                choices.append(
-                    {
-                        "message": {"content": generation.text},
-                        "finish_reason": generation.generation_info.get("finish_reason")
-                        if generation.generation_info
-                        else None,
-                        "logprobs": generation.generation_info.get("logprobs")
-                        if generation.generation_info
-                        else None,
-                    }
-                )
+    ) -> ChatResult:
+        if self.streaming:
+            stream_iter = self._stream(
+                messages, stop=stop, run_manager=run_manager, **kwargs
+            )
+            if stream_iter:
+                return generate_from_stream(stream_iter)
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs}
+        response = self.client.chat.completions.create(
+            model=params["model"], messages=message_dicts
+        )
+        message = AIMessage(content=response.choices[0].message.content)
+        return ChatResult(generations=[ChatGeneration(message=message)])
 
-            else:
-                messages, params = self._get_chat_messages([prompt], stop)
-                params = {**params, **kwargs}
-                response = completion_with_retry(
-                    self, messages=messages, run_manager=run_manager, **params
-                )
-                choices.extend(response["choices"])
-                update_token_usage(_keys, response, token_usage)
-        return create_llm_result(choices, prompts, token_usage, self.model_name)
+    @property
+    def _invocation_params(self) -> Mapping[str, Any]:
+        """Get the parameters used to invoke the model."""
+        pplx_creds: Dict[str, Any] = {
+            "api_key": self.pplx_api_key,
+            "api_base": "https://api.perplexity.ai",
+            "model": self.model,
+        }
+        return {**pplx_creds, **self._default_params}
 
-    async def _agenerate(
-        self,
-        prompts: List[str],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        choices = []
-        token_usage: Dict[str, int] = {}
-        _keys = {"completion_tokens", "prompt_tokens", "total_tokens"}
-        for prompt in prompts:
-            messages = self.prefix_messages + [{"role": "user", "content": prompt}]
-            if self.streaming:
-                generation: Optional[GenerationChunk] = None
-                async for chunk in self._astream(prompt, stop, run_manager, **kwargs):
-                    if generation is None:
-                        generation = chunk
-                    else:
-                        generation += chunk
-                assert generation is not None
-                choices.append(
-                    {
-                        "message": {"content": generation.text},
-                        "finish_reason": generation.generation_info.get("finish_reason")
-                        if generation.generation_info
-                        else None,
-                        "logprobs": generation.generation_info.get("logprobs")
-                        if generation.generation_info
-                        else None,
-                    }
-                )
-            else:
-                messages, params = self._get_chat_messages([prompt], stop)
-                params = {**params, **kwargs}
-                response = await acompletion_with_retry(
-                    self, messages=messages, run_manager=run_manager, **params
-                )
-                choices.extend(response["choices"])
-                update_token_usage(_keys, response, token_usage)
-        return create_llm_result(choices, prompts, token_usage, self.model_name)
+    @property
+    def _llm_type(self) -> str:
+        """Return type of chat model."""
+        return "perplexitychat"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/aphrodite.py` & `gigachain_community-0.2.0/langchain_community/llms/aphrodite.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/arcee.py` & `gigachain_community-0.2.0/langchain_community/llms/arcee.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/aviary.py` & `gigachain_community-0.2.0/langchain_community/llms/aviary.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/baidu_qianfan_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/baidu_qianfan_endpoint.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,15 +36,20 @@
         .. code-block:: python
 
             from langchain_community.llms import QianfanLLMEndpoint
             qianfan_model = QianfanLLMEndpoint(model="ERNIE-Bot",
                 endpoint="your_endpoint", qianfan_ak="your_ak", qianfan_sk="your_sk")
     """
 
+    init_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """init kwargs for qianfan client init, such as `query_per_second` which is 
+        associated with qianfan resource object to limit QPS"""
+
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """extra params for model invoke using with `do`."""
 
     client: Any
 
     qianfan_ak: Optional[str] = None
     qianfan_sk: Optional[str] = None
 
     streaming: Optional[bool] = False
@@ -87,14 +92,15 @@
                 "qianfan_sk",
                 "QIANFAN_SK",
                 default="",
             )
         )
 
         params = {
+            **values.get("init_kwargs", {}),
             "model": values["model"],
         }
         if values["qianfan_ak"].get_secret_value() != "":
             params["ak"] = values["qianfan_ak"].get_secret_value()
         if values["qianfan_sk"].get_secret_value() != "":
             params["sk"] = values["qianfan_sk"].get_secret_value()
         if values["endpoint"] is not None and values["endpoint"] != "":
@@ -162,22 +168,23 @@
             prompt: The prompt to pass into the model.
             stop: Optional list of stop words to use when generating.
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
-                response = qianfan_model("Tell me a joke.")
+                response = qianfan_model.invoke("Tell me a joke.")
         """
         if self.streaming:
             completion = ""
             for chunk in self._stream(prompt, stop, run_manager, **kwargs):
                 completion += chunk.text
             return completion
         params = self._convert_prompt_msg_params(prompt, **kwargs)
+        params["stop"] = stop
         response_payload = self.client.do(**params)
 
         return response_payload["result"]
 
     async def _acall(
         self,
         prompt: str,
@@ -188,41 +195,43 @@
         if self.streaming:
             completion = ""
             async for chunk in self._astream(prompt, stop, run_manager, **kwargs):
                 completion += chunk.text
             return completion
 
         params = self._convert_prompt_msg_params(prompt, **kwargs)
+        params["stop"] = stop
         response_payload = await self.client.ado(**params)
 
         return response_payload["result"]
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
         params = self._convert_prompt_msg_params(prompt, **{**kwargs, "stream": True})
+        params["stop"] = stop
         for res in self.client.do(**params):
             if res:
                 chunk = GenerationChunk(text=res["result"])
-                yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(chunk.text)
+                yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[GenerationChunk]:
         params = self._convert_prompt_msg_params(prompt, **{**kwargs, "stream": True})
+        params["stop"] = stop
         async for res in await self.client.ado(**params):
             if res:
                 chunk = GenerationChunk(text=res["result"])
-
-                yield chunk
                 if run_manager:
                     await run_manager.on_llm_new_token(chunk.text)
+                yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/bananadev.py` & `gigachain_community-0.2.0/langchain_community/llms/bananadev.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import logging
-from typing import Any, Dict, List, Mapping, Optional
+from typing import Any, Dict, List, Mapping, Optional, cast
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
 class Banana(LLM):
@@ -34,15 +34,15 @@
     model_url_slug: str = ""
     """model endpoint to use"""
 
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any model parameters valid for `create` call not
     explicitly specified."""
 
-    banana_api_key: Optional[str] = None
+    banana_api_key: Optional[SecretStr] = None
 
     class Config:
         """Configuration for this pydantic config."""
 
         extra = Extra.forbid
 
     @root_validator(pre=True)
@@ -62,16 +62,16 @@
                 extra[field_name] = values.pop(field_name)
         values["model_kwargs"] = extra
         return values
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        banana_api_key = get_from_dict_or_env(
-            values, "banana_api_key", "BANANA_API_KEY"
+        banana_api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "banana_api_key", "BANANA_API_KEY")
         )
         values["banana_api_key"] = banana_api_key
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
@@ -99,25 +99,25 @@
         except ImportError:
             raise ImportError(
                 "Could not import banana-dev python package. "
                 "Please install it with `pip install banana-dev`."
             )
         params = self.model_kwargs or {}
         params = {**params, **kwargs}
-        api_key = self.banana_api_key
+        api_key = cast(SecretStr, self.banana_api_key)
         model_key = self.model_key
         model_url_slug = self.model_url_slug
         model_inputs = {
             # a json specific to your model.
             "prompt": prompt,
             **params,
         }
         model = Client(
             # Found in main dashboard
-            api_key=api_key,
+            api_key=api_key.get_secret_value(),
             # Both found in model details page
             model_key=model_key,
             url=f"https://{model_url_slug}.run.banana.dev",
         )
         response, meta = model.call("/", model_inputs)
         try:
             text = response["outputs"]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/baseten.py` & `gigachain_community-0.2.0/langchain_community/llms/baseten.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/beam.py` & `gigachain_community-0.2.0/langchain_community/llms/beam.py`

 * *Files 1% similar despite different names*

```diff
@@ -183,15 +183,15 @@
             tokenizer = GPT2Tokenizer.from_pretrained(model_name)
             model = GPT2LMHeadModel.from_pretrained(model_name)
             encodedPrompt = tokenizer.encode(prompt, return_tensors='pt')
             outputs = model.generate(encodedPrompt, max_length=int(length),
               do_sample=True, pad_token_id=tokenizer.eos_token_id)
             output = tokenizer.decode(outputs[0], skip_special_tokens=True)
 
-            print(output)
+            print(output)  # noqa: T201
             return {{"text": output}}
 
         """
         )
 
         script_name = "run.py"
         with open(script_name, "w") as file:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/bedrock.py` & `gigachain_community-0.2.0/langchain_community/llms/textgen.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,458 +1,417 @@
-from __future__ import annotations
-
 import json
-import warnings
-from abc import ABC
-from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Mapping, Optional
+import logging
+from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
 
-from langchain_core.callbacks import CallbackManagerForLLMRun
+import requests
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForLLMRun,
+    CallbackManagerForLLMRun,
+)
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
-from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Field
 
-from langchain_community.llms.utils import enforce_stop_tokens
-from langchain_community.utilities.anthropic import (
-    get_num_tokens_anthropic,
-    get_token_ids_anthropic,
-)
+logger = logging.getLogger(__name__)
 
-if TYPE_CHECKING:
-    from botocore.config import Config
 
-HUMAN_PROMPT = "\n\nHuman:"
-ASSISTANT_PROMPT = "\n\nAssistant:"
-ALTERNATION_ERROR = (
-    "Error: Prompt must alternate between '\n\nHuman:' and '\n\nAssistant:'."
-)
+class TextGen(LLM):
+    """Text generation models from WebUI.
 
+    To use, you should have the text-generation-webui installed, a model loaded,
+    and --api added as a command-line option.
 
-def _add_newlines_before_ha(input_text: str) -> str:
-    new_text = input_text
-    for word in ["Human:", "Assistant:"]:
-        new_text = new_text.replace(word, "\n\n" + word)
-        for i in range(2):
-            new_text = new_text.replace("\n\n\n" + word, "\n\n" + word)
-    return new_text
-
-
-def _human_assistant_format(input_text: str) -> str:
-    if input_text.count("Human:") == 0 or (
-        input_text.find("Human:") > input_text.find("Assistant:")
-        and "Assistant:" in input_text
-    ):
-        input_text = HUMAN_PROMPT + " " + input_text  # SILENT CORRECTION
-    if input_text.count("Assistant:") == 0:
-        input_text = input_text + ASSISTANT_PROMPT  # SILENT CORRECTION
-    if input_text[: len("Human:")] == "Human:":
-        input_text = "\n\n" + input_text
-    input_text = _add_newlines_before_ha(input_text)
-    count = 0
-    # track alternation
-    for i in range(len(input_text)):
-        if input_text[i : i + len(HUMAN_PROMPT)] == HUMAN_PROMPT:
-            if count % 2 == 0:
-                count += 1
-            else:
-                warnings.warn(ALTERNATION_ERROR + f" Received {input_text}")
-        if input_text[i : i + len(ASSISTANT_PROMPT)] == ASSISTANT_PROMPT:
-            if count % 2 == 1:
-                count += 1
-            else:
-                warnings.warn(ALTERNATION_ERROR + f" Received {input_text}")
+    Suggested installation, use one-click installer for your OS:
+    https://github.com/oobabooga/text-generation-webui#one-click-installers
 
-    if count % 2 == 1:  # Only saw Human, no Assistant
-        input_text = input_text + ASSISTANT_PROMPT  # SILENT CORRECTION
+    Parameters below taken from text-generation-webui api example:
+    https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py
 
-    return input_text
+    Example:
+        .. code-block:: python
 
+            from langchain_community.llms import TextGen
+            llm = TextGen(model_url="http://localhost:8500")
+    """
 
-class LLMInputOutputAdapter:
-    """Adapter class to prepare the inputs from Langchain to a format
-    that LLM model expects.
-
-    It also provides helper function to extract
-    the generated text from the model response."""
-
-    provider_to_output_key_map = {
-        "anthropic": "completion",
-        "amazon": "outputText",
-        "cohere": "text",
-        "meta": "generation",
-    }
-
-    @classmethod
-    def prepare_input(
-        cls, provider: str, prompt: str, model_kwargs: Dict[str, Any]
-    ) -> Dict[str, Any]:
-        input_body = {**model_kwargs}
-        if provider == "anthropic":
-            input_body["prompt"] = _human_assistant_format(prompt)
-        elif provider in ("ai21", "cohere", "meta"):
-            input_body["prompt"] = prompt
-        elif provider == "amazon":
-            input_body = dict()
-            input_body["inputText"] = prompt
-            input_body["textGenerationConfig"] = {**model_kwargs}
-        else:
-            input_body["inputText"] = prompt
+    model_url: str
+    """The full URL to the textgen webui including http[s]://host:port """
 
-        if provider == "anthropic" and "max_tokens_to_sample" not in input_body:
-            input_body["max_tokens_to_sample"] = 256
+    preset: Optional[str] = None
+    """The preset to use in the textgen webui """
 
-        return input_body
+    max_new_tokens: Optional[int] = 250
+    """The maximum number of tokens to generate."""
 
-    @classmethod
-    def prepare_output(cls, provider: str, response: Any) -> str:
-        if provider == "anthropic":
-            response_body = json.loads(response.get("body").read().decode())
-            return response_body.get("completion")
-        else:
-            response_body = json.loads(response.get("body").read())
+    do_sample: bool = Field(True, alias="do_sample")
+    """Do sample"""
 
-        if provider == "ai21":
-            return response_body.get("completions")[0].get("data").get("text")
-        elif provider == "cohere":
-            return response_body.get("generations")[0].get("text")
-        elif provider == "meta":
-            return response_body.get("generation")
-        else:
-            return response_body.get("results")[0].get("outputText")
+    temperature: Optional[float] = 1.3
+    """Primary factor to control randomness of outputs. 0 = deterministic
+    (only the most likely token is used). Higher value = more randomness."""
 
-    @classmethod
-    def prepare_output_stream(
-        cls, provider: str, response: Any, stop: Optional[List[str]] = None
-    ) -> Iterator[GenerationChunk]:
-        stream = response.get("body")
+    top_p: Optional[float] = 0.1
+    """If not set to 1, select tokens with probabilities adding up to less than this
+    number. Higher value = higher range of possible random results."""
 
-        if not stream:
-            return
+    typical_p: Optional[float] = 1
+    """If not set to 1, select only tokens that are at least this much more likely to
+    appear than random tokens, given the prior text."""
 
-        if provider not in cls.provider_to_output_key_map:
-            raise ValueError(
-                f"Unknown streaming response output key for provider: {provider}"
-            )
+    epsilon_cutoff: Optional[float] = 0  # In units of 1e-4
+    """Epsilon cutoff"""
 
-        for event in stream:
-            chunk = event.get("chunk")
-            if chunk:
-                chunk_obj = json.loads(chunk.get("bytes").decode())
-                if provider == "cohere" and (
-                    chunk_obj["is_finished"]
-                    or chunk_obj[cls.provider_to_output_key_map[provider]]
-                    == "<EOS_TOKEN>"
-                ):
-                    return
-
-                # chunk obj format varies with provider
-                yield GenerationChunk(
-                    text=chunk_obj[cls.provider_to_output_key_map[provider]]
-                )
+    eta_cutoff: Optional[float] = 0  # In units of 1e-4
+    """ETA cutoff"""
 
+    repetition_penalty: Optional[float] = 1.18
+    """Exponential penalty factor for repeating prior tokens. 1 means no penalty,
+    higher value = less repetition, lower value = more repetition."""
 
-class BedrockBase(BaseModel, ABC):
-    """Base class for Bedrock models."""
+    top_k: Optional[float] = 40
+    """Similar to top_p, but select instead only the top_k most likely tokens.
+    Higher value = higher range of possible random results."""
 
-    client: Any = Field(exclude=True)  #: :meta private:
+    min_length: Optional[int] = 0
+    """Minimum generation length in tokens."""
 
-    region_name: Optional[str] = None
-    """The aws region e.g., `us-west-2`. Fallsback to AWS_DEFAULT_REGION env variable
-    or region specified in ~/.aws/config in case it is not provided here.
-    """
+    no_repeat_ngram_size: Optional[int] = 0
+    """If not set to 0, specifies the length of token sets that are completely blocked
+    from repeating at all. Higher values = blocks larger phrases,
+    lower values = blocks words or letters from repeating.
+    Only 0 or high values are a good idea in most cases."""
 
-    credentials_profile_name: Optional[str] = Field(default=None, exclude=True)
-    """The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
-    has either access keys or role information specified.
-    If not specified, the default credential profile or, if on an EC2 instance,
-    credentials from IMDS will be used.
-    See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
-    """
+    num_beams: Optional[int] = 1
+    """Number of beams"""
 
-    config: Optional[Config] = None
-    """An optional botocore.config.Config instance to pass to the client."""
+    penalty_alpha: Optional[float] = 0
+    """Penalty Alpha"""
 
-    model_id: str
-    """Id of the model to call, e.g., amazon.titan-text-express-v1, this is
-    equivalent to the modelId property in the list-foundation-models api"""
+    length_penalty: Optional[float] = 1
+    """Length Penalty"""
 
-    model_kwargs: Optional[Dict] = None
-    """Keyword arguments to pass to the model."""
+    early_stopping: bool = Field(False, alias="early_stopping")
+    """Early stopping"""
 
-    endpoint_url: Optional[str] = None
-    """Needed if you don't want to default to us-east-1 endpoint"""
+    seed: int = Field(-1, alias="seed")
+    """Seed (-1 for random)"""
 
-    streaming: bool = False
-    """Whether to stream the results."""
+    add_bos_token: bool = Field(True, alias="add_bos_token")
+    """Add the bos_token to the beginning of prompts.
+    Disabling this can make the replies more creative."""
 
-    provider_stop_sequence_key_name_map: Mapping[str, str] = {
-        "anthropic": "stop_sequences",
-        "amazon": "stopSequences",
-        "ai21": "stop_sequences",
-        "cohere": "stop_sequences",
-    }
-
-    @root_validator()
-    def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that AWS credentials to and python package exists in environment."""
-
-        # Skip creating new client if passed in constructor
-        if values["client"] is not None:
-            return values
+    truncation_length: Optional[int] = 2048
+    """Truncate the prompt up to this length. The leftmost tokens are removed if
+    the prompt exceeds this length. Most models require this to be at most 2048."""
 
-        try:
-            import boto3
-
-            if values["credentials_profile_name"] is not None:
-                session = boto3.Session(profile_name=values["credentials_profile_name"])
-            else:
-                # use default credentials
-                session = boto3.Session()
-
-            values["region_name"] = get_from_dict_or_env(
-                values,
-                "region_name",
-                "AWS_DEFAULT_REGION",
-                default=session.region_name,
-            )
+    ban_eos_token: bool = Field(False, alias="ban_eos_token")
+    """Ban the eos_token. Forces the model to never end the generation prematurely."""
 
-            client_params = {}
-            if values["region_name"]:
-                client_params["region_name"] = values["region_name"]
-            if values["endpoint_url"]:
-                client_params["endpoint_url"] = values["endpoint_url"]
-            if values["config"]:
-                client_params["config"] = values["config"]
+    skip_special_tokens: bool = Field(True, alias="skip_special_tokens")
+    """Skip special tokens. Some specific models need this unset."""
 
-            values["client"] = session.client("bedrock-runtime", **client_params)
+    stopping_strings: Optional[List[str]] = []
+    """A list of strings to stop generation when encountered."""
 
-        except ImportError:
-            raise ModuleNotFoundError(
-                "Could not import boto3 python package. "
-                "Please install it with `pip install boto3`."
-            )
-        except Exception as e:
-            raise ValueError(
-                "Could not load credentials to authenticate with AWS client. "
-                "Please check that credentials in the specified "
-                "profile name are valid."
-            ) from e
-
-        return values
+    streaming: bool = False
+    """Whether to stream the results, token by token."""
 
     @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
-        _model_kwargs = self.model_kwargs or {}
+    def _default_params(self) -> Dict[str, Any]:
+        """Get the default parameters for calling textgen."""
         return {
-            **{"model_kwargs": _model_kwargs},
+            "max_new_tokens": self.max_new_tokens,
+            "do_sample": self.do_sample,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            "typical_p": self.typical_p,
+            "epsilon_cutoff": self.epsilon_cutoff,
+            "eta_cutoff": self.eta_cutoff,
+            "repetition_penalty": self.repetition_penalty,
+            "top_k": self.top_k,
+            "min_length": self.min_length,
+            "no_repeat_ngram_size": self.no_repeat_ngram_size,
+            "num_beams": self.num_beams,
+            "penalty_alpha": self.penalty_alpha,
+            "length_penalty": self.length_penalty,
+            "early_stopping": self.early_stopping,
+            "seed": self.seed,
+            "add_bos_token": self.add_bos_token,
+            "truncation_length": self.truncation_length,
+            "ban_eos_token": self.ban_eos_token,
+            "skip_special_tokens": self.skip_special_tokens,
+            "stopping_strings": self.stopping_strings,
         }
 
-    def _get_provider(self) -> str:
-        return self.model_id.split(".")[0]
+    @property
+    def _identifying_params(self) -> Dict[str, Any]:
+        """Get the identifying parameters."""
+        return {**{"model_url": self.model_url}, **self._default_params}
 
     @property
-    def _model_is_anthropic(self) -> bool:
-        return self._get_provider() == "anthropic"
+    def _llm_type(self) -> str:
+        """Return type of llm."""
+        return "textgen"
 
-    def _prepare_input_and_invoke(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        _model_kwargs = self.model_kwargs or {}
+    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:
+        """
+        Performs sanity check, preparing parameters in format needed by textgen.
 
-        provider = self._get_provider()
-        params = {**_model_kwargs, **kwargs}
-        input_body = LLMInputOutputAdapter.prepare_input(provider, prompt, params)
-        body = json.dumps(input_body)
-        accept = "application/json"
-        contentType = "application/json"
+        Args:
+            stop (Optional[List[str]]): List of stop sequences for textgen.
 
-        try:
-            response = self.client.invoke_model(
-                body=body, modelId=self.model_id, accept=accept, contentType=contentType
-            )
-            text = LLMInputOutputAdapter.prepare_output(provider, response)
+        Returns:
+            Dictionary containing the combined parameters.
+        """
 
-        except Exception as e:
-            raise ValueError(f"Error raised by bedrock service: {e}")
+        # Raise error if stop sequences are in both input and default params
+        # if self.stop and stop is not None:
+        if self.stopping_strings and stop is not None:
+            raise ValueError("`stop` found in both the input and default params.")
 
-        if stop is not None:
-            text = enforce_stop_tokens(text, stop)
+        if self.preset is None:
+            params = self._default_params
+        else:
+            params = {"preset": self.preset}
+
+        # then sets it as configured, or default to an empty list:
+        params["stopping_strings"] = self.stopping_strings or stop or []
 
-        return text
+        return params
 
-    def _prepare_input_and_invoke_stream(
+    def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Iterator[GenerationChunk]:
-        _model_kwargs = self.model_kwargs or {}
-        provider = self._get_provider()
-
-        if stop:
-            if provider not in self.provider_stop_sequence_key_name_map:
-                raise ValueError(
-                    f"Stop sequence key name for {provider} is not supported."
-                )
-
-            # stop sequence from _generate() overrides
-            # stop sequences in the class attribute
-            _model_kwargs[self.provider_stop_sequence_key_name_map.get(provider)] = stop
-
-        if provider == "cohere":
-            _model_kwargs["stream"] = True
-
-        params = {**_model_kwargs, **kwargs}
-        input_body = LLMInputOutputAdapter.prepare_input(provider, prompt, params)
-        body = json.dumps(input_body)
-
-        try:
-            response = self.client.invoke_model_with_response_stream(
-                body=body,
-                modelId=self.model_id,
-                accept="application/json",
-                contentType="application/json",
-            )
-        except Exception as e:
-            raise ValueError(f"Error raised by bedrock service: {e}")
-
-        for chunk in LLMInputOutputAdapter.prepare_output_stream(
-            provider, response, stop
-        ):
-            yield chunk
-            if run_manager is not None:
-                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
-
+    ) -> str:
+        """Call the textgen web API and return the output.
 
-class Bedrock(LLM, BedrockBase):
-    """Bedrock models.
+        Args:
+            prompt: The prompt to use for generation.
+            stop: A list of strings to stop generation when encountered.
 
-    To authenticate, the AWS client uses the following methods to
-    automatically load credentials:
-    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
+        Returns:
+            The generated text.
 
-    If a specific credential profile should be used, you must pass
-    the name of the profile from the ~/.aws/credentials file that is to be used.
+        Example:
+            .. code-block:: python
 
-    Make sure the credentials / roles used have the required policies to
-    access the Bedrock service.
-    """
+                from langchain_community.llms import TextGen
+                llm = TextGen(model_url="http://localhost:5000")
+                llm.invoke("Write a story about llamas.")
+        """
+        if self.streaming:
+            combined_text_output = ""
+            for chunk in self._stream(
+                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
+            ):
+                combined_text_output += chunk.text
+            result = combined_text_output
 
-    """
-    Example:
-        .. code-block:: python
+        else:
+            url = f"{self.model_url}/api/v1/generate"
+            params = self._get_parameters(stop)
+            request = params.copy()
+            request["prompt"] = prompt
+            response = requests.post(url, json=request)
 
-            from bedrock_langchain.bedrock_llm import BedrockLLM
+            if response.status_code == 200:
+                result = response.json()["results"][0]["text"]
+            else:
+                print(f"ERROR: response: {response}")  # noqa: T201
+                result = ""
 
-            llm = BedrockLLM(
-                credentials_profile_name="default",
-                model_id="amazon.titan-text-express-v1",
-                streaming=True
-            )
+        return result
 
-    """
+    async def _acall(
+        self,
+        prompt: str,
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> str:
+        """Call the textgen web API and return the output.
 
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "amazon_bedrock"
+        Args:
+            prompt: The prompt to use for generation.
+            stop: A list of strings to stop generation when encountered.
 
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        """Return whether this model can be serialized by Langchain."""
-        return True
-
-    @classmethod
-    def get_lc_namespace(cls) -> List[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "llms", "bedrock"]
+        Returns:
+            The generated text.
 
-    @property
-    def lc_attributes(self) -> Dict[str, Any]:
-        attributes: Dict[str, Any] = {}
+        Example:
+            .. code-block:: python
 
-        if self.region_name:
-            attributes["region_name"] = self.region_name
+                from langchain_community.llms import TextGen
+                llm = TextGen(model_url="http://localhost:5000")
+                llm.invoke("Write a story about llamas.")
+        """
+        if self.streaming:
+            combined_text_output = ""
+            async for chunk in self._astream(
+                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
+            ):
+                combined_text_output += chunk.text
+            result = combined_text_output
 
-        return attributes
+        else:
+            url = f"{self.model_url}/api/v1/generate"
+            params = self._get_parameters(stop)
+            request = params.copy()
+            request["prompt"] = prompt
+            response = requests.post(url, json=request)
 
-    class Config:
-        """Configuration for this pydantic object."""
+            if response.status_code == 200:
+                result = response.json()["results"][0]["text"]
+            else:
+                print(f"ERROR: response: {response}")  # noqa: T201
+                result = ""
 
-        extra = Extra.forbid
+        return result
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
-        """Call out to Bedrock service with streaming.
+        """Yields results objects as they are generated in real time.
+
+        It also calls the callback manager's on_llm_new_token event with
+        similar parameters to the OpenAI LLM class method of the same name.
 
         Args:
-            prompt (str): The prompt to pass into the model
-            stop (Optional[List[str]], optional): Stop sequences. These will
-                override any stop sequences in the `model_kwargs` attribute.
-                Defaults to None.
-            run_manager (Optional[CallbackManagerForLLMRun], optional): Callback
-                run managers used to process the output. Defaults to None.
+            prompt: The prompts to pass into the model.
+            stop: Optional list of stop words to use when generating.
 
         Returns:
-            Iterator[GenerationChunk]: Generator that yields the streamed responses.
+            A generator representing the stream of tokens being generated.
 
         Yields:
-            Iterator[GenerationChunk]: Responses from the model.
+            A dictionary like objects containing a string token and metadata.
+            See text-generation-webui docs and below for more.
+
+        Example:
+            .. code-block:: python
+
+                from langchain_community.llms import TextGen
+                llm = TextGen(
+                    model_url = "ws://localhost:5005"
+                    streaming=True
+                )
+                for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
+                        stop=["'","\n"]):
+                    print(chunk, end='', flush=True)  # noqa: T201
+
         """
-        return self._prepare_input_and_invoke_stream(
-            prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
-        )
+        try:
+            import websocket
+        except ImportError:
+            raise ImportError(
+                "The `websocket-client` package is required for streaming."
+            )
 
-    def _call(
+        params = {**self._get_parameters(stop), **kwargs}
+
+        url = f"{self.model_url}/api/v1/stream"
+
+        request = params.copy()
+        request["prompt"] = prompt
+
+        websocket_client = websocket.WebSocket()
+
+        websocket_client.connect(url)
+
+        websocket_client.send(json.dumps(request))
+
+        while True:
+            result = websocket_client.recv()
+            result = json.loads(result)
+
+            if result["event"] == "text_stream":  # type: ignore[call-overload, index]
+                chunk = GenerationChunk(
+                    text=result["text"],  # type: ignore[call-overload, index]
+                    generation_info=None,
+                )
+                yield chunk
+            elif result["event"] == "stream_end":  # type: ignore[call-overload, index]
+                websocket_client.close()
+                return
+
+            if run_manager:
+                run_manager.on_llm_new_token(token=chunk.text)
+
+    async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> str:
-        """Call out to Bedrock service model.
+    ) -> AsyncIterator[GenerationChunk]:
+        """Yields results objects as they are generated in real time.
+
+        It also calls the callback manager's on_llm_new_token event with
+        similar parameters to the OpenAI LLM class method of the same name.
 
         Args:
-            prompt: The prompt to pass into the model.
+            prompt: The prompts to pass into the model.
             stop: Optional list of stop words to use when generating.
 
         Returns:
-            The string generated by the model.
+            A generator representing the stream of tokens being generated.
+
+        Yields:
+            A dictionary like objects containing a string token and metadata.
+            See text-generation-webui docs and below for more.
 
         Example:
             .. code-block:: python
 
-                response = llm("Tell me a joke.")
+                from langchain_community.llms import TextGen
+                llm = TextGen(
+                    model_url = "ws://localhost:5005"
+                    streaming=True
+                )
+                for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
+                        stop=["'","\n"]):
+                    print(chunk, end='', flush=True)  # noqa: T201
+
         """
+        try:
+            import websocket
+        except ImportError:
+            raise ImportError(
+                "The `websocket-client` package is required for streaming."
+            )
 
-        if self.streaming:
-            completion = ""
-            for chunk in self._stream(
-                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
-            ):
-                completion += chunk.text
-            return completion
+        params = {**self._get_parameters(stop), **kwargs}
 
-        return self._prepare_input_and_invoke(prompt=prompt, stop=stop, **kwargs)
+        url = f"{self.model_url}/api/v1/stream"
 
-    def get_num_tokens(self, text: str) -> int:
-        if self._model_is_anthropic:
-            return get_num_tokens_anthropic(text)
-        else:
-            return super().get_num_tokens(text)
+        request = params.copy()
+        request["prompt"] = prompt
 
-    def get_token_ids(self, text: str) -> List[int]:
-        if self._model_is_anthropic:
-            return get_token_ids_anthropic(text)
-        else:
-            return super().get_token_ids(text)
+        websocket_client = websocket.WebSocket()
+
+        websocket_client.connect(url)
+
+        websocket_client.send(json.dumps(request))
+
+        while True:
+            result = websocket_client.recv()
+            result = json.loads(result)
+
+            if result["event"] == "text_stream":  # type: ignore[call-overload, index]
+                chunk = GenerationChunk(
+                    text=result["text"],  # type: ignore[call-overload, index]
+                    generation_info=None,
+                )
+                yield chunk
+            elif result["event"] == "stream_end":  # type: ignore[call-overload, index]
+                websocket_client.close()
+                return
+
+            if run_manager:
+                await run_manager.on_llm_new_token(token=chunk.text)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/bittensor.py` & `gigachain_community-0.2.0/langchain_community/llms/bittensor.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/cerebriumai.py` & `gigachain_community-0.2.0/langchain_community/llms/cerebriumai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/chatglm.py` & `gigachain_community-0.2.0/langchain_community/llms/chatglm.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,15 +68,15 @@
 
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
-                response = chatglm_llm("Who are you?")
+                response = chatglm_llm.invoke("Who are you?")
         """
 
         _model_kwargs = self.model_kwargs or {}
 
         # HTTP headers for authorization
         headers = {"Content-Type": "application/json"}
 
@@ -121,9 +121,9 @@
                 f"Error raised during decoding response from inference endpoint: {e}."
                 f"\nResponse: {response.text}"
             )
 
         if stop is not None:
             text = enforce_stop_tokens(text, stop)
         if self.with_history:
-            self.history = self.history + [[None, parsed_response["response"]]]
+            self.history = parsed_response["history"]
         return text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/clarifai.py` & `gigachain_community-0.2.0/langchain_community/llms/clarifai.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import logging
 from typing import Any, Dict, List, Optional
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import Generation, LLMResult
-from langchain_core.pydantic_v1 import Extra, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, root_validator
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
 EXAMPLE_URL = "https://clarifai.com/openai/chat-completion/models/GPT-4"
@@ -38,42 +37,56 @@
     """Model id to use."""
     model_version_id: Optional[str] = None
     """Model version id to use."""
     app_id: Optional[str] = None
     """Clarifai application id to use."""
     user_id: Optional[str] = None
     """Clarifai user id to use."""
-    pat: Optional[str] = None
+    pat: Optional[str] = Field(default=None, exclude=True)  #: :meta private:
     """Clarifai personal access token to use."""
+    token: Optional[str] = Field(default=None, exclude=True)  #: :meta private:
+    """Clarifai session token to use."""
+    model: Any = Field(default=None, exclude=True)  #: :meta private:
     api_base: str = "https://api.clarifai.com"
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that we have all required info to access Clarifai
         platform and python package exists in environment."""
-        values["pat"] = get_from_dict_or_env(values, "pat", "CLARIFAI_PAT")
+        try:
+            from clarifai.client.model import Model
+        except ImportError:
+            raise ImportError(
+                "Could not import clarifai python package. "
+                "Please install it with `pip install clarifai`."
+            )
         user_id = values.get("user_id")
         app_id = values.get("app_id")
         model_id = values.get("model_id")
+        model_version_id = values.get("model_version_id")
         model_url = values.get("model_url")
-
-        if model_url is not None and model_id is not None:
-            raise ValueError("Please provide either model_url or model_id, not both.")
-
-        if model_url is None and model_id is None:
-            raise ValueError("Please provide one of model_url or model_id.")
-
-        if model_url is None and model_id is not None:
-            if user_id is None or app_id is None:
-                raise ValueError("Please provide a user_id and app_id.")
+        api_base = values.get("api_base")
+        pat = values.get("pat")
+        token = values.get("token")
+
+        values["model"] = Model(
+            url=model_url,
+            app_id=app_id,
+            user_id=user_id,
+            model_version=dict(id=model_version_id),
+            pat=pat,
+            token=token,
+            model_id=model_id,
+            base_url=api_base,
+        )
 
         return values
 
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Clarifai API."""
         return {}
@@ -111,38 +124,20 @@
 
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
-                response = clarifai_llm("Tell me a joke.")
+                response = clarifai_llm.invoke("Tell me a joke.")
         """
-        # If version_id None, Defaults to the latest model version
-        try:
-            from clarifai.client.model import Model
-        except ImportError:
-            raise ImportError(
-                "Could not import clarifai python package. "
-                "Please install it with `pip install clarifai`."
-            )
-        if self.pat is not None:
-            pat = self.pat
-        if self.model_url is not None:
-            _model_init = Model(url=self.model_url, pat=pat)
-        else:
-            _model_init = Model(
-                model_id=self.model_id,
-                user_id=self.user_id,
-                app_id=self.app_id,
-                pat=pat,
-            )
+
         try:
             (inference_params := {}) if inference_params is None else inference_params
-            predict_response = _model_init.predict_by_bytes(
+            predict_response = self.model.predict_by_bytes(
                 bytes(prompt, "utf-8"),
                 input_type="text",
                 inference_params=inference_params,
             )
             text = predict_response.outputs[0].data.text.raw
             if stop is not None:
                 text = enforce_stop_tokens(text, stop)
@@ -161,46 +156,34 @@
         **kwargs: Any,
     ) -> LLMResult:
         """Run the LLM on the given prompt and input."""
 
         # TODO: add caching here.
         try:
             from clarifai.client.input import Inputs
-            from clarifai.client.model import Model
         except ImportError:
             raise ImportError(
                 "Could not import clarifai python package. "
                 "Please install it with `pip install clarifai`."
             )
-        if self.pat is not None:
-            pat = self.pat
-        if self.model_url is not None:
-            _model_init = Model(url=self.model_url, pat=pat)
-        else:
-            _model_init = Model(
-                model_id=self.model_id,
-                user_id=self.user_id,
-                app_id=self.app_id,
-                pat=pat,
-            )
 
         generations = []
         batch_size = 32
-        input_obj = Inputs(pat=pat)
+        input_obj = Inputs.from_auth_helper(self.model.auth_helper)
         try:
             for i in range(0, len(prompts), batch_size):
                 batch = prompts[i : i + batch_size]
                 input_batch = [
                     input_obj.get_text_input(input_id=str(id), raw_text=inp)
                     for id, inp in enumerate(batch)
                 ]
                 (
                     inference_params := {}
                 ) if inference_params is None else inference_params
-                predict_response = _model_init.predict(
+                predict_response = self.model.predict(
                     inputs=input_batch, inference_params=inference_params
                 )
 
             for output in predict_response.outputs:
                 if stop is not None:
                     text = enforce_stop_tokens(output.data.text.raw, stop)
                 else:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/cloudflare_workersai.py` & `gigachain_community-0.2.0/langchain_community/llms/cloudflare_workersai.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
 
 logger = logging.getLogger(__name__)
 
 
 class CloudflareWorkersAI(LLM):
-    """Langchain LLM class to help to access Cloudflare Workers AI service.
+    """Cloudflare Workers AI service.
 
     To use, you must provide an API token and
     account ID to access Cloudflare Workers AI, and
     pass it as a named parameter to the constructor.
 
     Example:
         .. code-block:: python
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/cohere.py` & `gigachain_community-0.2.0/langchain_community/llms/cohere.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,83 +1,95 @@
 from __future__ import annotations
 
 import logging
 from typing import Any, Callable, Dict, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import LLM
 from langchain_core.load.serializable import Serializable
-from langchain_core.pydantic_v1 import Extra, Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     retry_if_exception_type,
     stop_after_attempt,
     wait_exponential,
 )
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
-def _create_retry_decorator(llm: Cohere) -> Callable[[Any], Any]:
+def _create_retry_decorator(max_retries: int) -> Callable[[Any], Any]:
     import cohere
 
+    # support v4 and v5
+    retry_conditions = (
+        retry_if_exception_type(cohere.error.CohereError)
+        if hasattr(cohere, "error")
+        else retry_if_exception_type(Exception)
+    )
+
     min_seconds = 4
     max_seconds = 10
     # Wait 2^x * 1 second between each retry starting with
     # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
     return retry(
         reraise=True,
-        stop=stop_after_attempt(llm.max_retries),
+        stop=stop_after_attempt(max_retries),
         wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
-        retry=(retry_if_exception_type(cohere.error.CohereError)),
+        retry=retry_conditions,
         before_sleep=before_sleep_log(logger, logging.WARNING),
     )
 
 
 def completion_with_retry(llm: Cohere, **kwargs: Any) -> Any:
     """Use tenacity to retry the completion call."""
-    retry_decorator = _create_retry_decorator(llm)
+    retry_decorator = _create_retry_decorator(llm.max_retries)
 
     @retry_decorator
     def _completion_with_retry(**kwargs: Any) -> Any:
         return llm.client.generate(**kwargs)
 
     return _completion_with_retry(**kwargs)
 
 
 def acompletion_with_retry(llm: Cohere, **kwargs: Any) -> Any:
     """Use tenacity to retry the completion call."""
-    retry_decorator = _create_retry_decorator(llm)
+    retry_decorator = _create_retry_decorator(llm.max_retries)
 
     @retry_decorator
     async def _completion_with_retry(**kwargs: Any) -> Any:
         return await llm.async_client.generate(**kwargs)
 
     return _completion_with_retry(**kwargs)
 
 
+@deprecated(
+    since="0.0.30", removal="0.3.0", alternative_import="langchain_cohere.BaseCohere"
+)
 class BaseCohere(Serializable):
     """Base class for Cohere models."""
 
     client: Any  #: :meta private:
     async_client: Any  #: :meta private:
     model: Optional[str] = Field(default=None)
     """Model name to use."""
 
     temperature: float = 0.75
     """A non-negative float that tunes the degree of randomness in generation."""
 
-    cohere_api_key: Optional[str] = None
+    cohere_api_key: Optional[SecretStr] = None
+    """Cohere API key. If not provided, will be read from the environment variable."""
 
     stop: Optional[List[str]] = None
 
     streaming: bool = Field(default=False)
     """Whether to stream the results."""
 
     user_agent: str = "langchain"
@@ -90,25 +102,32 @@
             import cohere
         except ImportError:
             raise ImportError(
                 "Could not import cohere python package. "
                 "Please install it with `pip install cohere`."
             )
         else:
-            cohere_api_key = get_from_dict_or_env(
-                values, "cohere_api_key", "COHERE_API_KEY"
+            values["cohere_api_key"] = convert_to_secret_str(
+                get_from_dict_or_env(values, "cohere_api_key", "COHERE_API_KEY")
             )
             client_name = values["user_agent"]
-            values["client"] = cohere.Client(cohere_api_key, client_name=client_name)
+            values["client"] = cohere.Client(
+                api_key=values["cohere_api_key"].get_secret_value(),
+                client_name=client_name,
+            )
             values["async_client"] = cohere.AsyncClient(
-                cohere_api_key, client_name=client_name
+                api_key=values["cohere_api_key"].get_secret_value(),
+                client_name=client_name,
             )
         return values
 
 
+@deprecated(
+    since="0.1.14", removal="0.3.0", alternative_import="langchain_cohere.Cohere"
+)
 class Cohere(LLM, BaseCohere):
     """Cohere large language models.
 
     To use, you should have the ``cohere`` python package installed, and the
     environment variable ``COHERE_API_KEY`` set with your API key, or pass
     it as a named parameter to the constructor.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/ctransformers.py` & `gigachain_community-0.2.0/langchain_community/llms/ctransformers.py`

 * *Files 2% similar despite different names*

```diff
@@ -93,15 +93,15 @@
 
         Returns:
             The generated text.
 
         Example:
             .. code-block:: python
 
-                response = llm("Tell me a joke.")
+                response = llm.invoke("Tell me a joke.")
         """
         text = []
         _run_manager = run_manager or CallbackManagerForLLMRun.get_noop_manager()
         for chunk in self.client(prompt, stop=stop, stream=True):
             text.append(chunk)
             _run_manager.on_llm_new_token(chunk, verbose=self.verbose)
         return "".join(text)
@@ -121,15 +121,15 @@
             stop: A list of strings to stop generation when encountered.
 
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
-                response = llm("Once upon a time, ")
+                response = llm.invoke("Once upon a time, ")
         """
         text_callback = None
         if run_manager:
             text_callback = partial(run_manager.on_llm_new_token, verbose=self.verbose)
 
         text = ""
         for token in self.client(prompt, stop=stop, stream=True):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/ctranslate2.py` & `gigachain_community-0.2.0/langchain_community/llms/ctranslate2.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/databricks.py` & `gigachain_community-0.2.0/langchain_community/llms/databricks.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import os
+import re
 import warnings
 from abc import ABC, abstractmethod
 from typing import Any, Callable, Dict, List, Mapping, Optional
 
 import requests
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models import LLM
@@ -156,29 +157,29 @@
         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
     ) -> Any:
         resp = self._post(self.api_url, request)
         return transform_output_fn(resp) if transform_output_fn else resp
 
 
 def get_repl_context() -> Any:
-    """Gets the notebook REPL context if running inside a Databricks notebook.
+    """Get the notebook REPL context if running inside a Databricks notebook.
     Returns None otherwise.
     """
     try:
         from dbruntime.databricks_repl_context import get_context
 
         return get_context()
     except ImportError:
         raise ImportError(
             "Cannot access dbruntime, not running inside a Databricks notebook."
         )
 
 
 def get_default_host() -> str:
-    """Gets the default Databricks workspace hostname.
+    """Get the default Databricks workspace hostname.
     Raises an error if the hostname cannot be automatically determined.
     """
     host = os.getenv("DATABRICKS_HOST")
     if not host:
         try:
             host = get_repl_context().browserHostName
             if not host:
@@ -190,15 +191,15 @@
             )
     # TODO: support Databricks CLI profile
     host = host.lstrip("https://").lstrip("http://").rstrip("/")
     return host
 
 
 def get_default_api_token() -> str:
-    """Gets the default Databricks personal access token.
+    """Get the default Databricks personal access token.
     Raises an error if the token cannot be automatically determined.
     """
     if api_token := os.getenv("DATABRICKS_TOKEN"):
         return api_token
     try:
         api_token = get_repl_context().apiToken
         if not api_token:
@@ -208,16 +209,64 @@
             "api_token was not set and cannot be automatically inferred. Set "
             f"environment variable 'DATABRICKS_TOKEN'. Received error: {e}"
         )
     # TODO: support Databricks CLI profile
     return api_token
 
 
-class Databricks(LLM):
+def _is_hex_string(data: str) -> bool:
+    """Checks if a data is a valid hexadecimal string using a regular expression."""
+    if not isinstance(data, str):
+        return False
+    pattern = r"^[0-9a-fA-F]+$"
+    return bool(re.match(pattern, data))
+
+
+def _load_pickled_fn_from_hex_string(
+    data: str, allow_dangerous_deserialization: Optional[bool]
+) -> Callable:
+    """Loads a pickled function from a hexadecimal string."""
+    if not allow_dangerous_deserialization:
+        raise ValueError(
+            "This code relies on the pickle module. "
+            "You will need to set allow_dangerous_deserialization=True "
+            "if you want to opt-in to allow deserialization of data using pickle."
+            "Data can be compromised by a malicious actor if "
+            "not handled properly to include "
+            "a malicious payload that when deserialized with "
+            "pickle can execute arbitrary code on your machine."
+        )
+
+    try:
+        import cloudpickle
+    except Exception as e:
+        raise ValueError(f"Please install cloudpickle>=2.0.0. Error: {e}")
+
+    try:
+        return cloudpickle.loads(bytes.fromhex(data))
+    except Exception as e:
+        raise ValueError(
+            f"Failed to load the pickled function from a hexadecimal string. Error: {e}"
+        )
 
+
+def _pickle_fn_to_hex_string(fn: Callable) -> str:
+    """Pickles a function and returns the hexadecimal string."""
+    try:
+        import cloudpickle
+    except Exception as e:
+        raise ValueError(f"Please install cloudpickle>=2.0.0. Error: {e}")
+
+    try:
+        return cloudpickle.dumps(fn).hex()
+    except Exception as e:
+        raise ValueError(f"Failed to pickle the function: {e}")
+
+
+class Databricks(LLM):
     """Databricks serving endpoint or a cluster driver proxy app for LLM.
 
     It supports two endpoint types:
 
     * **Serving endpoint** (recommended for both production and development).
       We assume that an LLM was deployed to a serving endpoint.
       To wrap it as an LLM you must have "Can Query" permission to the endpoint.
@@ -333,14 +382,23 @@
     extra_params: Dict[str, Any] = Field(default_factory=dict)
     """Any extra parameters to pass to the endpoint."""
     task: Optional[str] = None
     """The task of the endpoint. Only used when using a serving endpoint.
     If not provided, the task is automatically inferred from the endpoint.
     """
 
+    allow_dangerous_deserialization: bool = False
+    """Whether to allow dangerous deserialization of the data which 
+    involves loading data using pickle.
+    
+    If the data has been modified by a malicious actor, it can deliver a
+    malicious payload that results in execution of arbitrary code on the target
+    machine.
+    """
+
     _client: _DatabricksClientBase = PrivateAttr()
 
     class Config:
         extra = Extra.forbid
         underscore_attrs_are_private = True
 
     @property
@@ -394,14 +452,31 @@
     def set_model_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
         if v:
             assert "prompt" not in v, "model_kwargs must not contain key 'prompt'"
             assert "stop" not in v, "model_kwargs must not contain key 'stop'"
         return v
 
     def __init__(self, **data: Any):
+        if "transform_input_fn" in data and _is_hex_string(data["transform_input_fn"]):
+            data["transform_input_fn"] = _load_pickled_fn_from_hex_string(
+                data=data["transform_input_fn"],
+                allow_dangerous_deserialization=data.get(
+                    "allow_dangerous_deserialization"
+                ),
+            )
+        if "transform_output_fn" in data and _is_hex_string(
+            data["transform_output_fn"]
+        ):
+            data["transform_output_fn"] = _load_pickled_fn_from_hex_string(
+                data=data["transform_output_fn"],
+                allow_dangerous_deserialization=data.get(
+                    "allow_dangerous_deserialization"
+                ),
+            )
+
         super().__init__(**data)
         if self.model_kwargs is not None and self.extra_params is not None:
             raise ValueError("Cannot set both extra_params and extra_params.")
         elif self.model_kwargs is not None:
             warnings.warn(
                 "model_kwargs is deprecated. Please use extra_params instead.",
                 DeprecationWarning,
@@ -411,15 +486,15 @@
                 host=self.host,
                 api_token=self.api_token,
                 endpoint_name=self.endpoint_name,
                 databricks_uri=self.databricks_uri,
                 task=self.task,
             )
         elif self.cluster_id and self.cluster_driver_port:
-            self._client = _DatabricksClusterDriverProxyClient(
+            self._client = _DatabricksClusterDriverProxyClient(  # type: ignore[call-arg]
                 host=self.host,
                 api_token=self.api_token,
                 cluster_id=self.cluster_id,
                 cluster_driver_port=self.cluster_driver_port,
             )
         else:
             raise ValueError(
@@ -439,17 +514,20 @@
             "model_kwargs": self.model_kwargs,
             "temperature": self.temperature,
             "n": self.n,
             "stop": self.stop,
             "max_tokens": self.max_tokens,
             "extra_params": self.extra_params,
             "task": self.task,
-            # TODO: Support saving transform_input_fn and transform_output_fn
-            # "transform_input_fn": self.transform_input_fn,
-            # "transform_output_fn": self.transform_output_fn,
+            "transform_input_fn": None
+            if self.transform_input_fn is None
+            else _pickle_fn_to_hex_string(self.transform_input_fn),
+            "transform_output_fn": None
+            if self.transform_output_fn is None
+            else _pickle_fn_to_hex_string(self.transform_output_fn),
         }
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         return self._default_params
 
     @property
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/deepinfra.py` & `gigachain_community-0.2.0/langchain_community/llms/deepinfra.py`

 * *Ordering differences only*

 * *Files 1% similar despite different names*

```diff
@@ -151,17 +151,17 @@
             url=self._url(), data=self._body(prompt, {**kwargs, "stream": True})
         )
 
         self._handle_status(response.status_code, response.text)
         for line in _parse_stream(response.iter_lines()):
             chunk = _handle_sse_line(line)
             if chunk:
-                yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(chunk.text)
+                yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -170,17 +170,17 @@
         async with request.apost(
             url=self._url(), data=self._body(prompt, {**kwargs, "stream": True})
         ) as response:
             self._handle_status(response.status, response.text)
             async for line in _parse_stream_async(response.content):
                 chunk = _handle_sse_line(line)
                 if chunk:
-                    yield chunk
                     if run_manager:
                         await run_manager.on_llm_new_token(chunk.text)
+                    yield chunk
 
 
 def _parse_stream(rbody: Iterator[bytes]) -> Iterator[str]:
     for line in rbody:
         _line = _parse_stream_helper(line)
         if _line is not None:
             yield _line
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/deepsparse.py` & `gigachain_community-0.2.0/langchain_community/llms/deepsparse.py`

 * *Files 4% similar despite different names*

```diff
@@ -88,15 +88,15 @@
             stop: A list of strings to stop generation when encountered.
         Returns:
             The generated text.
         Example:
             .. code-block:: python
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
-                llm("Tell me a joke.")
+                llm.invoke("Tell me a joke.")
         """
         if self.streaming:
             combined_output = ""
             for chunk in self._stream(
                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
             ):
                 combined_output += chunk.text
@@ -126,15 +126,15 @@
             stop: A list of strings to stop generation when encountered.
         Returns:
             The generated text.
         Example:
             .. code-block:: python
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
-                llm("Tell me a joke.")
+                llm.invoke("Tell me a joke.")
         """
         if self.streaming:
             combined_output = ""
             async for chunk in self._astream(
                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
             ):
                 combined_output += chunk.text
@@ -173,15 +173,15 @@
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(
                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
                     streaming=True
                 )
                 for chunk in llm.stream("Tell me a joke",
                         stop=["'","\n"]):
-                    print(chunk, end='', flush=True)
+                    print(chunk, end='', flush=True)  # noqa: T201
         """
         inference = self.pipeline(
             sequences=prompt, streaming=True, **self.generation_config
         )
         for token in inference:
             chunk = GenerationChunk(text=token.generations[0].text)
             yield chunk
@@ -211,15 +211,15 @@
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(
                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
                     streaming=True
                 )
                 for chunk in llm.stream("Tell me a joke",
                         stop=["'","\n"]):
-                    print(chunk, end='', flush=True)
+                    print(chunk, end='', flush=True)  # noqa: T201
         """
         inference = self.pipeline(
             sequences=prompt, streaming=True, **self.generation_config
         )
         for token in inference:
             chunk = GenerationChunk(text=token.generations[0].text)
             yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/edenai.py` & `gigachain_community-0.2.0/langchain_community/llms/edenai.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around EdenAI's Generation API."""
+
 import logging
 from typing import Any, Dict, List, Literal, Optional
 
 from aiohttp import ClientSession
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
@@ -14,15 +15,15 @@
 from langchain_community.llms.utils import enforce_stop_tokens
 from langchain_community.utilities.requests import Requests
 
 logger = logging.getLogger(__name__)
 
 
 class EdenAI(LLM):
-    """Wrapper around edenai models.
+    """EdenAI models.
 
     To use, you should have
     the environment variable ``EDENAI_API_KEY`` set with your API token.
     You can find your token here: https://app.edenai.run/admin/account/settings
 
     `feature` and `subfeature` are required, but any other model parameters can also be
     passed in with the format params={model_param: value, ...}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/fake.py` & `gigachain_community-0.2.0/langchain_community/llms/fake.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/fireworks.py` & `gigachain_community-0.2.0/langchain_community/llms/fireworks.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import asyncio
 from concurrent.futures import ThreadPoolExecutor
 from typing import Any, AsyncIterator, Callable, Dict, Iterator, List, Optional, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import BaseLLM, create_base_retry_decorator
 from langchain_core.outputs import Generation, GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
@@ -22,14 +23,19 @@
         generation_info=dict(
             finish_reason=stream_response.choices[0].finish_reason,
             logprobs=stream_response.choices[0].logprobs,
         ),
     )
 
 
+@deprecated(
+    since="0.0.26",
+    removal="0.3",
+    alternative_import="langchain_fireworks.Fireworks",
+)
 class Fireworks(BaseLLM):
     """Fireworks models."""
 
     model: str = "accounts/fireworks/models/llama-v2-7b-chat"
     model_kwargs: dict = Field(
         default_factory=lambda: {
             "temperature": 0.7,
@@ -176,17 +182,17 @@
             "stream": True,
             **self.model_kwargs,
         }
         for stream_resp in completion_with_retry(
             self, self.use_retry, run_manager=run_manager, stop=stop, **params
         ):
             chunk = _stream_response_to_generation_chunk(stream_resp)
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+            yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -197,17 +203,17 @@
             "stream": True,
             **self.model_kwargs,
         }
         async for stream_resp in await acompletion_with_retry_streaming(
             self, self.use_retry, run_manager=run_manager, stop=stop, **params
         ):
             chunk = _stream_response_to_generation_chunk(stream_resp)
-            yield chunk
             if run_manager:
                 await run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+            yield chunk
 
 
 def conditional_decorator(
     condition: bool, decorator: Callable[[Any], Any]
 ) -> Callable[[Any], Any]:
     """Conditionally apply a decorator.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/forefrontai.py` & `gigachain_community-0.2.0/langchain_community/llms/forefrontai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/gigachat.py` & `gigachain_community-0.2.0/langchain_community/llms/gigachat.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,37 @@
 from __future__ import annotations
 
+import json
 import logging
 from functools import cached_property
-from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    AsyncIterator,
+    Dict,
+    Iterator,
+    List,
+    Literal,
+    Optional,
+)
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import BaseLLM
 from langchain_core.load.serializable import Serializable
 from langchain_core.outputs import Generation, GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import root_validator
 
+if TYPE_CHECKING:
+    import gigachat
+    import gigachat.models as gm
+    from gigachat._types import FileTypes
+
 logger = logging.getLogger(__name__)
 
 
 class _BaseGigaChat(Serializable):
     base_url: Optional[str] = None
     """ Base API URL """
     auth_url: Optional[str] = None
@@ -50,18 +65,27 @@
     profanity: bool = True
     """ DEPRECATED: Check for profanity """
     profanity_check: Optional[bool] = None
     """ Check for profanity """
     streaming: bool = False
     """ Whether to stream the results or not. """
     temperature: Optional[float] = None
-    """What sampling temperature to use."""
+    """ What sampling temperature to use. """
     max_tokens: Optional[int] = None
     """ Maximum number of tokens to generate """
     use_api_for_tokens: bool = False
+    """ Use GigaChat API for tokens count """
+    verbose: bool = False
+    """ Verbose logging """
+    top_p: Optional[float] = None
+    """ top_p value to use for nucleus sampling. Must be between 0.0 and 1.0 """
+    repetition_penalty: Optional[float] = None
+    """ The penalty applied to repeated tokens """
+    update_interval: Optional[float] = None
+    """ Minimum interval in seconds that elapses between sending tokens """
 
     @property
     def _llm_type(self) -> str:
         return "giga-chat-model"
 
     @property
     def lc_secrets(self) -> Dict[str, str]:
@@ -73,15 +97,15 @@
         }
 
     @property
     def lc_serializable(self) -> bool:
         return True
 
     @cached_property
-    def _client(self) -> Any:
+    def _client(self) -> gigachat.GigaChat:
         """Returns GigaChat API client"""
         import gigachat
 
         return gigachat.GigaChat(
             base_url=self.base_url,
             auth_url=self.auth_url,
             credentials=self.credentials,
@@ -93,14 +117,15 @@
             password=self.password,
             timeout=self.timeout,
             verify_ssl_certs=self.verify_ssl_certs,
             ca_bundle_file=self.ca_bundle_file,
             cert_file=self.cert_file,
             key_file=self.key_file,
             key_file_password=self.key_file_password,
+            verbose=self.verbose,
         )
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate authenticate data in environment and python package is installed."""
         try:
             import gigachat  # noqa: F401
@@ -109,93 +134,116 @@
                 "Could not import gigachat python package. "
                 "Please install it with `pip install gigachat`."
             )
         fields = set(cls.__fields__.keys())
         diff = set(values.keys()) - fields
         if diff:
             logger.warning(f"Extra fields {diff} in GigaChat class")
-        if "profanity" in fields:
+        if "profanity" in fields and values.get("profanity") is False:
             logger.warning(
-                "Profanity field is deprecated. Use 'profanity_check' instead."
+                "'profanity' field is deprecated. Use 'profanity_check' instead."
             )
             if values.get("profanity_check") is None:
                 values["profanity_check"] = values.get("profanity")
         return values
 
     @property
     def _identifying_params(self) -> Dict[str, Any]:
         """Get the identifying parameters."""
         return {
             "temperature": self.temperature,
             "model": self.model,
-            "profanity": self.profanity and self.profanity_check,
+            "profanity": self.profanity_check,
             "streaming": self.streaming,
             "max_tokens": self.max_tokens,
+            "top_p": self.top_p,
+            "repetition_penalty": self.repetition_penalty,
         }
 
-    def tokens_count(self, input_: List[str], model=None) -> List[Any]:
+    def tokens_count(
+        self, input_: List[str], model: Optional[str] = None
+    ) -> List[gm.TokensCount]:
         """Get tokens of string list"""
         return self._client.tokens_count(input_, model)
 
-    async def atokens_count(self, input_: List[str], model=None) -> List[Any]:
+    async def atokens_count(
+        self, input_: List[str], model: Optional[str] = None
+    ) -> List[gm.TokensCount]:
         """Get tokens of strings list (async)"""
         return await self._client.atokens_count(input_, model)
 
-    def get_models(self) -> Any:
+    def get_models(self) -> gm.Models:
         """Get available models of Gigachat"""
         return self._client.get_models()
 
-    async def aget_models(self) -> Any:
+    async def aget_models(self) -> gm.Models:
         """Get available models of Gigachat (async)"""
         return await self._client.aget_models()
 
-    def get_model(self, model: str) -> Any:
+    def get_model(self, model: str) -> gm.Model:
         """Get info about model"""
         return self._client.get_model(model)
 
-    async def aget_model(self, model: str) -> Any:
+    async def aget_model(self, model: str) -> gm.Model:
         """Get info about model (async)"""
         return await self._client.aget_model(model)
 
     def get_num_tokens(self, text: str) -> int:
         """Count approximate number of tokens"""
         if self.use_api_for_tokens:
             return self.tokens_count([text])[0].tokens  # type: ignore
         else:
             return round(len(text) / 4.6)
 
+    def upload_file(
+        self, file: FileTypes, purpose: Literal["general", "assistant"] = "general"
+    ) -> gm.UploadedFile:
+        return self._client.upload_file(file, purpose)
+
+    async def aupload_file(
+        self, file: FileTypes, purpose: Literal["general", "assistant"] = "general"
+    ) -> gm.UploadedFile:
+        return await self._client.aupload_file(file, purpose)
+
 
 class GigaChat(_BaseGigaChat, BaseLLM):
     """`GigaChat` large language models API.
 
     To use, you should pass login and password to access GigaChat API or use token.
 
     Example:
         .. code-block:: python
 
             from langchain_community.llms import GigaChat
-            giga = GigaChat(credentials=..., verify_ssl_certs=False)
+            giga = GigaChat(credentials=..., scope=..., verify_ssl_certs=False)
     """
 
     payload_role: str = "user"
 
     def _build_payload(self, messages: List[str]) -> Dict[str, Any]:
         payload: Dict[str, Any] = {
             "messages": [{"role": self.payload_role, "content": m} for m in messages],
-            "profanity_check": self.profanity_check,
         }
+        if self.model:
+            payload["model"] = self.model
+        if self.profanity_check is not None:
+            payload["profanity_check"] = self.profanity_check
         if self.temperature is not None:
             payload["temperature"] = self.temperature
+        if self.top_p is not None:
+            payload["top_p"] = self.top_p
         if self.max_tokens is not None:
             payload["max_tokens"] = self.max_tokens
-        if self.model:
-            payload["model"] = self.model
+        if self.repetition_penalty is not None:
+            payload["repetition_penalty"] = self.repetition_penalty
+        if self.update_interval is not None:
+            payload["update_interval"] = self.update_interval
 
         if self.verbose:
-            logger.info("Giga request: %s", payload)
+            logger.warning("Giga request: %s", json.dumps(payload, ensure_ascii=False))
 
         return payload
 
     def _create_llm_result(self, response: Any) -> LLMResult:
         generations = []
         for res in response.choices:
             finish_reason = res.finish_reason
@@ -207,14 +255,15 @@
             if finish_reason != "stop":
                 logger.warning(
                     "Giga generation stopped with reason: %s",
                     finish_reason,
                 )
             if self.verbose:
                 logger.info("Giga response: %s", res.message.content)
+
         token_usage = response.usage
         llm_output = {"token_usage": token_usage, "model_name": response.model}
         return LLMResult(generations=generations, llm_output=llm_output)
 
     def _generate(
         self,
         prompts: List[str],
@@ -277,29 +326,31 @@
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
         payload = self._build_payload([prompt])
 
         for chunk in self._client.stream(payload):
             if chunk.choices:
                 content = chunk.choices[0].delta.content
-                yield GenerationChunk(text=content)
+                yield GenerationChunk(text=content)  # type: ignore
                 if run_manager:
-                    run_manager.on_llm_new_token(content)
+                    run_manager.on_llm_new_token(content if content is not None else "")
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[GenerationChunk]:
         payload = self._build_payload([prompt])
 
         async for chunk in self._client.astream(payload):
             if chunk.choices:
                 content = chunk.choices[0].delta.content
-                yield GenerationChunk(text=content)
+                yield GenerationChunk(text=content)  # type: ignore
                 if run_manager:
-                    await run_manager.on_llm_new_token(content)
+                    await run_manager.on_llm_new_token(
+                        content if content is not None else ""
+                    )
 
     class Config:
         extra = "allow"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/google_palm.py` & `gigachain_community-0.2.0/langchain_community/llms/google_palm.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,15 +55,15 @@
     has_leading_space = all(not line or line[0] == " " for line in text.split("\n")[1:])
     if has_leading_space:
         return text.replace("\n ", "\n")
     else:
         return text
 
 
-@deprecated("0.0.351", alternative="langchain_google_genai.GoogleGenerativeAI")
+@deprecated("0.0.12", alternative_import="langchain_google_genai.GoogleGenerativeAI")
 class GooglePalm(BaseLLM, BaseModel):
     """
     DEPRECATED: Use `langchain_google_genai.GoogleGenerativeAI` instead.
 
     Google PaLM models.
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/gooseai.py` & `gigachain_community-0.2.0/langchain_community/llms/gooseai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/gpt4all.py` & `gigachain_community-0.2.0/langchain_community/llms/gpt4all.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     Example:
         .. code-block:: python
 
             from langchain_community.llms import GPT4All
             model = GPT4All(model="./models/gpt4all-model.bin", n_threads=8)
 
             # Simplest invocation
-            response = model("Once upon a time, ")
+            response = model.invoke("Once upon a time, ")
     """
 
     model: str
     """Path to the pre-trained GPT4All model file."""
 
     backend: Optional[str] = Field(None, alias="backend")
 
@@ -107,26 +107,28 @@
             "n_predict",
             "top_k",
             "top_p",
             "temp",
             "n_batch",
             "repeat_penalty",
             "repeat_last_n",
+            "streaming",
         }
 
     def _default_params(self) -> Dict[str, Any]:
         return {
             "max_tokens": self.max_tokens,
             "n_predict": self.n_predict,
             "top_k": self.top_k,
             "top_p": self.top_p,
             "temp": self.temp,
             "n_batch": self.n_batch,
             "repeat_penalty": self.repeat_penalty,
             "repeat_last_n": self.repeat_last_n,
+            "streaming": self.streaming,
         }
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that the python package exists in the environment."""
         try:
             from gpt4all import GPT4All as GPT4AllModel
@@ -191,15 +193,15 @@
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
                 prompt = "Once upon a time, "
-                response = model(prompt, n_predict=55)
+                response = model.invoke(prompt, n_predict=55)
         """
         text_callback = None
         if run_manager:
             text_callback = partial(run_manager.on_llm_new_token, verbose=self.verbose)
         text = ""
         params = {**self._default_params(), **kwargs}
         for token in self.client.generate(prompt, **params):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/gradient_ai.py` & `gigachain_community-0.2.0/langchain_community/llms/gradient_ai.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/grammars/json.gbnf` & `gigachain_community-0.2.0/langchain_community/llms/grammars/json.gbnf`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/huggingface_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/huggingface_hub.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,47 +1,56 @@
+import json
 from typing import Any, Dict, List, Mapping, Optional
 
-import requests
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.pydantic_v1 import Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
-VALID_TASKS = ("text2text-generation", "text-generation", "summarization")
+# key: task
+# value: key in the output dictionary
+VALID_TASKS_DICT = {
+    "translation": "translation_text",
+    "summarization": "summary_text",
+    "conversational": "generated_text",
+    "text-generation": "generated_text",
+    "text2text-generation": "generated_text",
+}
 
 
-class HuggingFaceEndpoint(LLM):
-    """HuggingFace Endpoint models.
+@deprecated("0.0.21", removal="0.3.0", alternative="HuggingFaceEndpoint")
+class HuggingFaceHub(LLM):
+    """HuggingFaceHub  models.
+    ! This class is deprecated, you should use HuggingFaceEndpoint instead.
 
     To use, you should have the ``huggingface_hub`` python package installed, and the
     environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass
     it as a named parameter to the constructor.
 
-    Only supports `text-generation` and `text2text-generation` for now.
+    Supports `text-generation`, `text2text-generation`, `conversational`, `translation`,
+     and `summarization`.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.llms import HuggingFaceEndpoint
-            endpoint_url = (
-                "https://abcdefghijklmnop.us-east-1.aws.endpoints.huggingface.cloud"
-            )
-            hf = HuggingFaceEndpoint(
-                endpoint_url=endpoint_url,
-                huggingfacehub_api_token="my-api-key"
-            )
+            from langchain_community.llms import HuggingFaceHub
+            hf = HuggingFaceHub(repo_id="gpt2", huggingfacehub_api_token="my-api-key")
     """
 
-    endpoint_url: str = ""
-    """Endpoint URL to use."""
+    client: Any  #: :meta private:
+    repo_id: Optional[str] = None
+    """Model name to use. 
+    If not provided, the default model for the chosen task will be used."""
     task: Optional[str] = None
     """Task to call the model with.
-    Should be a task that returns `generated_text` or `summary_text`."""
+    Should be a task that returns `generated_text`, `summary_text`, 
+    or `translation_text`."""
     model_kwargs: Optional[dict] = None
     """Keyword arguments to pass to the model."""
 
     huggingfacehub_api_token: Optional[str] = None
 
     class Config:
         """Configuration for this pydantic object."""
@@ -51,48 +60,57 @@
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
         huggingfacehub_api_token = get_from_dict_or_env(
             values, "huggingfacehub_api_token", "HUGGINGFACEHUB_API_TOKEN"
         )
         try:
-            from huggingface_hub.hf_api import HfApi
+            from huggingface_hub import HfApi, InferenceClient
 
-            try:
-                HfApi(
-                    endpoint="https://huggingface.co",  # Can be a Private Hub endpoint.
-                    token=huggingfacehub_api_token,
-                ).whoami()
-            except Exception as e:
+            repo_id = values["repo_id"]
+            client = InferenceClient(
+                model=repo_id,
+                token=huggingfacehub_api_token,
+            )
+            if not values["task"]:
+                if not repo_id:
+                    raise ValueError(
+                        "Must specify either `repo_id` or `task`, or both."
+                    )
+                # Use the recommended task for the chosen model
+                model_info = HfApi(token=huggingfacehub_api_token).model_info(
+                    repo_id=repo_id
+                )
+                values["task"] = model_info.pipeline_tag
+            if values["task"] not in VALID_TASKS_DICT:
                 raise ValueError(
-                    "Could not authenticate with huggingface_hub. "
-                    "Please check your API token."
-                ) from e
-
+                    f"Got invalid task {values['task']}, "
+                    f"currently only {VALID_TASKS_DICT.keys()} are supported"
+                )
+            values["client"] = client
         except ImportError:
             raise ImportError(
                 "Could not import huggingface_hub python package. "
                 "Please install it with `pip install huggingface_hub`."
             )
-        values["huggingfacehub_api_token"] = huggingfacehub_api_token
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         _model_kwargs = self.model_kwargs or {}
         return {
-            **{"endpoint_url": self.endpoint_url, "task": self.task},
+            **{"repo_id": self.repo_id, "task": self.task},
             **{"model_kwargs": _model_kwargs},
         }
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
-        return "huggingface_endpoint"
+        return "huggingface_hub"
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -108,49 +126,27 @@
 
         Example:
             .. code-block:: python
 
                 response = hf("Tell me a joke.")
         """
         _model_kwargs = self.model_kwargs or {}
+        parameters = {**_model_kwargs, **kwargs}
 
-        # payload samples
-        params = {**_model_kwargs, **kwargs}
-        parameter_payload = {"inputs": prompt, "parameters": params}
-
-        # HTTP headers for authorization
-        headers = {
-            "Authorization": f"Bearer {self.huggingfacehub_api_token}",
-            "Content-Type": "application/json",
-        }
-
-        # send request
-        try:
-            response = requests.post(
-                self.endpoint_url, headers=headers, json=parameter_payload
-            )
-        except requests.exceptions.RequestException as e:  # This is the correct syntax
-            raise ValueError(f"Error raised by inference endpoint: {e}")
-        generated_text = response.json()
-        if "error" in generated_text:
-            raise ValueError(
-                f"Error raised by inference API: {generated_text['error']}"
-            )
-        if self.task == "text-generation":
-            text = generated_text[0]["generated_text"]
-            # Remove prompt if included in generated text.
-            if text.startswith(prompt):
-                text = text[len(prompt) :]
-        elif self.task == "text2text-generation":
-            text = generated_text[0]["generated_text"]
-        elif self.task == "summarization":
-            text = generated_text[0]["summary_text"]
+        response = self.client.post(
+            json={"inputs": prompt, "parameters": parameters}, task=self.task
+        )
+        response = json.loads(response.decode())
+        if "error" in response:
+            raise ValueError(f"Error raised by inference API: {response['error']}")
+
+        response_key = VALID_TASKS_DICT[self.task]  # type: ignore
+        if isinstance(response, list):
+            text = response[0][response_key]
         else:
-            raise ValueError(
-                f"Got invalid task {self.task}, "
-                f"currently only {VALID_TASKS} are supported"
-            )
+            text = response[response_key]
+
         if stop is not None:
             # This is a bit hacky, but I can't figure out a better way to enforce
             # stop tokens when making calls to huggingface_hub.
             text = enforce_stop_tokens(text, stop)
         return text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/huggingface_hub.py` & `gigachain_community-0.2.0/langchain_community/llms/petals.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,130 +1,153 @@
+import logging
 from typing import Any, Dict, List, Mapping, Optional
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
-DEFAULT_REPO_ID = "gpt2"
-VALID_TASKS = ("text2text-generation", "text-generation", "summarization")
+logger = logging.getLogger(__name__)
 
 
-class HuggingFaceHub(LLM):
-    """HuggingFaceHub  models.
+class Petals(LLM):
+    """Petals Bloom models.
 
-    To use, you should have the ``huggingface_hub`` python package installed, and the
-    environment variable ``HUGGINGFACEHUB_API_TOKEN`` set with your API token, or pass
-    it as a named parameter to the constructor.
+    To use, you should have the ``petals`` python package installed, and the
+    environment variable ``HUGGINGFACE_API_KEY`` set with your API key.
 
-    Only supports `text-generation`, `text2text-generation` and `summarization` for now.
+    Any parameters that are valid to be passed to the call can be passed
+    in, even if not explicitly saved on this class.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.llms import HuggingFaceHub
-            hf = HuggingFaceHub(repo_id="gpt2", huggingfacehub_api_token="my-api-key")
+            from langchain_community.llms import petals
+            petals = Petals()
+
     """
 
-    client: Any  #: :meta private:
-    repo_id: str = DEFAULT_REPO_ID
-    """Model name to use."""
-    task: Optional[str] = None
-    """Task to call the model with.
-    Should be a task that returns `generated_text` or `summary_text`."""
-    model_kwargs: Optional[dict] = None
-    """Keyword arguments to pass to the model."""
+    client: Any
+    """The client to use for the API calls."""
+
+    tokenizer: Any
+    """The tokenizer to use for the API calls."""
+
+    model_name: str = "bigscience/bloom-petals"
+    """The model to use."""
+
+    temperature: float = 0.7
+    """What sampling temperature to use"""
+
+    max_new_tokens: int = 256
+    """The maximum number of new tokens to generate in the completion."""
+
+    top_p: float = 0.9
+    """The cumulative probability for top-p sampling."""
 
-    huggingfacehub_api_token: Optional[str] = None
+    top_k: Optional[int] = None
+    """The number of highest probability vocabulary tokens
+    to keep for top-k-filtering."""
+
+    do_sample: bool = True
+    """Whether or not to use sampling; use greedy decoding otherwise."""
+
+    max_length: Optional[int] = None
+    """The maximum length of the sequence to be generated."""
+
+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """Holds any model parameters valid for `create` call
+    not explicitly specified."""
+
+    huggingface_api_key: Optional[SecretStr] = None
 
     class Config:
-        """Configuration for this pydantic object."""
+        """Configuration for this pydantic config."""
 
         extra = Extra.forbid
 
+    @root_validator(pre=True)
+    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        """Build extra kwargs from additional params that were passed in."""
+        all_required_field_names = {field.alias for field in cls.__fields__.values()}
+
+        extra = values.get("model_kwargs", {})
+        for field_name in list(values):
+            if field_name not in all_required_field_names:
+                if field_name in extra:
+                    raise ValueError(f"Found {field_name} supplied twice.")
+                logger.warning(
+                    f"""WARNING! {field_name} is not default parameter.
+                    {field_name} was transferred to model_kwargs.
+                    Please confirm that {field_name} is what you intended."""
+                )
+                extra[field_name] = values.pop(field_name)
+        values["model_kwargs"] = extra
+        return values
+
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        huggingfacehub_api_token = get_from_dict_or_env(
-            values, "huggingfacehub_api_token", "HUGGINGFACEHUB_API_TOKEN"
+        huggingface_api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "huggingface_api_key", "HUGGINGFACE_API_KEY")
         )
         try:
-            from huggingface_hub.inference_api import InferenceApi
+            from petals import AutoDistributedModelForCausalLM
+            from transformers import AutoTokenizer
 
-            repo_id = values["repo_id"]
-            client = InferenceApi(
-                repo_id=repo_id,
-                token=huggingfacehub_api_token,
-                task=values.get("task"),
+            model_name = values["model_name"]
+            values["tokenizer"] = AutoTokenizer.from_pretrained(model_name)
+            values["client"] = AutoDistributedModelForCausalLM.from_pretrained(
+                model_name
             )
-            if client.task not in VALID_TASKS:
-                raise ValueError(
-                    f"Got invalid task {client.task}, "
-                    f"currently only {VALID_TASKS} are supported"
-                )
-            values["client"] = client
+            values["huggingface_api_key"] = huggingface_api_key.get_secret_value()
+
         except ImportError:
-            raise ValueError(
-                "Could not import huggingface_hub python package. "
-                "Please install it with `pip install huggingface_hub`."
+            raise ImportError(
+                "Could not import transformers or petals python package."
+                "Please install with `pip install -U transformers petals`."
             )
         return values
 
     @property
+    def _default_params(self) -> Dict[str, Any]:
+        """Get the default parameters for calling Petals API."""
+        normal_params = {
+            "temperature": self.temperature,
+            "max_new_tokens": self.max_new_tokens,
+            "top_p": self.top_p,
+            "top_k": self.top_k,
+            "do_sample": self.do_sample,
+            "max_length": self.max_length,
+        }
+        return {**normal_params, **self.model_kwargs}
+
+    @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
-        _model_kwargs = self.model_kwargs or {}
-        return {
-            **{"repo_id": self.repo_id, "task": self.task},
-            **{"model_kwargs": _model_kwargs},
-        }
+        return {**{"model_name": self.model_name}, **self._default_params}
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
-        return "huggingface_hub"
+        return "petals"
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call out to HuggingFace Hub's inference endpoint.
-
-        Args:
-            prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
-
-        Returns:
-            The string generated by the model.
-
-        Example:
-            .. code-block:: python
-
-                response = hf("Tell me a joke.")
-        """
-        _model_kwargs = self.model_kwargs or {}
-        params = {**_model_kwargs, **kwargs}
-        response = self.client(inputs=prompt, params=params)
-        if "error" in response:
-            raise ValueError(f"Error raised by inference API: {response['error']}")
-        if self.client.task == "text-generation":
-            # Text generation return includes the starter text.
-            text = response[0]["generated_text"][len(prompt) :]
-        elif self.client.task == "text2text-generation":
-            text = response[0]["generated_text"]
-        elif self.client.task == "summarization":
-            text = response[0]["summary_text"]
-        else:
-            raise ValueError(
-                f"Got invalid task {self.client.task}, "
-                f"currently only {VALID_TASKS} are supported"
-            )
+        """Call the Petals API."""
+        params = self._default_params
+        params = {**params, **kwargs}
+        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"]
+        outputs = self.client.generate(inputs, **params)
+        text = self.tokenizer.decode(outputs[0])
         if stop is not None:
-            # This is a bit hacky, but I can't figure out a better way to enforce
-            # stop tokens when making calls to huggingface_hub.
+            # I believe this is required since the stop tokens
+            # are not enforced by the model parameters
             text = enforce_stop_tokens(text, stop)
         return text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/huggingface_pipeline.py` & `gigachain_community-0.2.0/langchain_community/llms/self_hosted_hugging_face.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,247 +1,213 @@
-from __future__ import annotations
-
 import importlib.util
 import logging
-from typing import Any, List, Mapping, Optional
+from typing import Any, Callable, List, Mapping, Optional
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
-from langchain_core.language_models.llms import BaseLLM
-from langchain_core.outputs import Generation, LLMResult
 from langchain_core.pydantic_v1 import Extra
 
+from langchain_community.llms.self_hosted import SelfHostedPipeline
 from langchain_community.llms.utils import enforce_stop_tokens
 
 DEFAULT_MODEL_ID = "gpt2"
 DEFAULT_TASK = "text-generation"
 VALID_TASKS = ("text2text-generation", "text-generation", "summarization")
-DEFAULT_BATCH_SIZE = 4
 
 logger = logging.getLogger(__name__)
 
 
-class HuggingFacePipeline(BaseLLM):
-    """HuggingFace Pipeline API.
+def _generate_text(
+    pipeline: Any,
+    prompt: str,
+    *args: Any,
+    stop: Optional[List[str]] = None,
+    **kwargs: Any,
+) -> str:
+    """Inference function to send to the remote hardware.
+
+    Accepts a Hugging Face pipeline (or more likely,
+    a key pointing to such a pipeline on the cluster's object store)
+    and returns generated text.
+    """
+    response = pipeline(prompt, *args, **kwargs)
+    if pipeline.task == "text-generation":
+        # Text generation return includes the starter text.
+        text = response[0]["generated_text"][len(prompt) :]
+    elif pipeline.task == "text2text-generation":
+        text = response[0]["generated_text"]
+    elif pipeline.task == "summarization":
+        text = response[0]["summary_text"]
+    else:
+        raise ValueError(
+            f"Got invalid task {pipeline.task}, "
+            f"currently only {VALID_TASKS} are supported"
+        )
+    if stop is not None:
+        text = enforce_stop_tokens(text, stop)
+    return text
+
+
+def _load_transformer(
+    model_id: str = DEFAULT_MODEL_ID,
+    task: str = DEFAULT_TASK,
+    device: int = 0,
+    model_kwargs: Optional[dict] = None,
+) -> Any:
+    """Inference function to send to the remote hardware.
+
+    Accepts a huggingface model_id and returns a pipeline for the task.
+    """
+    from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer
+    from transformers import pipeline as hf_pipeline
+
+    _model_kwargs = model_kwargs or {}
+    tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)
+
+    try:
+        if task == "text-generation":
+            model = AutoModelForCausalLM.from_pretrained(model_id, **_model_kwargs)
+        elif task in ("text2text-generation", "summarization"):
+            model = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)
+        else:
+            raise ValueError(
+                f"Got invalid task {task}, "
+                f"currently only {VALID_TASKS} are supported"
+            )
+    except ImportError as e:
+        raise ImportError(
+            f"Could not load the {task} model due to missing dependencies."
+        ) from e
+
+    if importlib.util.find_spec("torch") is not None:
+        import torch
+
+        cuda_device_count = torch.cuda.device_count()
+        if device < -1 or (device >= cuda_device_count):
+            raise ValueError(
+                f"Got device=={device}, "
+                f"device is required to be within [-1, {cuda_device_count})"
+            )
+        if device < 0 and cuda_device_count > 0:
+            logger.warning(
+                "Device has %d GPUs available. "
+                "Provide device={deviceId} to `from_model_id` to use available"
+                "GPUs for execution. deviceId is -1 for CPU and "
+                "can be a positive integer associated with CUDA device id.",
+                cuda_device_count,
+            )
+
+    pipeline = hf_pipeline(
+        task=task,
+        model=model,
+        tokenizer=tokenizer,
+        device=device,
+        model_kwargs=_model_kwargs,
+    )
+    if pipeline.task not in VALID_TASKS:
+        raise ValueError(
+            f"Got invalid task {pipeline.task}, "
+            f"currently only {VALID_TASKS} are supported"
+        )
+    return pipeline
+
+
+class SelfHostedHuggingFaceLLM(SelfHostedPipeline):
+    """HuggingFace Pipeline API to run on self-hosted remote hardware.
+
+    Supported hardware includes auto-launched instances on AWS, GCP, Azure,
+    and Lambda, as well as servers specified
+    by IP address and SSH credentials (such as on-prem, or another cloud
+    like Paperspace, Coreweave, etc.).
 
-    To use, you should have the ``transformers`` python package installed.
+    To use, you should have the ``runhouse`` python package installed.
 
     Only supports `text-generation`, `text2text-generation` and `summarization` for now.
 
     Example using from_model_id:
         .. code-block:: python
 
-            from langchain_community.llms import HuggingFacePipeline
-            hf = HuggingFacePipeline.from_model_id(
-                model_id="gpt2",
-                task="text-generation",
-                pipeline_kwargs={"max_new_tokens": 10},
+            from langchain_community.llms import SelfHostedHuggingFaceLLM
+            import runhouse as rh
+            gpu = rh.cluster(name="rh-a10x", instance_type="A100:1")
+            hf = SelfHostedHuggingFaceLLM(
+                model_id="google/flan-t5-large", task="text2text-generation",
+                hardware=gpu
             )
-    Example passing pipeline in directly:
+    Example passing fn that generates a pipeline (bc the pipeline is not serializable):
         .. code-block:: python
 
-            from langchain_community.llms import HuggingFacePipeline
+            from langchain_community.llms import SelfHostedHuggingFaceLLM
             from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
+            import runhouse as rh
 
-            model_id = "gpt2"
-            tokenizer = AutoTokenizer.from_pretrained(model_id)
-            model = AutoModelForCausalLM.from_pretrained(model_id)
-            pipe = pipeline(
-                "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=10
-            )
-            hf = HuggingFacePipeline(pipeline=pipe)
+            def get_pipeline():
+                model_id = "gpt2"
+                tokenizer = AutoTokenizer.from_pretrained(model_id)
+                model = AutoModelForCausalLM.from_pretrained(model_id)
+                pipe = pipeline(
+                    "text-generation", model=model, tokenizer=tokenizer
+                )
+                return pipe
+            hf = SelfHostedHuggingFaceLLM(
+                model_load_fn=get_pipeline, model_id="gpt2", hardware=gpu)
     """
 
-    pipeline: Any  #: :meta private:
     model_id: str = DEFAULT_MODEL_ID
-    """Model name to use."""
+    """Hugging Face model_id to load the model."""
+    task: str = DEFAULT_TASK
+    """Hugging Face task ("text-generation", "text2text-generation" or
+    "summarization")."""
+    device: int = 0
+    """Device to use for inference. -1 for CPU, 0 for GPU, 1 for second GPU, etc."""
     model_kwargs: Optional[dict] = None
-    """Keyword arguments passed to the model."""
-    pipeline_kwargs: Optional[dict] = None
-    """Keyword arguments passed to the pipeline."""
-    batch_size: int = DEFAULT_BATCH_SIZE
-    """Batch size to use when passing multiple documents to generate."""
+    """Keyword arguments to pass to the model."""
+    hardware: Any
+    """Remote hardware to send the inference function to."""
+    model_reqs: List[str] = ["./", "transformers", "torch"]
+    """Requirements to install on hardware to inference the model."""
+    model_load_fn: Callable = _load_transformer
+    """Function to load the model remotely on the server."""
+    inference_fn: Callable = _generate_text  #: :meta private:
+    """Inference function to send to the remote hardware."""
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
-    @classmethod
-    def from_model_id(
-        cls,
-        model_id: str,
-        task: str,
-        device: Optional[int] = -1,
-        device_map: Optional[str] = None,
-        model_kwargs: Optional[dict] = None,
-        pipeline_kwargs: Optional[dict] = None,
-        batch_size: int = DEFAULT_BATCH_SIZE,
-        **kwargs: Any,
-    ) -> HuggingFacePipeline:
-        """Construct the pipeline object from model_id and task."""
-        try:
-            from transformers import (
-                AutoModelForCausalLM,
-                AutoModelForSeq2SeqLM,
-                AutoTokenizer,
-            )
-            from transformers import pipeline as hf_pipeline
-
-        except ImportError:
-            raise ValueError(
-                "Could not import transformers python package. "
-                "Please install it with `pip install transformers`."
-            )
-
-        _model_kwargs = model_kwargs or {}
-        tokenizer = AutoTokenizer.from_pretrained(model_id, **_model_kwargs)
-
-        try:
-            if task == "text-generation":
-                model = AutoModelForCausalLM.from_pretrained(model_id, **_model_kwargs)
-            elif task in ("text2text-generation", "summarization"):
-                model = AutoModelForSeq2SeqLM.from_pretrained(model_id, **_model_kwargs)
-            else:
-                raise ValueError(
-                    f"Got invalid task {task}, "
-                    f"currently only {VALID_TASKS} are supported"
-                )
-        except ImportError as e:
-            raise ValueError(
-                f"Could not load the {task} model due to missing dependencies."
-            ) from e
-
-        if tokenizer.pad_token is None:
-            tokenizer.pad_token_id = model.config.eos_token_id
-
-        if (
-            getattr(model, "is_loaded_in_4bit", False)
-            or getattr(model, "is_loaded_in_8bit", False)
-        ) and device is not None:
-            logger.warning(
-                f"Setting the `device` argument to None from {device} to avoid "
-                "the error caused by attempting to move the model that was already "
-                "loaded on the GPU using the Accelerate module to the same or "
-                "another device."
-            )
-            device = None
+    def __init__(self, **kwargs: Any):
+        """Construct the pipeline remotely using an auxiliary function.
 
-        if device is not None and importlib.util.find_spec("torch") is not None:
-            import torch
-
-            cuda_device_count = torch.cuda.device_count()
-            if device < -1 or (device >= cuda_device_count):
-                raise ValueError(
-                    f"Got device=={device}, "
-                    f"device is required to be within [-1, {cuda_device_count})"
-                )
-            if device_map is not None and device < 0:
-                device = None
-            if device is not None and device < 0 and cuda_device_count > 0:
-                logger.warning(
-                    "Device has %d GPUs available. "
-                    "Provide device={deviceId} to `from_model_id` to use available"
-                    "GPUs for execution. deviceId is -1 (default) for CPU and "
-                    "can be a positive integer associated with CUDA device id.",
-                    cuda_device_count,
-                )
-        if "trust_remote_code" in _model_kwargs:
-            _model_kwargs = {
-                k: v for k, v in _model_kwargs.items() if k != "trust_remote_code"
-            }
-        _pipeline_kwargs = pipeline_kwargs or {}
-        pipeline = hf_pipeline(
-            task=task,
-            model=model,
-            tokenizer=tokenizer,
-            device=device,
-            device_map=device_map,
-            batch_size=batch_size,
-            model_kwargs=_model_kwargs,
-            **_pipeline_kwargs,
-        )
-        if pipeline.task not in VALID_TASKS:
-            raise ValueError(
-                f"Got invalid task {pipeline.task}, "
-                f"currently only {VALID_TASKS} are supported"
-            )
-        return cls(
-            pipeline=pipeline,
-            model_id=model_id,
-            model_kwargs=_model_kwargs,
-            pipeline_kwargs=_pipeline_kwargs,
-            batch_size=batch_size,
-            **kwargs,
-        )
+        The load function needs to be importable to be imported
+        and run on the server, i.e. in a module and not a REPL or closure.
+        Then, initialize the remote inference function.
+        """
+        load_fn_kwargs = {
+            "model_id": kwargs.get("model_id", DEFAULT_MODEL_ID),
+            "task": kwargs.get("task", DEFAULT_TASK),
+            "device": kwargs.get("device", 0),
+            "model_kwargs": kwargs.get("model_kwargs", None),
+        }
+        super().__init__(load_fn_kwargs=load_fn_kwargs, **kwargs)
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {
-            "model_id": self.model_id,
-            "model_kwargs": self.model_kwargs,
-            "pipeline_kwargs": self.pipeline_kwargs,
+            **{"model_id": self.model_id},
+            **{"model_kwargs": self.model_kwargs},
         }
 
     @property
     def _llm_type(self) -> str:
-        return "huggingface_pipeline"
+        return "selfhosted_huggingface_pipeline"
 
-    def _generate(
+    def _call(
         self,
-        prompts: List[str],
+        prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> LLMResult:
-        # List to hold all results
-        text_generations: List[str] = []
-
-        for i in range(0, len(prompts), self.batch_size):
-            batch_prompts = prompts[i : i + self.batch_size]
-
-            # Process batch of prompts
-            responses = self.pipeline(batch_prompts)
-
-            # Process each response in the batch
-            for j, response in enumerate(responses):
-                if isinstance(response, list):
-                    # if model returns multiple generations, pick the top one
-                    response = response[0]
-
-                if self.pipeline.task == "text-generation":
-                    try:
-                        from transformers.pipelines.text_generation import ReturnType
-
-                        remove_prompt = (
-                            self.pipeline._postprocess_params.get("return_type")
-                            != ReturnType.NEW_TEXT
-                        )
-                    except Exception as e:
-                        logger.warning(
-                            f"Unable to extract pipeline return_type. "
-                            f"Received error:\n\n{e}"
-                        )
-                        remove_prompt = True
-                    if remove_prompt:
-                        text = response["generated_text"][len(batch_prompts[j]) :]
-                    else:
-                        text = response["generated_text"]
-                elif self.pipeline.task == "text2text-generation":
-                    text = response["generated_text"]
-                elif self.pipeline.task == "summarization":
-                    text = response["summary_text"]
-                else:
-                    raise ValueError(
-                        f"Got invalid task {self.pipeline.task}, "
-                        f"currently only {VALID_TASKS} are supported"
-                    )
-                if stop:
-                    # Enforce stop tokens
-                    text = enforce_stop_tokens(text, stop)
-
-                # Append the processed text to results
-                text_generations.append(text)
-
-        return LLMResult(
-            generations=[[Generation(text=text)] for text in text_generations]
+    ) -> str:
+        return self.client(
+            pipeline=self.pipeline_ref, prompt=prompt, stop=stop, **kwargs
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/huggingface_text_gen_inference.py` & `gigachain_community-0.2.0/langchain_community/llms/huggingface_text_gen_inference.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,28 @@
 import logging
 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
 from langchain_core.pydantic_v1 import Extra, Field, root_validator
 from langchain_core.utils import get_pydantic_field_names
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated("0.0.21", removal="0.3.0", alternative="HuggingFaceEndpoint")
 class HuggingFaceTextGenInference(LLM):
     """
     HuggingFace text generation API.
+    ! This class is deprecated, you should use HuggingFaceEndpoint instead !
 
     To use, you should have the `text-generation` python package installed and
     a text-generation server running.
 
     Example:
         .. code-block:: python
 
@@ -29,15 +32,15 @@
                 max_new_tokens=512,
                 top_k=10,
                 top_p=0.95,
                 typical_p=0.95,
                 temperature=0.01,
                 repetition_penalty=1.03,
             )
-            print(llm("What is Deep Learning?"))
+            print(llm.invoke("What is Deep Learning?"))  # noqa: T201
 
             # Streaming response example
             from langchain_community.callbacks import streaming_stdout
 
             callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]
             llm = HuggingFaceTextGenInference(
                 inference_server_url="http://localhost:8010/",
@@ -46,15 +49,15 @@
                 top_p=0.95,
                 typical_p=0.95,
                 temperature=0.01,
                 repetition_penalty=1.03,
                 callbacks=callbacks,
                 streaming=True
             )
-            print(llm("What is Deep Learning?"))
+            print(llm.invoke("What is Deep Learning?"))  # noqa: T201
 
     """
 
     max_new_tokens: int = 512
     """Maximum number of generated tokens"""
     top_k: Optional[int] = None
     """The number of highest probability vocabulary tokens to keep for
@@ -252,17 +255,18 @@
                 text = res.token.text[: res.token.text.index(stop_seq_found)]
             else:
                 text = res.token.text
 
             # yield text, if any
             if text:
                 chunk = GenerationChunk(text=text)
-                yield chunk
+
                 if run_manager:
                     run_manager.on_llm_new_token(chunk.text)
+                yield chunk
 
             # break if stop sequence found
             if stop_seq_found:
                 break
 
     async def _astream(
         self,
@@ -288,14 +292,15 @@
                 text = res.token.text[: res.token.text.index(stop_seq_found)]
             else:
                 text = res.token.text
 
             # yield text, if any
             if text:
                 chunk = GenerationChunk(text=text)
-                yield chunk
+
                 if run_manager:
                     await run_manager.on_llm_new_token(chunk.text)
+                yield chunk
 
             # break if stop sequence found
             if stop_seq_found:
                 break
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/human.py` & `gigachain_community-0.2.0/langchain_community/llms/human.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from langchain_core.pydantic_v1 import Field
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 
 def _display_prompt(prompt: str) -> None:
     """Displays the given prompt to the user."""
-    print(f"\n{prompt}")
+    print(f"\n{prompt}")  # noqa: T201
 
 
 def _collect_user_input(
     separator: Optional[str] = None, stop: Optional[List[str]] = None
 ) -> str:
     """Collects and returns user input as a single string."""
     separator = separator or "\n"
@@ -29,17 +29,15 @@
             break
     # Combine all lines into a single string
     multi_line_input = separator.join(lines)
     return multi_line_input
 
 
 class HumanInputLLM(LLM):
-    """
-    It returns user input as the response.
-    """
+    """User input as the response."""
 
     input_func: Callable = Field(default_factory=lambda: _collect_user_input)
     prompt_func: Callable[[str], None] = Field(default_factory=lambda: _display_prompt)
     separator: str = "\n"
     input_kwargs: Mapping[str, Any] = {}
     prompt_kwargs: Mapping[str, Any] = {}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/javelin_ai_gateway.py` & `gigachain_community-0.2.0/langchain_community/llms/javelin_ai_gateway.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/koboldai.py` & `gigachain_community-0.2.0/langchain_community/llms/koboldai.py`

 * *Files 0% similar despite different names*

```diff
@@ -143,15 +143,15 @@
 
         Example:
             .. code-block:: python
 
                 from langchain_community.llms import KoboldApiLLM
 
                 llm = KoboldApiLLM(endpoint="http://localhost:5000")
-                llm("Write a story about dragons.")
+                llm.invoke("Write a story about dragons.")
         """
         data: Dict[str, Any] = {
             "prompt": prompt,
             "use_story": self.use_story,
             "use_authors_note": self.use_authors_note,
             "use_world_info": self.use_world_info,
             "use_memory": self.use_memory,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/llamacpp.py` & `gigachain_community-0.2.0/langchain_community/llms/llamacpp.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,20 @@
 from __future__ import annotations
 
 import logging
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union
+from typing import Any, Dict, Iterator, List, Optional, Union
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
 from langchain_core.pydantic_v1 import Field, root_validator
 from langchain_core.utils import get_pydantic_field_names
 from langchain_core.utils.utils import build_extra_kwargs
 
-if TYPE_CHECKING:
-    from llama_cpp import LlamaGrammar
-
 logger = logging.getLogger(__name__)
 
 
 class LlamaCpp(LLM):
     """llama.cpp model.
 
     To use, you should have the llama-cpp-python library installed, and provide the
@@ -122,15 +119,15 @@
     grammar_path: Optional[Union[str, Path]] = None
     """
     grammar_path: Path to the .gbnf file that defines formal grammars
     for constraining model outputs. For instance, the grammar can be used
     to force the model to generate valid JSON or to speak exclusively in emojis. At most
     one of grammar_path and grammar should be passed in.
     """
-    grammar: Optional[Union[str, LlamaGrammar]] = None
+    grammar: Optional[Union[str, Any]] = None
     """
     grammar: formal grammar for constraining model outputs. For instance, the grammar 
     can be used to force the model to generate valid JSON or to speak exclusively in 
     emojis. At most one of grammar_path and grammar should be passed in.
     """
 
     verbose: bool = True
@@ -277,15 +274,15 @@
             The generated text.
 
         Example:
             .. code-block:: python
 
                 from langchain_community.llms import LlamaCpp
                 llm = LlamaCpp(model_path="/path/to/local/llama/model.bin")
-                llm("This is a prompt.")
+                llm.invoke("This is a prompt.")
         """
         if self.streaming:
             # If streaming is enabled, we use the stream
             # method that yields as they are generated
             # and return the combined strings from the first choices's text:
             combined_text_output = ""
             for chunk in self._stream(
@@ -332,27 +329,27 @@
                 llm = LlamaCpp(
                     model_path="/path/to/local/model.bin",
                     temperature = 0.5
                 )
                 for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
                         stop=["'","\n"]):
                     result = chunk["choices"][0]
-                    print(result["text"], end='', flush=True)
+                    print(result["text"], end='', flush=True)  # noqa: T201
 
         """
         params = {**self._get_parameters(stop), **kwargs}
         result = self.client(prompt=prompt, stream=True, **params)
         for part in result:
             logprobs = part["choices"][0].get("logprobs", None)
             chunk = GenerationChunk(
                 text=part["choices"][0]["text"],
                 generation_info={"logprobs": logprobs},
             )
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(
                     token=chunk.text, verbose=self.verbose, log_probs=logprobs
                 )
+            yield chunk
 
     def get_num_tokens(self, text: str) -> int:
         tokenized_text = self.client.tokenize(text.encode("utf-8"))
         return len(tokenized_text)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/loading.py` & `gigachain_community-0.2.0/langchain_community/llms/loading.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,44 +1,54 @@
 """Base interface for loading large language model APIs."""
+
 import json
 from pathlib import Path
-from typing import Union
+from typing import Any, Union
 
 import yaml
 from langchain_core.language_models.llms import BaseLLM
 
 from langchain_community.llms import get_type_to_cls_dict
 
+_ALLOW_DANGEROUS_DESERIALIZATION_ARG = "allow_dangerous_deserialization"
+
 
-def load_llm_from_config(config: dict) -> BaseLLM:
+def load_llm_from_config(config: dict, **kwargs: Any) -> BaseLLM:
     """Load LLM from Config Dict."""
     if "_type" not in config:
         raise ValueError("Must specify an LLM Type in config")
     config_type = config.pop("_type")
 
     type_to_cls_dict = get_type_to_cls_dict()
 
     if config_type not in type_to_cls_dict:
         raise ValueError(f"Loading {config_type} LLM not supported")
 
     llm_cls = type_to_cls_dict[config_type]()
-    return llm_cls(**config)
+
+    load_kwargs = {}
+    if _ALLOW_DANGEROUS_DESERIALIZATION_ARG in llm_cls.__fields__:
+        load_kwargs[_ALLOW_DANGEROUS_DESERIALIZATION_ARG] = kwargs.get(
+            _ALLOW_DANGEROUS_DESERIALIZATION_ARG, False
+        )
+
+    return llm_cls(**config, **load_kwargs)
 
 
-def load_llm(file: Union[str, Path]) -> BaseLLM:
-    """Load LLM from file."""
+def load_llm(file: Union[str, Path], **kwargs: Any) -> BaseLLM:
+    """Load LLM from a file."""
     # Convert file to Path object.
     if isinstance(file, str):
         file_path = Path(file)
     else:
         file_path = file
     # Load from either json or yaml.
     if file_path.suffix == ".json":
-        with open(file_path) as f:
+        with open(file_path, encoding="utf-8") as f:
             config = json.load(f)
-    elif file_path.suffix == ".yaml":
-        with open(file_path, "r") as f:
+    elif file_path.suffix.endswith((".yaml", ".yml")):
+        with open(file_path, "r", encoding="utf-8") as f:
             config = yaml.safe_load(f)
     else:
         raise ValueError("File type must be json or yaml")
     # Load the LLM from the config now.
-    return load_llm_from_config(config)
+    return load_llm_from_config(config, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/manifest.py` & `gigachain_community-0.2.0/langchain_community/llms/manifest.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/minimax.py` & `gigachain_community-0.2.0/langchain_community/llms/minimax.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around Minimax APIs."""
+
 from __future__ import annotations
 
 import logging
 from typing import (
     Any,
     Dict,
     List,
@@ -19,15 +20,15 @@
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
 class _MinimaxEndpointClient(BaseModel):
-    """An API client that talks to a Minimax llm endpoint."""
+    """API client for the Minimax LLM endpoint."""
 
     host: str
     group_id: str
     api_key: SecretStr
     api_url: str
 
     @root_validator(pre=True, allow_reuse=True)
@@ -83,15 +84,15 @@
         # Get custom api url from environment.
         values["minimax_api_host"] = get_from_dict_or_env(
             values,
             "minimax_api_host",
             "MINIMAX_API_HOST",
             default="https://api.minimax.chat",
         )
-        values["_client"] = _MinimaxEndpointClient(
+        values["_client"] = _MinimaxEndpointClient(  # type: ignore[call-arg]
             host=values["minimax_api_host"],
             api_key=values["minimax_api_key"],
             group_id=values["minimax_group_id"],
         )
         return values
 
     @property
@@ -113,15 +114,16 @@
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
         return "minimax"
 
 
 class Minimax(MinimaxCommon, LLM):
-    """Wrapper around Minimax large language models.
+    """Minimax large language models.
+
     To use, you should have the environment variable
     ``MINIMAX_API_KEY`` and ``MINIMAX_GROUP_ID`` set with your API key,
     or pass them as a named parameter to the constructor.
     Example:
      . code-block:: python
          from langchain_community.llms.minimax import Minimax
          minimax = Minimax(model="<model_name>", minimax_api_key="my-api-key",
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/mlflow.py` & `gigachain_community-0.2.0/langchain_community/llms/mlflow.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,43 +1,32 @@
 from __future__ import annotations
 
 from typing import Any, Dict, List, Mapping, Optional
 from urllib.parse import urlparse
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models import LLM
-from langchain_core.pydantic_v1 import BaseModel, Extra, Field, PrivateAttr
-
-
-# Ignoring type because below is valid pydantic code
-# Unexpected keyword argument "extra" for "__init_subclass__" of "object"
-class Params(BaseModel, extra=Extra.allow):  # type: ignore[call-arg]
-    """Parameters for MLflow"""
-
-    temperature: float = 0.0
-    n: int = 1
-    stop: Optional[List[str]] = None
-    max_tokens: Optional[int] = None
+from langchain_core.pydantic_v1 import Field, PrivateAttr
 
 
 class Mlflow(LLM):
-    """Wrapper around completions LLMs in MLflow.
+    """MLflow LLM service.
 
     To use, you should have the `mlflow[genai]` python package installed.
-    For more information, see https://mlflow.org/docs/latest/llms/deployments/server.html.
+    For more information, see https://mlflow.org/docs/latest/llms/deployments.
 
     Example:
         .. code-block:: python
 
             from langchain_community.llms import Mlflow
 
             completions = Mlflow(
                 target_uri="http://localhost:5000",
                 endpoint="test",
-                params={"temperature": 0.1}
+                temperature=0.1,
             )
     """
 
     endpoint: str
     """The endpoint to use."""
     target_uri: str
     """The target URI to use."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/mlflow_ai_gateway.py` & `gigachain_community-0.2.0/langchain_community/llms/mlflow_ai_gateway.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,16 +17,15 @@
     candidate_count: int = 1
     """The number of candidates to return."""
     stop: Optional[List[str]] = None
     max_tokens: Optional[int] = None
 
 
 class MlflowAIGateway(LLM):
-    """
-    Wrapper around completions LLMs in the MLflow AI Gateway.
+    """MLflow AI Gateway LLMs.
 
     To use, you should have the ``mlflow[gateway]`` python package installed.
     For more information, see https://mlflow.org/docs/latest/gateway/index.html.
 
     Example:
         .. code-block:: python
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/modal.py` & `gigachain_community-0.2.0/langchain_community/llms/modal.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/mosaicml.py` & `gigachain_community-0.2.0/langchain_community/llms/mosaicml.py`

 * *Files 0% similar despite different names*

```diff
@@ -111,15 +111,15 @@
 
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
-                response = mosaic_llm("Tell me a joke.")
+                response = mosaic_llm.invoke("Tell me a joke.")
         """
         _model_kwargs = self.model_kwargs or {}
 
         prompt = self._transform_prompt(prompt)
 
         payload = {"inputs": [prompt]}
         payload.update(_model_kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/nlpcloud.py` & `gigachain_community-0.2.0/langchain_community/llms/nlpcloud.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,15 +34,15 @@
     """Whether min_length and max_length should include the length of the input."""
     remove_input: bool = True
     """Remove input text from API response"""
     remove_end_sequence: bool = True
     """Whether or not to remove the end sequence token."""
     bad_words: List[str] = []
     """List of tokens not allowed to be generated."""
-    top_p: int = 1
+    top_p: float = 1.0
     """Total probability mass of tokens to consider at each step."""
     top_k: int = 50
     """The number of highest probability tokens to keep for top-k filtering."""
     repetition_penalty: float = 1.0
     """Penalizes repeated tokens. 1.0 means no penalty."""
     num_beams: int = 1
     """Number of beams for beam search."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/oci_data_science_model_deployment_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/oci_data_science_model_deployment_endpoint.py`

 * *Files 0% similar despite different names*

```diff
@@ -224,15 +224,15 @@
     Make sure to have the required policies to access the OCI Data
     Science Model Deployment endpoint. See:
     https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
 
     Example:
         .. code-block:: python
 
-            from langchain.llms import ModelDeploymentTGI
+            from langchain_community.llms import ModelDeploymentTGI
 
             oci_md = ModelDeploymentTGI(endpoint="https://<MD_OCID>/predict")
 
     """
 
     do_sample: bool = True
     """If set to True, this parameter enables decoding strategies such as
@@ -290,15 +290,15 @@
     Make sure to have the required policies to access the OCI Data
     Science Model Deployment endpoint. See:
     https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
 
     Example:
         .. code-block:: python
 
-            from langchain.llms import OCIModelDeploymentVLLM
+            from langchain_community.llms import OCIModelDeploymentVLLM
 
             oci_md = OCIModelDeploymentVLLM(
                 endpoint="https://<MD_OCID>/predict",
                 model="mymodel"
             )
 
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/octoai_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/stochasticai.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,151 +1,137 @@
+import logging
+import time
 from typing import Any, Dict, List, Mapping, Optional
 
+import requests
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
+logger = logging.getLogger(__name__)
 
-class OctoAIEndpoint(LLM):
-    """OctoAI LLM Endpoints.
 
-    OctoAIEndpoint is a class to interact with OctoAI
-     Compute Service large language model endpoints.
+class StochasticAI(LLM):
+    """StochasticAI large language models.
 
-    To use, you should have the ``octoai`` python package installed, and the
-    environment variable ``OCTOAI_API_TOKEN`` set with your API token, or pass
-    it as a named parameter to the constructor.
+    To use, you should have the environment variable ``STOCHASTICAI_API_KEY``
+    set with your API key.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.llms.octoai_endpoint  import OctoAIEndpoint
-            OctoAIEndpoint(
-                octoai_api_token="octoai-api-key",
-                endpoint_url="https://mpt-7b-demo-f1kzsig6xes9.octoai.run/generate",
-                model_kwargs={
-                    "max_new_tokens": 200,
-                    "temperature": 0.75,
-                    "top_p": 0.95,
-                    "repetition_penalty": 1,
-                    "seed": None,
-                    "stop": [],
-                },
-            )
-
-            from langchain_community.llms.octoai_endpoint  import OctoAIEndpoint
-            OctoAIEndpoint(
-                octoai_api_token="octoai-api-key",
-                endpoint_url="https://llama-2-7b-chat-demo-kk0powt97tmb.octoai.run/v1/chat/completions",
-                model_kwargs={
-                    "model": "llama-2-7b-chat",
-                    "messages": [
-                        {
-                            "role": "system",
-                            "content": "Below is an instruction that describes a task.
-                                Write a response that completes the request."
-                        }
-                    ],
-                    "stream": False,
-                    "max_tokens": 256
-                }
-            )
-
+            from langchain_community.llms import StochasticAI
+            stochasticai = StochasticAI(api_url="")
     """
 
-    endpoint_url: Optional[str] = None
-    """Endpoint URL to use."""
+    api_url: str = ""
+    """Model name to use."""
 
-    model_kwargs: Optional[dict] = None
-    """Keyword arguments to pass to the model."""
+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """Holds any model parameters valid for `create` call not
+    explicitly specified."""
 
-    octoai_api_token: Optional[str] = None
-    """OCTOAI API Token"""
-
-    streaming: bool = False
-    """Whether to generate a stream of tokens asynchronously"""
+    stochasticai_api_key: Optional[SecretStr] = None
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
-    @root_validator(allow_reuse=True)
+    @root_validator(pre=True)
+    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        """Build extra kwargs from additional params that were passed in."""
+        all_required_field_names = {field.alias for field in cls.__fields__.values()}
+
+        extra = values.get("model_kwargs", {})
+        for field_name in list(values):
+            if field_name not in all_required_field_names:
+                if field_name in extra:
+                    raise ValueError(f"Found {field_name} supplied twice.")
+                logger.warning(
+                    f"""{field_name} was transferred to model_kwargs.
+                    Please confirm that {field_name} is what you intended."""
+                )
+                extra[field_name] = values.pop(field_name)
+        values["model_kwargs"] = extra
+        return values
+
+    @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that api key and python package exists in environment."""
-        octoai_api_token = get_from_dict_or_env(
-            values, "octoai_api_token", "OCTOAI_API_TOKEN"
-        )
-        values["endpoint_url"] = get_from_dict_or_env(
-            values, "endpoint_url", "ENDPOINT_URL"
+        """Validate that api key exists in environment."""
+        stochasticai_api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "stochasticai_api_key", "STOCHASTICAI_API_KEY")
         )
-
-        values["octoai_api_token"] = octoai_api_token
+        values["stochasticai_api_key"] = stochasticai_api_key
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
-        _model_kwargs = self.model_kwargs or {}
         return {
-            **{"endpoint_url": self.endpoint_url},
-            **{"model_kwargs": _model_kwargs},
+            **{"endpoint_url": self.api_url},
+            **{"model_kwargs": self.model_kwargs},
         }
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
-        return "octoai_endpoint"
+        return "stochasticai"
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call out to OctoAI's inference endpoint.
+        """Call out to StochasticAI's complete endpoint.
 
         Args:
             prompt: The prompt to pass into the model.
             stop: Optional list of stop words to use when generating.
 
         Returns:
             The string generated by the model.
 
-        """
-        _model_kwargs = self.model_kwargs or {}
-
-        try:
-            # Initialize the OctoAI client
-            from octoai import client
-
-            octoai_client = client.Client(token=self.octoai_api_token)
-
-            if "model" in _model_kwargs:
-                parameter_payload = _model_kwargs
-                parameter_payload["messages"].append(
-                    {"role": "user", "content": prompt}
-                )
-                # Send the request using the OctoAI client
-                output = octoai_client.infer(self.endpoint_url, parameter_payload)
-                text = output.get("choices")[0].get("message").get("content")
-            else:
-                # Prepare the payload JSON
-                parameter_payload = {"inputs": prompt, "parameters": _model_kwargs}
-
-                # Send the request using the OctoAI client
-                resp_json = octoai_client.infer(self.endpoint_url, parameter_payload)
-                text = resp_json["generated_text"]
-
-        except Exception as e:
-            # Handle any errors raised by the inference endpoint
-            raise ValueError(f"Error raised by the inference endpoint: {e}") from e
+        Example:
+            .. code-block:: python
 
+                response = StochasticAI("Tell me a joke.")
+        """
+        params = self.model_kwargs or {}
+        params = {**params, **kwargs}
+        response_post = requests.post(
+            url=self.api_url,
+            json={"prompt": prompt, "params": params},
+            headers={
+                "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",  # type: ignore[union-attr]
+                "Accept": "application/json",
+                "Content-Type": "application/json",
+            },
+        )
+        response_post.raise_for_status()
+        response_post_json = response_post.json()
+        completed = False
+        while not completed:
+            response_get = requests.get(
+                url=response_post_json["data"]["responseUrl"],
+                headers={
+                    "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",  # type: ignore[union-attr]
+                    "Accept": "application/json",
+                    "Content-Type": "application/json",
+                },
+            )
+            response_get.raise_for_status()
+            response_get_json = response_get.json()["data"]
+            text = response_get_json.get("completion")
+            completed = text is not None
+            time.sleep(0.5)
+        text = text[0]
         if stop is not None:
-            # Apply stop tokens when making calls to OctoAI
+            # I believe this is required since the stop tokens
+            # are not enforced by the model parameters
             text = enforce_stop_tokens(text, stop)
-
         return text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/ollama.py` & `gigachain_community-0.2.0/langchain_community/llms/ollama.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import json
-from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional
+from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional, Union
 
 import aiohttp
 import requests
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
@@ -60,14 +60,18 @@
 
     num_thread: Optional[int] = None
     """Sets the number of threads to use during computation.
     By default, Ollama will detect this for optimal performance.
     It is recommended to set this value to the number of physical
     CPU cores your system has (as opposed to the logical number of cores)."""
 
+    num_predict: Optional[int] = None
+    """Maximum number of tokens to predict when generating text.
+    (Default: 128, -1 = infinite generation, -2 = fill context)"""
+
     repeat_last_n: Optional[int] = None
     """Sets how far back for the model to look back to prevent
     repetition. (Default: 64, 0 = disabled, -1 = num_ctx)"""
 
     repeat_penalty: Optional[float] = None
     """Sets how strongly to penalize repetitions. A higher value (e.g., 1.5)
     will penalize repetitions more strongly, while a lower value (e.g., 0.9)
@@ -86,15 +90,15 @@
     impact more, while a value of 1.0 disables this setting. (default: 1)"""
 
     top_k: Optional[int] = None
     """Reduces the probability of generating nonsense. A higher value (e.g. 100)
     will give more diverse answers, while a lower value (e.g. 10)
     will be more conservative. (Default: 40)"""
 
-    top_p: Optional[int] = None
+    top_p: Optional[float] = None
     """Works together with top-k. A higher value (e.g., 0.95) will lead
     to more diverse text, while a lower value (e.g., 0.5) will
     generate more focused and conservative text. (Default: 0.9)"""
 
     system: Optional[str] = None
     """system prompt (overrides what is defined in the Modelfile)"""
 
@@ -103,37 +107,57 @@
 
     format: Optional[str] = None
     """Specify the format of the output (e.g., json)"""
 
     timeout: Optional[int] = None
     """Timeout for the request stream"""
 
+    keep_alive: Optional[Union[int, str]] = None
+    """How long the model will stay loaded into memory.
+
+    The parameter (Default: 5 minutes) can be set to:
+    1. a duration string in Golang (such as "10m" or "24h");
+    2. a number in seconds (such as 3600);
+    3. any negative number which will keep the model loaded \
+        in memory (e.g. -1 or "-1m");
+    4. 0 which will unload the model immediately after generating a response;
+
+    See the [Ollama documents](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately)"""
+
+    headers: Optional[dict] = None
+    """Additional headers to pass to endpoint (e.g. Authorization, Referer).
+    This is useful when Ollama is hosted on cloud services that require
+    tokens for authentication.
+    """
+
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Ollama."""
         return {
             "model": self.model,
             "format": self.format,
             "options": {
                 "mirostat": self.mirostat,
                 "mirostat_eta": self.mirostat_eta,
                 "mirostat_tau": self.mirostat_tau,
                 "num_ctx": self.num_ctx,
                 "num_gpu": self.num_gpu,
                 "num_thread": self.num_thread,
+                "num_predict": self.num_predict,
                 "repeat_last_n": self.repeat_last_n,
                 "repeat_penalty": self.repeat_penalty,
                 "temperature": self.temperature,
                 "stop": self.stop,
                 "tfs_z": self.tfs_z,
                 "top_k": self.top_k,
                 "top_p": self.top_p,
             },
             "system": self.system,
             "template": self.template,
+            "keep_alive": self.keep_alive,
         }
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {**{"model": self.model, "format": self.format}, **self._default_params}
 
@@ -144,30 +168,30 @@
         images: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         payload = {"prompt": prompt, "images": images}
         yield from self._create_stream(
             payload=payload,
             stop=stop,
-            api_url=f"{self.base_url}/api/generate/",
+            api_url=f"{self.base_url}/api/generate",
             **kwargs,
         )
 
     async def _acreate_generate_stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         images: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> AsyncIterator[str]:
         payload = {"prompt": prompt, "images": images}
         async for item in self._acreate_stream(
             payload=payload,
             stop=stop,
-            api_url=f"{self.base_url}/api/generate/",
+            api_url=f"{self.base_url}/api/generate",
             **kwargs,
         ):
             yield item
 
     def _create_stream(
         self,
         api_url: str,
@@ -175,57 +199,59 @@
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         if self.stop is not None and stop is not None:
             raise ValueError("`stop` found in both the input and default params.")
         elif self.stop is not None:
             stop = self.stop
-        elif stop is None:
-            stop = []
 
         params = self._default_params
 
-        if "model" in kwargs:
-            params["model"] = kwargs["model"]
+        for key in self._default_params:
+            if key in kwargs:
+                params[key] = kwargs[key]
 
         if "options" in kwargs:
             params["options"] = kwargs["options"]
         else:
             params["options"] = {
                 **params["options"],
                 "stop": stop,
-                **kwargs,
+                **{k: v for k, v in kwargs.items() if k not in self._default_params},
             }
 
         if payload.get("messages"):
             request_payload = {"messages": payload.get("messages", []), **params}
         else:
             request_payload = {
                 "prompt": payload.get("prompt"),
                 "images": payload.get("images", []),
                 **params,
             }
 
         response = requests.post(
             url=api_url,
-            headers={"Content-Type": "application/json"},
+            headers={
+                "Content-Type": "application/json",
+                **(self.headers if isinstance(self.headers, dict) else {}),
+            },
             json=request_payload,
             stream=True,
             timeout=self.timeout,
         )
         response.encoding = "utf-8"
         if response.status_code != 200:
             if response.status_code == 404:
                 raise OllamaEndpointNotFoundError(
                     "Ollama call failed with status code 404. "
                     "Maybe your model is not found "
                     f"and you should pull the model with `ollama pull {self.model}`."
                 )
             else:
-                optional_detail = response.json().get("error")
+                optional_detail = response.text
                 raise ValueError(
                     f"Ollama call failed with status code {response.status_code}."
                     f" Details: {optional_detail}"
                 )
         return response.iter_lines(decode_unicode=True)
 
     async def _acreate_stream(
@@ -235,54 +261,56 @@
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> AsyncIterator[str]:
         if self.stop is not None and stop is not None:
             raise ValueError("`stop` found in both the input and default params.")
         elif self.stop is not None:
             stop = self.stop
-        elif stop is None:
-            stop = []
 
         params = self._default_params
 
-        if "model" in kwargs:
-            params["model"] = kwargs["model"]
+        for key in self._default_params:
+            if key in kwargs:
+                params[key] = kwargs[key]
 
         if "options" in kwargs:
             params["options"] = kwargs["options"]
         else:
             params["options"] = {
                 **params["options"],
                 "stop": stop,
-                **kwargs,
+                **{k: v for k, v in kwargs.items() if k not in self._default_params},
             }
 
         if payload.get("messages"):
             request_payload = {"messages": payload.get("messages", []), **params}
         else:
             request_payload = {
                 "prompt": payload.get("prompt"),
                 "images": payload.get("images", []),
                 **params,
             }
 
         async with aiohttp.ClientSession() as session:
             async with session.post(
                 url=api_url,
-                headers={"Content-Type": "application/json"},
+                headers={
+                    "Content-Type": "application/json",
+                    **(self.headers if isinstance(self.headers, dict) else {}),
+                },
                 json=request_payload,
                 timeout=self.timeout,
             ) as response:
                 if response.status != 200:
                     if response.status == 404:
                         raise OllamaEndpointNotFoundError(
                             "Ollama call failed with status code 404."
                         )
                     else:
-                        optional_detail = await response.json().get("error")
+                        optional_detail = response.text
                         raise ValueError(
                             f"Ollama call failed with status code {response.status}."
                             f" Details: {optional_detail}"
                         )
                 async for line in response.content:
                     yield line.decode("utf-8")
 
@@ -357,15 +385,15 @@
         extra = Extra.forbid
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
         return "ollama-llm"
 
-    def _generate(
+    def _generate(  # type: ignore[override]
         self,
         prompts: List[str],
         stop: Optional[List[str]] = None,
         images: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> LLMResult:
@@ -391,17 +419,17 @@
                 stop=stop,
                 images=images,
                 run_manager=run_manager,
                 verbose=self.verbose,
                 **kwargs,
             )
             generations.append([final_chunk])
-        return LLMResult(generations=generations)
+        return LLMResult(generations=generations)  # type: ignore[arg-type]
 
-    async def _agenerate(
+    async def _agenerate(  # type: ignore[override]
         self,
         prompts: List[str],
         stop: Optional[List[str]] = None,
         images: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> LLMResult:
@@ -422,47 +450,47 @@
         # TODO: add caching here.
         generations = []
         for prompt in prompts:
             final_chunk = await super()._astream_with_aggregation(
                 prompt,
                 stop=stop,
                 images=images,
-                run_manager=run_manager,
+                run_manager=run_manager,  # type: ignore[arg-type]
                 verbose=self.verbose,
                 **kwargs,
             )
             generations.append([final_chunk])
-        return LLMResult(generations=generations)
+        return LLMResult(generations=generations)  # type: ignore[arg-type]
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
-        for stream_resp in self._create_stream(prompt, stop, **kwargs):
+        for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
             if stream_resp:
                 chunk = _stream_response_to_generation_chunk(stream_resp)
-                yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
                         chunk.text,
                         verbose=self.verbose,
                     )
+                yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[GenerationChunk]:
-        async for stream_resp in self._acreate_stream(prompt, stop, **kwargs):
+        async for stream_resp in self._acreate_generate_stream(prompt, stop, **kwargs):
             if stream_resp:
                 chunk = _stream_response_to_generation_chunk(stream_resp)
-                yield chunk
                 if run_manager:
                     await run_manager.on_llm_new_token(
                         chunk.text,
                         verbose=self.verbose,
                     )
+                yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/opaqueprompts.py` & `gigachain_community-0.2.0/langchain_community/llms/opaqueprompts.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import logging
 from typing import Any, Dict, List, Optional
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.language_models.llms import LLM
+from langchain_core.messages import AIMessage
 from langchain_core.pydantic_v1 import Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 logger = logging.getLogger(__name__)
 
 
 class OpaquePrompts(LLM):
-    """An LLM wrapper that uses OpaquePrompts to sanitize prompts.
+    """LLM that uses OpaquePrompts to sanitize prompts.
 
     Wraps another LLM and sanitizes prompts before passing it to the LLM, then
         de-sanitizes the response.
 
     To use, you should have the ``opaqueprompts`` python package installed,
     and the environment variable ``OPAQUEPROMPTS_API_KEY`` set with
     your API key, or pass it as a named parameter to the constructor.
@@ -79,30 +80,31 @@
 
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
-                response = op_llm("Tell me a joke.")
+                response = op_llm.invoke("Tell me a joke.")
         """
         import opaqueprompts as op
 
         _run_manager = run_manager or CallbackManagerForLLMRun.get_noop_manager()
 
         # sanitize the prompt by replacing the sensitive information with a placeholder
         sanitize_response: op.SanitizeResponse = op.sanitize([prompt])
         sanitized_prompt_value_str = sanitize_response.sanitized_texts[0]
 
         # TODO: Add in callbacks once child runs for LLMs are supported by LangSmith.
         # call the LLM with the sanitized prompt and get the response
-        llm_response = self.base_llm.predict(
+        llm_response = self.base_llm.bind(stop=stop).invoke(
             sanitized_prompt_value_str,
-            stop=stop,
         )
+        if isinstance(llm_response, AIMessage):
+            llm_response = llm_response.content
 
         # desanitize the response by restoring the original sensitive information
         desanitize_response: op.DesanitizeResponse = op.desanitize(
             llm_response,
             secure_context=sanitize_response.secure_context,
         )
         return desanitize_response.desanitized_text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/openai.py` & `gigachain_community-0.2.0/langchain_community/llms/openai.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,15 @@
     Mapping,
     Optional,
     Set,
     Tuple,
     Union,
 )
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import BaseLLM, create_base_retry_decorator
 from langchain_core.outputs import Generation, GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import Field, root_validator
@@ -360,24 +361,24 @@
         self.get_sub_prompts(params, [prompt], stop)  # this mutates params
         for stream_resp in completion_with_retry(
             self, prompt=prompt, run_manager=run_manager, **params
         ):
             if not isinstance(stream_resp, dict):
                 stream_resp = stream_resp.dict()
             chunk = _stream_response_to_generation_chunk(stream_resp)
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(
                     chunk.text,
                     chunk=chunk,
                     verbose=self.verbose,
                     logprobs=chunk.generation_info["logprobs"]
                     if chunk.generation_info
                     else None,
                 )
+            yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -386,24 +387,24 @@
         self.get_sub_prompts(params, [prompt], stop)  # this mutates params
         async for stream_resp in await acompletion_with_retry(
             self, prompt=prompt, run_manager=run_manager, **params
         ):
             if not isinstance(stream_resp, dict):
                 stream_resp = stream_resp.dict()
             chunk = _stream_response_to_generation_chunk(stream_resp)
-            yield chunk
             if run_manager:
                 await run_manager.on_llm_new_token(
                     chunk.text,
                     chunk=chunk,
                     verbose=self.verbose,
                     logprobs=chunk.generation_info["logprobs"]
                     if chunk.generation_info
                     else None,
                 )
+            yield chunk
 
     def _generate(
         self,
         prompts: List[str],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -720,14 +721,17 @@
 
                 max_tokens = openai.max_token_for_prompt("Tell me a joke.")
         """
         num_tokens = self.get_num_tokens(prompt)
         return self.max_context_size - num_tokens
 
 
+@deprecated(
+    since="0.0.10", removal="0.3.0", alternative_import="langchain_openai.OpenAI"
+)
 class OpenAI(BaseOpenAI):
     """OpenAI large language models.
 
     To use, you should have the ``openai`` python package installed, and the
     environment variable ``OPENAI_API_KEY`` set with your API key.
 
     Any parameters that are valid to be passed to the openai.create call can be passed
@@ -746,27 +750,31 @@
         return ["langchain", "llms", "openai"]
 
     @property
     def _invocation_params(self) -> Dict[str, Any]:
         return {**{"model": self.model_name}, **super()._invocation_params}
 
 
+@deprecated(
+    since="0.0.10", removal="0.3.0", alternative_import="langchain_openai.AzureOpenAI"
+)
 class AzureOpenAI(BaseOpenAI):
     """Azure-specific OpenAI large language models.
 
     To use, you should have the ``openai`` python package installed, and the
     environment variable ``OPENAI_API_KEY`` set with your API key.
 
     Any parameters that are valid to be passed to the openai.create call can be passed
     in, even if not explicitly saved on this class.
 
     Example:
         .. code-block:: python
 
             from langchain_community.llms import AzureOpenAI
+
             openai = AzureOpenAI(model_name="gpt-3.5-turbo-instruct")
     """
 
     azure_endpoint: Union[str, None] = None
     """Your Azure endpoint, including the resource.
 
         Automatically inferred from env var `AZURE_OPENAI_ENDPOINT` if not provided.
@@ -949,14 +957,19 @@
     def lc_attributes(self) -> Dict[str, Any]:
         return {
             "openai_api_type": self.openai_api_type,
             "openai_api_version": self.openai_api_version,
         }
 
 
+@deprecated(
+    since="0.0.1",
+    removal="0.3.0",
+    alternative_import="langchain_openai.ChatOpenAI",
+)
 class OpenAIChat(BaseLLM):
     """OpenAI Chat large language models.
 
     To use, you should have the ``openai`` python package installed, and the
     environment variable ``OPENAI_API_KEY`` set with your API key.
 
     Any parameters that are valid to be passed to the openai.create call can be passed
@@ -1096,17 +1109,17 @@
         for stream_resp in completion_with_retry(
             self, messages=messages, run_manager=run_manager, **params
         ):
             if not isinstance(stream_resp, dict):
                 stream_resp = stream_resp.dict()
             token = stream_resp["choices"][0]["delta"].get("content", "")
             chunk = GenerationChunk(text=token)
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(token, chunk=chunk)
+            yield chunk
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
@@ -1116,17 +1129,17 @@
         async for stream_resp in await acompletion_with_retry(
             self, messages=messages, run_manager=run_manager, **params
         ):
             if not isinstance(stream_resp, dict):
                 stream_resp = stream_resp.dict()
             token = stream_resp["choices"][0]["delta"].get("content", "")
             chunk = GenerationChunk(text=token)
-            yield chunk
             if run_manager:
                 await run_manager.on_llm_new_token(token, chunk=chunk)
+            yield chunk
 
     def _generate(
         self,
         prompts: List[str],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/openllm.py` & `gigachain_community-0.2.0/langchain_community/llms/openllm.py`

 * *Files 7% similar despite different names*

```diff
@@ -59,33 +59,35 @@
         .. code-block:: python
 
             from langchain_community.llms import OpenLLM
             llm = OpenLLM(
                 model_name='flan-t5',
                 model_id='google/flan-t5-large',
             )
-            llm("What is the difference between a duck and a goose?")
+            llm.invoke("What is the difference between a duck and a goose?")
 
     For all available supported models, you can run 'openllm models'.
 
     If you have a OpenLLM server running, you can also use it remotely:
         .. code-block:: python
 
             from langchain_community.llms import OpenLLM
             llm = OpenLLM(server_url='http://localhost:3000')
-            llm("What is the difference between a duck and a goose?")
+            llm.invoke("What is the difference between a duck and a goose?")
     """
 
     model_name: Optional[str] = None
     """Model name to use. See 'openllm models' for all available models."""
     model_id: Optional[str] = None
     """Model Id to use. If not provided, will use the default model for the model name.
     See 'openllm models' for all available model variants."""
     server_url: Optional[str] = None
     """Optional server URL that currently runs a LLMServer with 'openllm start'."""
+    timeout: int = 30
+    """"Time out for the openllm client"""
     server_type: ServerType = "http"
     """Optional server type. Either 'http' or 'grpc'."""
     embedded: bool = True
     """Initialize this LLM instance in current process by default. Should
     only set to False when using in conjunction with BentoML Service."""
     llm_kwargs: Dict[str, Any]
     """Keyword arguments to be passed to openllm.LLM"""
@@ -121,14 +123,15 @@
 
     def __init__(
         self,
         model_name: Optional[str] = None,
         *,
         model_id: Optional[str] = None,
         server_url: Optional[str] = None,
+        timeout: int = 30,
         server_type: Literal["grpc", "http"] = "http",
         embedded: bool = True,
         **llm_kwargs: Any,
     ):
         try:
             import openllm
         except ImportError as e:
@@ -145,19 +148,20 @@
                 model_id is None and model_name is None
             ), "'server_url' and {'model_id', 'model_name'} are mutually exclusive"
             client_cls = (
                 openllm.client.HTTPClient
                 if server_type == "http"
                 else openllm.client.GrpcClient
             )
-            client = client_cls(server_url)
+            client = client_cls(server_url, timeout)
 
             super().__init__(
-                **{
+                **{  # type: ignore[arg-type]
                     "server_url": server_url,
+                    "timeout": timeout,
                     "server_type": server_type,
                     "llm_kwargs": llm_kwargs,
                 }
             )
             self._runner = None  # type: ignore
             self._client = client
         else:
@@ -172,15 +176,15 @@
                 model_name=model_name,
                 model_id=model_id,
                 init_local=embedded,
                 ensure_available=True,
                 **llm_kwargs,
             )
             super().__init__(
-                **{
+                **{  # type: ignore[arg-type]
                     "model_name": model_name,
                     "model_id": model_id,
                     "embedded": embedded,
                     "llm_kwargs": llm_kwargs,
                 }
             )
             self._client = None  # type: ignore
@@ -213,17 +217,17 @@
             raise ValueError("OpenLLM must be initialized locally with 'model_name'")
         return self._runner
 
     @property
     def _identifying_params(self) -> IdentifyingParams:
         """Get the identifying parameters."""
         if self._client is not None:
-            self.llm_kwargs.update(self._client._config())
-            model_name = self._client._metadata()["model_name"]
-            model_id = self._client._metadata()["model_id"]
+            self.llm_kwargs.update(self._client._config)
+            model_name = self._client._metadata.model_dump()["model_name"]
+            model_id = self._client._metadata.model_dump()["model_id"]
         else:
             if self._runner is None:
                 raise ValueError("Runner must be initialized.")
             model_name = self.model_name
             model_id = self.model_id
             try:
                 self.llm_kwargs.update(
@@ -261,17 +265,19 @@
 
         copied = copy.deepcopy(self.llm_kwargs)
         copied.update(kwargs)
         config = openllm.AutoConfig.for_model(
             self._identifying_params["model_name"], **copied
         )
         if self._client:
-            res = self._client.generate(
-                prompt, **config.model_dump(flatten=True)
-            ).responses[0]
+            res = (
+                self._client.generate(prompt, **config.model_dump(flatten=True))
+                .outputs[0]
+                .text
+            )
         else:
             assert self._runner is not None
             res = self._runner(prompt, **config.model_dump(flatten=True))
         if isinstance(res, dict) and "text" in res:
             return res["text"]
         elif isinstance(res, str):
             return res
@@ -298,18 +304,20 @@
 
         copied = copy.deepcopy(self.llm_kwargs)
         copied.update(kwargs)
         config = openllm.AutoConfig.for_model(
             self._identifying_params["model_name"], **copied
         )
         if self._client:
-            async_client = openllm.client.AsyncHTTPClient(self.server_url)
+            async_client = openllm.client.AsyncHTTPClient(self.server_url, self.timeout)
             res = (
-                await async_client.generate(prompt, **config.model_dump(flatten=True))
-            ).responses[0]
+                (await async_client.generate(prompt, **config.model_dump(flatten=True)))
+                .outputs[0]
+                .text
+            )
         else:
             assert self._runner is not None
             (
                 prompt,
                 generate_kwargs,
                 postprocess_kwargs,
             ) = self._runner.llm.sanitize_parameters(prompt, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/openlm.py` & `gigachain_community-0.2.0/langchain_community/llms/openlm.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/pai_eas_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/pai_eas_endpoint.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -227,14 +227,14 @@
                         text = output[: output.index(stop_seq_found)]
                     else:
                         text = output
 
                     # yield text, if any
                     if text:
                         res = GenerationChunk(text=text)
-                        yield res
                         if run_manager:
                             run_manager.on_llm_new_token(res.text)
+                        yield res
 
                     # break if stop sequence found
                     if stop_seq_found:
                         break
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/petals.py` & `gigachain_community-0.2.0/langchain_community/llms/vllm.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,153 +1,176 @@
-import logging
-from typing import Any, Dict, List, Mapping, Optional
+from typing import Any, Dict, List, Optional
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
-from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.language_models.llms import BaseLLM
+from langchain_core.outputs import Generation, LLMResult
+from langchain_core.pydantic_v1 import Field, root_validator
 
-from langchain_community.llms.utils import enforce_stop_tokens
+from langchain_community.llms.openai import BaseOpenAI
+from langchain_community.utils.openai import is_openai_v1
 
-logger = logging.getLogger(__name__)
 
+class VLLM(BaseLLM):
+    """VLLM language model."""
 
-class Petals(LLM):
-    """Petals Bloom models.
+    model: str = ""
+    """The name or path of a HuggingFace Transformers model."""
 
-    To use, you should have the ``petals`` python package installed, and the
-    environment variable ``HUGGINGFACE_API_KEY`` set with your API key.
+    tensor_parallel_size: Optional[int] = 1
+    """The number of GPUs to use for distributed execution with tensor parallelism."""
 
-    Any parameters that are valid to be passed to the call can be passed
-    in, even if not explicitly saved on this class.
+    trust_remote_code: Optional[bool] = False
+    """Trust remote code (e.g., from HuggingFace) when downloading the model 
+    and tokenizer."""
 
-    Example:
-        .. code-block:: python
+    n: int = 1
+    """Number of output sequences to return for the given prompt."""
 
-            from langchain_community.llms import petals
-            petals = Petals()
+    best_of: Optional[int] = None
+    """Number of output sequences that are generated from the prompt."""
 
-    """
+    presence_penalty: float = 0.0
+    """Float that penalizes new tokens based on whether they appear in the 
+    generated text so far"""
 
-    client: Any
-    """The client to use for the API calls."""
+    frequency_penalty: float = 0.0
+    """Float that penalizes new tokens based on their frequency in the 
+    generated text so far"""
 
-    tokenizer: Any
-    """The tokenizer to use for the API calls."""
+    temperature: float = 1.0
+    """Float that controls the randomness of the sampling."""
 
-    model_name: str = "bigscience/bloom-petals"
-    """The model to use."""
+    top_p: float = 1.0
+    """Float that controls the cumulative probability of the top tokens to consider."""
 
-    temperature: float = 0.7
-    """What sampling temperature to use"""
+    top_k: int = -1
+    """Integer that controls the number of top tokens to consider."""
 
-    max_new_tokens: int = 256
-    """The maximum number of new tokens to generate in the completion."""
+    use_beam_search: bool = False
+    """Whether to use beam search instead of sampling."""
 
-    top_p: float = 0.9
-    """The cumulative probability for top-p sampling."""
+    stop: Optional[List[str]] = None
+    """List of strings that stop the generation when they are generated."""
 
-    top_k: Optional[int] = None
-    """The number of highest probability vocabulary tokens
-    to keep for top-k-filtering."""
+    ignore_eos: bool = False
+    """Whether to ignore the EOS token and continue generating tokens after 
+    the EOS token is generated."""
 
-    do_sample: bool = True
-    """Whether or not to use sampling; use greedy decoding otherwise."""
+    max_new_tokens: int = 512
+    """Maximum number of tokens to generate per output sequence."""
 
-    max_length: Optional[int] = None
-    """The maximum length of the sequence to be generated."""
+    logprobs: Optional[int] = None
+    """Number of log probabilities to return per output token."""
 
-    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
-    """Holds any model parameters valid for `create` call
-    not explicitly specified."""
+    dtype: str = "auto"
+    """The data type for the model weights and activations."""
 
-    huggingface_api_key: Optional[SecretStr] = None
+    download_dir: Optional[str] = None
+    """Directory to download and load the weights. (Default to the default 
+    cache dir of huggingface)"""
 
-    class Config:
-        """Configuration for this pydantic config."""
+    vllm_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """Holds any model parameters valid for `vllm.LLM` call not explicitly specified."""
 
-        extra = Extra.forbid
-
-    @root_validator(pre=True)
-    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
-        """Build extra kwargs from additional params that were passed in."""
-        all_required_field_names = {field.alias for field in cls.__fields__.values()}
-
-        extra = values.get("model_kwargs", {})
-        for field_name in list(values):
-            if field_name not in all_required_field_names:
-                if field_name in extra:
-                    raise ValueError(f"Found {field_name} supplied twice.")
-                logger.warning(
-                    f"""WARNING! {field_name} is not default parameter.
-                    {field_name} was transferred to model_kwargs.
-                    Please confirm that {field_name} is what you intended."""
-                )
-                extra[field_name] = values.pop(field_name)
-        values["model_kwargs"] = extra
-        return values
+    client: Any  #: :meta private:
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that api key and python package exists in environment."""
-        huggingface_api_key = convert_to_secret_str(
-            get_from_dict_or_env(values, "huggingface_api_key", "HUGGINGFACE_API_KEY")
-        )
-        try:
-            from petals import AutoDistributedModelForCausalLM
-            from transformers import AutoTokenizer
-
-            model_name = values["model_name"]
-            values["tokenizer"] = AutoTokenizer.from_pretrained(model_name)
-            values["client"] = AutoDistributedModelForCausalLM.from_pretrained(
-                model_name
-            )
-            values["huggingface_api_key"] = huggingface_api_key.get_secret_value()
+        """Validate that python package exists in environment."""
 
+        try:
+            from vllm import LLM as VLLModel
         except ImportError:
             raise ImportError(
-                "Could not import transformers or petals python package."
-                "Please install with `pip install -U transformers petals`."
+                "Could not import vllm python package. "
+                "Please install it with `pip install vllm`."
             )
+
+        values["client"] = VLLModel(
+            model=values["model"],
+            tensor_parallel_size=values["tensor_parallel_size"],
+            trust_remote_code=values["trust_remote_code"],
+            dtype=values["dtype"],
+            download_dir=values["download_dir"],
+            **values["vllm_kwargs"],
+        )
+
         return values
 
     @property
     def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for calling Petals API."""
-        normal_params = {
-            "temperature": self.temperature,
-            "max_new_tokens": self.max_new_tokens,
-            "top_p": self.top_p,
+        """Get the default parameters for calling vllm."""
+        return {
+            "n": self.n,
+            "best_of": self.best_of,
+            "max_tokens": self.max_new_tokens,
             "top_k": self.top_k,
-            "do_sample": self.do_sample,
-            "max_length": self.max_length,
+            "top_p": self.top_p,
+            "temperature": self.temperature,
+            "presence_penalty": self.presence_penalty,
+            "frequency_penalty": self.frequency_penalty,
+            "stop": self.stop,
+            "ignore_eos": self.ignore_eos,
+            "use_beam_search": self.use_beam_search,
+            "logprobs": self.logprobs,
         }
-        return {**normal_params, **self.model_kwargs}
 
-    @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
-        return {**{"model_name": self.model_name}, **self._default_params}
+    def _generate(
+        self,
+        prompts: List[str],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> LLMResult:
+        """Run the LLM on the given prompt and input."""
+
+        from vllm import SamplingParams
+
+        # build sampling parameters
+        params = {**self._default_params, **kwargs, "stop": stop}
+        sampling_params = SamplingParams(**params)
+        # call the model
+        outputs = self.client.generate(prompts, sampling_params)
+
+        generations = []
+        for output in outputs:
+            text = output.outputs[0].text
+            generations.append([Generation(text=text)])
+
+        return LLMResult(generations=generations)
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
-        return "petals"
+        return "vllm"
 
-    def _call(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Call the Petals API."""
-        params = self._default_params
-        params = {**params, **kwargs}
-        inputs = self.tokenizer(prompt, return_tensors="pt")["input_ids"]
-        outputs = self.client.generate(inputs, **params)
-        text = self.tokenizer.decode(outputs[0])
-        if stop is not None:
-            # I believe this is required since the stop tokens
-            # are not enforced by the model parameters
-            text = enforce_stop_tokens(text, stop)
-        return text
+
+class VLLMOpenAI(BaseOpenAI):
+    """vLLM OpenAI-compatible API client"""
+
+    @classmethod
+    def is_lc_serializable(cls) -> bool:
+        return False
+
+    @property
+    def _invocation_params(self) -> Dict[str, Any]:
+        """Get the parameters used to invoke the model."""
+
+        params: Dict[str, Any] = {
+            "model": self.model_name,
+            **self._default_params,
+            "logit_bias": None,
+        }
+        if not is_openai_v1():
+            params.update(
+                {
+                    "api_key": self.openai_api_key,
+                    "api_base": self.openai_api_base,
+                }
+            )
+
+        return params
+
+    @property
+    def _llm_type(self) -> str:
+        """Return type of llm."""
+        return "vllm-openai"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/pipelineai.py` & `gigachain_community-0.2.0/langchain_community/llms/pipelineai.py`

 * *Files 2% similar despite different names*

```diff
@@ -98,15 +98,15 @@
         try:
             from pipeline import PipelineCloud
         except ImportError:
             raise ImportError(
                 "Could not import pipeline-ai python package. "
                 "Please install it with `pip install pipeline-ai`."
             )
-        client = PipelineCloud(token=self.pipeline_api_key.get_secret_value())
+        client = PipelineCloud(token=self.pipeline_api_key.get_secret_value())  # type: ignore[union-attr]
         params = self.pipeline_kwargs or {}
         params = {**params, **kwargs}
 
         run = client.run_pipeline(self.pipeline_key, [prompt, params])
         try:
             text = run.result_preview[0][0]
         except AttributeError:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/predictionguard.py` & `gigachain_community-0.2.0/langchain_community/llms/predictionguard.py`

 * *Files 1% similar despite different names*

```diff
@@ -96,15 +96,15 @@
         """Call out to Prediction Guard's model API.
         Args:
             prompt: The prompt to pass into the model.
         Returns:
             The string generated by the model.
         Example:
             .. code-block:: python
-                response = pgllm("Tell me a joke.")
+                response = pgllm.invoke("Tell me a joke.")
         """
         import predictionguard as pg
 
         params = self._default_params
         if self.stop is not None and stop is not None:
             raise ValueError("`stop` found in both the input and default params.")
         elif self.stop is not None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/promptlayer_openai.py` & `gigachain_community-0.2.0/langchain_community/llms/promptlayer_openai.py`

 * *Files 1% similar despite different names*

```diff
@@ -120,15 +120,15 @@
                 ):
                     generation.generation_info = {}
                 generation.generation_info["pl_request_id"] = pl_request_id
         return generated_responses
 
 
 class PromptLayerOpenAIChat(OpenAIChat):
-    """Wrapper around OpenAI large language models.
+    """PromptLayer OpenAI large language models.
 
     To use, you should have the ``openai`` and ``promptlayer`` python
     package installed, and the environment variable ``OPENAI_API_KEY``
     and ``PROMPTLAYER_API_KEY`` set with your openAI API key and
     promptlayer key respectively.
 
     All parameters that can be passed to the OpenAIChat LLM can also
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/replicate.py` & `gigachain_community-0.2.0/langchain_community/llms/replicate.py`

 * *Files 10% similar despite different names*

```diff
@@ -40,15 +40,15 @@
     """
 
     model: str
     model_kwargs: Dict[str, Any] = Field(default_factory=dict, alias="input")
     replicate_api_token: Optional[str] = None
     prompt_key: Optional[str] = None
     version_obj: Any = Field(default=None, exclude=True)
-    """Optionally pass in the model version object during initialization to avoid 
+    """Optionally pass in the model version object during initialization to avoid
         having to make an extra API call to retrieve it during streaming. NOTE: not
         serializable, is excluded from serialization.
     """
 
     streaming: bool = False
     """Whether to stream the results."""
 
@@ -173,37 +173,41 @@
                     # Potentially some tokens that should still be yielded before ending
                     # stream.
                     stop_index = max(output.find(s), 0)
                     output = output[:stop_index]
                     if not output:
                         break
             if output:
-                yield GenerationChunk(text=output)
                 if run_manager:
                     run_manager.on_llm_new_token(
                         output,
                         verbose=self.verbose,
                     )
+                yield GenerationChunk(text=output)
             if stop_condition_reached:
                 break
 
     def _create_prediction(self, prompt: str, **kwargs: Any) -> Prediction:
         try:
             import replicate as replicate_python
         except ImportError:
             raise ImportError(
                 "Could not import replicate python package. "
                 "Please install it with `pip install replicate`."
             )
 
         # get the model and version
         if self.version_obj is None:
-            model_str, version_str = self.model.split(":")
-            model = replicate_python.models.get(model_str)
-            self.version_obj = model.versions.get(version_str)
+            if ":" in self.model:
+                model_str, version_str = self.model.split(":")
+                model = replicate_python.models.get(model_str)
+                self.version_obj = model.versions.get(version_str)
+            else:
+                model = replicate_python.models.get(self.model)
+                self.version_obj = model.latest_version
 
         if self.prompt_key is None:
             # sort through the openapi schema to get the name of the first input
             input_properties = sorted(
                 self.version_obj.openapi_schema["components"]["schemas"]["Input"][
                     "properties"
                 ].items(),
@@ -213,10 +217,15 @@
             self.prompt_key = input_properties[0][0]
 
         input_: Dict = {
             self.prompt_key: prompt,
             **self.model_kwargs,
             **kwargs,
         }
-        return replicate_python.predictions.create(
-            version=self.version_obj, input=input_
-        )
+
+        # if it's an official model
+        if ":" not in self.model:
+            return replicate_python.models.predictions.create(self.model, input=input_)
+        else:
+            return replicate_python.predictions.create(
+                version=self.version_obj, input=input_
+            )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/rwkv.py` & `gigachain_community-0.2.0/langchain_community/llms/rwkv.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """RWKV models.
 
 Based on https://github.com/saharNooby/rwkv.cpp/blob/master/rwkv/chat_with_bot.py
          https://github.com/BlinkDL/ChatRWKV/blob/main/v2/chat.py
 """
+
 from typing import Any, Dict, List, Mapping, Optional, Set
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 
 from langchain_community.llms.utils import enforce_stop_tokens
@@ -21,15 +22,15 @@
     Example:
         .. code-block:: python
 
             from langchain_community.llms import RWKV
             model = RWKV(model="./models/rwkv-3b-fp16.bin", strategy="cpu fp32")
 
             # Simplest invocation
-            response = model("Once upon a time, ")
+            response = model.invoke("Once upon a time, ")
     """
 
     model: str
     """Path to the pre-trained RWKV model file."""
 
     tokens_path: str
     """Path to the RWKV tokens file."""
@@ -221,14 +222,14 @@
         Returns:
             The string generated by the model.
 
         Example:
             .. code-block:: python
 
                 prompt = "Once upon a time, "
-                response = model(prompt, n_predict=55)
+                response = model.invoke(prompt, n_predict=55)
         """
         text = self.rwkv_generate(prompt)
 
         if stop is not None:
             text = enforce_stop_tokens(text, stop)
         return text
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/sagemaker_endpoint.py` & `gigachain_community-0.2.0/langchain_community/llms/sagemaker_endpoint.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Sagemaker InvokeEndpoint API."""
+
 import io
 import json
 from abc import abstractmethod
 from typing import Any, Dict, Generic, Iterator, List, Mapping, Optional, TypeVar, Union
 
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
@@ -11,16 +12,15 @@
 from langchain_community.llms.utils import enforce_stop_tokens
 
 INPUT_TYPE = TypeVar("INPUT_TYPE", bound=Union[str, List[str]])
 OUTPUT_TYPE = TypeVar("OUTPUT_TYPE", bound=Union[str, List[List[float]], Iterator])
 
 
 class LineIterator:
-    """
-    A helper class for parsing the byte stream input.
+    """Parse the byte stream input.
 
     The output of the model will be in the following format:
 
     b'{"outputs": [" a"]}\n'
     b'{"outputs": [" challenging"]}\n'
     b'{"outputs": [" problem"]}\n'
     ...
@@ -70,15 +70,15 @@
                 # Unknown Event Type
                 continue
             self.buffer.seek(0, io.SEEK_END)
             self.buffer.write(chunk["PayloadPart"]["Bytes"])
 
 
 class ContentHandlerBase(Generic[INPUT_TYPE, OUTPUT_TYPE]):
-    """A handler class to transform input from LLM to a
+    """Handler class to transform input from LLM to a
     format that SageMaker endpoint expects.
 
     Similarly, the class handles transforming output from the
     SageMaker endpoint to a format that LLM class expects.
     """
 
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/self_hosted.py` & `gigachain_community-0.2.0/langchain_community/llms/self_hosted.py`

 * *Files 4% similar despite different names*

```diff
@@ -133,26 +133,41 @@
     model_load_fn: Callable
     """Function to load the model remotely on the server."""
     load_fn_kwargs: Optional[dict] = None
     """Keyword arguments to pass to the model load function."""
     model_reqs: List[str] = ["./", "torch"]
     """Requirements to install on hardware to inference the model."""
 
+    allow_dangerous_deserialization: bool = False
+    """Allow deserialization using pickle which can be dangerous if 
+    loading compromised data.
+    """
+
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     def __init__(self, **kwargs: Any):
         """Init the pipeline with an auxiliary function.
 
         The load function must be in global scope to be imported
         and run on the server, i.e. in a module and not a REPL or closure.
         Then, initialize the remote inference function.
         """
+        if not kwargs.get("allow_dangerous_deserialization"):
+            raise ValueError(
+                "SelfHostedPipeline relies on the pickle module. "
+                "You will need to set allow_dangerous_deserialization=True "
+                "if you want to opt-in to allow deserialization of data using pickle."
+                "Data can be compromised by a malicious actor if "
+                "not handled properly to include "
+                "a malicious payload that when deserialized with "
+                "pickle can execute arbitrary code. "
+            )
         super().__init__(**kwargs)
         try:
             import runhouse as rh
 
         except ImportError:
             raise ImportError(
                 "Could not import runhouse python package. "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/symblai_nebula.py` & `gigachain_community-0.2.0/langchain_community/llms/symblai_nebula.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/textgen.py` & `gigachain_community-0.2.0/langchain_community/llms/friendli.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,417 +1,350 @@
-import json
-import logging
+from __future__ import annotations
+
+import os
 from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
 
-import requests
-from langchain_core.callbacks import (
+from langchain_core.callbacks.manager import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import LLM
-from langchain_core.outputs import GenerationChunk
-from langchain_core.pydantic_v1 import Field
-
-logger = logging.getLogger(__name__)
-
-
-class TextGen(LLM):
-    """Text generation models from WebUI.
-
-    To use, you should have the text-generation-webui installed, a model loaded,
-    and --api added as a command-line option.
-
-    Suggested installation, use one-click installer for your OS:
-    https://github.com/oobabooga/text-generation-webui#one-click-installers
-
-    Parameters below taken from text-generation-webui api example:
-    https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py
+from langchain_core.load.serializable import Serializable
+from langchain_core.outputs import GenerationChunk, LLMResult
+from langchain_core.pydantic_v1 import Field, SecretStr, root_validator
+from langchain_core.utils.env import get_from_dict_or_env
+from langchain_core.utils.utils import convert_to_secret_str
+
+
+def _stream_response_to_generation_chunk(stream_response: Any) -> GenerationChunk:
+    """Convert a stream response to a generation chunk."""
+    if stream_response.event == "token_sampled":
+        return GenerationChunk(
+            text=stream_response.text,
+            generation_info={"token": str(stream_response.token)},
+        )
+    return GenerationChunk(text="")
+
+
+class BaseFriendli(Serializable):
+    """Base class of Friendli."""
+
+    # Friendli client.
+    client: Any = Field(default=None, exclude=True)
+    # Friendli Async client.
+    async_client: Any = Field(default=None, exclude=True)
+    # Model name to use.
+    model: str = "mixtral-8x7b-instruct-v0-1"
+    # Friendli personal access token to run as.
+    friendli_token: Optional[SecretStr] = None
+    # Friendli team ID to run as.
+    friendli_team: Optional[str] = None
+    # Whether to enable streaming mode.
+    streaming: bool = False
+    # Number between -2.0 and 2.0. Positive values penalizes tokens that have been
+    # sampled, taking into account their frequency in the preceding text. This
+    # penalization diminishes the model's tendency to reproduce identical lines
+    # verbatim.
+    frequency_penalty: Optional[float] = None
+    # Number between -2.0 and 2.0. Positive values penalizes tokens that have been
+    # sampled at least once in the existing text.
+    presence_penalty: Optional[float] = None
+    # The maximum number of tokens to generate. The length of your input tokens plus
+    # `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI
+    # GPT-3)
+    max_tokens: Optional[int] = None
+    # When one of the stop phrases appears in the generation result, the API will stop
+    # generation. The phrase is included in the generated result. If you are using
+    # beam search, all of the active beams should contain the stop phrase to terminate
+    # generation. Before checking whether a stop phrase is included in the result, the
+    # phrase is converted into tokens.
+    stop: Optional[List[str]] = None
+    # Sampling temperature. Smaller temperature makes the generation result closer to
+    # greedy, argmax (i.e., `top_k = 1`) sampling. If it is `None`, then 1.0 is used.
+    temperature: Optional[float] = None
+    # Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers
+    # between 0.0 (exclusive) and 1.0 (inclusive) are allowed. If it is `None`, then 1.0
+    # is used by default.
+    top_p: Optional[float] = None
+
+    @root_validator()
+    def validate_environment(cls, values: Dict) -> Dict:
+        """Validate if personal access token is provided in environment."""
+        try:
+            import friendli
+        except ImportError as e:
+            raise ImportError(
+                "Could not import friendli-client python package. "
+                "Please install it with `pip install friendli-client`."
+            ) from e
+
+        friendli_token = convert_to_secret_str(
+            get_from_dict_or_env(values, "friendli_token", "FRIENDLI_TOKEN")
+        )
+        values["friendli_token"] = friendli_token
+        friendli_token_str = friendli_token.get_secret_value()
+        friendli_team = values["friendli_team"] or os.getenv("FRIENDLI_TEAM")
+        values["friendli_team"] = friendli_team
+        values["client"] = values["client"] or friendli.Friendli(
+            token=friendli_token_str, team_id=friendli_team
+        )
+        values["async_client"] = values["async_client"] or friendli.AsyncFriendli(
+            token=friendli_token_str, team_id=friendli_team
+        )
+        return values
+
+
+class Friendli(LLM, BaseFriendli):
+    """Friendli LLM.
+
+    ``friendli-client`` package should be installed with `pip install friendli-client`.
+    You must set ``FRIENDLI_TOKEN`` environment variable or provide the value of your
+    personal access token for the ``friendli_token`` argument.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.llms import TextGen
-            llm = TextGen(model_url="http://localhost:8500")
-    """
-
-    model_url: str
-    """The full URL to the textgen webui including http[s]://host:port """
-
-    preset: Optional[str] = None
-    """The preset to use in the textgen webui """
-
-    max_new_tokens: Optional[int] = 250
-    """The maximum number of tokens to generate."""
-
-    do_sample: bool = Field(True, alias="do_sample")
-    """Do sample"""
-
-    temperature: Optional[float] = 1.3
-    """Primary factor to control randomness of outputs. 0 = deterministic
-    (only the most likely token is used). Higher value = more randomness."""
-
-    top_p: Optional[float] = 0.1
-    """If not set to 1, select tokens with probabilities adding up to less than this
-    number. Higher value = higher range of possible random results."""
-
-    typical_p: Optional[float] = 1
-    """If not set to 1, select only tokens that are at least this much more likely to
-    appear than random tokens, given the prior text."""
-
-    epsilon_cutoff: Optional[float] = 0  # In units of 1e-4
-    """Epsilon cutoff"""
-
-    eta_cutoff: Optional[float] = 0  # In units of 1e-4
-    """ETA cutoff"""
-
-    repetition_penalty: Optional[float] = 1.18
-    """Exponential penalty factor for repeating prior tokens. 1 means no penalty,
-    higher value = less repetition, lower value = more repetition."""
-
-    top_k: Optional[float] = 40
-    """Similar to top_p, but select instead only the top_k most likely tokens.
-    Higher value = higher range of possible random results."""
-
-    min_length: Optional[int] = 0
-    """Minimum generation length in tokens."""
-
-    no_repeat_ngram_size: Optional[int] = 0
-    """If not set to 0, specifies the length of token sets that are completely blocked
-    from repeating at all. Higher values = blocks larger phrases,
-    lower values = blocks words or letters from repeating.
-    Only 0 or high values are a good idea in most cases."""
-
-    num_beams: Optional[int] = 1
-    """Number of beams"""
-
-    penalty_alpha: Optional[float] = 0
-    """Penalty Alpha"""
-
-    length_penalty: Optional[float] = 1
-    """Length Penalty"""
+            from langchain_community.llms import Friendli
 
-    early_stopping: bool = Field(False, alias="early_stopping")
-    """Early stopping"""
-
-    seed: int = Field(-1, alias="seed")
-    """Seed (-1 for random)"""
-
-    add_bos_token: bool = Field(True, alias="add_bos_token")
-    """Add the bos_token to the beginning of prompts.
-    Disabling this can make the replies more creative."""
-
-    truncation_length: Optional[int] = 2048
-    """Truncate the prompt up to this length. The leftmost tokens are removed if
-    the prompt exceeds this length. Most models require this to be at most 2048."""
-
-    ban_eos_token: bool = Field(False, alias="ban_eos_token")
-    """Ban the eos_token. Forces the model to never end the generation prematurely."""
-
-    skip_special_tokens: bool = Field(True, alias="skip_special_tokens")
-    """Skip special tokens. Some specific models need this unset."""
-
-    stopping_strings: Optional[List[str]] = []
-    """A list of strings to stop generation when encountered."""
+            friendli = Friendli(
+                model="mixtral-8x7b-instruct-v0-1", friendli_token="YOUR FRIENDLI TOKEN"
+            )
+    """
 
-    streaming: bool = False
-    """Whether to stream the results, token by token."""
+    @property
+    def lc_secrets(self) -> Dict[str, str]:
+        return {"friendli_token": "FRIENDLI_TOKEN"}
 
     @property
     def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for calling textgen."""
+        """Get the default parameters for calling Friendli completions API."""
         return {
-            "max_new_tokens": self.max_new_tokens,
-            "do_sample": self.do_sample,
+            "frequency_penalty": self.frequency_penalty,
+            "presence_penalty": self.presence_penalty,
+            "max_tokens": self.max_tokens,
+            "stop": self.stop,
             "temperature": self.temperature,
             "top_p": self.top_p,
-            "typical_p": self.typical_p,
-            "epsilon_cutoff": self.epsilon_cutoff,
-            "eta_cutoff": self.eta_cutoff,
-            "repetition_penalty": self.repetition_penalty,
-            "top_k": self.top_k,
-            "min_length": self.min_length,
-            "no_repeat_ngram_size": self.no_repeat_ngram_size,
-            "num_beams": self.num_beams,
-            "penalty_alpha": self.penalty_alpha,
-            "length_penalty": self.length_penalty,
-            "early_stopping": self.early_stopping,
-            "seed": self.seed,
-            "add_bos_token": self.add_bos_token,
-            "truncation_length": self.truncation_length,
-            "ban_eos_token": self.ban_eos_token,
-            "skip_special_tokens": self.skip_special_tokens,
-            "stopping_strings": self.stopping_strings,
         }
 
     @property
     def _identifying_params(self) -> Dict[str, Any]:
         """Get the identifying parameters."""
-        return {**{"model_url": self.model_url}, **self._default_params}
+        return {"model": self.model, **self._default_params}
 
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
-        return "textgen"
-
-    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:
-        """
-        Performs sanity check, preparing parameters in format needed by textgen.
-
-        Args:
-            stop (Optional[List[str]]): List of stop sequences for textgen.
-
-        Returns:
-            Dictionary containing the combined parameters.
-        """
+        return "friendli"
 
-        # Raise error if stop sequences are in both input and default params
-        # if self.stop and stop is not None:
-        if self.stopping_strings and stop is not None:
+    def _get_invocation_params(
+        self, stop: Optional[List[str]] = None, **kwargs: Any
+    ) -> Dict[str, Any]:
+        """Get the parameters used to invoke the model."""
+        params = self._default_params
+        if self.stop is not None and stop is not None:
             raise ValueError("`stop` found in both the input and default params.")
-
-        if self.preset is None:
-            params = self._default_params
+        elif self.stop is not None:
+            params["stop"] = self.stop
         else:
-            params = {"preset": self.preset}
-
-        # then sets it as configured, or default to an empty list:
-        params["stopping_strings"] = self.stopping_strings or stop or []
-
-        return params
+            params["stop"] = stop
+        return {**params, **kwargs}
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call the textgen web API and return the output.
+        """Call out Friendli's completions API.
 
         Args:
-            prompt: The prompt to use for generation.
-            stop: A list of strings to stop generation when encountered.
+            prompt (str): The text prompt to generate completion for.
+            stop (Optional[List[str]], optional): When one of the stop phrases appears
+                in the generation result, the API will stop generation. The stop phrases
+                are excluded from the result. If beam search is enabled, all of the
+                active beams should contain the stop phrase to terminate generation.
+                Before checking whether a stop phrase is included in the result, the
+                phrase is converted into tokens. We recommend using stop_tokens because
+                it is clearer. For example, after tokenization, phrases "clear" and
+                " clear" can result in different token sequences due to the prepended
+                space character. Defaults to None.
 
         Returns:
-            The generated text.
+            str: The generated text output.
 
         Example:
             .. code-block:: python
 
-                from langchain_community.llms import TextGen
-                llm = TextGen(model_url="http://localhost:5000")
-                llm("Write a story about llamas.")
+                response = frienldi("Give me a recipe for the Old Fashioned cocktail.")
         """
-        if self.streaming:
-            combined_text_output = ""
-            for chunk in self._stream(
-                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
-            ):
-                combined_text_output += chunk.text
-            result = combined_text_output
-
-        else:
-            url = f"{self.model_url}/api/v1/generate"
-            params = self._get_parameters(stop)
-            request = params.copy()
-            request["prompt"] = prompt
-            response = requests.post(url, json=request)
-
-            if response.status_code == 200:
-                result = response.json()["results"][0]["text"]
-            else:
-                print(f"ERROR: response: {response}")
-                result = ""
-
-        return result
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        completion = self.client.completions.create(
+            model=self.model, prompt=prompt, stream=False, **params
+        )
+        return completion.choices[0].text
 
     async def _acall(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call the textgen web API and return the output.
+        """Call out Friendli's completions API Asynchronously.
 
         Args:
-            prompt: The prompt to use for generation.
-            stop: A list of strings to stop generation when encountered.
+            prompt (str): The text prompt to generate completion for.
+            stop (Optional[List[str]], optional): When one of the stop phrases appears
+                in the generation result, the API will stop generation. The stop phrases
+                are excluded from the result. If beam search is enabled, all of the
+                active beams should contain the stop phrase to terminate generation.
+                Before checking whether a stop phrase is included in the result, the
+                phrase is converted into tokens. We recommend using stop_tokens because
+                it is clearer. For example, after tokenization, phrases "clear" and
+                " clear" can result in different token sequences due to the prepended
+                space character. Defaults to None.
 
         Returns:
-            The generated text.
+            str: The generated text output.
 
         Example:
             .. code-block:: python
 
-                from langchain_community.llms import TextGen
-                llm = TextGen(model_url="http://localhost:5000")
-                llm("Write a story about llamas.")
+                response = await frienldi("Tell me a joke.")
         """
-        if self.streaming:
-            combined_text_output = ""
-            async for chunk in self._astream(
-                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
-            ):
-                combined_text_output += chunk.text
-            result = combined_text_output
-
-        else:
-            url = f"{self.model_url}/api/v1/generate"
-            params = self._get_parameters(stop)
-            request = params.copy()
-            request["prompt"] = prompt
-            response = requests.post(url, json=request)
-
-            if response.status_code == 200:
-                result = response.json()["results"][0]["text"]
-            else:
-                print(f"ERROR: response: {response}")
-                result = ""
-
-        return result
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        completion = await self.async_client.completions.create(
+            model=self.model, prompt=prompt, stream=False, **params
+        )
+        return completion.choices[0].text
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
-        """Yields results objects as they are generated in real time.
-
-        It also calls the callback manager's on_llm_new_token event with
-        similar parameters to the OpenAI LLM class method of the same name.
-
-        Args:
-            prompt: The prompts to pass into the model.
-            stop: Optional list of stop words to use when generating.
-
-        Returns:
-            A generator representing the stream of tokens being generated.
-
-        Yields:
-            A dictionary like objects containing a string token and metadata.
-            See text-generation-webui docs and below for more.
-
-        Example:
-            .. code-block:: python
-
-                from langchain_community.llms import TextGen
-                llm = TextGen(
-                    model_url = "ws://localhost:5005"
-                    streaming=True
-                )
-                for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
-                        stop=["'","\n"]):
-                    print(chunk, end='', flush=True)
-
-        """
-        try:
-            import websocket
-        except ImportError:
-            raise ImportError(
-                "The `websocket-client` package is required for streaming."
-            )
-
-        params = {**self._get_parameters(stop), **kwargs}
-
-        url = f"{self.model_url}/api/v1/stream"
-
-        request = params.copy()
-        request["prompt"] = prompt
-
-        websocket_client = websocket.WebSocket()
-
-        websocket_client.connect(url)
-
-        websocket_client.send(json.dumps(request))
-
-        while True:
-            result = websocket_client.recv()
-            result = json.loads(result)
-
-            if result["event"] == "text_stream":
-                chunk = GenerationChunk(
-                    text=result["text"],
-                    generation_info=None,
-                )
-                yield chunk
-            elif result["event"] == "stream_end":
-                websocket_client.close()
-                return
-
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        stream = self.client.completions.create(
+            model=self.model, prompt=prompt, stream=True, **params
+        )
+        for line in stream:
+            chunk = _stream_response_to_generation_chunk(line)
+            yield chunk
             if run_manager:
-                run_manager.on_llm_new_token(token=chunk.text)
+                run_manager.on_llm_new_token(line.text, chunk=chunk)
 
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> AsyncIterator[GenerationChunk]:
-        """Yields results objects as they are generated in real time.
+        params = self._get_invocation_params(stop=stop, **kwargs)
+        stream = await self.async_client.completions.create(
+            model=self.model, prompt=prompt, stream=True, **params
+        )
+        async for line in stream:
+            chunk = _stream_response_to_generation_chunk(line)
+            yield chunk
+            if run_manager:
+                await run_manager.on_llm_new_token(line.text, chunk=chunk)
 
-        It also calls the callback manager's on_llm_new_token event with
-        similar parameters to the OpenAI LLM class method of the same name.
+    def _generate(
+        self,
+        prompts: list[str],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> LLMResult:
+        """Call out Friendli's completions API with k unique prompts.
 
         Args:
-            prompt: The prompts to pass into the model.
-            stop: Optional list of stop words to use when generating.
+            prompt (str): The text prompt to generate completion for.
+            stop (Optional[List[str]], optional): When one of the stop phrases appears
+                in the generation result, the API will stop generation. The stop phrases
+                are excluded from the result. If beam search is enabled, all of the
+                active beams should contain the stop phrase to terminate generation.
+                Before checking whether a stop phrase is included in the result, the
+                phrase is converted into tokens. We recommend using stop_tokens because
+                it is clearer. For example, after tokenization, phrases "clear" and
+                " clear" can result in different token sequences due to the prepended
+                space character. Defaults to None.
 
         Returns:
-            A generator representing the stream of tokens being generated.
-
-        Yields:
-            A dictionary like objects containing a string token and metadata.
-            See text-generation-webui docs and below for more.
+            str: The generated text output.
 
         Example:
             .. code-block:: python
 
-                from langchain_community.llms import TextGen
-                llm = TextGen(
-                    model_url = "ws://localhost:5005"
-                    streaming=True
-                )
-                for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
-                        stop=["'","\n"]):
-                    print(chunk, end='', flush=True)
-
+                response = frienldi.generate(["Tell me a joke."])
         """
-        try:
-            import websocket
-        except ImportError:
-            raise ImportError(
-                "The `websocket-client` package is required for streaming."
-            )
-
-        params = {**self._get_parameters(stop), **kwargs}
-
-        url = f"{self.model_url}/api/v1/stream"
+        llm_output = {"model": self.model}
+        if self.streaming:
+            if len(prompts) > 1:
+                raise ValueError("Cannot stream results with multiple prompts.")
 
-        request = params.copy()
-        request["prompt"] = prompt
+            generation: Optional[GenerationChunk] = None
+            for chunk in self._stream(prompts[0], stop, run_manager, **kwargs):
+                if generation is None:
+                    generation = chunk
+                else:
+                    generation += chunk
+            assert generation is not None
+            return LLMResult(generations=[[generation]], llm_output=llm_output)
+
+        llm_result = super()._generate(prompts, stop, run_manager, **kwargs)
+        llm_result.llm_output = llm_output
+        return llm_result
 
-        websocket_client = websocket.WebSocket()
+    async def _agenerate(
+        self,
+        prompts: list[str],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> LLMResult:
+        """Call out Friendli's completions API asynchronously with k unique prompts.
 
-        websocket_client.connect(url)
+        Args:
+            prompt (str): The text prompt to generate completion for.
+            stop (Optional[List[str]], optional): When one of the stop phrases appears
+                in the generation result, the API will stop generation. The stop phrases
+                are excluded from the result. If beam search is enabled, all of the
+                active beams should contain the stop phrase to terminate generation.
+                Before checking whether a stop phrase is included in the result, the
+                phrase is converted into tokens. We recommend using stop_tokens because
+                it is clearer. For example, after tokenization, phrases "clear" and
+                " clear" can result in different token sequences due to the prepended
+                space character. Defaults to None.
 
-        websocket_client.send(json.dumps(request))
+        Returns:
+            str: The generated text output.
 
-        while True:
-            result = websocket_client.recv()
-            result = json.loads(result)
+        Example:
+            .. code-block:: python
 
-            if result["event"] == "text_stream":
-                chunk = GenerationChunk(
-                    text=result["text"],
-                    generation_info=None,
+                response = await frienldi.agenerate(
+                    ["Give me a recipe for the Old Fashioned cocktail."]
                 )
-                yield chunk
-            elif result["event"] == "stream_end":
-                websocket_client.close()
-                return
+        """
+        llm_output = {"model": self.model}
+        if self.streaming:
+            if len(prompts) > 1:
+                raise ValueError("Cannot stream results with multiple prompts.")
 
-            if run_manager:
-                await run_manager.on_llm_new_token(token=chunk.text)
+            generation = None
+            async for chunk in self._astream(prompts[0], stop, run_manager, **kwargs):
+                if generation is None:
+                    generation = chunk
+                else:
+                    generation += chunk
+            assert generation is not None
+            return LLMResult(generations=[[generation]], llm_output=llm_output)
+
+        llm_result = await super()._agenerate(prompts, stop, run_manager, **kwargs)
+        llm_result.llm_output = llm_output
+        return llm_result
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/titan_takeoff.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/edenai_base_tool.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,161 +1,158 @@
-from typing import Any, Iterator, List, Mapping, Optional
+from __future__ import annotations
 
-import requests
-from langchain_core.callbacks import CallbackManagerForLLMRun
-from langchain_core.language_models.llms import LLM
-from langchain_core.outputs import GenerationChunk
-from requests.exceptions import ConnectionError
+import logging
+from abc import abstractmethod
+from typing import Any, Dict, List, Optional
 
-from langchain_community.llms.utils import enforce_stop_tokens
+import requests
+from langchain_core.callbacks import CallbackManagerForToolRun
+from langchain_core.pydantic_v1 import root_validator
+from langchain_core.tools import BaseTool
+from langchain_core.utils import get_from_dict_or_env
 
+logger = logging.getLogger(__name__)
 
-class TitanTakeoff(LLM):
-    """Wrapper around Titan Takeoff APIs."""
 
-    base_url: str = "http://localhost:8000"
-    """Specifies the baseURL to use for the Titan Takeoff API. 
-    Default = http://localhost:8000.
+class EdenaiTool(BaseTool):
+    """
+    the base tool for all the EdenAI Tools .
+    you should have
+    the environment variable ``EDENAI_API_KEY`` set with your API token.
+    You can find your token here: https://app.edenai.run/admin/account/settings
     """
 
-    generate_max_length: int = 128
-    """Maximum generation length. Default = 128."""
+    feature: str
+    subfeature: str
+    edenai_api_key: Optional[str] = None
+    is_async: bool = False
+
+    providers: List[str]
+    """provider to use for the API call."""
+
+    @root_validator(allow_reuse=True)
+    def validate_environment(cls, values: Dict) -> Dict:
+        """Validate that api key exists in environment."""
+        values["edenai_api_key"] = get_from_dict_or_env(
+            values, "edenai_api_key", "EDENAI_API_KEY"
+        )
+        return values
+
+    @staticmethod
+    def get_user_agent() -> str:
+        from langchain_community import __version__
 
-    sampling_topk: int = 1
-    """Sample predictions from the top K most probable candidates. Default = 1."""
+        return f"langchain/{__version__}"
 
-    sampling_topp: float = 1.0
-    """Sample from predictions whose cumulative probability exceeds this value.
-    Default = 1.0.
-    """
+    def _call_eden_ai(self, query_params: Dict[str, Any]) -> str:
+        """
+        Make an API call to the EdenAI service with the specified query parameters.
 
-    sampling_temperature: float = 1.0
-    """Sample with randomness. Bigger temperatures are associated with 
-    more randomness and 'creativity'. Default = 1.0.
-    """
+        Args:
+            query_params (dict): The parameters to include in the API call.
 
-    repetition_penalty: float = 1.0
-    """Penalise the generation of tokens that have been generated before. 
-    Set to > 1 to penalize. Default = 1 (no penalty).
-    """
+        Returns:
+            requests.Response: The response from the EdenAI API call.
 
-    no_repeat_ngram_size: int = 0
-    """Prevent repetitions of ngrams of this size. Default = 0 (turned off)."""
+        """
 
-    streaming: bool = False
-    """Whether to stream the output. Default = False."""
+        # faire l'API call
 
-    @property
-    def _default_params(self) -> Mapping[str, Any]:
-        """Get the default parameters for calling Titan Takeoff Server."""
-        params = {
-            "generate_max_length": self.generate_max_length,
-            "sampling_topk": self.sampling_topk,
-            "sampling_topp": self.sampling_topp,
-            "sampling_temperature": self.sampling_temperature,
-            "repetition_penalty": self.repetition_penalty,
-            "no_repeat_ngram_size": self.no_repeat_ngram_size,
+        headers = {
+            "Authorization": f"Bearer {self.edenai_api_key}",
+            "User-Agent": self.get_user_agent(),
         }
-        return params
 
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "titan_takeoff"
-
-    def _call(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Call out to Titan Takeoff generate endpoint.
+        url = f"https://api.edenai.run/v2/{self.feature}/{self.subfeature}"
 
-        Args:
-            prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
+        payload = {
+            "providers": str(self.providers),
+            "response_as_dict": False,
+            "attributes_as_list": True,
+            "show_original_response": False,
+        }
 
-        Returns:
-            The string generated by the model.
+        payload.update(query_params)
 
-        Example:
-            .. code-block:: python
+        response = requests.post(url, json=payload, headers=headers)
 
-                prompt = "What is the capital of the United Kingdom?"
-                response = model(prompt)
+        self._raise_on_error(response)
 
-        """
         try:
-            if self.streaming:
-                text_output = ""
-                for chunk in self._stream(
-                    prompt=prompt,
-                    stop=stop,
-                    run_manager=run_manager,
-                ):
-                    text_output += chunk.text
-                return text_output
-
-            url = f"{self.base_url}/generate"
-            params = {"text": prompt, **self._default_params}
-
-            response = requests.post(url, json=params)
-            response.raise_for_status()
-            response.encoding = "utf-8"
-            text = ""
-
-            if "message" in response.json():
-                text = response.json()["message"]
-            else:
-                raise ValueError("Something went wrong.")
-            if stop is not None:
-                text = enforce_stop_tokens(text, stop)
-            return text
-        except ConnectionError:
-            raise ConnectionError(
-                "Could not connect to Titan Takeoff server. \
-                Please make sure that the server is running."
+            return self._parse_response(response.json())
+        except Exception as e:
+            raise RuntimeError(f"An error occurred while running tool: {e}")
+
+    def _raise_on_error(self, response: requests.Response) -> None:
+        if response.status_code >= 500:
+            raise Exception(f"EdenAI Server: Error {response.status_code}")
+        elif response.status_code >= 400:
+            raise ValueError(f"EdenAI received an invalid payload: {response.text}")
+        elif response.status_code != 200:
+            raise Exception(
+                f"EdenAI returned an unexpected response with status "
+                f"{response.status_code}: {response.text}"
             )
 
-    def _stream(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> Iterator[GenerationChunk]:
-        """Call out to Titan Takeoff stream endpoint.
+        # case where edenai call succeeded but provider returned an error
+        # (eg: rate limit, server error, etc.)
+        if self.is_async is False:
+            # async call are different and only return a job_id,
+            # not the provider response directly
+            provider_response = response.json()[0]
+            if provider_response.get("status") == "fail":
+                err_msg = provider_response["error"]["message"]
+                raise ValueError(err_msg)
+
+    @abstractmethod
+    def _run(
+        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None
+    ) -> str:
+        pass
 
-        Args:
-            prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
+    @abstractmethod
+    def _parse_response(self, response: Any) -> str:
+        """Take a dict response and condense it's data in a human readable string"""
+        pass
+
+    def _get_edenai(self, url: str) -> requests.Response:
+        headers = {
+            "accept": "application/json",
+            "authorization": f"Bearer {self.edenai_api_key}",
+            "User-Agent": self.get_user_agent(),
+        }
 
-        Returns:
-            The string generated by the model.
+        response = requests.get(url, headers=headers)
 
-        Yields:
-            A dictionary like object containing a string token.
+        self._raise_on_error(response)
 
-        Example:
-            .. code-block:: python
+        return response
 
-                prompt = "What is the capital of the United Kingdom?"
-                response = model(prompt)
+    def _parse_json_multilevel(
+        self, extracted_data: dict, formatted_list: list, level: int = 0
+    ) -> None:
+        for section, subsections in extracted_data.items():
+            indentation = "  " * level
+            if isinstance(subsections, str):
+                subsections = subsections.replace("\n", ",")
+                formatted_list.append(f"{indentation}{section} : {subsections}")
+
+            elif isinstance(subsections, list):
+                formatted_list.append(f"{indentation}{section} : ")
+                self._list_handling(subsections, formatted_list, level + 1)
+
+            elif isinstance(subsections, dict):
+                formatted_list.append(f"{indentation}{section} : ")
+                self._parse_json_multilevel(subsections, formatted_list, level + 1)
+
+    def _list_handling(
+        self, subsection_list: list, formatted_list: list, level: int
+    ) -> None:
+        for list_item in subsection_list:
+            if isinstance(list_item, dict):
+                self._parse_json_multilevel(list_item, formatted_list, level)
 
-        """
-        url = f"{self.base_url}/generate_stream"
-        params = {"text": prompt, **self._default_params}
+            elif isinstance(list_item, list):
+                self._list_handling(list_item, formatted_list, level + 1)
 
-        response = requests.post(url, json=params, stream=True)
-        response.encoding = "utf-8"
-        for text in response.iter_content(chunk_size=1, decode_unicode=True):
-            if text:
-                chunk = GenerationChunk(text=text)
-                yield chunk
-                if run_manager:
-                    run_manager.on_llm_new_token(token=chunk.text)
-
-    @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
-        return {"base_url": self.base_url, **{}, **self._default_params}
+            else:
+                formatted_list.append(f"{'  ' * level}{list_item}")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/titan_takeoff_pro.py` & `gigachain_community-0.2.0/langchain_community/llms/konko.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,217 +1,201 @@
-from typing import Any, Iterator, List, Mapping, Optional
+"""Wrapper around Konko AI's Completion API."""
 
-import requests
-from langchain_core.callbacks import CallbackManagerForLLMRun
+import logging
+import warnings
+from typing import Any, Dict, List, Optional
+
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForLLMRun,
+    CallbackManagerForLLMRun,
+)
 from langchain_core.language_models.llms import LLM
-from langchain_core.outputs import GenerationChunk
-from requests.exceptions import ConnectionError
+from langchain_core.pydantic_v1 import Extra, SecretStr, root_validator
 
-from langchain_community.llms.utils import enforce_stop_tokens
+from langchain_community.utils.openai import is_openai_v1
 
+logger = logging.getLogger(__name__)
 
-class TitanTakeoffPro(LLM):
-    """Titan Takeoff Pro is a language model that can be used to generate text."""
 
-    base_url: Optional[str] = "http://localhost:3000"
-    """Specifies the baseURL to use for the Titan Takeoff Pro API.
-    Default = http://localhost:3000.
-    """
-
-    max_new_tokens: Optional[int] = None
-    """Maximum tokens generated."""
-
-    min_new_tokens: Optional[int] = None
-    """Minimum tokens generated."""
+class Konko(LLM):
+    """Konko AI models.
 
-    sampling_topk: Optional[int] = None
-    """Sample predictions from the top K most probable candidates."""
+    To use, you'll need an API key. This can be passed in as init param
+    ``konko_api_key`` or set as environment variable ``KONKO_API_KEY``.
 
-    sampling_topp: Optional[float] = None
-    """Sample from predictions whose cumulative probability exceeds this value.
+    Konko AI API reference: https://docs.konko.ai/reference/
     """
 
-    sampling_temperature: Optional[float] = None
-    """Sample with randomness. Bigger temperatures are associated with 
-    more randomness and 'creativity'.
+    base_url: str = "https://api.konko.ai/v1/completions"
+    """Base inference API URL."""
+    konko_api_key: SecretStr
+    """Konko AI API key."""
+    model: str
+    """Model name. Available models listed here: 
+       https://docs.konko.ai/reference/get_models
     """
-
+    temperature: Optional[float] = None
+    """Model temperature."""
+    top_p: Optional[float] = None
+    """Used to dynamically adjust the number of choices for each predicted token based 
+        on the cumulative probabilities. A value of 1 will always yield the same 
+        output. A temperature less than 1 favors more correctness and is appropriate 
+        for question answering or summarization. A value greater than 1 introduces more 
+        randomness in the output.
+    """
+    top_k: Optional[int] = None
+    """Used to limit the number of choices for the next predicted word or token. It 
+        specifies the maximum number of tokens to consider at each step, based on their 
+        probability of occurrence. This technique helps to speed up the generation 
+        process and can improve the quality of the generated text by focusing on the 
+        most likely options.
+    """
+    max_tokens: Optional[int] = None
+    """The maximum number of tokens to generate."""
     repetition_penalty: Optional[float] = None
-    """Penalise the generation of tokens that have been generated before. 
-    Set to > 1 to penalize.
+    """A number that controls the diversity of generated text by reducing the 
+        likelihood of repeated sequences. Higher values decrease repetition.
+    """
+    logprobs: Optional[int] = None
+    """An integer that specifies how many top token log probabilities are included in 
+        the response for each token generation step.
     """
 
-    regex_string: Optional[str] = None
-    """A regex string for constrained generation."""
+    class Config:
+        """Configuration for this pydantic object."""
 
-    no_repeat_ngram_size: Optional[int] = None
-    """Prevent repetitions of ngrams of this size. Default = 0 (turned off)."""
+        extra = Extra.forbid
 
-    streaming: bool = False
-    """Whether to stream the output. Default = False."""
+    @root_validator(pre=True)
+    def validate_environment(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        """Validate that python package exists in environment."""
+        try:
+            import konko
 
-    @property
-    def _default_params(self) -> Mapping[str, Any]:
-        """Get the default parameters for calling Titan Takeoff Server (Pro)."""
-        return {
-            **(
-                {"regex_string": self.regex_string}
-                if self.regex_string is not None
-                else {}
-            ),
-            **(
-                {"sampling_temperature": self.sampling_temperature}
-                if self.sampling_temperature is not None
-                else {}
-            ),
-            **(
-                {"sampling_topp": self.sampling_topp}
-                if self.sampling_topp is not None
-                else {}
-            ),
-            **(
-                {"repetition_penalty": self.repetition_penalty}
-                if self.repetition_penalty is not None
-                else {}
-            ),
-            **(
-                {"max_new_tokens": self.max_new_tokens}
-                if self.max_new_tokens is not None
-                else {}
-            ),
-            **(
-                {"min_new_tokens": self.min_new_tokens}
-                if self.min_new_tokens is not None
-                else {}
-            ),
-            **(
-                {"sampling_topk": self.sampling_topk}
-                if self.sampling_topk is not None
-                else {}
-            ),
-            **(
-                {"no_repeat_ngram_size": self.no_repeat_ngram_size}
-                if self.no_repeat_ngram_size is not None
-                else {}
-            ),
+        except ImportError:
+            raise ImportError(
+                "Could not import konko python package. "
+                "Please install it with `pip install konko`."
+            )
+        if not hasattr(konko, "_is_legacy_openai"):
+            warnings.warn(
+                "You are using an older version of the 'konko' package. "
+                "Please consider upgrading to access new features"
+                "including the completion endpoint."
+            )
+        return values
+
+    def construct_payload(
+        self,
+        prompt: str,
+        stop: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> Dict[str, Any]:
+        stop_to_use = stop[0] if stop and len(stop) == 1 else stop
+        payload: Dict[str, Any] = {
+            **self.default_params,
+            "prompt": prompt,
+            "stop": stop_to_use,
+            **kwargs,
         }
+        return {k: v for k, v in payload.items() if v is not None}
 
     @property
     def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "titan_takeoff_pro"
+        """Return type of model."""
+        return "konko"
+
+    @staticmethod
+    def get_user_agent() -> str:
+        from langchain_community import __version__
+
+        return f"langchain/{__version__}"
+
+    @property
+    def default_params(self) -> Dict[str, Any]:
+        return {
+            "model": self.model,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            "top_k": self.top_k,
+            "max_tokens": self.max_tokens,
+            "repetition_penalty": self.repetition_penalty,
+        }
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call out to Titan Takeoff (Pro) generate endpoint.
+        """Call out to Konko's text generation endpoint.
 
         Args:
             prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
 
         Returns:
-            The string generated by the model.
-
-        Example:
-            .. code-block:: python
+            The string generated by the model..
+        """
+        import konko
 
-                prompt = "What is the capital of the United Kingdom?"
-                response = model(prompt)
+        payload = self.construct_payload(prompt, stop, **kwargs)
 
-        """
         try:
-            if self.streaming:
-                text_output = ""
-                for chunk in self._stream(
-                    prompt=prompt,
-                    stop=stop,
-                    run_manager=run_manager,
-                ):
-                    text_output += chunk.text
-                return text_output
-            url = f"{self.base_url}/generate"
-            params = {"text": prompt, **self._default_params}
-
-            response = requests.post(url, json=params)
-            response.raise_for_status()
-            response.encoding = "utf-8"
-
-            text = ""
-            if "text" in response.json():
-                text = response.json()["text"]
-                text = text.replace("</s>", "")
+            if is_openai_v1():
+                response = konko.completions.create(**payload)
             else:
-                raise ValueError("Something went wrong.")
-            if stop is not None:
-                text = enforce_stop_tokens(text, stop)
-            return text
-        except ConnectionError:
-            raise ConnectionError(
-                "Could not connect to Titan Takeoff (Pro) server. \
-                Please make sure that the server is running."
+                response = konko.Completion.create(**payload)
+
+        except AttributeError:
+            raise ValueError(
+                "`konko` has no `Completion` attribute, this is likely "
+                "due to an old version of the konko package. Try upgrading it "
+                "with `pip install --upgrade konko`."
             )
 
-    def _stream(
+        if is_openai_v1():
+            output = response.choices[0].text
+        else:
+            output = response["choices"][0]["text"]
+
+        return output
+
+    async def _acall(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Iterator[GenerationChunk]:
-        """Call out to Titan Takeoff (Pro) stream endpoint.
+    ) -> str:
+        """Asynchronously call out to Konko's text generation endpoint.
 
         Args:
             prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
 
         Returns:
             The string generated by the model.
+        """
+        import konko
 
-        Yields:
-            A dictionary like object containing a string token.
-
-        Example:
-            .. code-block:: python
+        payload = self.construct_payload(prompt, stop, **kwargs)
 
-                prompt = "What is the capital of the United Kingdom?"
-                response = model(prompt)
+        try:
+            if is_openai_v1():
+                client = konko.AsyncKonko()
+                response = await client.completions.create(**payload)
+            else:
+                response = await konko.Completion.acreate(**payload)
 
-        """
-        url = f"{self.base_url}/generate_stream"
-        params = {"text": prompt, **self._default_params}
+        except AttributeError:
+            raise ValueError(
+                "`konko` has no `Completion` attribute, this is likely "
+                "due to an old version of the konko package. Try upgrading it "
+                "with `pip install --upgrade konko`."
+            )
 
-        response = requests.post(url, json=params, stream=True)
-        response.encoding = "utf-8"
-        buffer = ""
-        for text in response.iter_content(chunk_size=1, decode_unicode=True):
-            buffer += text
-            if "data:" in buffer:
-                # Remove the first instance of "data:" from the buffer.
-                if buffer.startswith("data:"):
-                    buffer = ""
-                if len(buffer.split("data:", 1)) == 2:
-                    content, _ = buffer.split("data:", 1)
-                    buffer = content.rstrip("\n")
-                # Trim the buffer to only have content after the "data:" part.
-                if buffer:  # Ensure that there's content to process.
-                    chunk = GenerationChunk(text=buffer)
-                    buffer = ""  # Reset buffer for the next set of data.
-                    yield chunk
-                    if run_manager:
-                        run_manager.on_llm_new_token(token=chunk.text)
-
-        # Yield any remaining content in the buffer.
-        if buffer:
-            chunk = GenerationChunk(text=buffer.replace("</s>", ""))
-            yield chunk
-            if run_manager:
-                run_manager.on_llm_new_token(token=chunk.text)
+        if is_openai_v1():
+            output = response.choices[0].text
+        else:
+            output = response["choices"][0]["text"]
 
-    @property
-    def _identifying_params(self) -> Mapping[str, Any]:
-        """Get the identifying parameters."""
-        return {"base_url": self.base_url, **{}, **self._default_params}
+        return output
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/together.py` & `gigachain_community-0.2.0/langchain_community/llms/together.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,25 +1,30 @@
 """Wrapper around Together AI's Completion API."""
+
 import logging
 from typing import Any, Dict, List, Optional
 
 from aiohttp import ClientSession
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import LLM
 from langchain_core.pydantic_v1 import Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.utilities.requests import Requests
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.12", removal="0.3", alternative_import="langchain_together.Together"
+)
 class Together(LLM):
     """LLM models from `Together`.
 
     To use, you'll need an API key which you can find here:
     https://api.together.xyz/settings/api-keys. This can be passed in as init param
     ``together_api_key`` or set as environment variable ``TOGETHER_API_KEY``.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/tongyi.py` & `gigachain_community-0.2.0/langchain_community/llms/llamafile.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,277 +1,320 @@
 from __future__ import annotations
 
-import logging
-from typing import Any, Callable, Dict, List, Optional
+import json
+from io import StringIO
+from typing import Any, Dict, Iterator, List, Optional
 
-from langchain_core.callbacks import CallbackManagerForLLMRun
+import requests
+from langchain_core.callbacks.manager import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.outputs import Generation, LLMResult
-from langchain_core.pydantic_v1 import Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
-from requests.exceptions import HTTPError
-from tenacity import (
-    before_sleep_log,
-    retry,
-    retry_if_exception_type,
-    stop_after_attempt,
-    wait_exponential,
-)
-
-logger = logging.getLogger(__name__)
-
-
-def _create_retry_decorator(llm: Tongyi) -> Callable[[Any], Any]:
-    min_seconds = 1
-    max_seconds = 4
-    # Wait 2^x * 1 second between each retry starting with
-    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
-    return retry(
-        reraise=True,
-        stop=stop_after_attempt(llm.max_retries),
-        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
-        retry=(retry_if_exception_type(HTTPError)),
-        before_sleep=before_sleep_log(logger, logging.WARNING),
-    )
-
-
-def generate_with_retry(llm: Tongyi, **kwargs: Any) -> Any:
-    """Use tenacity to retry the completion call."""
-    retry_decorator = _create_retry_decorator(llm)
-
-    @retry_decorator
-    def _generate_with_retry(**_kwargs: Any) -> Any:
-        resp = llm.client.call(**_kwargs)
-        if resp.status_code == 200:
-            return resp
-        elif resp.status_code in [400, 401]:
-            raise ValueError(
-                f"status_code: {resp.status_code} \n "
-                f"code: {resp.code} \n message: {resp.message}"
-            )
-        else:
-            raise HTTPError(
-                f"HTTP error occurred: status_code: {resp.status_code} \n "
-                f"code: {resp.code} \n message: {resp.message}",
-                response=resp,
-            )
+from langchain_core.outputs import GenerationChunk
+from langchain_core.pydantic_v1 import Extra
+from langchain_core.utils import get_pydantic_field_names
 
-    return _generate_with_retry(**kwargs)
 
+class Llamafile(LLM):
+    """Llamafile lets you distribute and run large language models with a
+    single file.
 
-def stream_generate_with_retry(llm: Tongyi, **kwargs: Any) -> Any:
-    """Use tenacity to retry the completion call."""
-    retry_decorator = _create_retry_decorator(llm)
-
-    @retry_decorator
-    def _stream_generate_with_retry(**_kwargs: Any) -> Any:
-        stream_resps = []
-        resps = llm.client.call(**_kwargs)
-        for resp in resps:
-            if resp.status_code == 200:
-                stream_resps.append(resp)
-            elif resp.status_code in [400, 401]:
-                raise ValueError(
-                    f"status_code: {resp.status_code} \n "
-                    f"code: {resp.code} \n message: {resp.message}"
-                )
-            else:
-                raise HTTPError(
-                    f"HTTP error occurred: status_code: {resp.status_code} \n "
-                    f"code: {resp.code} \n message: {resp.message}",
-                    response=resp,
-                )
-        return stream_resps
-
-    return _stream_generate_with_retry(**kwargs)
+    To get started, see: https://github.com/Mozilla-Ocho/llamafile
 
+    To use this class, you will need to first:
 
-class Tongyi(LLM):
-    """Tongyi Qwen large language models.
+    1. Download a llamafile.
+    2. Make the downloaded file executable: `chmod +x path/to/model.llamafile`
+    3. Start the llamafile in server mode:
 
-    To use, you should have the ``dashscope`` python package installed, and the
-    environment variable ``DASHSCOPE_API_KEY`` set with your API key, or pass
-    it as a named parameter to the constructor.
+        `./path/to/model.llamafile --server --nobrowser`
 
     Example:
         .. code-block:: python
 
-            from langchain_community.llms import Tongyi
-            Tongyi = tongyi()
+            from langchain_community.llms import Llamafile
+            llm = Llamafile()
+            llm.invoke("Tell me a joke.")
     """
 
-    @property
-    def lc_secrets(self) -> Dict[str, str]:
-        return {"dashscope_api_key": "DASHSCOPE_API_KEY"}
+    base_url: str = "http://localhost:8080"
+    """Base url where the llamafile server is listening."""
 
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return False
+    request_timeout: Optional[int] = None
+    """Timeout for server requests"""
 
-    client: Any  #: :meta private:
-    model_name: str = "qwen-plus"
+    streaming: bool = False
+    """Allows receiving each predicted token in real-time instead of
+    waiting for the completion to finish. To enable this, set to true."""
 
-    """Model name to use."""
-    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    # Generation options
 
-    top_p: float = 0.8
-    """Total probability mass of tokens to consider at each step."""
+    seed: int = -1
+    """Random Number Generator (RNG) seed. A random seed is used if this is 
+    less than zero. Default: -1"""
 
-    dashscope_api_key: Optional[str] = None
-    """Dashscope api key provide by alicloud."""
+    temperature: float = 0.8
+    """Temperature. Default: 0.8"""
 
-    n: int = 1
-    """How many completions to generate for each prompt."""
+    top_k: int = 40
+    """Limit the next token selection to the K most probable tokens. 
+    Default: 40."""
 
-    streaming: bool = False
-    """Whether to stream the results or not."""
+    top_p: float = 0.95
+    """Limit the next token selection to a subset of tokens with a cumulative 
+    probability above a threshold P. Default: 0.95."""
+
+    min_p: float = 0.05
+    """The minimum probability for a token to be considered, relative to 
+    the probability of the most likely token. Default: 0.05."""
+
+    n_predict: int = -1
+    """Set the maximum number of tokens to predict when generating text. 
+    Note: May exceed the set limit slightly if the last token is a partial 
+    multibyte character. When 0, no tokens will be generated but the prompt 
+    is evaluated into the cache. Default: -1 = infinity."""
+
+    n_keep: int = 0
+    """Specify the number of tokens from the prompt to retain when the 
+    context size is exceeded and tokens need to be discarded. By default, 
+    this value is set to 0 (meaning no tokens are kept). Use -1 to retain all 
+    tokens from the prompt."""
+
+    tfs_z: float = 1.0
+    """Enable tail free sampling with parameter z. Default: 1.0 = disabled."""
+
+    typical_p: float = 1.0
+    """Enable locally typical sampling with parameter p. 
+    Default: 1.0 = disabled."""
+
+    repeat_penalty: float = 1.1
+    """Control the repetition of token sequences in the generated text. 
+    Default: 1.1"""
+
+    repeat_last_n: int = 64
+    """Last n tokens to consider for penalizing repetition. Default: 64, 
+    0 = disabled, -1 = ctx-size."""
+
+    penalize_nl: bool = True
+    """Penalize newline tokens when applying the repeat penalty. 
+    Default: true."""
+
+    presence_penalty: float = 0.0
+    """Repeat alpha presence penalty. Default: 0.0 = disabled."""
+
+    frequency_penalty: float = 0.0
+    """Repeat alpha frequency penalty. Default: 0.0 = disabled"""
+
+    mirostat: int = 0
+    """Enable Mirostat sampling, controlling perplexity during text 
+    generation. 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0. 
+    Default: disabled."""
 
-    max_retries: int = 10
-    """Maximum number of retries to make when generating."""
+    mirostat_tau: float = 5.0
+    """Set the Mirostat target entropy, parameter tau. Default: 5.0."""
 
-    prefix_messages: List = Field(default_factory=list)
-    """Series of messages for Chat input."""
+    mirostat_eta: float = 0.1
+    """Set the Mirostat learning rate, parameter eta. Default: 0.1."""
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        extra = Extra.forbid
 
     @property
     def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "tongyi"
-
-    @root_validator()
-    def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that api key and python package exists in environment."""
-        get_from_dict_or_env(values, "dashscope_api_key", "DASHSCOPE_API_KEY")
-        try:
-            import dashscope
-        except ImportError:
-            raise ImportError(
-                "Could not import dashscope python package. "
-                "Please install it with `pip install dashscope`."
-            )
-        try:
-            values["client"] = dashscope.Generation
-        except AttributeError:
-            raise ValueError(
-                "`dashscope` has no `Generation` attribute, this is likely "
-                "due to an old version of the dashscope package. Try upgrading it "
-                "with `pip install --upgrade dashscope`."
-            )
+        return "llamafile"
 
-        return values
+    @property
+    def _param_fieldnames(self) -> List[str]:
+        # Return the list of fieldnames that will be passed as configurable
+        # generation options to the llamafile server. Exclude 'builtin' fields
+        # from the BaseLLM class like 'metadata' as well as fields that should
+        # not be passed in requests (base_url, request_timeout).
+        ignore_keys = [
+            "base_url",
+            "cache",
+            "callback_manager",
+            "callbacks",
+            "metadata",
+            "name",
+            "request_timeout",
+            "streaming",
+            "tags",
+            "verbose",
+            "custom_get_token_ids",
+        ]
+        attrs = [
+            k for k in get_pydantic_field_names(self.__class__) if k not in ignore_keys
+        ]
+        return attrs
 
     @property
     def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for calling OpenAI API."""
-        normal_params = {
-            "top_p": self.top_p,
-        }
+        params = {}
+        for fieldname in self._param_fieldnames:
+            params[fieldname] = getattr(self, fieldname)
+        return params
+
+    def _get_parameters(
+        self, stop: Optional[List[str]] = None, **kwargs: Any
+    ) -> Dict[str, Any]:
+        params = self._default_params
+
+        # Only update keys that are already present in the default config.
+        # This way, we don't accidentally post unknown/unhandled key/values
+        # in the request to the llamafile server
+        for k, v in kwargs.items():
+            if k in params:
+                params[k] = v
 
-        return {**normal_params, **self.model_kwargs}
+        if stop is not None and len(stop) > 0:
+            params["stop"] = stop
+
+        if self.streaming:
+            params["stream"] = True
+
+        return params
 
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        """Call out to Tongyi's generate endpoint.
+        """Request prompt completion from the llamafile server and return the
+        output.
 
         Args:
-            prompt: The prompt to pass into the model.
+            prompt: The prompt to use for generation.
+            stop: A list of strings to stop generation when encountered.
+            run_manager:
+            **kwargs: Any additional options to pass as part of the
+            generation request.
 
         Returns:
             The string generated by the model.
 
-        Example:
-            .. code-block:: python
-
-                response = tongyi("Tell me a joke.")
         """
-        params: Dict[str, Any] = {
-            **{"model": self.model_name},
-            **self._default_params,
-            **kwargs,
-        }
-
-        completion = generate_with_retry(
-            self,
-            prompt=prompt,
-            **params,
-        )
-        return completion["output"]["text"]
 
-    def _generate(
+        if self.streaming:
+            with StringIO() as buff:
+                for chunk in self._stream(
+                    prompt, stop=stop, run_manager=run_manager, **kwargs
+                ):
+                    buff.write(chunk.text)
+
+                text = buff.getvalue()
+
+            return text
+
+        else:
+            params = self._get_parameters(stop=stop, **kwargs)
+            payload = {"prompt": prompt, **params}
+
+            try:
+                response = requests.post(
+                    url=f"{self.base_url}/completion",
+                    headers={
+                        "Content-Type": "application/json",
+                    },
+                    json=payload,
+                    stream=False,
+                    timeout=self.request_timeout,
+                )
+            except requests.exceptions.ConnectionError:
+                raise requests.exceptions.ConnectionError(
+                    f"Could not connect to Llamafile server. Please make sure "
+                    f"that a server is running at {self.base_url}."
+                )
+
+            response.raise_for_status()
+            response.encoding = "utf-8"
+
+            text = response.json()["content"]
+
+            return text
+
+    def _stream(
         self,
-        prompts: List[str],
+        prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> LLMResult:
-        generations = []
-        params: Dict[str, Any] = {
-            **{"model": self.model_name},
-            **self._default_params,
-            **kwargs,
-        }
-        if self.streaming:
-            if len(prompts) > 1:
-                raise ValueError("Cannot stream results with multiple prompts.")
+    ) -> Iterator[GenerationChunk]:
+        """Yields results objects as they are generated in real time.
+
+        It also calls the callback manager's on_llm_new_token event with
+        similar parameters to the OpenAI LLM class method of the same name.
+
+        Args:
+            prompt: The prompts to pass into the model.
+            stop: Optional list of stop words to use when generating.
+            run_manager:
+            **kwargs: Any additional options to pass as part of the
+            generation request.
+
+        Returns:
+            A generator representing the stream of tokens being generated.
+
+        Yields:
+            Dictionary-like objects each containing a token
+
+        Example:
+        .. code-block:: python
+
+            from langchain_community.llms import Llamafile
+            llm = Llamafile(
+                temperature = 0.0
+            )
+            for chunk in llm.stream("Ask 'Hi, how are you?' like a pirate:'",
+                    stop=["'","\n"]):
+                result = chunk["choices"][0]
+                print(result["text"], end='', flush=True)
+
+        """
+        params = self._get_parameters(stop=stop, **kwargs)
+        if "stream" not in params:
             params["stream"] = True
-            temp = ""
-            for stream_resp in stream_generate_with_retry(
-                self, prompt=prompts[0], **params
-            ):
-                if run_manager:
-                    stream_resp_text = stream_resp["output"]["text"]
-                    stream_resp_text = stream_resp_text.replace(temp, "")
-                    # Ali Cloud's streaming transmission interface, each return content
-                    # will contain the output
-                    # of the previous round(as of September 20, 2023, future updates to
-                    # the Alibaba Cloud API may vary)
-                    run_manager.on_llm_new_token(stream_resp_text)
-                    # The implementation of streaming transmission primarily relies on
-                    # the "on_llm_new_token" method
-                    # of the streaming callback.
-                temp = stream_resp["output"]["text"]
-
-                generations.append(
-                    [
-                        Generation(
-                            text=stream_resp["output"]["text"],
-                            generation_info=dict(
-                                finish_reason=stream_resp["output"]["finish_reason"],
-                            ),
-                        )
-                    ]
-                )
-            generations.reverse()
-            # In the official implementation of the OpenAI API,
-            # the "generations" parameter passed to LLMResult seems to be a 1*1*1
-            # two-dimensional list
-            # (including in non-streaming mode).
-            # Considering that Alibaba Cloud's streaming transmission
-            # (as of September 20, 2023, future updates to the Alibaba Cloud API may
-            # vary)
-            # includes the output of the previous round in each return,
-            # reversing this "generations" list should suffice
-            # (This is the solution with the least amount of changes to the source code,
-            # while still allowing for convenient modifications in the future,
-            # although it may result in slightly more memory consumption).
+
+        payload = {"prompt": prompt, **params}
+
+        try:
+            response = requests.post(
+                url=f"{self.base_url}/completion",
+                headers={
+                    "Content-Type": "application/json",
+                },
+                json=payload,
+                stream=True,
+                timeout=self.request_timeout,
+            )
+        except requests.exceptions.ConnectionError:
+            raise requests.exceptions.ConnectionError(
+                f"Could not connect to Llamafile server. Please make sure "
+                f"that a server is running at {self.base_url}."
+            )
+
+        response.encoding = "utf8"
+
+        for raw_chunk in response.iter_lines(decode_unicode=True):
+            content = self._get_chunk_content(raw_chunk)
+            chunk = GenerationChunk(text=content)
+
+            if run_manager:
+                run_manager.on_llm_new_token(token=chunk.text)
+            yield chunk
+
+    def _get_chunk_content(self, chunk: str) -> str:
+        """When streaming is turned on, llamafile server returns lines like:
+
+        'data: {"content":" They","multimodal":true,"slot_id":0,"stop":false}'
+
+        Here, we convert this to a dict and return the value of the 'content'
+        field
+        """
+
+        if chunk.startswith("data:"):
+            cleaned = chunk.lstrip("data: ")
+            data = json.loads(cleaned)
+            return data["content"]
         else:
-            for prompt in prompts:
-                completion = generate_with_retry(
-                    self,
-                    prompt=prompt,
-                    **params,
-                )
-                generations.append(
-                    [
-                        Generation(
-                            text=completion["output"]["text"],
-                            generation_info=dict(
-                                finish_reason=completion["output"]["finish_reason"],
-                            ),
-                        )
-                    ]
-                )
-        return LLMResult(generations=generations)
+            return chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/vertexai.py` & `gigachain_community-0.2.0/langchain_community/llms/vertexai.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 from concurrent.futures import Executor, ThreadPoolExecutor
 from typing import TYPE_CHECKING, Any, ClassVar, Dict, Iterator, List, Optional, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks.manager import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import BaseLLM
 from langchain_core.outputs import Generation, GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
@@ -35,24 +36,24 @@
 # We can remove after `langchain` stops importing it
 _response_to_generation = None
 completion_with_retry = None
 stream_completion_with_retry = None
 
 
 def is_codey_model(model_name: str) -> bool:
-    """Returns True if the model name is a Codey model."""
+    """Return True if the model name is a Codey model."""
     return "code" in model_name
 
 
 def is_gemini_model(model_name: str) -> bool:
-    """Returns True if the model name is a Gemini model."""
+    """Return True if the model name is a Gemini model."""
     return model_name is not None and "gemini" in model_name
 
 
-def completion_with_retry(
+def completion_with_retry(  # type: ignore[no-redef]
     llm: VertexAI,
     prompt: List[Union[str, "Image"]],
     stream: bool = False,
     is_gemini: bool = False,
     run_manager: Optional[CallbackManagerForLLMRun] = None,
     **kwargs: Any,
 ) -> Any:
@@ -196,14 +197,19 @@
         params = {params_mapping.get(k, k): v for k, v in kwargs.items()}
         params = {**self._default_params, "stop_sequences": stop_sequences, **params}
         if stream or self.streaming:
             params.pop("candidate_count")
         return params
 
 
+@deprecated(
+    since="0.0.12",
+    removal="0.3.0",
+    alternative_import="langchain_google_vertexai.VertexAI",
+)
 class VertexAI(_VertexAICommon, BaseLLM):
     """Google Vertex AI large language models."""
 
     model_name: str = "text-bison"
     "The name of the Vertex AI large language model."
     tuned_model_name: Optional[str] = None
     "The name of a tuned model. If provided, model_name is ignored."
@@ -320,15 +326,15 @@
                 generation = GenerationChunk(text="")
                 for chunk in self._stream(
                     prompt, stop=stop, run_manager=run_manager, **kwargs
                 ):
                     generation += chunk
                 generations.append([generation])
             else:
-                res = completion_with_retry(
+                res = completion_with_retry(  # type: ignore[misc]
                     self,
                     [prompt],
                     stream=should_stream,
                     is_gemini=self._is_gemini_model,
                     run_manager=run_manager,
                     **params,
                 )
@@ -353,44 +359,49 @@
                 is_gemini=self._is_gemini_model,
                 run_manager=run_manager,
                 **params,
             )
             generations.append(
                 [self._response_to_generation(r) for r in res.candidates]
             )
-        return LLMResult(generations=generations)
+        return LLMResult(generations=generations)  # type: ignore[arg-type]
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
         params = self._prepare_params(stop=stop, stream=True, **kwargs)
-        for stream_resp in completion_with_retry(
+        for stream_resp in completion_with_retry(  # type: ignore[misc]
             self,
             [prompt],
             stream=True,
             is_gemini=self._is_gemini_model,
             run_manager=run_manager,
             **params,
         ):
             chunk = self._response_to_generation(stream_resp)
-            yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(
                     chunk.text,
                     chunk=chunk,
                     verbose=self.verbose,
                 )
+            yield chunk
 
 
+@deprecated(
+    since="0.0.12",
+    removal="0.3.0",
+    alternative_import="langchain_google_vertexai.VertexAIModelGarden",
+)
 class VertexAIModelGarden(_VertexAIBase, BaseLLM):
-    """Large language models served from Vertex AI Model Garden."""
+    """Vertex AI Model Garden large language models."""
 
     client: "PredictionServiceClient" = None  #: :meta private:
     async_client: "PredictionServiceAsyncClient" = None  #: :meta private:
     endpoint_id: str
     "A name of an endpoint where the model has been deployed."
     allowed_model_args: Optional[List[str]] = None
     "Allowed optional args to be passed to the model."
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/volcengine_maas.py` & `gigachain_community-0.2.0/langchain_community/llms/volcengine_maas.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -171,10 +171,10 @@
     ) -> Iterator[GenerationChunk]:
         params = self._convert_prompt_msg_params(prompt, **kwargs)
         for res in self.client.stream_chat(params):
             if res:
                 chunk = GenerationChunk(
                     text=res.get("choice", {}).get("message", {}).get("content", "")
                 )
-                yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+                yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/watsonxllm.py` & `gigachain_community-0.2.0/langchain_community/llms/watsonxllm.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,55 +1,62 @@
 import logging
 import os
 from typing import Any, Dict, Iterator, List, Mapping, Optional, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import BaseLLM
 from langchain_core.outputs import Generation, GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.18", removal="0.3", alternative_import="langchain_ibm.WatsonxLLM"
+)
 class WatsonxLLM(BaseLLM):
     """
     IBM watsonx.ai large language models.
 
-    To use, you should have ``ibm_watson_machine_learning`` python package installed,
+    To use, you should have ``ibm_watsonx_ai`` python package installed,
     and the environment variable ``WATSONX_APIKEY`` set with your API key, or pass
     it as a named parameter to the constructor.
 
 
     Example:
         .. code-block:: python
 
-            from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames
+            from ibm_watsonx_ai.metanames import GenTextParamsMetaNames
             parameters = {
                 GenTextParamsMetaNames.DECODING_METHOD: "sample",
                 GenTextParamsMetaNames.MAX_NEW_TOKENS: 100,
                 GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,
                 GenTextParamsMetaNames.TEMPERATURE: 0.5,
                 GenTextParamsMetaNames.TOP_K: 50,
                 GenTextParamsMetaNames.TOP_P: 1,
             }
 
             from langchain_community.llms import WatsonxLLM
-            llm = WatsonxLLM(
+            watsonx_llm = WatsonxLLM(
                 model_id="google/flan-ul2",
                 url="https://us-south.ml.cloud.ibm.com",
                 apikey="*****",
                 project_id="*****",
                 params=parameters,
             )
     """
 
     model_id: str = ""
     """Type of model to use."""
 
+    deployment_id: str = ""
+    """Type of deployed model to use."""
+
     project_id: str = ""
     """ID of the Watson Studio project."""
 
     space_id: str = ""
     """ID of the Watson Studio space."""
 
     url: Optional[SecretStr] = None
@@ -155,63 +162,71 @@
                 )
             if not values["instance_id"] or "WATSONX_INSTANCE_ID" not in os.environ:
                 values["instance_id"] = convert_to_secret_str(
                     get_from_dict_or_env(values, "instance_id", "WATSONX_INSTANCE_ID")
                 )
 
         try:
-            from ibm_watson_machine_learning.foundation_models import Model
+            from ibm_watsonx_ai.foundation_models import ModelInference
 
             credentials = {
                 "url": values["url"].get_secret_value() if values["url"] else None,
-                "apikey": values["apikey"].get_secret_value()
-                if values["apikey"]
-                else None,
-                "token": values["token"].get_secret_value()
-                if values["token"]
-                else None,
-                "password": values["password"].get_secret_value()
-                if values["password"]
-                else None,
-                "username": values["username"].get_secret_value()
-                if values["username"]
-                else None,
-                "instance_id": values["instance_id"].get_secret_value()
-                if values["instance_id"]
-                else None,
-                "version": values["version"].get_secret_value()
-                if values["version"]
-                else None,
+                "apikey": (
+                    values["apikey"].get_secret_value() if values["apikey"] else None
+                ),
+                "token": (
+                    values["token"].get_secret_value() if values["token"] else None
+                ),
+                "password": (
+                    values["password"].get_secret_value()
+                    if values["password"]
+                    else None
+                ),
+                "username": (
+                    values["username"].get_secret_value()
+                    if values["username"]
+                    else None
+                ),
+                "instance_id": (
+                    values["instance_id"].get_secret_value()
+                    if values["instance_id"]
+                    else None
+                ),
+                "version": (
+                    values["version"].get_secret_value() if values["version"] else None
+                ),
             }
             credentials_without_none_value = {
                 key: value for key, value in credentials.items() if value is not None
             }
 
-            watsonx_model = Model(
+            watsonx_model = ModelInference(
                 model_id=values["model_id"],
+                deployment_id=values["deployment_id"],
                 credentials=credentials_without_none_value,
                 params=values["params"],
                 project_id=values["project_id"],
                 space_id=values["space_id"],
                 verify=values["verify"],
             )
             values["watsonx_model"] = watsonx_model
 
         except ImportError:
             raise ImportError(
-                "Could not import ibm_watson_machine_learning python package. "
-                "Please install it with `pip install ibm_watson_machine_learning`."
+                "Could not import ibm_watsonx_ai python package. "
+                "Please install it with `pip install ibm_watsonx_ai`."
             )
         return values
 
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {
             "model_id": self.model_id,
+            "deployment_id": self.deployment_id,
             "params": self.params,
             "project_id": self.project_id,
             "space_id": self.space_id,
         }
 
     @property
     def _llm_type(self) -> str:
@@ -240,30 +255,58 @@
                 )
 
         return {
             "generated_token_count": generated_token_count,
             "input_token_count": input_token_count,
         }
 
+    def _get_chat_params(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:
+        params: Dict[str, Any] = {**self.params} if self.params else {}
+        if stop is not None:
+            params["stop_sequences"] = stop
+        return params
+
     def _create_llm_result(self, response: List[dict]) -> LLMResult:
         """Create the LLMResult from the choices and prompts."""
         generations = []
         for res in response:
             results = res.get("results")
             if results:
                 finish_reason = results[0].get("stop_reason")
                 gen = Generation(
                     text=results[0].get("generated_text"),
                     generation_info={"finish_reason": finish_reason},
                 )
                 generations.append([gen])
         final_token_usage = self._extract_token_usage(response)
-        llm_output = {"token_usage": final_token_usage, "model_id": self.model_id}
+        llm_output = {
+            "token_usage": final_token_usage,
+            "model_id": self.model_id,
+            "deployment_id": self.deployment_id,
+        }
         return LLMResult(generations=generations, llm_output=llm_output)
 
+    def _stream_response_to_generation_chunk(
+        self,
+        stream_response: Dict[str, Any],
+    ) -> GenerationChunk:
+        """Convert a stream response to a generation chunk."""
+        if not stream_response["results"]:
+            return GenerationChunk(text="")
+        return GenerationChunk(
+            text=stream_response["results"][0]["generated_text"],
+            generation_info=dict(
+                finish_reason=stream_response["results"][0].get("stop_reason", None),
+                llm_output={
+                    "model_id": self.model_id,
+                    "deployment_id": self.deployment_id,
+                },
+            ),
+        )
+
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
@@ -273,15 +316,15 @@
             stop: Optional list of stop words to use when generating.
             run_manager: Optional callback manager.
         Returns:
             The string generated by the model.
         Example:
             .. code-block:: python
 
-                response = watsonxllm("What is a molecule")
+                response = watsonx_llm.invoke("What is a molecule")
         """
         result = self._generate(
             prompts=[prompt], stop=stop, run_manager=run_manager, **kwargs
         )
         return result.generations[0][0].text
 
     def _generate(
@@ -298,16 +341,17 @@
             stop: Optional list of stop words to use when generating.
             run_manager: Optional callback manager.
         Returns:
             The full LLMResult output.
         Example:
             .. code-block:: python
 
-                response = watsonxllm.generate(["What is a molecule"])
+                response = watsonx_llm.generate(["What is a molecule"])
         """
+        params = self._get_chat_params(stop=stop)
         should_stream = stream if stream is not None else self.streaming
         if should_stream:
             if len(prompts) > 1:
                 raise ValueError(
                     f"WatsonxLLM currently only supports single prompt, got {prompts}"
                 )
             generation = GenerationChunk(text="")
@@ -316,17 +360,20 @@
             )
             for chunk in stream_iter:
                 if generation is None:
                     generation = chunk
                 else:
                     generation += chunk
             assert generation is not None
+            if isinstance(generation.generation_info, dict):
+                llm_output = generation.generation_info.pop("llm_output")
+                return LLMResult(generations=[[generation]], llm_output=llm_output)
             return LLMResult(generations=[[generation]])
         else:
-            response = self.watsonx_model.generate(prompt=prompts)
+            response = self.watsonx_model.generate(prompt=prompts, params=params)
             return self._create_llm_result(response)
 
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
@@ -338,16 +385,20 @@
             stop: Optional list of stop words to use when generating.
             run_manager: Optional callback manager.
         Returns:
             The iterator which yields generation chunks.
         Example:
             .. code-block:: python
 
-                response = watsonxllm.stream("What is a molecule")
+                response = watsonx_llm.stream("What is a molecule")
                 for chunk in response:
-                    print(chunk, end='')
+                    print(chunk, end='')  # noqa: T201
         """
-        for chunk in self.watsonx_model.generate_text_stream(prompt=prompt):
-            if chunk:
-                yield GenerationChunk(text=chunk)
-                if run_manager:
-                    run_manager.on_llm_new_token(chunk)
+        params = self._get_chat_params(stop=stop)
+        for stream_resp in self.watsonx_model.generate_text_stream(
+            prompt=prompt, raw_response=True, params=params
+        ):
+            chunk = self._stream_response_to_generation_chunk(stream_resp)
+
+            if run_manager:
+                run_manager.on_llm_new_token(chunk.text, chunk=chunk)
+            yield chunk
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/writer.py` & `gigachain_community-0.2.0/langchain_community/llms/writer.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/xinference.py` & `gigachain_community-0.2.0/langchain_community/llms/xinference.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,15 +58,15 @@
         from langchain_community.llms import Xinference
 
         llm = Xinference(
             server_url="http://0.0.0.0:9997",
             model_uid = {model_uid} # replace model_uid with the model UID return from launching the model
         )
 
-        llm(
+        llm.invoke(
             prompt="Q: where can we visit in the capital of France? A:",
             generate_config={"max_tokens": 1024, "stream": True},
         )
 
     To view all the supported builtin models, run:
 
     .. code-block:: bash
@@ -96,15 +96,15 @@
                 "Could not import RESTfulClient from xinference. Please install it"
                 " with `pip install xinference`."
             ) from e
 
         model_kwargs = model_kwargs or {}
 
         super().__init__(
-            **{
+            **{  # type: ignore[arg-type]
                 "server_url": server_url,
                 "model_uid": model_uid,
                 "model_kwargs": model_kwargs,
             }
         )
 
         if self.server_url is None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/llms/yandex.py` & `gigachain_community-0.2.0/langchain_community/llms/yandex.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 from __future__ import annotations
 
 import logging
-from typing import Any, Callable, Dict, List, Mapping, Optional
+from typing import Any, Callable, Dict, List, Optional, Sequence
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.llms import LLM
 from langchain_core.load.serializable import Serializable
-from langchain_core.pydantic_v1 import root_validator
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import SecretStr, root_validator
+from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     retry_if_exception_type,
     stop_after_attempt,
     wait_exponential,
 )
 
 from langchain_community.llms.utils import enforce_stop_tokens
 
 logger = logging.getLogger(__name__)
 
 
 class _BaseYandexGPT(Serializable):
-    iam_token: str = ""
+    iam_token: SecretStr = ""  # type: ignore[assignment]
     """Yandex Cloud IAM token for service or user account
     with the `ai.languageModels.user` role"""
-    api_key: str = ""
+    api_key: SecretStr = ""  # type: ignore[assignment]
     """Yandex Cloud Api Key for service account
     with the `ai.languageModels.user` role"""
     folder_id: str = ""
     """Yandex Cloud folder ID"""
     model_uri: str = ""
     """Model uri to use."""
     model_name: str = "yandexgpt-lite"
@@ -48,59 +48,76 @@
     Must be greater than zero and not exceed 7400 tokens."""
     stop: Optional[List[str]] = None
     """Sequences when completion generation will stop."""
     url: str = "llm.api.cloud.yandex.net:443"
     """The url of the API."""
     max_retries: int = 6
     """Maximum number of retries to make when generating."""
+    sleep_interval: float = 1.0
+    """Delay between API requests"""
+    disable_request_logging: bool = False
+    """YandexGPT API logs all request data by default. 
+    If you provide personal data, confidential information, disable logging."""
+    _grpc_metadata: Sequence
 
     @property
     def _llm_type(self) -> str:
         return "yandex_gpt"
 
     @property
-    def _identifying_params(self) -> Mapping[str, Any]:
+    def _identifying_params(self) -> Dict[str, Any]:
         """Get the identifying parameters."""
         return {
             "model_uri": self.model_uri,
             "temperature": self.temperature,
             "max_tokens": self.max_tokens,
             "stop": self.stop,
             "max_retries": self.max_retries,
         }
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that iam token exists in environment."""
 
-        iam_token = get_from_dict_or_env(values, "iam_token", "YC_IAM_TOKEN", "")
+        iam_token = convert_to_secret_str(
+            get_from_dict_or_env(values, "iam_token", "YC_IAM_TOKEN", "")
+        )
         values["iam_token"] = iam_token
-        api_key = get_from_dict_or_env(values, "api_key", "YC_API_KEY", "")
+        api_key = convert_to_secret_str(
+            get_from_dict_or_env(values, "api_key", "YC_API_KEY", "")
+        )
         values["api_key"] = api_key
         folder_id = get_from_dict_or_env(values, "folder_id", "YC_FOLDER_ID", "")
         values["folder_id"] = folder_id
-        if api_key == "" and iam_token == "":
+        if api_key.get_secret_value() == "" and iam_token.get_secret_value() == "":
             raise ValueError("Either 'YC_API_KEY' or 'YC_IAM_TOKEN' must be provided.")
 
         if values["iam_token"]:
             values["_grpc_metadata"] = [
-                ("authorization", f"Bearer {values['iam_token']}")
+                ("authorization", f"Bearer {values['iam_token'].get_secret_value()}")
             ]
             if values["folder_id"]:
                 values["_grpc_metadata"].append(("x-folder-id", values["folder_id"]))
         else:
             values["_grpc_metadata"] = (
-                ("authorization", f"Api-Key {values['api_key']}"),
+                ("authorization", f"Api-Key {values['api_key'].get_secret_value()}"),
             )
         if values["model_uri"] == "" and values["folder_id"] == "":
             raise ValueError("Either 'model_uri' or 'folder_id' must be provided.")
         if not values["model_uri"]:
             values[
                 "model_uri"
             ] = f"gpt://{values['folder_id']}/{values['model_name']}/{values['model_version']}"
+        if values["disable_request_logging"]:
+            values["_grpc_metadata"].append(
+                (
+                    "x-data-logging-enabled",
+                    "false",
+                )
+            )
         return values
 
 
 class YandexGPT(_BaseYandexGPT, LLM):
     """Yandex large language models.
 
     To use, you should have the ``yandexcloud`` python package installed.
@@ -175,101 +192,131 @@
 def _make_request(
     self: YandexGPT,
     prompt: str,
 ) -> str:
     try:
         import grpc
         from google.protobuf.wrappers_pb2 import DoubleValue, Int64Value
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
-            CompletionOptions,
-            Message,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
-            CompletionRequest,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
-            TextGenerationServiceStub,
-        )
+
+        try:
+            from yandex.cloud.ai.foundation_models.v1.text_common_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2_grpc import (  # noqa: E501
+                TextGenerationServiceStub,
+            )
+        except ModuleNotFoundError:
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
+                TextGenerationServiceStub,
+            )
     except ImportError as e:
         raise ImportError(
-            "Please install YandexCloud SDK" " with `pip install yandexcloud`."
+            "Please install YandexCloud SDK  with `pip install yandexcloud` \
+            or upgrade it to recent version."
         ) from e
     channel_credentials = grpc.ssl_channel_credentials()
     channel = grpc.secure_channel(self.url, channel_credentials)
     request = CompletionRequest(
         model_uri=self.model_uri,
         completion_options=CompletionOptions(
             temperature=DoubleValue(value=self.temperature),
             max_tokens=Int64Value(value=self.max_tokens),
         ),
         messages=[Message(role="user", text=prompt)],
     )
     stub = TextGenerationServiceStub(channel)
-    res = stub.Completion(request, metadata=self._grpc_metadata)
+    res = stub.Completion(request, metadata=self._grpc_metadata)  # type: ignore[attr-defined]
     return list(res)[0].alternatives[0].message.text
 
 
 async def _amake_request(self: YandexGPT, prompt: str) -> str:
     try:
         import asyncio
 
         import grpc
         from google.protobuf.wrappers_pb2 import DoubleValue, Int64Value
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
-            CompletionOptions,
-            Message,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
-            CompletionRequest,
-            CompletionResponse,
-        )
-        from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
-            TextGenerationAsyncServiceStub,
-        )
+
+        try:
+            from yandex.cloud.ai.foundation_models.v1.text_common_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+                CompletionResponse,
+            )
+            from yandex.cloud.ai.foundation_models.v1.text_generation.text_generation_service_pb2_grpc import (  # noqa: E501
+                TextGenerationAsyncServiceStub,
+            )
+        except ModuleNotFoundError:
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_pb2 import (
+                CompletionOptions,
+                Message,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2 import (  # noqa: E501
+                CompletionRequest,
+                CompletionResponse,
+            )
+            from yandex.cloud.ai.foundation_models.v1.foundation_models_service_pb2_grpc import (  # noqa: E501
+                TextGenerationAsyncServiceStub,
+            )
         from yandex.cloud.operation.operation_service_pb2 import GetOperationRequest
         from yandex.cloud.operation.operation_service_pb2_grpc import (
             OperationServiceStub,
         )
     except ImportError as e:
         raise ImportError(
-            "Please install YandexCloud SDK" " with `pip install yandexcloud`."
+            "Please install YandexCloud SDK  with `pip install yandexcloud` \
+            or upgrade it to recent version."
         ) from e
     operation_api_url = "operation.api.cloud.yandex.net:443"
     channel_credentials = grpc.ssl_channel_credentials()
     async with grpc.aio.secure_channel(self.url, channel_credentials) as channel:
         request = CompletionRequest(
             model_uri=self.model_uri,
             completion_options=CompletionOptions(
                 temperature=DoubleValue(value=self.temperature),
                 max_tokens=Int64Value(value=self.max_tokens),
             ),
             messages=[Message(role="user", text=prompt)],
         )
         stub = TextGenerationAsyncServiceStub(channel)
-        operation = await stub.Completion(request, metadata=self._grpc_metadata)
+        operation = await stub.Completion(request, metadata=self._grpc_metadata)  # type: ignore[attr-defined]
         async with grpc.aio.secure_channel(
             operation_api_url, channel_credentials
         ) as operation_channel:
             operation_stub = OperationServiceStub(operation_channel)
             while not operation.done:
                 await asyncio.sleep(1)
                 operation_request = GetOperationRequest(operation_id=operation.id)
                 operation = await operation_stub.Get(
-                    operation_request, metadata=self._grpc_metadata
+                    operation_request,
+                    metadata=self._grpc_metadata,  # type: ignore[attr-defined]
                 )
 
         completion_response = CompletionResponse()
         operation.response.Unpack(completion_response)
         return completion_response.alternatives[0].message.text
 
 
 def _create_retry_decorator(llm: YandexGPT) -> Callable[[Any], Any]:
     from grpc import RpcError
 
-    min_seconds = 1
+    min_seconds = llm.sleep_interval
     max_seconds = 60
     return retry(
         reraise=True,
         stop=stop_after_attempt(llm.max_retries),
         wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
         retry=(retry_if_exception_type((RpcError))),
         before_sleep=before_sleep_log(logger, logging.WARNING),
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/arcee.py` & `gigachain_community-0.2.0/langchain_community/retrievers/arcee.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,30 +6,30 @@
 from langchain_core.retrievers import BaseRetriever
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 from langchain_community.utilities.arcee import ArceeWrapper, DALMFilter
 
 
 class ArceeRetriever(BaseRetriever):
-    """Document retriever for Arcee's Domain Adapted Language Models (DALMs).
+    """Arcee Domain Adapted Language Models (DALMs) retriever.
 
     To use, set the ``ARCEE_API_KEY`` environment variable with your Arcee API key,
     or pass ``arcee_api_key`` as a named parameter.
 
     Example:
         .. code-block:: python
 
             from langchain_community.retrievers import ArceeRetriever
 
             retriever = ArceeRetriever(
                 model="DALM-PubMed",
                 arcee_api_key="ARCEE-API-KEY"
             )
 
-            documents = retriever.get_relevant_documents("AI-driven music therapy")
+            documents = retriever.invoke("AI-driven music therapy")
     """
 
     _client: Optional[ArceeWrapper] = None  #: :meta private:
     """Arcee client."""
 
     arcee_api_key: SecretStr
     """Arcee API Key"""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/arxiv.py` & `gigachain_community-0.2.0/langchain_community/retrievers/arxiv.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/azure_cognitive_search.py` & `gigachain_community-0.2.0/langchain_community/retrievers/azure_ai_search.py`

 * *Files 22% similar despite different names*

```diff
@@ -14,25 +14,25 @@
 from langchain_core.retrievers import BaseRetriever
 from langchain_core.utils import get_from_dict_or_env, get_from_env
 
 DEFAULT_URL_SUFFIX = "search.windows.net"
 """Default URL Suffix for endpoint connection - commercial cloud"""
 
 
-class AzureCognitiveSearchRetriever(BaseRetriever):
-    """`Azure Cognitive Search` service retriever."""
+class AzureAISearchRetriever(BaseRetriever):
+    """`Azure AI Search` service retriever."""
 
     service_name: str = ""
-    """Name of Azure Cognitive Search service"""
+    """Name of Azure AI Search service"""
     index_name: str = ""
-    """Name of Index inside Azure Cognitive Search service"""
+    """Name of Index inside Azure AI Search service"""
     api_key: str = ""
     """API Key. Both Admin and Query keys work, but for reading data it's
     recommended to use a Query key."""
-    api_version: str = "2020-06-30"
+    api_version: str = "2023-11-01"
     """API version"""
     aiosession: Optional[aiohttp.ClientSession] = None
     """ClientSession, in case we want to reuse connection for better performance."""
     content_key: str = "content"
     """Key in a retrieved result to set as the Document page_content."""
     top_k: Optional[int] = None
     """Number of results to retrieve. Set to None to retrieve all results."""
@@ -41,29 +41,39 @@
         extra = Extra.forbid
         arbitrary_types_allowed = True
 
     @root_validator(pre=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that service name, index name and api key exists in environment."""
         values["service_name"] = get_from_dict_or_env(
-            values, "service_name", "AZURE_COGNITIVE_SEARCH_SERVICE_NAME"
+            values, "service_name", "AZURE_AI_SEARCH_SERVICE_NAME"
         )
         values["index_name"] = get_from_dict_or_env(
-            values, "index_name", "AZURE_COGNITIVE_SEARCH_INDEX_NAME"
+            values, "index_name", "AZURE_AI_SEARCH_INDEX_NAME"
         )
         values["api_key"] = get_from_dict_or_env(
-            values, "api_key", "AZURE_COGNITIVE_SEARCH_API_KEY"
+            values, "api_key", "AZURE_AI_SEARCH_API_KEY"
         )
         return values
 
     def _build_search_url(self, query: str) -> str:
-        url_suffix = get_from_env(
-            "", "AZURE_COGNITIVE_SEARCH_URL_SUFFIX", DEFAULT_URL_SUFFIX
-        )
-        base_url = f"https://{self.service_name}.{url_suffix}/"
+        url_suffix = get_from_env("", "AZURE_AI_SEARCH_URL_SUFFIX", DEFAULT_URL_SUFFIX)
+        if url_suffix in self.service_name and "https://" in self.service_name:
+            base_url = f"{self.service_name}/"
+        elif url_suffix in self.service_name and "https://" not in self.service_name:
+            base_url = f"https://{self.service_name}/"
+        elif url_suffix not in self.service_name and "https://" in self.service_name:
+            base_url = f"{self.service_name}.{url_suffix}/"
+        elif (
+            url_suffix not in self.service_name and "https://" not in self.service_name
+        ):
+            base_url = f"https://{self.service_name}.{url_suffix}/"
+        else:
+            # pass to Azure to throw a specific error
+            base_url = self.service_name
         endpoint_path = f"indexes/{self.index_name}/docs?api-version={self.api_version}"
         top_param = f"&$top={self.top_k}" if self.top_k else ""
         return base_url + endpoint_path + f"&search={query}" + top_param
 
     @property
     def _headers(self) -> Dict[str, str]:
         return {
@@ -108,7 +118,15 @@
     ) -> List[Document]:
         search_results = await self._asearch(query)
 
         return [
             Document(page_content=result.pop(self.content_key), metadata=result)
             for result in search_results
         ]
+
+
+# For backwards compatibility
+class AzureCognitiveSearchRetriever(AzureAISearchRetriever):
+    """`Azure Cognitive Search` service retriever.
+    This version of the retriever will soon be
+    depreciated. Please switch to AzureAISearchRetriever
+    """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/bedrock.py` & `gigachain_community-0.2.0/langchain_community/retrievers/bedrock.py`

 * *Files 14% similar despite different names*

```diff
@@ -84,20 +84,20 @@
             if values.get("endpoint_url"):
                 client_params["endpoint_url"] = values["endpoint_url"]
 
             values["client"] = session.client("bedrock-agent-runtime", **client_params)
 
             return values
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import boto3 python package. "
                 "Please install it with `pip install boto3`."
             )
         except UnknownServiceError as e:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Ensure that you have installed the latest boto3 package "
                 "that contains the API for `bedrock-runtime-agent`."
             ) from e
         except Exception as e:
             raise ValueError(
                 "Could not load credentials to authenticate with AWS client. "
                 "Please check that credentials in the specified "
@@ -111,18 +111,21 @@
             retrievalQuery={"text": query.strip()},
             knowledgeBaseId=self.knowledge_base_id,
             retrievalConfiguration=self.retrieval_config.dict(),
         )
         results = response["retrievalResults"]
         documents = []
         for result in results:
+            content = result["content"]["text"]
+            result.pop("content")
+            if "score" not in result:
+                result["score"] = 0
+            if "metadata" in result:
+                result["source_metadata"] = result.pop("metadata")
             documents.append(
                 Document(
-                    page_content=result["content"]["text"],
-                    metadata={
-                        "location": result["location"],
-                        "score": result["score"] if "score" in result else 0,
-                    },
+                    page_content=content,
+                    metadata=result,
                 )
             )
 
         return documents
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/bm25.py` & `gigachain_community-0.2.0/langchain_community/retrievers/bm25.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 from __future__ import annotations
 
 from typing import Any, Callable, Dict, Iterable, List, Optional
 
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
+from langchain_core.pydantic_v1 import Field
 from langchain_core.retrievers import BaseRetriever
 
 
 def default_preprocessing_func(text: str) -> List[str]:
     return text.split()
 
 
 class BM25Retriever(BaseRetriever):
     """`BM25` retriever without Elasticsearch."""
 
     vectorizer: Any
     """ BM25 vectorizer."""
-    docs: List[Document]
+    docs: List[Document] = Field(repr=False)
     """ List of documents."""
     k: int = 4
     """ Number of documents to return."""
     preprocess_func: Callable[[str], List[str]] = default_preprocessing_func
     """ Preprocessing function to use on the text before BM25 vectorization."""
 
     class Config:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/chaindesk.py` & `gigachain_community-0.2.0/langchain_community/retrievers/chaindesk.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/chatgpt_plugin_retriever.py` & `gigachain_community-0.2.0/langchain_community/retrievers/remote_retriever.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,90 +1,56 @@
-from __future__ import annotations
-
 from typing import List, Optional
 
 import aiohttp
 import requests
 from langchain_core.callbacks import (
     AsyncCallbackManagerForRetrieverRun,
     CallbackManagerForRetrieverRun,
 )
 from langchain_core.documents import Document
 from langchain_core.retrievers import BaseRetriever
 
 
-class ChatGPTPluginRetriever(BaseRetriever):
-    """`ChatGPT plugin` retriever."""
+class RemoteLangChainRetriever(BaseRetriever):
+    """`LangChain API` retriever."""
 
     url: str
-    """URL of the ChatGPT plugin."""
-    bearer_token: str
-    """Bearer token for the ChatGPT plugin."""
-    top_k: int = 3
-    """Number of documents to return."""
-    filter: Optional[dict] = None
-    """Filter to apply to the results."""
-    aiosession: Optional[aiohttp.ClientSession] = None
-    """Aiohttp session to use for requests."""
-
-    class Config:
-        """Configuration for this pydantic object."""
-
-        arbitrary_types_allowed = True
-        """Allow arbitrary types."""
+    """URL of the remote LangChain API."""
+    headers: Optional[dict] = None
+    """Headers to use for the request."""
+    input_key: str = "message"
+    """Key to use for the input in the request."""
+    response_key: str = "response"
+    """Key to use for the response in the request."""
+    page_content_key: str = "page_content"
+    """Key to use for the page content in the response."""
+    metadata_key: str = "metadata"
+    """Key to use for the metadata in the response."""
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
-        url, json, headers = self._create_request(query)
-        response = requests.post(url, json=json, headers=headers)
-        results = response.json()["results"][0]["results"]
-        docs = []
-        for d in results:
-            content = d.pop("text")
-            metadata = d.pop("metadata", d)
-            if metadata.get("source_id"):
-                metadata["source"] = metadata.pop("source_id")
-            docs.append(Document(page_content=content, metadata=metadata))
-        return docs
+        response = requests.post(
+            self.url, json={self.input_key: query}, headers=self.headers
+        )
+        result = response.json()
+        return [
+            Document(
+                page_content=r[self.page_content_key], metadata=r[self.metadata_key]
+            )
+            for r in result[self.response_key]
+        ]
 
     async def _aget_relevant_documents(
         self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
     ) -> List[Document]:
-        url, json, headers = self._create_request(query)
-
-        if not self.aiosession:
-            async with aiohttp.ClientSession() as session:
-                async with session.post(url, headers=headers, json=json) as response:
-                    res = await response.json()
-        else:
-            async with self.aiosession.post(
-                url, headers=headers, json=json
+        async with aiohttp.ClientSession() as session:
+            async with session.request(
+                "POST", self.url, headers=self.headers, json={self.input_key: query}
             ) as response:
-                res = await response.json()
-
-        results = res["results"][0]["results"]
-        docs = []
-        for d in results:
-            content = d.pop("text")
-            metadata = d.pop("metadata", d)
-            if metadata.get("source_id"):
-                metadata["source"] = metadata.pop("source_id")
-            docs.append(Document(page_content=content, metadata=metadata))
-        return docs
-
-    def _create_request(self, query: str) -> tuple[str, dict, dict]:
-        url = f"{self.url}/query"
-        json = {
-            "queries": [
-                {
-                    "query": query,
-                    "filter": self.filter,
-                    "top_k": self.top_k,
-                }
-            ]
-        }
-        headers = {
-            "Content-Type": "application/json",
-            "Authorization": f"Bearer {self.bearer_token}",
-        }
-        return url, json, headers
+                result = await response.json()
+        return [
+            Document(
+                page_content=r[self.page_content_key], metadata=r[self.metadata_key]
+            )
+            for r in result[self.response_key]
+        ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/cohere_rag_retriever.py` & `gigachain_community-0.2.0/langchain_community/retrievers/cohere_rag_retriever.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Any, Dict, List
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForRetrieverRun,
     CallbackManagerForRetrieverRun,
 )
 from langchain_core.documents import Document
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import HumanMessage
@@ -13,18 +14,22 @@
 from langchain_core.retrievers import BaseRetriever
 
 if TYPE_CHECKING:
     from langchain_core.messages import BaseMessage
 
 
 def _get_docs(response: Any) -> List[Document]:
-    docs = [
-        Document(page_content=doc["snippet"], metadata=doc)
-        for doc in response.generation_info["documents"]
-    ]
+    docs = (
+        []
+        if "documents" not in response.generation_info
+        else [
+            Document(page_content=doc["snippet"], metadata=doc)
+            for doc in response.generation_info["documents"]
+        ]
+    )
     docs.append(
         Document(
             page_content=response.message.content,
             metadata={
                 "type": "model_response",
                 "citations": response.generation_info["citations"],
                 "search_results": response.generation_info["search_results"],
@@ -32,14 +37,19 @@
                 "token_count": response.generation_info["token_count"],
             },
         )
     )
     return docs
 
 
+@deprecated(
+    since="0.0.30",
+    removal="0.3.0",
+    alternative_import="langchain_cohere.CohereRagRetriever",
+)
 class CohereRagRetriever(BaseRetriever):
     """Cohere Chat API with RAG."""
 
     connectors: List[Dict] = Field(default_factory=lambda: [{"id": "web-search"}])
     """
     When specified, the model's reply will be enriched with information found by
     querying each of the connectors (RAG). These will be returned as langchain
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/databerry.py` & `gigachain_community-0.2.0/langchain_community/retrievers/databerry.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/docarray.py` & `gigachain_community-0.2.0/langchain_community/retrievers/docarray.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/elastic_search_bm25.py` & `gigachain_community-0.2.0/langchain_community/retrievers/elastic_search_bm25.py`

 * *Files 1% similar despite different names*

```diff
@@ -99,15 +99,15 @@
 
         Returns:
             List of ids from adding the texts into the retriever.
         """
         try:
             from elasticsearch.helpers import bulk
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import elasticsearch python package. "
                 "Please install it with `pip install elasticsearch`."
             )
         requests = []
         ids = []
         for i, text in enumerate(texts):
             _id = str(uuid.uuid4())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/embedchain.py` & `gigachain_community-0.2.0/langchain_community/retrievers/embedchain.py`

 * *Files 5% similar despite different names*

```diff
@@ -61,11 +61,14 @@
         res = self.client.search(query)
 
         docs = []
         for r in res:
             docs.append(
                 Document(
                     page_content=r["context"],
-                    metadata={"source": r["source"], "document_id": r["document_id"]},
+                    metadata={
+                        "source": r["metadata"]["url"],
+                        "document_id": r["metadata"]["doc_id"],
+                    },
                 )
             )
         return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/google_cloud_documentai_warehouse.py` & `gigachain_community-0.2.0/langchain_community/retrievers/google_cloud_documentai_warehouse.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 """Retriever wrapper for Google Cloud Document AI Warehouse."""
+
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import root_validator
 from langchain_core.retrievers import BaseRetriever
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.utilities.vertexai import get_client_info
@@ -16,14 +18,19 @@
         SearchDocumentsRequest,
     )
     from google.cloud.contentwarehouse_v1.services.document_service.pagers import (
         SearchDocumentsPager,
     )
 
 
+@deprecated(
+    since="0.0.32",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.DocumentAIWarehouseRetriever",
+)
 class GoogleDocumentAIWarehouseRetriever(BaseRetriever):
     """A retriever based on Document AI Warehouse.
 
     Documents should be created and documents should be uploaded
         in a separate flow, and this retriever uses only Document AI
         schema_id provided to search for revelant documents.
 
@@ -40,15 +47,15 @@
     qa_size_limit: int = 5
     """The limit on the number of documents returned."""
     client: "DocumentServiceClient" = None  #: :meta private:
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validates the environment."""
-        try:  # noqa: F401
+        try:
             from google.cloud.contentwarehouse_v1 import DocumentServiceClient
         except ImportError as exc:
             raise ImportError(
                 "google.cloud.contentwarehouse is not installed."
                 "Please install it with pip install google-cloud-contentwarehouse"
             ) from exc
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/google_vertex_ai_search.py` & `gigachain_community-0.2.0/langchain_community/retrievers/google_vertex_ai_search.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """Retriever wrapper for Google Vertex AI Search."""
+
 from __future__ import annotations
 
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.retrievers import BaseRetriever
 from langchain_core.utils import get_from_dict_or_env
 
 from langchain_community.utilities.vertexai import get_client_info
@@ -20,83 +22,77 @@
         SearchServiceClient,
     )
 
 
 class _BaseGoogleVertexAISearchRetriever(BaseModel):
     project_id: str
     """Google Cloud Project ID."""
-    data_store_id: str
+    data_store_id: Optional[str] = None
     """Vertex AI Search data store ID."""
+    search_engine_id: Optional[str] = None
+    """Vertex AI Search app ID."""
     location_id: str = "global"
     """Vertex AI Search data store location."""
     serving_config_id: str = "default_config"
     """Vertex AI Search serving config ID."""
     credentials: Any = None
     """The default custom credentials (google.auth.credentials.Credentials) to use
     when making API calls. If not provided, credentials will be ascertained from
     the environment."""
-    engine_data_type: int = Field(default=0, ge=0, le=2)
-    """ Defines the Vertex AI Search data type
+    engine_data_type: int = Field(default=0, ge=0, le=3)
+    """ Defines the Vertex AI Search app data type
     0 - Unstructured data 
     1 - Structured data
     2 - Website data
+    3 - Blended search
     """
 
     @root_validator(pre=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validates the environment."""
         try:
             from google.cloud import discoveryengine_v1beta  # noqa: F401
         except ImportError as exc:
             raise ImportError(
                 "google.cloud.discoveryengine is not installed."
                 "Please install it with pip install "
-                "google-cloud-discoveryengine>=0.11.0"
+                "google-cloud-discoveryengine>=0.11.10"
             ) from exc
         try:
             from google.api_core.exceptions import InvalidArgument  # noqa: F401
         except ImportError as exc:
             raise ImportError(
                 "google.api_core.exceptions is not installed. "
                 "Please install it with pip install google-api-core"
             ) from exc
 
         values["project_id"] = get_from_dict_or_env(values, "project_id", "PROJECT_ID")
 
         try:
-            # For backwards compatibility
-            search_engine_id = get_from_dict_or_env(
+            values["data_store_id"] = get_from_dict_or_env(
+                values, "data_store_id", "DATA_STORE_ID"
+            )
+            values["search_engine_id"] = get_from_dict_or_env(
                 values, "search_engine_id", "SEARCH_ENGINE_ID"
             )
-
-            if search_engine_id:
-                import warnings
-
-                warnings.warn(
-                    "The `search_engine_id` parameter is deprecated. Use `data_store_id` instead.",  # noqa: E501
-                    DeprecationWarning,
-                )
-                values["data_store_id"] = search_engine_id
-        except:  # noqa: E722
+        except Exception:
             pass
 
-        values["data_store_id"] = get_from_dict_or_env(
-            values, "data_store_id", "DATA_STORE_ID"
-        )
-
         return values
 
     @property
     def client_options(self) -> "ClientOptions":
         from google.api_core.client_options import ClientOptions
 
         return ClientOptions(
-            api_endpoint=f"{self.location_id}-discoveryengine.googleapis.com"
-            if self.location_id != "global"
-            else None
+            api_endpoint=(
+                f"{self.location_id}-discoveryengine.googleapis.com"
+                if self.location_id != "global"
+                else None
+            )
         )
 
     def _convert_structured_search_response(
         self, results: Sequence[SearchResult]
     ) -> List[Document]:
         """Converts a sequence of search results to a list of LangChain documents."""
         import json
@@ -138,22 +134,23 @@
             doc_metadata = document_dict.get("struct_data", {})
             doc_metadata["id"] = document_dict["id"]
 
             if chunk_type not in derived_struct_data:
                 continue
 
             for chunk in derived_struct_data[chunk_type]:
-                doc_metadata["source"] = derived_struct_data.get("link", "")
+                chunk_metadata = doc_metadata.copy()
+                chunk_metadata["source"] = derived_struct_data.get("link", "")
 
                 if chunk_type == "extractive_answers":
-                    doc_metadata["source"] += f":{chunk.get('pageNumber', '')}"
+                    chunk_metadata["source"] += f":{chunk.get('pageNumber', '')}"
 
                 documents.append(
                     Document(
-                        page_content=chunk.get("content", ""), metadata=doc_metadata
+                        page_content=chunk.get("content", ""), metadata=chunk_metadata
                     )
                 )
 
         return documents
 
     def _convert_website_search_response(
         self, results: Sequence[SearchResult], chunk_type: str
@@ -184,25 +181,30 @@
                 documents.append(
                     Document(
                         page_content=chunk.get(text_field, ""), metadata=doc_metadata
                     )
                 )
 
         if not documents:
-            print(f"No {chunk_type} could be found.")
+            print(f"No {chunk_type} could be found.")  # noqa: T201
             if chunk_type == "extractive_answers":
-                print(
+                print(  # noqa: T201
                     "Make sure that your data store is using Advanced Website "
                     "Indexing.\n"
                     "https://cloud.google.com/generative-ai-app-builder/docs/about-advanced-features#advanced-website-indexing"  # noqa: E501
                 )
 
         return documents
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.VertexAISearchRetriever",
+)
 class GoogleVertexAISearchRetriever(BaseRetriever, _BaseGoogleVertexAISearchRetriever):
     """`Google Vertex AI Search` retriever.
 
     For a detailed explanation of the Vertex AI Search concepts
     and configuration parameters, refer to the product documentation.
     https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction
     """
@@ -266,20 +268,32 @@
         # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store
         self._client = SearchServiceClient(
             credentials=self.credentials,
             client_options=self.client_options,
             client_info=get_client_info(module="vertex-ai-search"),
         )
 
-        self._serving_config = self._client.serving_config_path(
-            project=self.project_id,
-            location=self.location_id,
-            data_store=self.data_store_id,
-            serving_config=self.serving_config_id,
-        )
+        if self.engine_data_type == 3 and not self.search_engine_id:
+            raise ValueError(
+                "search_engine_id must be specified for blended search apps."
+            )
+
+        if self.search_engine_id:
+            self._serving_config = f"projects/{self.project_id}/locations/{self.location_id}/collections/default_collection/engines/{self.search_engine_id}/servingConfigs/default_config"  # noqa: E501
+        elif self.data_store_id:
+            self._serving_config = self._client.serving_config_path(
+                project=self.project_id,
+                location=self.location_id,
+                data_store=self.data_store_id,
+                serving_config=self.serving_config_id,
+            )
+        else:
+            raise ValueError(
+                "Either data_store_id or search_engine_id must be specified."
+            )
 
     def _create_search_request(self, query: str) -> SearchRequest:
         """Prepares a SearchRequest object."""
         from google.cloud.discoveryengine_v1beta import SearchRequest
 
         query_expansion_spec = SearchRequest.QueryExpansionSpec(
             condition=self.query_expansion_condition,
@@ -303,27 +317,27 @@
                     )
                 )
             content_search_spec = SearchRequest.ContentSearchSpec(
                 extractive_content_spec=extractive_content_spec
             )
         elif self.engine_data_type == 1:
             content_search_spec = None
-        elif self.engine_data_type == 2:
+        elif self.engine_data_type in (2, 3):
             content_search_spec = SearchRequest.ContentSearchSpec(
                 extractive_content_spec=SearchRequest.ContentSearchSpec.ExtractiveContentSpec(
                     max_extractive_answer_count=self.max_extractive_answer_count,
                 ),
                 snippet_spec=SearchRequest.ContentSearchSpec.SnippetSpec(
                     return_snippet=True
                 ),
             )
         else:
             raise NotImplementedError(
                 "Only data store type 0 (Unstructured), 1 (Structured),"
-                "or 2 (Website) are supported currently."
+                "2 (Website), or 3 (Blended) are supported currently."
                 + f" Got {self.engine_data_type}"
             )
 
         return SearchRequest(
             query=query,
             filter=self.filter,
             serving_config=self._serving_config,
@@ -333,14 +347,19 @@
             spell_correction_spec=spell_correction_spec,
         )
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
         """Get documents relevant for a query."""
+        return self.get_relevant_documents_with_response(query)[0]
+
+    def get_relevant_documents_with_response(
+        self, query: str
+    ) -> Tuple[List[Document], Any]:
         from google.api_core.exceptions import InvalidArgument
 
         search_request = self._create_search_request(query)
 
         try:
             response = self._client.search(search_request)
         except InvalidArgument as exc:
@@ -356,31 +375,36 @@
                 else "extractive_segments"
             )
             documents = self._convert_unstructured_search_response(
                 response.results, chunk_type
             )
         elif self.engine_data_type == 1:
             documents = self._convert_structured_search_response(response.results)
-        elif self.engine_data_type == 2:
+        elif self.engine_data_type in (2, 3):
             chunk_type = (
                 "extractive_answers" if self.get_extractive_answers else "snippets"
             )
             documents = self._convert_website_search_response(
                 response.results, chunk_type
             )
         else:
             raise NotImplementedError(
                 "Only data store type 0 (Unstructured), 1 (Structured),"
-                "or 2 (Website) are supported currently."
+                "2 (Website), or 3 (Blended) are supported currently."
                 + f" Got {self.engine_data_type}"
             )
 
-        return documents
+        return documents, response
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.VertexAIMultiTurnSearchRetriever",
+)
 class GoogleVertexAIMultiTurnSearchRetriever(
     BaseRetriever, _BaseGoogleVertexAISearchRetriever
 ):
     """`Google Vertex AI Search` retriever for multi-turn conversations."""
 
     conversation_id: str = "-"
     """Vertex AI Search Conversation ID."""
@@ -403,24 +427,27 @@
 
         self._client = ConversationalSearchServiceClient(
             credentials=self.credentials,
             client_options=self.client_options,
             client_info=get_client_info(module="vertex-ai-search"),
         )
 
+        if not self.data_store_id:
+            raise ValueError("data_store_id is required for MultiTurnSearchRetriever.")
+
         self._serving_config = self._client.serving_config_path(
             project=self.project_id,
             location=self.location_id,
             data_store=self.data_store_id,
             serving_config=self.serving_config_id,
         )
 
-        if self.engine_data_type == 1:
+        if self.engine_data_type == 1 or self.engine_data_type == 3:
             raise NotImplementedError(
-                "Data store type 1 (Structured)"
+                "Data store type 1 (Structured) and 3 (Blended)"
                 "is not currently supported for multi-turn search."
                 + f" Got {self.engine_data_type}"
             )
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/kay.py` & `gigachain_community-0.2.0/langchain_community/retrievers/kay.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/kendra.py` & `gigachain_community-0.2.0/langchain_community/retrievers/kendra.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,31 @@
 import re
 from abc import ABC, abstractmethod
-from typing import Any, Callable, Dict, List, Literal, Optional, Sequence, Union
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    List,
+    Literal,
+    Optional,
+    Sequence,
+    Union,
+)
 
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
-from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator, validator
+from langchain_core.pydantic_v1 import (
+    BaseModel,
+    Extra,
+    Field,
+    root_validator,
+    validator,
+)
 from langchain_core.retrievers import BaseRetriever
+from typing_extensions import Annotated
 
 
 def clean_excerpt(excerpt: str) -> str:
     """Clean an excerpt from Kendra.
 
     Args:
         excerpt: The excerpt to clean.
@@ -149,14 +165,16 @@
     """The ID of the relevant result item."""
     DocumentId: Optional[str]
     """The document ID."""
     DocumentURI: Optional[str]
     """The document URI."""
     DocumentAttributes: Optional[List[DocumentAttribute]] = []
     """The document attributes."""
+    ScoreAttributes: Optional[dict]
+    """The kendra score confidence"""
 
     @abstractmethod
     def get_title(self) -> str:
         """Document title."""
 
     @abstractmethod
     def get_excerpt(self) -> str:
@@ -174,31 +192,38 @@
         """
         return {}
 
     def get_document_attributes_dict(self) -> Dict[str, DocumentAttributeValueType]:
         """Document attributes dict."""
         return {attr.Key: attr.Value.value for attr in (self.DocumentAttributes or [])}
 
+    def get_score_attribute(self) -> str:
+        """Document Score Confidence"""
+        if self.ScoreAttributes is not None:
+            return self.ScoreAttributes["ScoreConfidence"]
+        else:
+            return "NOT_AVAILABLE"
+
     def to_doc(
         self, page_content_formatter: Callable[["ResultItem"], str] = combined_text
     ) -> Document:
         """Converts this item to a Document."""
         page_content = page_content_formatter(self)
         metadata = self.get_additional_metadata()
         metadata.update(
             {
                 "result_id": self.Id,
                 "document_id": self.DocumentId,
                 "source": self.DocumentURI,
                 "title": self.get_title(),
                 "excerpt": self.get_excerpt(),
                 "document_attributes": self.get_document_attributes_dict(),
+                "score": self.get_score_attribute(),
             }
         )
-
         return Document(page_content=page_content, metadata=metadata)
 
 
 class QueryResultItem(ResultItem):
     """Query API result item."""
 
     DocumentTitle: TextWithHighLights
@@ -286,14 +311,23 @@
 
     QueryId: str
     """The ID of the query."""
     ResultItems: List[RetrieveResultItem]
     """The result items."""
 
 
+KENDRA_CONFIDENCE_MAPPING = {
+    "NOT_AVAILABLE": 0.0,
+    "LOW": 0.25,
+    "MEDIUM": 0.50,
+    "HIGH": 0.75,
+    "VERY_HIGH": 1.0,
+}
+
+
 class AmazonKendraRetriever(BaseRetriever):
     """`Amazon Kendra Index` retriever.
 
     Args:
         index_id: Kendra index id
 
         region_name: The aws region e.g., `us-west-2`.
@@ -306,14 +340,18 @@
             EC2 instance, credentials from IMDS will be used.
 
         top_k: No of results to return
 
         attribute_filter: Additional filtering of results based on metadata
             See: https://docs.aws.amazon.com/kendra/latest/APIReference
 
+        document_relevance_override_configurations: Overrides relevance tuning
+            configurations of fields/attributes set at the index level
+            See: https://docs.aws.amazon.com/kendra/latest/APIReference
+
         page_content_formatter: generates the Document page_content
             allowing access to all result item attributes. By default, it uses
             the item's title and excerpt.
 
         client: boto3 client for Kendra
 
         user_context: Provides information about the user context
@@ -329,17 +367,19 @@
     """
 
     index_id: str
     region_name: Optional[str] = None
     credentials_profile_name: Optional[str] = None
     top_k: int = 3
     attribute_filter: Optional[Dict] = None
+    document_relevance_override_configurations: Optional[List[Dict]] = None
     page_content_formatter: Callable[[ResultItem], str] = combined_text
     client: Any
     user_context: Optional[Dict] = None
+    min_score_confidence: Annotated[Optional[float], Field(ge=0.0, le=1.0)]
 
     @validator("top_k")
     def validate_top_k(cls, value: int) -> int:
         if value < 0:
             raise ValueError(f"top_k ({value}) cannot be negative.")
         return value
 
@@ -361,15 +401,15 @@
             if values.get("region_name"):
                 client_params["region_name"] = values["region_name"]
 
             values["client"] = session.client("kendra", **client_params)
 
             return values
         except ImportError:
-            raise ModuleNotFoundError(
+            raise ImportError(
                 "Could not import boto3 python package. "
                 "Please install it with `pip install boto3`."
             )
         except Exception as e:
             raise ValueError(
                 "Could not load credentials to authenticate with AWS client. "
                 "Please check that credentials in the specified "
@@ -382,14 +422,18 @@
             # truncate the query to ensure that
             # there is no validation exception from Kendra.
             "QueryText": query.strip()[0:999],
             "PageSize": self.top_k,
         }
         if self.attribute_filter is not None:
             kendra_kwargs["AttributeFilter"] = self.attribute_filter
+        if self.document_relevance_override_configurations is not None:
+            kendra_kwargs[
+                "DocumentRelevanceOverrideConfigurations"
+            ] = self.document_relevance_override_configurations
         if self.user_context is not None:
             kendra_kwargs["UserContext"] = self.user_context
 
         response = self.client.retrieve(**kendra_kwargs)
         r_result = RetrieveResult.parse_obj(response)
         if r_result.ResultItems:
             return r_result.ResultItems
@@ -402,24 +446,43 @@
     def _get_top_k_docs(self, result_items: Sequence[ResultItem]) -> List[Document]:
         top_docs = [
             item.to_doc(self.page_content_formatter)
             for item in result_items[: self.top_k]
         ]
         return top_docs
 
+    def _filter_by_score_confidence(self, docs: List[Document]) -> List[Document]:
+        """
+        Filter out the records that have a score confidence
+        greater than the required threshold.
+        """
+        if not self.min_score_confidence:
+            return docs
+        filtered_docs = [
+            item
+            for item in docs
+            if (
+                item.metadata.get("score") is not None
+                and isinstance(item.metadata["score"], str)
+                and KENDRA_CONFIDENCE_MAPPING.get(item.metadata["score"], 0.0)
+                >= self.min_score_confidence
+            )
+        ]
+        return filtered_docs
+
     def _get_relevant_documents(
         self,
         query: str,
         *,
         run_manager: CallbackManagerForRetrieverRun,
     ) -> List[Document]:
         """Run search on Kendra index and get top k documents
 
         Example:
         .. code-block:: python
 
-            docs = retriever.get_relevant_documents('This is my query')
+            docs = retriever.invoke('This is my query')
 
         """
         result_items = self._kendra_query(query)
         top_k_docs = self._get_top_k_docs(result_items)
-        return top_k_docs
+        return self._filter_by_score_confidence(top_k_docs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/knn.py` & `gigachain_community-0.2.0/langchain_community/retrievers/knn.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """KNN Retriever.
 Largely based on
 https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.ipynb"""
 
 from __future__ import annotations
 
 import concurrent.futures
-from typing import Any, List, Optional
+from typing import Any, Iterable, List, Optional
 
 import numpy as np
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.retrievers import BaseRetriever
 
@@ -34,31 +34,54 @@
 
     embeddings: Embeddings
     """Embeddings model to use."""
     index: Any
     """Index of embeddings."""
     texts: List[str]
     """List of texts to index."""
+    metadatas: Optional[List[dict]] = None
+    """List of metadatas corresponding with each text."""
     k: int = 4
     """Number of results to return."""
     relevancy_threshold: Optional[float] = None
     """Threshold for relevancy."""
 
     class Config:
-
         """Configuration for this pydantic object."""
 
         arbitrary_types_allowed = True
 
     @classmethod
     def from_texts(
-        cls, texts: List[str], embeddings: Embeddings, **kwargs: Any
+        cls,
+        texts: List[str],
+        embeddings: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        **kwargs: Any,
     ) -> KNNRetriever:
         index = create_index(texts, embeddings)
-        return cls(embeddings=embeddings, index=index, texts=texts, **kwargs)
+        return cls(
+            embeddings=embeddings,
+            index=index,
+            texts=texts,
+            metadatas=metadatas,
+            **kwargs,
+        )
+
+    @classmethod
+    def from_documents(
+        cls,
+        documents: Iterable[Document],
+        embeddings: Embeddings,
+        **kwargs: Any,
+    ) -> KNNRetriever:
+        texts, metadatas = zip(*((d.page_content, d.metadata) for d in documents))
+        return cls.from_texts(
+            texts=texts, embeddings=embeddings, metadatas=metadatas, **kwargs
+        )
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
         query_embeds = np.array(self.embeddings.embed_query(query))
         # calc L2 norm
         index_embeds = self.index / np.sqrt((self.index**2).sum(1, keepdims=True))
@@ -67,15 +90,18 @@
         similarities = index_embeds.dot(query_embeds)
         sorted_ix = np.argsort(-similarities)
 
         denominator = np.max(similarities) - np.min(similarities) + 1e-6
         normalized_similarities = (similarities - np.min(similarities)) / denominator
 
         top_k_results = [
-            Document(page_content=self.texts[row])
+            Document(
+                page_content=self.texts[row],
+                metadata=self.metadatas[row] if self.metadatas else {},
+            )
             for row in sorted_ix[0 : self.k]
             if (
                 self.relevancy_threshold is None
                 or normalized_similarities[row] >= self.relevancy_threshold
             )
         ]
         return top_k_results
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/llama_index.py` & `gigachain_community-0.2.0/langchain_community/retrievers/llama_index.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,30 +18,30 @@
     """Keyword arguments to pass to the query method."""
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
         """Get documents relevant for a query."""
         try:
-            from llama_index.indices.base import BaseGPTIndex
-            from llama_index.response.schema import Response
+            from llama_index.core.base.response.schema import Response
+            from llama_index.core.indices.base import BaseGPTIndex
         except ImportError:
             raise ImportError(
                 "You need to install `pip install llama-index` to use this retriever."
             )
         index = cast(BaseGPTIndex, self.index)
 
-        response = index.query(query, response_mode="no_text", **self.query_kwargs)
+        response = index.query(query, **self.query_kwargs)
         response = cast(Response, response)
         # parse source nodes
         docs = []
         for source_node in response.source_nodes:
-            metadata = source_node.extra_info or {}
+            metadata = source_node.metadata or {}
             docs.append(
-                Document(page_content=source_node.source_text, metadata=metadata)
+                Document(page_content=source_node.get_content(), metadata=metadata)
             )
         return docs
 
 
 class LlamaIndexGraphRetriever(BaseRetriever):
     """`LlamaIndex` graph data structure retriever.
 
@@ -54,19 +54,19 @@
     """List of query configs to pass to the query method."""
 
     def _get_relevant_documents(
         self, query: str, *, run_manager: CallbackManagerForRetrieverRun
     ) -> List[Document]:
         """Get documents relevant for a query."""
         try:
-            from llama_index.composability.graph import (
+            from llama_index.core.base.response.schema import Response
+            from llama_index.core.composability.base import (
                 QUERY_CONFIG_TYPE,
                 ComposableGraph,
             )
-            from llama_index.response.schema import Response
         except ImportError:
             raise ImportError(
                 "You need to install `pip install llama-index` to use this retriever."
             )
         graph = cast(ComposableGraph, self.graph)
 
         # for now, inject response_mode="no_text" into query configs
@@ -75,12 +75,12 @@
         query_configs = cast(List[QUERY_CONFIG_TYPE], self.query_configs)
         response = graph.query(query, query_configs=query_configs)
         response = cast(Response, response)
 
         # parse source nodes
         docs = []
         for source_node in response.source_nodes:
-            metadata = source_node.extra_info or {}
+            metadata = source_node.metadata or {}
             docs.append(
-                Document(page_content=source_node.source_text, metadata=metadata)
+                Document(page_content=source_node.get_content(), metadata=metadata)
             )
         return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/metal.py` & `gigachain_community-0.2.0/langchain_community/retrievers/metal.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/milvus.py` & `gigachain_community-0.2.0/langchain_community/retrievers/milvus.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Milvus Retriever"""
+
 import warnings
 from typing import Any, Dict, List, Optional
 
 from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import root_validator
@@ -14,27 +15,29 @@
 
 
 class MilvusRetriever(BaseRetriever):
     """`Milvus API` retriever."""
 
     embedding_function: Embeddings
     collection_name: str = "LangChainCollection"
+    collection_properties: Optional[Dict[str, Any]] = None
     connection_args: Optional[Dict[str, Any]] = None
     consistency_level: str = "Session"
     search_params: Optional[dict] = None
 
     store: Milvus
     retriever: BaseRetriever
 
     @root_validator(pre=True)
     def create_retriever(cls, values: Dict) -> Dict:
         """Create the Milvus store and retriever."""
         values["store"] = Milvus(
             values["embedding_function"],
             values["collection_name"],
+            values["collection_properties"],
             values["connection_args"],
             values["consistency_level"],
         )
         values["retriever"] = values["store"].as_retriever(
             search_kwargs={"param": values["search_params"]}
         )
         return values
@@ -53,15 +56,15 @@
     def _get_relevant_documents(
         self,
         query: str,
         *,
         run_manager: CallbackManagerForRetrieverRun,
         **kwargs: Any,
     ) -> List[Document]:
-        return self.retriever.get_relevant_documents(
+        return self.retriever.invoke(
             query, run_manager=run_manager.get_child(), **kwargs
         )
 
 
 def MilvusRetreiver(*args: Any, **kwargs: Any) -> MilvusRetriever:
     """Deprecated MilvusRetreiver. Please use MilvusRetriever ('i' before 'e') instead.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/outline.py` & `gigachain_community-0.2.0/langchain_community/retrievers/outline.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/pinecone_hybrid_search.py` & `gigachain_community-0.2.0/langchain_community/retrievers/pinecone_hybrid_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/pubmed.py` & `gigachain_community-0.2.0/langchain_community/retrievers/pubmed.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/qdrant_sparse_vector_retriever.py` & `gigachain_community-0.2.0/langchain_community/retrievers/qdrant_sparse_vector_retriever.py`

 * *Files 1% similar despite different names*

```diff
@@ -116,15 +116,18 @@
             ),
             limit=self.k,
             with_vectors=False,
             **self.search_options,
         )
         return [
             Qdrant._document_from_scored_point(
-                point, self.content_payload_key, self.metadata_payload_key
+                point,
+                self.collection_name,
+                self.content_payload_key,
+                self.metadata_payload_key,
             )
             for point in results
         ]
 
     def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:
         """Run more documents through the embeddings and add to the vectorstore.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/svm.py` & `gigachain_community-0.2.0/langchain_community/retrievers/svm.py`

 * *Files 0% similar despite different names*

```diff
@@ -42,15 +42,14 @@
     """List of metadatas corresponding with each text."""
     k: int = 4
     """Number of results to return."""
     relevancy_threshold: Optional[float] = None
     """Threshold for relevancy."""
 
     class Config:
-
         """Configuration for this pydantic object."""
 
         arbitrary_types_allowed = True
 
     @classmethod
     def from_texts(
         cls,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/tavily_search_api.py` & `gigachain_community-0.2.0/langchain_community/retrievers/tavily_search_api.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/vespa_retriever.py` & `gigachain_community-0.2.0/langchain_community/retrievers/vespa_retriever.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/weaviate_hybrid_search.py` & `gigachain_community-0.2.0/langchain_community/retrievers/weaviate_hybrid_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/wikipedia.py` & `gigachain_community-0.2.0/langchain_community/retrievers/wikipedia.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/you.py` & `gigachain_community-0.2.0/langchain_community/document_loaders/surrealdb.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,64 +1,95 @@
+import asyncio
+import json
+import logging
 from typing import Any, Dict, List, Optional
 
-from langchain_core.callbacks import CallbackManagerForRetrieverRun
 from langchain_core.documents import Document
-from langchain_core.pydantic_v1 import root_validator
-from langchain_core.retrievers import BaseRetriever
-from langchain_core.utils import get_from_dict_or_env
-
-
-class YouRetriever(BaseRetriever):
-    """`You` retriever that uses You.com's search API.
-
-    To connect to the You.com api requires an API key which
-    you can get by emailing api@you.com.
-    You can check out our docs at https://documentation.you.com.
-
-    You need to set the environment variable `YDC_API_KEY` for retriever to operate.
-    """
-
-    ydc_api_key: str
-    k: Optional[int] = None
-    n_hits: Optional[int] = None
-    n_snippets_per_hit: Optional[int] = None
-    endpoint_type: str = "web"
-
-    @root_validator(pre=True)
-    def validate_client(
-        cls,
-        values: Dict[str, Any],
-    ) -> Dict[str, Any]:
-        values["ydc_api_key"] = get_from_dict_or_env(
-            values, "ydc_api_key", "YDC_API_KEY"
-        )
-        return values
 
-    def _get_relevant_documents(
-        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
-    ) -> List[Document]:
-        import requests
-
-        headers = {"X-API-Key": self.ydc_api_key}
-        if self.endpoint_type == "web":
-            results = requests.get(
-                f"https://api.ydc-index.io/search?query={query}",
-                headers=headers,
-            ).json()
-
-            docs = []
-            n_hits = self.n_hits or len(results["hits"])
-            for hit in results["hits"][:n_hits]:
-                n_snippets_per_hit = self.n_snippets_per_hit or len(hit["snippets"])
-                for snippet in hit["snippets"][:n_snippets_per_hit]:
-                    docs.append(Document(page_content=snippet))
-                    if self.k is not None and len(docs) >= self.k:
-                        return docs
-            return docs
-        elif self.endpoint_type == "snippet":
-            results = requests.get(
-                f"https://api.ydc-index.io/snippet_search?query={query}",
-                headers=headers,
-            ).json()
-            return [Document(page_content=snippet) for snippet in results]
+from langchain_community.document_loaders.base import BaseLoader
+
+logger = logging.getLogger(__name__)
+
+
+class SurrealDBLoader(BaseLoader):
+    """Load SurrealDB documents."""
+
+    def __init__(
+        self,
+        filter_criteria: Optional[Dict] = None,
+        **kwargs: Any,
+    ) -> None:
+        try:
+            from surrealdb import Surreal
+        except ImportError as e:
+            raise ImportError(
+                """Cannot import from surrealdb.
+                please install with `pip install surrealdb`."""
+            ) from e
+
+        self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
+
+        if self.dburl[0:2] == "ws":
+            self.sdb = Surreal(self.dburl)
         else:
-            raise RuntimeError(f"Invalid endpoint type provided {self.endpoint_type}")
+            raise ValueError("Only websocket connections are supported at this time.")
+
+        self.filter_criteria = filter_criteria or {}
+
+        if "table" in self.filter_criteria:
+            raise ValueError(
+                "key `table` is not a valid criteria for `filter_criteria` argument."
+            )
+
+        self.ns = kwargs.pop("ns", "langchain")
+        self.db = kwargs.pop("db", "database")
+        self.table = kwargs.pop("table", "documents")
+        self.sdb = Surreal(self.dburl)
+        self.kwargs = kwargs
+
+    async def initialize(self) -> None:
+        """
+        Initialize connection to surrealdb database
+        and authenticate if credentials are provided
+        """
+        await self.sdb.connect()
+        if "db_user" in self.kwargs and "db_pass" in self.kwargs:
+            user = self.kwargs.get("db_user")
+            password = self.kwargs.get("db_pass")
+            await self.sdb.signin({"user": user, "pass": password})
+
+        await self.sdb.use(self.ns, self.db)
+
+    def load(self) -> List[Document]:
+        async def _load() -> List[Document]:
+            await self.initialize()
+            return await self.aload()
+
+        return asyncio.run(_load())
+
+    async def aload(self) -> List[Document]:
+        """Load data into Document objects."""
+
+        query = "SELECT * FROM type::table($table)"
+        if self.filter_criteria is not None and len(self.filter_criteria) > 0:
+            query += " WHERE "
+            for idx, key in enumerate(self.filter_criteria):
+                query += f""" {"AND" if idx > 0 else ""} {key} = ${key}"""
+
+        metadata = {
+            "ns": self.ns,
+            "db": self.db,
+            "table": self.table,
+        }
+        results = await self.sdb.query(
+            query, {"table": self.table, **self.filter_criteria}
+        )
+
+        return [
+            (
+                Document(
+                    page_content=json.dumps(result),
+                    metadata={"id": result["id"], **result["metadata"], **metadata},
+                )
+            )
+            for result in results[0]["result"]
+        ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/zep.py` & `gigachain_community-0.2.0/langchain_community/retrievers/zep.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/retrievers/zilliz.py` & `gigachain_community-0.2.0/langchain_community/retrievers/zilliz.py`

 * *Files 4% similar despite different names*

```diff
@@ -57,15 +57,15 @@
     def _get_relevant_documents(
         self,
         query: str,
         *,
         run_manager: CallbackManagerForRetrieverRun,
         **kwargs: Any,
     ) -> List[Document]:
-        return self.retriever.get_relevant_documents(
+        return self.retriever.invoke(
             query, run_manager=run_manager.get_child(), **kwargs
         )
 
 
 def ZillizRetreiver(*args: Any, **kwargs: Any) -> ZillizRetriever:
     """Deprecated ZillizRetreiver.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/storage/redis.py` & `gigachain_community-0.2.0/langchain_community/storage/redis.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,15 +28,15 @@
             # [b"value1", b"value2"]
 
             # Delete keys
             redis_store.mdelete(["key1"])
 
             # Iterate over keys
             for key in redis_store.yield_keys():
-                print(key)
+                print(key)  # noqa: T201
     """
 
     def __init__(
         self,
         *,
         client: Any = None,
         redis_url: Optional[str] = None,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/storage/upstash_redis.py` & `gigachain_community-0.2.0/langchain_community/storage/upstash_redis.py`

 * *Files 0% similar despite different names*

```diff
@@ -116,15 +116,15 @@
                 if self.namespace:
                     relative_key = key[len(self.namespace) + 1 :]
                     yield relative_key
                 else:
                     yield key
 
 
-@deprecated("0.0.335", alternative="UpstashRedisByteStore")
+@deprecated("0.0.1", alternative="UpstashRedisByteStore")
 class UpstashRedisStore(_UpstashRedisStore):
     """
     BaseStore implementation using Upstash Redis
     as the underlying store to store strings.
 
     Deprecated in favor of the more generic UpstashRedisByteStore.
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/app.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/app.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/base.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/base.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/owner.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/owner.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/rule.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/rule.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/transfer.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/transfer.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """AINetwork Blockchain tool utils."""
+
 from __future__ import annotations
 
 import os
 from typing import TYPE_CHECKING, Literal, Optional
 
 if TYPE_CHECKING:
     from ain.ain import Ain
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ainetwork/value.py` & `gigachain_community-0.2.0/langchain_community/tools/ainetwork/value.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/amadeus/closest_airport.py` & `gigachain_community-0.2.0/langchain_community/tools/amadeus/closest_airport.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-from typing import Optional, Type
+from typing import Any, Dict, Optional, Type
 
 from langchain_core.callbacks import CallbackManagerForToolRun
-from langchain_core.pydantic_v1 import BaseModel, Field
+from langchain_core.language_models import BaseLanguageModel
+from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 
 from langchain_community.chat_models import ChatOpenAI
 from langchain_community.tools.amadeus.base import AmadeusBaseTool
 
 
 class ClosestAirportSchema(BaseModel):
     """Schema for the AmadeusClosestAirport tool."""
@@ -31,20 +32,30 @@
 
     name: str = "closest_airport"
     description: str = (
         "Use this tool to find the closest airport to a particular location."
     )
     args_schema: Type[ClosestAirportSchema] = ClosestAirportSchema
 
+    llm: Optional[BaseLanguageModel] = Field(default=None)
+    """Tool's llm used for calculating the closest airport. Defaults to `ChatOpenAI`."""
+
+    @root_validator(pre=True)
+    def set_llm(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        if not values.get("llm"):
+            # For backward-compatibility
+            values["llm"] = ChatOpenAI(temperature=0)
+        return values
+
     def _run(
         self,
         location: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         content = (
             f" What is the nearest airport to {location}? Please respond with the "
             " airport's International Air Transport Association (IATA) Location "
             ' Identifier in the following JSON format. JSON: "iataCode": "IATA '
             ' Location Identifier" '
         )
 
-        return ChatOpenAI(temperature=0).predict(content)
+        return self.llm.invoke(content)  # type: ignore[union-attr]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/amadeus/flight_search.py` & `gigachain_community-0.2.0/langchain_community/tools/amadeus/flight_search.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,24 +27,24 @@
             " search's destination airport. "
         )
     )
     departureDateTimeEarliest: str = Field(
         description=(
             " The earliest departure datetime from the origin airport "
             " for the flight search in the following format: "
-            ' "YYYY-MM-DDTHH:MM", where "T" separates the date and time '
+            ' "YYYY-MM-DDTHH:MM:SS", where "T" separates the date and time '
             ' components. For example: "2023-06-09T10:30:00" represents '
             " June 9th, 2023, at 10:30 AM. "
         )
     )
     departureDateTimeLatest: str = Field(
         description=(
             " The latest departure datetime from the origin airport "
             " for the flight search in the following format: "
-            ' "YYYY-MM-DDTHH:MM", where "T" separates the date and time '
+            ' "YYYY-MM-DDTHH:MM:SS", where "T" separates the date and time '
             ' components. For example: "2023-06-09T10:30:00" represents '
             " June 9th, 2023, at 10:30 AM. "
         )
     )
     page_number: int = Field(
         default=1,
         description="The specific page number of flight results to retrieve",
@@ -93,52 +93,53 @@
                 " same date. If you're trying to search for round-trip "
                 " flights, call this function for the outbound flight first, "
                 " and then call again for the return flight. "
             )
             return [None]
 
         # Collect all results from the Amadeus Flight Offers Search API
+        response = None
         try:
             response = client.shopping.flight_offers_search.get(
                 originLocationCode=originLocationCode,
                 destinationLocationCode=destinationLocationCode,
                 departureDate=latestDeparture.strftime("%Y-%m-%d"),
                 adults=1,
             )
         except ResponseError as error:
-            print(error)
+            print(error)  # noqa: T201
 
         # Generate output dictionary
         output = []
+        if response is not None:
+            for offer in response.data:
+                itinerary: Dict = {}
+                itinerary["price"] = {}
+                itinerary["price"]["total"] = offer["price"]["total"]
+                currency = offer["price"]["currency"]
+                currency = response.result["dictionaries"]["currencies"][currency]
+                itinerary["price"]["currency"] = {}
+                itinerary["price"]["currency"] = currency
+
+                segments = []
+                for segment in offer["itineraries"][0]["segments"]:
+                    flight = {}
+                    flight["departure"] = segment["departure"]
+                    flight["arrival"] = segment["arrival"]
+                    flight["flightNumber"] = segment["number"]
+                    carrier = segment["carrierCode"]
+                    carrier = response.result["dictionaries"]["carriers"][carrier]
+                    flight["carrier"] = carrier
 
-        for offer in response.data:
-            itinerary: Dict = {}
-            itinerary["price"] = {}
-            itinerary["price"]["total"] = offer["price"]["total"]
-            currency = offer["price"]["currency"]
-            currency = response.result["dictionaries"]["currencies"][currency]
-            itinerary["price"]["currency"] = {}
-            itinerary["price"]["currency"] = currency
-
-            segments = []
-            for segment in offer["itineraries"][0]["segments"]:
-                flight = {}
-                flight["departure"] = segment["departure"]
-                flight["arrival"] = segment["arrival"]
-                flight["flightNumber"] = segment["number"]
-                carrier = segment["carrierCode"]
-                carrier = response.result["dictionaries"]["carriers"][carrier]
-                flight["carrier"] = carrier
+                    segments.append(flight)
 
-                segments.append(flight)
+                itinerary["segments"] = []
+                itinerary["segments"] = segments
 
-            itinerary["segments"] = []
-            itinerary["segments"] = segments
-
-            output.append(itinerary)
+                output.append(itinerary)
 
         # Filter out flights after latest departure time
         for index, offer in enumerate(output):
             offerDeparture = dt.strptime(
                 offer["segments"][0]["departure"]["at"], "%Y-%m-%dT%H:%M:%S"
             )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/amadeus/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/amadeus/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """O365 tool utils."""
+
 from __future__ import annotations
 
 import logging
 import os
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/arxiv/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/arxiv/tool.py`

 * *Files 9% similar despite different names*

```diff
@@ -23,15 +23,15 @@
         "A wrapper around Arxiv.org "
         "Useful for when you need to answer questions about Physics, Mathematics, "
         "Computer Science, Quantitative Biology, Quantitative Finance, Statistics, "
         "Electrical Engineering, and Economics "
         "from scientific articles on arxiv.org. "
         "Input should be a search query."
     )
-    api_wrapper: ArxivAPIWrapper = Field(default_factory=ArxivAPIWrapper)
+    api_wrapper: ArxivAPIWrapper = Field(default_factory=ArxivAPIWrapper)  # type: ignore[arg-type]
     args_schema: Type[BaseModel] = ArxivInput
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/form_recognizer.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/form_recognizer.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/image_analysis.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/image_analysis.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/speech2text.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/speech2text.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/text2speech.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/text2speech.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/text_analytics_health.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_cognitive_services/text_analytics_health.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/azure_cognitive_services/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/azure_ai_services/utils.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/bearly/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/bearly/tool.py`

 * *Files 3% similar despite different names*

```diff
@@ -55,15 +55,15 @@
 The code should NOT be wrapped in backticks. \
 All python packages including requests, matplotlib, scipy, numpy, pandas, \
 etc are available. \
 If you have any files outputted write them to "output/" relative to the execution \
 path. Output can only be read from the directory, stdout, and stdin. \
 Do not use things like plot.show() as it will \
 not work instead write them out `output/` and a link to the file will be returned. \
-print() any output and results so you can capture the output."""
+print() any output and results so you can capture the output."""  # noqa: T201
 
 
 class FileInfo(BaseModel):
     """Information about a file to be uploaded."""
 
     source_path: str
     description: str
@@ -121,20 +121,24 @@
                     "outputDir": "output/",
                     "outputAsLinks": True,
                 }
             ),
             headers={"Authorization": self.api_key},
         ).json()
         return {
-            "stdout": base64.b64decode(resp["stdoutBasesixtyfour"]).decode()
-            if resp["stdoutBasesixtyfour"]
-            else "",
-            "stderr": base64.b64decode(resp["stderrBasesixtyfour"]).decode()
-            if resp["stderrBasesixtyfour"]
-            else "",
+            "stdout": (
+                base64.b64decode(resp["stdoutBasesixtyfour"]).decode()
+                if resp["stdoutBasesixtyfour"]
+                else ""
+            ),
+            "stderr": (
+                base64.b64decode(resp["stderrBasesixtyfour"]).decode()
+                if resp["stderrBasesixtyfour"]
+                else ""
+            ),
             "fileLinks": resp["fileLinks"],
             "exitCode": resp["exitCode"],
         }
 
     async def _arun(self, query: str) -> str:
         """Use the tool asynchronously."""
         raise NotImplementedError("custom_search does not support async")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/bing_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/bing_search/tool.py`

 * *Files 6% similar despite different names*

```diff
@@ -27,15 +27,15 @@
         """Use the tool."""
         return self.api_wrapper.run(query)
 
 
 class BingSearchResults(BaseTool):
     """Tool that queries the Bing Search API and gets back json."""
 
-    name: str = "Bing Search Results JSON"
+    name: str = "bing_search_results_json"
     description: str = (
         "A wrapper around Bing Search. "
         "Useful for when you need to answer questions about current events. "
         "Input should be a search query. Output is a JSON array of the query results"
     )
     num_results: int = 4
     api_wrapper: BingSearchAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/brave_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/brave_search/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/clickup/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/clickup/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/clickup/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/clickup/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 from langchain_community.agent_toolkits.clickup.toolkit import ClickupToolkit
 from langchain_community.utilities.clickup import ClickupAPIWrapper
 
 clickup = ClickupAPIWrapper()
 toolkit = ClickupToolkit.from_clickup_api_wrapper(clickup)
 ```
 """
+
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.clickup import ClickupAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/convert_to_openai.py` & `gigachain_community-0.2.0/langchain_community/tools/render.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,19 @@
+"""Different methods for rendering Tools to be passed to LLMs.
+
+Depending on the LLM you are using and the prompting strategy you are using,
+you may want Tools to be rendered in a different way.
+This module contains various ways to render tools.
+"""
+
 from langchain_core.tools import BaseTool
 
+from langchain_community.utils.gigachat_functions import (
+    convert_pydantic_to_gigachat_function,
+)
 from langchain_community.utils.openai_functions import (
     FunctionDescription,
     ToolDescription,
     convert_pydantic_to_openai_function,
 )
 
 
@@ -14,14 +24,39 @@
             tool.args_schema, name=tool.name, description=tool.description
         )
     else:
         return {
             "name": tool.name,
             "description": tool.description,
             "parameters": {
+                # This is a hack to get around the fact that some tools
+                # do not expose an args_schema, and expect an argument
+                # which is a string.
+                # And Open AI does not support an array type for the
+                # parameters.
+                "properties": {
+                    "__arg1": {"title": "__arg1", "type": "string"},
+                },
+                "required": ["__arg1"],
+                "type": "object",
+            },
+        }
+
+
+def format_tool_to_gigachat_function(tool: BaseTool) -> FunctionDescription:
+    """Format tool into the OpenAI function API."""
+    if tool.args_schema:
+        return convert_pydantic_to_gigachat_function(
+            tool.args_schema, name=tool.name, description=tool.description
+        )
+    else:
+        return {
+            "name": tool.name,
+            "description": tool.description,
+            "parameters": {
                 # This is a hack to get around the fact that some tools
                 # do not expose an args_schema, and expect an argument
                 # which is a string.
                 # And Open AI does not support an array type for the
                 # parameters.
                 "properties": {
                     "__arg1": {"title": "__arg1", "type": "string"},
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/dataforseo_api_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/dataforseo_api_search/tool.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,15 +40,15 @@
         return (await self.api_wrapper.arun(query)).__str__()
 
 
 class DataForSeoAPISearchResults(BaseTool):
     """Tool that queries the DataForSeo Google Search API
     and get back json."""
 
-    name: str = "DataForSeo-Results-JSON"
+    name: str = "dataforseo_results_json"
     description: str = (
         "A comprehensive Google Search API provided by DataForSeo."
         "This tool is useful for obtaining real-time data on current events "
         "or popular searches."
         "The input should be a search query and the output is a JSON object "
         "of the query results."
     )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ddg_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/tavily_search/tool.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,85 +1,107 @@
-"""Tool for the DuckDuckGo search API."""
+"""Tool for the Tavily search API."""
 
-import warnings
-from typing import Any, Optional, Type
+from typing import Dict, List, Optional, Type, Union
 
-from langchain_core.callbacks import CallbackManagerForToolRun
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForToolRun,
+    CallbackManagerForToolRun,
+)
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
-from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper
+from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
 
 
-class DDGInput(BaseModel):
-    """Input for the DuckDuckGo search tool."""
+class TavilyInput(BaseModel):
+    """Input for the Tavily tool."""
 
     query: str = Field(description="search query to look up")
 
 
-class DuckDuckGoSearchRun(BaseTool):
-    """Tool that queries the DuckDuckGo search API."""
+class TavilySearchResults(BaseTool):
+    """Tool that queries the Tavily Search API and gets back json."""
 
-    name: str = "duckduckgo_search"
+    name: str = "tavily_search_results_json"
     description: str = (
-        "A wrapper around DuckDuckGo Search. "
+        "A search engine optimized for comprehensive, accurate, and trusted results. "
         "Useful for when you need to answer questions about current events. "
         "Input should be a search query."
     )
-    api_wrapper: DuckDuckGoSearchAPIWrapper = Field(
-        default_factory=DuckDuckGoSearchAPIWrapper
-    )
-    args_schema: Type[BaseModel] = DDGInput
+    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)  # type: ignore[arg-type]
+    max_results: int = 5
+    args_schema: Type[BaseModel] = TavilyInput
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[List[Dict], str]:
         """Use the tool."""
-        return self.api_wrapper.run(query)
+        try:
+            return self.api_wrapper.results(
+                query,
+                self.max_results,
+            )
+        except Exception as e:
+            return repr(e)
 
+    async def _arun(
+        self,
+        query: str,
+        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
+    ) -> Union[List[Dict], str]:
+        """Use the tool asynchronously."""
+        try:
+            return await self.api_wrapper.results_async(
+                query,
+                self.max_results,
+            )
+        except Exception as e:
+            return repr(e)
 
-class DuckDuckGoSearchResults(BaseTool):
-    """Tool that queries the DuckDuckGo search API and gets back json."""
 
-    name: str = "DuckDuckGo Results JSON"
+class TavilyAnswer(BaseTool):
+    """Tool that queries the Tavily Search API and gets back an answer."""
+
+    name: str = "tavily_answer"
     description: str = (
-        "A wrapper around Duck Duck Go Search. "
+        "A search engine optimized for comprehensive, accurate, and trusted results. "
         "Useful for when you need to answer questions about current events. "
-        "Input should be a search query. Output is a JSON array of the query results"
-    )
-    max_results: int = Field(alias="num_results", default=4)
-    api_wrapper: DuckDuckGoSearchAPIWrapper = Field(
-        default_factory=DuckDuckGoSearchAPIWrapper
+        "Input should be a search query. "
+        "This returns only the answer - not the original source data."
     )
-    backend: str = "text"
-    args_schema: Type[BaseModel] = DDGInput
+    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)  # type: ignore[arg-type]
+    args_schema: Type[BaseModel] = TavilyInput
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[List[Dict], str]:
         """Use the tool."""
-        res = self.api_wrapper.results(query, self.max_results, source=self.backend)
-        res_strs = [", ".join([f"{k}: {v}" for k, v in d.items()]) for d in res]
-        return ", ".join([f"[{rs}]" for rs in res_strs])
-
-
-def DuckDuckGoSearchTool(*args: Any, **kwargs: Any) -> DuckDuckGoSearchRun:
-    """
-    Deprecated. Use DuckDuckGoSearchRun instead.
-
-    Args:
-        *args:
-        **kwargs:
-
-    Returns:
-        DuckDuckGoSearchRun
-    """
-    warnings.warn(
-        "DuckDuckGoSearchTool will be deprecated in the future. "
-        "Please use DuckDuckGoSearchRun instead.",
-        DeprecationWarning,
-    )
-    return DuckDuckGoSearchRun(*args, **kwargs)
+        try:
+            return self.api_wrapper.raw_results(
+                query,
+                max_results=5,
+                include_answer=True,
+                search_depth="basic",
+            )["answer"]
+        except Exception as e:
+            return repr(e)
+
+    async def _arun(
+        self,
+        query: str,
+        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
+    ) -> Union[List[Dict], str]:
+        """Use the tool asynchronously."""
+        try:
+            result = await self.api_wrapper.raw_results_async(
+                query,
+                max_results=5,
+                include_answer=True,
+                search_depth="basic",
+            )
+            return result["answer"]
+        except Exception as e:
+            return repr(e)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/e2b_data_analysis/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/e2b_data_analysis/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/e2b_data_analysis/unparse.py` & `gigachain_community-0.2.0/langchain_community/tools/e2b_data_analysis/unparse.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 # mypy: disable-error-code=no-untyped-def
 # Because Python >3.9 doesn't support ast.unparse,
 # we copied the unparse functionality from here:
 # https://github.com/python/cpython/blob/3.8/Tools/parser/unparse.py
 "Usage: unparse.py <path to source file>"
+
 import ast
 import io
 import sys
 import tokenize
 
 # Large float and imaginary literals get turned into infinities in the AST.
 # We unparse those infinities to INFSTR.
@@ -23,15 +24,15 @@
     else:
         for x in seq:
             inter()
             f(x)
 
 
 class Unparser:
-    """Methods in this class recursively traverse an AST and
+    """Traverse an AST and
     output source code for the abstract syntax; original formatting
     is disregarded."""
 
     def __init__(self, tree, file=sys.stdout):
         """Unparser(tree, file=sys.stdout) -> None.
         Print the source for tree to file."""
         self.f = file
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Edenai Tools."""
+
 from langchain_community.tools.edenai.audio_speech_to_text import (
     EdenAiSpeechToTextTool,
 )
 from langchain_community.tools.edenai.audio_text_to_speech import (
     EdenAiTextToSpeechTool,
 )
 from langchain_community.tools.edenai.edenai_base_tool import EdenaiTool
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/audio_speech_to_text.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/audio_speech_to_text.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/audio_text_to_speech.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/audio_text_to_speech.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/edenai_base_tool.py` & `gigachain_community-0.2.0/langchain_community/embeddings/oracleai.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,159 +1,182 @@
+# Authors:
+#   Harichandan Roy (hroy)
+#   David Jiang (ddjiang)
+#
+# -----------------------------------------------------------------------------
+# oracleai.py
+# -----------------------------------------------------------------------------
+
 from __future__ import annotations
 
+import json
 import logging
-from abc import abstractmethod
-from typing import Any, Dict, List, Optional
+import traceback
+from typing import TYPE_CHECKING, Any, Dict, List, Optional
+
+from langchain_core.embeddings import Embeddings
+from langchain_core.pydantic_v1 import BaseModel, Extra
 
-import requests
-from langchain_core.callbacks import CallbackManagerForToolRun
-from langchain_core.pydantic_v1 import root_validator
-from langchain_core.tools import BaseTool
-from langchain_core.utils import get_from_dict_or_env
+if TYPE_CHECKING:
+    from oracledb import Connection
 
 logger = logging.getLogger(__name__)
 
+"""OracleEmbeddings class"""
 
-class EdenaiTool(BaseTool):
 
-    """
-    the base tool for all the EdenAI Tools .
-    you should have
-    the environment variable ``EDENAI_API_KEY`` set with your API token.
-    You can find your token here: https://app.edenai.run/admin/account/settings
-    """
+class OracleEmbeddings(BaseModel, Embeddings):
+    """Get Embeddings"""
 
-    feature: str
-    subfeature: str
-    edenai_api_key: Optional[str] = None
-    is_async: bool = False
-
-    providers: List[str]
-    """provider to use for the API call."""
-
-    @root_validator(allow_reuse=True)
-    def validate_environment(cls, values: Dict) -> Dict:
-        """Validate that api key exists in environment."""
-        values["edenai_api_key"] = get_from_dict_or_env(
-            values, "edenai_api_key", "EDENAI_API_KEY"
-        )
-        return values
+    """Oracle Connection"""
+    conn: Any
+    """Embedding Parameters"""
+    params: Dict[str, Any]
+    """Proxy"""
+    proxy: Optional[str] = None
 
-    @staticmethod
-    def get_user_agent() -> str:
-        from langchain_community import __version__
-
-        return f"langchain/{__version__}"
+    def __init__(self, **kwargs: Any):
+        super().__init__(**kwargs)
 
-    def _call_eden_ai(self, query_params: Dict[str, Any]) -> str:
-        """
-        Make an API call to the EdenAI service with the specified query parameters.
+    class Config:
+        """Configuration for this pydantic object."""
 
-        Args:
-            query_params (dict): The parameters to include in the API call.
+        extra = Extra.forbid
 
-        Returns:
-            requests.Response: The response from the EdenAI API call.
+    """
+    1 - user needs to have create procedure, 
+        create mining model, create any directory privilege.
+    2 - grant create procedure, create mining model, 
+        create any directory to <user>;
+    """
 
+    @staticmethod
+    def load_onnx_model(
+        conn: Connection, dir: str, onnx_file: str, model_name: str
+    ) -> None:
+        """Load an ONNX model to Oracle Database.
+        Args:
+            conn: Oracle Connection,
+            dir: Oracle Directory,
+            onnx_file: ONNX file name,
+            model_name: Name of the model.
         """
 
-        # faire l'API call
+        try:
+            if conn is None or dir is None or onnx_file is None or model_name is None:
+                raise Exception("Invalid input")
 
-        headers = {
-            "Authorization": f"Bearer {self.edenai_api_key}",
-            "User-Agent": self.get_user_agent(),
-        }
+            cursor = conn.cursor()
+            cursor.execute(
+                """
+                begin
+                    dbms_data_mining.drop_model(model_name => :model, force => true);
+                    SYS.DBMS_VECTOR.load_onnx_model(:path, :filename, :model, 
+                        json('{"function" : "embedding", 
+                            "embeddingOutput" : "embedding", 
+                            "input": {"input": ["DATA"]}}'));
+                end;""",
+                path=dir,
+                filename=onnx_file,
+                model=model_name,
+            )
 
-        url = f"https://api.edenai.run/v2/{self.feature}/{self.subfeature}"
+            cursor.close()
 
-        payload = {
-            "providers": str(self.providers),
-            "response_as_dict": False,
-            "attributes_as_list": True,
-            "show_original_response": False,
-        }
+        except Exception as ex:
+            logger.info(f"An exception occurred :: {ex}")
+            traceback.print_exc()
+            cursor.close()
+            raise
 
-        payload.update(query_params)
+    def embed_documents(self, texts: List[str]) -> List[List[float]]:
+        """Compute doc embeddings using an OracleEmbeddings.
+        Args:
+            texts: The list of texts to embed.
+        Returns:
+            List of embeddings, one for each input text.
+        """
 
-        response = requests.post(url, json=payload, headers=headers)
+        try:
+            import oracledb
+        except ImportError as e:
+            raise ImportError(
+                "Unable to import oracledb, please install with "
+                "`pip install -U oracledb`."
+            ) from e
 
-        self._raise_on_error(response)
+        if texts is None:
+            return None
 
+        embeddings: List[List[float]] = []
         try:
-            return self._parse_response(response.json())
-        except Exception as e:
-            raise RuntimeError(f"An error occurred while running tool: {e}")
-
-    def _raise_on_error(self, response: requests.Response) -> None:
-        if response.status_code >= 500:
-            raise Exception(f"EdenAI Server: Error {response.status_code}")
-        elif response.status_code >= 400:
-            raise ValueError(f"EdenAI received an invalid payload: {response.text}")
-        elif response.status_code != 200:
-            raise Exception(
-                f"EdenAI returned an unexpected response with status "
-                f"{response.status_code}: {response.text}"
-            )
+            # returns strings or bytes instead of a locator
+            oracledb.defaults.fetch_lobs = False
+            cursor = self.conn.cursor()
+
+            if self.proxy:
+                cursor.execute(
+                    "begin utl_http.set_proxy(:proxy); end;", proxy=self.proxy
+                )
+
+            for text in texts:
+                cursor.execute(
+                    "select t.* "
+                    + "from dbms_vector_chain.utl_to_embeddings(:content, "
+                    + "json(:params)) t",
+                    content=text,
+                    params=json.dumps(self.params),
+                )
+
+                for row in cursor:
+                    if row is None:
+                        embeddings.append([])
+                    else:
+                        rdata = json.loads(row[0])
+                        # dereference string as array
+                        vec = json.loads(rdata["embed_vector"])
+                        embeddings.append(vec)
+
+            cursor.close()
+            return embeddings
+        except Exception as ex:
+            logger.info(f"An exception occurred :: {ex}")
+            traceback.print_exc()
+            cursor.close()
+            raise
 
-        # case where edenai call succeeded but provider returned an error
-        # (eg: rate limit, server error, etc.)
-        if self.is_async is False:
-            # async call are different and only return a job_id,
-            # not the provider response directly
-            provider_response = response.json()[0]
-            if provider_response.get("status") == "fail":
-                err_msg = provider_response["error"]["message"]
-                raise ValueError(err_msg)
-
-    @abstractmethod
-    def _run(
-        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None
-    ) -> str:
-        pass
-
-    @abstractmethod
-    def _parse_response(self, response: Any) -> str:
-        """Take a dict response and condense it's data in a human readable string"""
-        pass
-
-    def _get_edenai(self, url: str) -> requests.Response:
-        headers = {
-            "accept": "application/json",
-            "authorization": f"Bearer {self.edenai_api_key}",
-            "User-Agent": self.get_user_agent(),
-        }
+    def embed_query(self, text: str) -> List[float]:
+        """Compute query embedding using an OracleEmbeddings.
+        Args:
+            text: The text to embed.
+        Returns:
+            Embedding for the text.
+        """
+        return self.embed_documents([text])[0]
 
-        response = requests.get(url, headers=headers)
 
-        self._raise_on_error(response)
+# uncomment the following code block to run the test
 
-        return response
+"""
+# A sample unit test.
 
-    def _parse_json_multilevel(
-        self, extracted_data: dict, formatted_list: list, level: int = 0
-    ) -> None:
-        for section, subsections in extracted_data.items():
-            indentation = "  " * level
-            if isinstance(subsections, str):
-                subsections = subsections.replace("\n", ",")
-                formatted_list.append(f"{indentation}{section} : {subsections}")
-
-            elif isinstance(subsections, list):
-                formatted_list.append(f"{indentation}{section} : ")
-                self._list_handling(subsections, formatted_list, level + 1)
-
-            elif isinstance(subsections, dict):
-                formatted_list.append(f"{indentation}{section} : ")
-                self._parse_json_multilevel(subsections, formatted_list, level + 1)
+''' get the Oracle connection '''
+conn = oracledb.connect(
+    user="",
+    password="",
+    dsn="")
+print("Oracle connection is established...")
 
-    def _list_handling(
-        self, subsection_list: list, formatted_list: list, level: int
-    ) -> None:
-        for list_item in subsection_list:
-            if isinstance(list_item, dict):
-                self._parse_json_multilevel(list_item, formatted_list, level)
+''' params '''
+embedder_params = {"provider":"database", "model":"demo_model"}
+proxy = ""
+
+''' instance '''
+embedder = OracleEmbeddings(conn=conn, params=embedder_params, proxy=proxy)
+
+embed = embedder.embed_query("Hello World!")
+print(f"Embedding generated by OracleEmbeddings: {embed}")
 
-            elif isinstance(list_item, list):
-                self._list_handling(list_item, formatted_list, level + 1)
+conn.close()
+print("Connection is closed.")
 
-            else:
-                formatted_list.append(f"{'  ' * level}{list_item}")
+"""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/image_explicitcontent.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/image_explicitcontent.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 
 from langchain_community.tools.edenai.edenai_base_tool import EdenaiTool
 
 logger = logging.getLogger(__name__)
 
 
 class EdenAiExplicitImageTool(EdenaiTool):
-
     """Tool that queries the Eden AI Explicit image detection.
 
     for api reference check edenai documentation:
     https://docs.edenai.co/reference/image_explicit_content_create.
 
     To use, you should have
     the environment variable ``EDENAI_API_KEY`` set with your API token.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/image_objectdetection.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/image_objectdetection.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/ocr_identityparser.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/ocr_identityparser.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/ocr_invoiceparser.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/ocr_invoiceparser.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/edenai/text_moderation.py` & `gigachain_community-0.2.0/langchain_community/tools/edenai/text_moderation.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/eleven_labs/text2speech.py` & `gigachain_community-0.2.0/langchain_community/tools/eleven_labs/text2speech.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/copy.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/copy.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/delete.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/delete.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/file_search.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/file_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/list_dir.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/list_dir.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/move.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/move.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/read.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/read.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/utils.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/file_management/write.py` & `gigachain_community-0.2.0/langchain_community/tools/file_management/write.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/github/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/github/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/github/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/github/tool.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,27 +3,28 @@
 and operate on a GitHub repository.
 
 To use this tool, you must first set as environment variables:
     GITHUB_API_TOKEN
     GITHUB_REPOSITORY -> format: {owner}/{repo}
 
 """
+
 from typing import Optional, Type
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.github import GitHubAPIWrapper
 
 
 class GitHubAction(BaseTool):
     """Tool for interacting with the GitHub API."""
 
-    api_wrapper: GitHubAPIWrapper = Field(default_factory=GitHubAPIWrapper)
+    api_wrapper: GitHubAPIWrapper = Field(default_factory=GitHubAPIWrapper)  # type: ignore[arg-type]
     mode: str
     name: str = ""
     description: str = ""
     args_schema: Optional[Type[BaseModel]] = None
 
     def _run(
         self,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gitlab/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/gitlab/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gitlab/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/gitlab/tool.py`

 * *Files 7% similar despite different names*

```diff
@@ -3,27 +3,28 @@
 and operate on a GitLab repository.
 
 To use this tool, you must first set as environment variables:
     GITLAB_PRIVATE_ACCESS_TOKEN
     GITLAB_REPOSITORY -> format: {owner}/{repo}
 
 """
+
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.gitlab import GitLabAPIWrapper
 
 
 class GitLabAction(BaseTool):
     """Tool for interacting with the GitLab API."""
 
-    api_wrapper: GitLabAPIWrapper = Field(default_factory=GitLabAPIWrapper)
+    api_wrapper: GitLabAPIWrapper = Field(default_factory=GitLabAPIWrapper)  # type: ignore[arg-type]
     mode: str
     name: str = ""
     description: str = ""
 
     def _run(
         self,
         instructions: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/base.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Base class for Gmail tools."""
+
 from __future__ import annotations
 
 from typing import TYPE_CHECKING
 
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool
 
@@ -30,8 +31,8 @@
 
         Args:
             api_resource: The api resource to use.
 
         Returns:
             A tool.
         """
-        return cls(service=api_resource)
+        return cls(service=api_resource)  # type: ignore[call-arg]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/create_draft.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/create_draft.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/get_message.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/get_message.py`

 * *Files 10% similar despite different names*

```diff
@@ -49,18 +49,18 @@
 
         message_body = ""
         if email_msg.is_multipart():
             for part in email_msg.walk():
                 ctype = part.get_content_type()
                 cdispo = str(part.get("Content-Disposition"))
                 if ctype == "text/plain" and "attachment" not in cdispo:
-                    message_body = part.get_payload(decode=True).decode("utf-8")
+                    message_body = part.get_payload(decode=True).decode("utf-8")  # type: ignore[union-attr]
                     break
         else:
-            message_body = email_msg.get_payload(decode=True).decode("utf-8")
+            message_body = email_msg.get_payload(decode=True).decode("utf-8")  # type: ignore[union-attr]
 
         body = clean_email_body(message_body)
 
         return {
             "id": message_id,
             "threadId": message_data["threadId"],
             "snippet": message_data["snippet"],
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/get_thread.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/get_thread.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/search.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/search.py`

 * *Files 2% similar despite different names*

```diff
@@ -94,18 +94,23 @@
 
             message_body = ""
             if email_msg.is_multipart():
                 for part in email_msg.walk():
                     ctype = part.get_content_type()
                     cdispo = str(part.get("Content-Disposition"))
                     if ctype == "text/plain" and "attachment" not in cdispo:
-                        message_body = part.get_payload(decode=True).decode("utf-8")
+                        try:
+                            message_body = part.get_payload(decode=True).decode("utf-8")  # type: ignore[union-attr]
+                        except UnicodeDecodeError:
+                            message_body = part.get_payload(decode=True).decode(  # type: ignore[union-attr]
+                                "latin-1"
+                            )
                         break
             else:
-                message_body = email_msg.get_payload(decode=True).decode("utf-8")
+                message_body = email_msg.get_payload(decode=True).decode("utf-8")  # type: ignore[union-attr]
 
             body = clean_email_body(message_body)
 
             results.append(
                 {
                     "id": message["id"],
                     "threadId": message_data["threadId"],
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/send_message.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/send_message.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Send Gmail messages."""
+
 import base64
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
 from typing import Any, Dict, List, Optional, Type, Union
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import BaseModel, Field
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/gmail/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/gmail/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 """Gmail tool utils."""
+
 from __future__ import annotations
 
 import logging
 import os
 from typing import TYPE_CHECKING, List, Optional, Tuple
 
+from langchain_core.utils import guard_import
+
 if TYPE_CHECKING:
     from google.auth.transport.requests import Request
     from google.oauth2.credentials import Credentials
     from google_auth_oauthlib.flow import InstalledAppFlow
     from googleapiclient.discovery import Resource
     from googleapiclient.discovery import build as build_resource
 
@@ -17,56 +20,45 @@
 
 def import_google() -> Tuple[Request, Credentials]:
     """Import google libraries.
 
     Returns:
         Tuple[Request, Credentials]: Request and Credentials classes.
     """
-    # google-auth-httplib2
-    try:
-        from google.auth.transport.requests import Request  # noqa: F401
-        from google.oauth2.credentials import Credentials  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "You need to install google-auth-httplib2 to use this toolkit. "
-            "Try running pip install --upgrade google-auth-httplib2"
-        )
-    return Request, Credentials
+    return (
+        guard_import(
+            module_name="google.auth.transport.requests",
+            pip_name="google-auth-httplib2",
+        ).Request,
+        guard_import(
+            module_name="google.oauth2.credentials", pip_name="google-auth-httplib2"
+        ).Credentials,
+    )
 
 
 def import_installed_app_flow() -> InstalledAppFlow:
     """Import InstalledAppFlow class.
 
     Returns:
         InstalledAppFlow: InstalledAppFlow class.
     """
-    try:
-        from google_auth_oauthlib.flow import InstalledAppFlow
-    except ImportError:
-        raise ImportError(
-            "You need to install google-auth-oauthlib to use this toolkit. "
-            "Try running pip install --upgrade google-auth-oauthlib"
-        )
-    return InstalledAppFlow
+    return guard_import(
+        module_name="google_auth_oauthlib.flow", pip_name="google-auth-oauthlib"
+    ).InstalledAppFlow
 
 
 def import_googleapiclient_resource_builder() -> build_resource:
     """Import googleapiclient.discovery.build function.
 
     Returns:
         build_resource: googleapiclient.discovery.build function.
     """
-    try:
-        from googleapiclient.discovery import build
-    except ImportError:
-        raise ImportError(
-            "You need to install googleapiclient to use this toolkit. "
-            "Try running pip install --upgrade google-api-python-client"
-        )
-    return build
+    return guard_import(
+        module_name="googleapiclient.discovery", pip_name="google-api-python-client"
+    ).build
 
 
 DEFAULT_SCOPES = ["https://mail.google.com/"]
 DEFAULT_CREDS_TOKEN_FILE = "token.json"
 DEFAULT_CLIENT_SECRETS_FILE = "credentials.json"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/golden_query/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/golden_query/tool.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from langchain_community.utilities.golden_query import GoldenQueryAPIWrapper
 
 
 class GoldenQueryRun(BaseTool):
     """Tool that adds the capability to query using the Golden API and get back JSON."""
 
-    name: str = "Golden-Query"
+    name: str = "golden_query"
     description: str = (
         "A wrapper around Golden Query API."
         " Useful for getting entities that match"
         " a natural language query from Golden's Knowledge Base."
         "\nExample queries:"
         "\n- companies in nanotech"
         "\n- list of cloud providers starting in 2019"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_cloud/texttospeech.py` & `gigachain_community-0.2.0/langchain_community/tools/google_cloud/texttospeech.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import tempfile
 from typing import TYPE_CHECKING, Any, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.vertexai import get_client_info
 
 if TYPE_CHECKING:
     from google.cloud import texttospeech
@@ -32,14 +33,19 @@
         texttospeech.AudioEncoding.OGG_OPUS: ".ogg",
         texttospeech.AudioEncoding.MULAW: ".wav",
         texttospeech.AudioEncoding.ALAW: ".wav",
     }
     return ENCODING_FILE_EXTENSION_MAP.get(encoding)
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.TextToSpeechTool",
+)
 class GoogleCloudTextToSpeechTool(BaseTool):
     """Tool that queries the Google Cloud Text to Speech API.
 
     In order to set this up, follow instructions at:
     https://cloud.google.com/text-to-speech/docs/before-you-begin
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_finance/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_finance/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_jobs/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_jobs/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_lens/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_lens/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from langchain_community.utilities.google_lens import GoogleLensAPIWrapper
 
 
 class GoogleLensQueryRun(BaseTool):
     """Tool that queries the Google Lens API."""
 
-    name: str = "google_Lens"
+    name: str = "google_lens"
     description: str = (
         "A wrapper around Google Lens Search. "
         "Useful for when you need to get information related"
         "to an image from Google Lens"
         "Input should be a url to an image."
     )
     api_wrapper: GoogleLensAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_places/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_places/tool.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,41 @@
 """Tool for the Google search API."""
 
 from typing import Optional, Type
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.google_places_api import GooglePlacesAPIWrapper
 
 
 class GooglePlacesSchema(BaseModel):
     """Input for GooglePlacesTool."""
 
     query: str = Field(..., description="Query for google maps")
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GooglePlacesTool",
+)
 class GooglePlacesTool(BaseTool):
     """Tool that queries the Google places API."""
 
     name: str = "google_places"
     description: str = (
         "A wrapper around Google Places. "
         "Useful for when you need to validate or "
         "discover addressed from ambiguous text. "
         "Input should be a search query."
     )
-    api_wrapper: GooglePlacesAPIWrapper = Field(default_factory=GooglePlacesAPIWrapper)
+    api_wrapper: GooglePlacesAPIWrapper = Field(default_factory=GooglePlacesAPIWrapper)  # type: ignore[arg-type]
     args_schema: Type[BaseModel] = GooglePlacesSchema
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_scholar/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_scholar/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_search/tool.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,23 @@
 """Tool for the Google search API."""
 
 from typing import Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.google_search import GoogleSearchAPIWrapper
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GoogleSearchRun",
+)
 class GoogleSearchRun(BaseTool):
     """Tool that queries the Google search API."""
 
     name: str = "google_search"
     description: str = (
         "A wrapper around Google Search. "
         "Useful for when you need to answer questions about current events. "
@@ -24,18 +30,23 @@
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Use the tool."""
         return self.api_wrapper.run(query)
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GoogleSearchResults",
+)
 class GoogleSearchResults(BaseTool):
     """Tool that queries the Google Search API and gets back json."""
 
-    name: str = "Google Search Results JSON"
+    name: str = "google_search_results_json"
     description: str = (
         "A wrapper around Google Search. "
         "Useful for when you need to answer questions about current events. "
         "Input should be a search query. Output is a JSON array of the query results"
     )
     num_results: int = 4
     api_wrapper: GoogleSearchAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_serper/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_serper/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/google_trends/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/google_trends/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/graphql/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/graphql/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/human/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/human/tool.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool
 
 
 def _print_func(text: str) -> None:
-    print("\n")
-    print(text)
+    print("\n")  # noqa: T201
+    print(text)  # noqa: T201
 
 
 class HumanInputRun(BaseTool):
     """Tool that asks user for input."""
 
     name: str = "human"
     description: str = (
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/ifttt.py` & `gigachain_community-0.2.0/langchain_community/tools/ifttt.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,14 +28,15 @@
 service, and you're ready to start receiving data and triggering actions 
 
 # Finishing up
 - To get your webhook URL go to https://ifttt.com/maker_webhooks/settings
 - Copy the IFTTT key value from there. The URL is of the form
 https://maker.ifttt.com/use/YOUR_IFTTT_KEY. Grab the YOUR_IFTTT_KEY value.
 """
+
 from typing import Optional
 
 import requests
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.tools import BaseTool
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/jira/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/jira/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/jira/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/jira/tool.py`

 * *Files 11% similar despite different names*

```diff
@@ -14,27 +14,28 @@
 from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit
 from langchain_community.utilities.jira import JiraAPIWrapper
 
 jira = JiraAPIWrapper()
 toolkit = JiraToolkit.from_jira_api_wrapper(jira)
 ```
 """
+
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.jira import JiraAPIWrapper
 
 
 class JiraAction(BaseTool):
     """Tool that queries the Atlassian Jira API."""
 
-    api_wrapper: JiraAPIWrapper = Field(default_factory=JiraAPIWrapper)
+    api_wrapper: JiraAPIWrapper = Field(default_factory=JiraAPIWrapper)  # type: ignore[arg-type]
     mode: str
     name: str = ""
     description: str = ""
 
     def _run(
         self,
         instructions: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/json/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/json/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # flake8: noqa
 """Tools for working with JSON specs."""
+
 from __future__ import annotations
 
 import json
 import re
 from pathlib import Path
 from typing import Dict, List, Optional, Union
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/memorize/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/memorize/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     ) -> TrainResult:
         ...
 
 
 class Memorize(BaseTool):
     """Tool that trains a language model."""
 
-    name: str = "Memorize"
+    name: str = "memorize"
     description: str = (
         "Useful whenever you observed novel information "
         "from previous conversation history, "
         "i.e., another tool's action outputs or human comments. "
         "The action input should include observed information in detail, "
         "then the tool will fine-tune yourself to remember it."
     )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/merriam_webster/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/merriam_webster/tool.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from langchain_community.utilities.merriam_webster import MerriamWebsterAPIWrapper
 
 
 class MerriamWebsterQueryRun(BaseTool):
     """Tool that searches the Merriam-Webster API."""
 
-    name: str = "MerriamWebster"
+    name: str = "merriam_webster"
     description: str = (
         "A wrapper around Merriam-Webster. "
         "Useful for when you need to get the definition of a word."
         "Input should be the word you want the definition of."
     )
     api_wrapper: MerriamWebsterAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/metaphor_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/metaphor_search/tool.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,20 +1,26 @@
 """Tool for the Metaphor search API."""
 
 from typing import Dict, List, Optional, Union
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.metaphor_search import MetaphorSearchAPIWrapper
 
 
+@deprecated(
+    since="0.0.15",
+    removal="0.3.0",
+    alternative="langchain_exa.ExaSearchResults",
+)
 class MetaphorSearchResults(BaseTool):
     """Tool that queries the Metaphor Search API and gets back json."""
 
     name: str = "metaphor_search_results_json"
     description: str = (
         "A wrapper around Metaphor Search. "
         "Input should be a Metaphor-optimized query. "
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/multion/close_session.py` & `gigachain_community-0.2.0/langchain_community/tools/multion/close_session.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,10 @@
-import asyncio
 from typing import TYPE_CHECKING, Optional, Type
 
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 if TYPE_CHECKING:
     # This is for linting and IDE typehints
@@ -50,18 +48,10 @@
         sessionId: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> None:
         try:
             try:
                 multion.close_session(sessionId)
             except Exception as e:
-                print(f"{e}, retrying...")
+                print(f"{e}, retrying...")  # noqa: T201
         except Exception as e:
             raise Exception(f"An error occurred: {e}")
-
-    async def _arun(
-        self,
-        sessionId: str,
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> None:
-        loop = asyncio.get_running_loop()
-        await loop.run_in_executor(None, self._run, sessionId)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/multion/create_session.py` & `gigachain_community-0.2.0/langchain_community/tools/multion/create_session.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,10 @@
-import asyncio
 from typing import TYPE_CHECKING, Optional, Type
 
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 if TYPE_CHECKING:
     # This is for linting and IDE typehints
@@ -63,18 +61,7 @@
             response = multion.new_session({"input": query, "url": url})
             return {
                 "sessionId": response["session_id"],
                 "Response": response["message"],
             }
         except Exception as e:
             raise Exception(f"An error occurred: {e}")
-
-    async def _arun(
-        self,
-        query: str,
-        url: Optional[str] = "https://www.google.com/",
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> dict:
-        loop = asyncio.get_running_loop()
-        result = await loop.run_in_executor(None, self._run, query, url)
-
-        return result
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/multion/update_session.py` & `gigachain_community-0.2.0/langchain_community/tools/multion/update_session.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,10 @@
-import asyncio
 from typing import TYPE_CHECKING, Optional, Type
 
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 if TYPE_CHECKING:
     # This is for linting and IDE typehints
@@ -66,23 +64,11 @@
                 response = multion.update_session(
                     sessionId, {"input": query, "url": url}
                 )
                 content = {"sessionId": sessionId, "Response": response["message"]}
                 self.sessionId = sessionId
                 return content
             except Exception as e:
-                print(f"{e}, retrying...")
+                print(f"{e}, retrying...")  # noqa: T201
                 return {"error": f"{e}", "Response": "retrying..."}
         except Exception as e:
             raise Exception(f"An error occurred: {e}")
-
-    async def _arun(
-        self,
-        sessionId: str,
-        query: str,
-        url: Optional[str] = "https://www.google.com/",
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> dict:
-        loop = asyncio.get_running_loop()
-        result = await loop.run_in_executor(None, self._run, sessionId, query, url)
-
-        return result
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/nasa/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/nasa/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/nasa/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/nasa/tool.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 """
-This tool allows agents to interact with the NASA API, specifically 
+This tool allows agents to interact with the NASA API, specifically
 the the NASA Image & Video Library and Exoplanet
 """
 
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import Field
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/nuclia/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/nuclia/tool.py`

 * *Files 2% similar despite different names*

```diff
@@ -71,15 +71,15 @@
         self._config["BACKEND"] = f"https://{zone}.nuclia.cloud/api/v1"
         key = os.environ.get("NUCLIA_NUA_KEY")
         if not key:
             raise ValueError("NUCLIA_NUA_KEY environment variable not set")
         else:
             self._config["NUA_KEY"] = key
         self._config["enable_ml"] = enable_ml
-        super().__init__()
+        super().__init__()  # type: ignore[call-arg]
 
     def _run(
         self,
         action: str,
         id: str,
         path: Optional[str],
         text: Optional[str],
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/create_draft_message.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/create_draft_message.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/events_search.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/events_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,15 +50,15 @@
             "False for searches that will retrieve small events, otherwise, set to "
             "True."
         ),
     )
 
 
 class O365SearchEvents(O365BaseTool):
-    """Class for searching calendar events in Office 365
+    """Search calendar events in Office 365.
 
     Free, but setup is required
     """
 
     name: str = "events_search"
     args_schema: Type[BaseModel] = SearchEventsInput
     description: str = (
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/messages_search.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/messages_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,17 +49,17 @@
             "False for searches that will retrieve small messages, otherwise, set to "
             "True"
         ),
     )
 
 
 class O365SearchEmails(O365BaseTool):
-    """Class for searching email messages in Office 365
+    """Search email messages in Office 365.
 
-    Free, but setup is required
+    Free, but setup is required.
     """
 
     name: str = "messages_search"
     args_schema: Type[BaseModel] = SearchEmailsInput
     description: str = (
         "Use this tool to search for email messages."
         " The input must be a valid Microsoft Graph v1.0 $search query."
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/send_event.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/send_event.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/send_message.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/send_message.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
     bcc: Optional[List[str]] = Field(
         None,
         description="The list of BCC recipients.",
     )
 
 
 class O365SendMessage(O365BaseTool):
-    """Tool for sending an email in Office 365."""
+    """Send an email in Office 365."""
 
     name: str = "send_email"
     description: str = (
         "Use this tool to send an email with the provided message fields."
     )
     args_schema: Type[SendMessageSchema] = SendMessageSchema
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/office365/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/office365/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """O365 tool utils."""
+
 from __future__ import annotations
 
 import logging
 import os
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
@@ -31,15 +32,15 @@
         except Exception:
             return str(body)
     except ImportError:
         return str(body)
 
 
 def authenticate() -> Account:
-    """Authenticate using the Microsoft Grah API"""
+    """Authenticate using the Microsoft Graph API"""
     try:
         from O365 import Account
     except ImportError as e:
         raise ImportError(
             "Cannot import 0365. Please install the package with `pip install O365`."
         ) from e
 
@@ -62,15 +63,15 @@
             scopes=[
                 "https://graph.microsoft.com/Mail.ReadWrite",
                 "https://graph.microsoft.com/Mail.Send",
                 "https://graph.microsoft.com/Calendars.ReadWrite",
                 "https://graph.microsoft.com/MailboxSettings.ReadWrite",
             ]
         ):
-            print("Error: Could not authenticate")
+            print("Error: Could not authenticate")  # noqa: T201
             return None
         else:
             return account
     else:
         return account
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/openapi/utils/api_models.py` & `gigachain_community-0.2.0/langchain_community/tools/openapi/utils/api_models.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Pydantic models for parsing an OpenAPI spec."""
+
 from __future__ import annotations
 
 import logging
 from enum import Enum
 from typing import (
     TYPE_CHECKING,
     Any,
@@ -526,15 +527,15 @@
         if not description and spec.paths is not None:
             description = spec.paths[path].description or spec.paths[path].summary
         return cls(
             operation_id=operation_id,
             description=description or "",
             base_url=spec.base_url,
             path=path,
-            method=method,
+            method=method,  # type: ignore[arg-type]
             properties=properties,
             request_body=api_request_body,
         )
 
     @staticmethod
     def ts_type_from_python(type_: SCHEMA_TYPE) -> str:
         if type_ is None:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/openweathermap/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/openweathermap/tool.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,18 +9,18 @@
 from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper
 
 
 class OpenWeatherMapQueryRun(BaseTool):
     """Tool that queries the OpenWeatherMap API."""
 
     api_wrapper: OpenWeatherMapAPIWrapper = Field(
-        default_factory=OpenWeatherMapAPIWrapper
+        default_factory=OpenWeatherMapAPIWrapper  # type: ignore[arg-type]
     )
 
-    name: str = "OpenWeatherMap"
+    name: str = "open_weather_map"
     description: str = (
         "A wrapper around OpenWeatherMap API. "
         "Useful for fetching current weather information for a specified location. "
         "Input should be a location string (e.g. London,GB)."
     )
 
     def _run(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/__init__.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/__init__.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/base.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from __future__ import annotations
 
 from typing import TYPE_CHECKING, Optional, Tuple, Type
 
 from langchain_core.pydantic_v1 import root_validator
 from langchain_core.tools import BaseTool
+from langchain_core.utils import guard_import
 
 if TYPE_CHECKING:
     from playwright.async_api import Browser as AsyncBrowser
     from playwright.sync_api import Browser as SyncBrowser
 else:
     try:
         # We do this so pydantic can resolve the types when instantiating
@@ -21,23 +22,18 @@
     """
     Lazy import playwright browsers.
 
     Returns:
         Tuple[Type[AsyncBrowser], Type[SyncBrowser]]:
             AsyncBrowser and SyncBrowser classes.
     """
-    try:
-        from playwright.async_api import Browser as AsyncBrowser  # noqa: F401
-        from playwright.sync_api import Browser as SyncBrowser  # noqa: F401
-    except ImportError:
-        raise ImportError(
-            "The 'playwright' package is required to use the playwright tools."
-            " Please install it with 'pip install playwright'."
-        )
-    return AsyncBrowser, SyncBrowser
+    return (
+        guard_import(module_name="playwright.async_api").Browser,
+        guard_import(module_name="playwright.sync_api").Browser,
+    )
 
 
 class BaseBrowserTool(BaseTool):
     """Base class for browser tools."""
 
     sync_browser: Optional["SyncBrowser"] = None
     async_browser: Optional["AsyncBrowser"] = None
@@ -54,8 +50,8 @@
     def from_browser(
         cls,
         sync_browser: Optional[SyncBrowser] = None,
         async_browser: Optional[AsyncBrowser] = None,
     ) -> BaseBrowserTool:
         """Instantiate the tool."""
         lazy_import_playwright_browsers()
-        return cls(sync_browser=sync_browser, async_browser=async_browser)
+        return cls(sync_browser=sync_browser, async_browser=async_browser)  # type: ignore[call-arg]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/click.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/click.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/current_page.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/current_page.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/extract_hyperlinks.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/extract_hyperlinks.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/extract_text.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/extract_text.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/get_elements.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/get_elements.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/navigate.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/navigate.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/navigate_back.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/navigate_back.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/playwright/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/playwright/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Utilities for the Playwright browser tools."""
+
 from __future__ import annotations
 
 import asyncio
 from typing import TYPE_CHECKING, Any, Coroutine, List, Optional, TypeVar
 
 if TYPE_CHECKING:
     from playwright.async_api import Browser as AsyncBrowser
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/plugin.py` & `gigachain_community-0.2.0/langchain_community/tools/plugin.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/powerbi/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/powerbi/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/powerbi/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/powerbi/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Tools for interacting with a Power BI dataset."""
+
 import logging
 from time import perf_counter
 from typing import Any, Dict, Optional, Tuple
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/pubmed/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/pubmed/tool.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,23 +6,23 @@
 
 from langchain_community.utilities.pubmed import PubMedAPIWrapper
 
 
 class PubmedQueryRun(BaseTool):
     """Tool that searches the PubMed API."""
 
-    name: str = "PubMed"
+    name: str = "pub_med"
     description: str = (
         "A wrapper around PubMed. "
         "Useful for when you need to answer questions about medicine, health, "
         "and biomedical topics "
         "from biomedical literature, MEDLINE, life science journals, and online books. "
         "Input should be a search query."
     )
-    api_wrapper: PubMedAPIWrapper = Field(default_factory=PubMedAPIWrapper)
+    api_wrapper: PubMedAPIWrapper = Field(default_factory=PubMedAPIWrapper)  # type: ignore[arg-type]
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Use the PubMed tool."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/reddit_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/reddit_search/tool.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,15 +38,15 @@
     """Tool that queries for posts on a subreddit."""
 
     name: str = "reddit_search"
     description: str = (
         "A tool that searches for posts on Reddit."
         "Useful when you need to know post information on a subreddit."
     )
-    api_wrapper: RedditSearchAPIWrapper = Field(default_factory=RedditSearchAPIWrapper)
+    api_wrapper: RedditSearchAPIWrapper = Field(default_factory=RedditSearchAPIWrapper)  # type: ignore[arg-type]
     args_schema: Type[BaseModel] = RedditSearchSchema
 
     def _run(
         self,
         query: str,
         sort: str,
         time_filter: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/requests/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/requests/tool.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 # flake8: noqa
 """Tools for making requests to an API endpoint."""
+
 import json
-from typing import Any, Dict, Optional
+from typing import Any, Dict, Optional, Union
 
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 
-from langchain_community.utilities.requests import TextRequestsWrapper
+from langchain_community.utilities.requests import GenericRequestsWrapper
 from langchain_core.tools import BaseTool
 
 
 def _parse_input(text: str) -> Dict[str, Any]:
     """Parse the json string into a dict."""
     return json.loads(text)
 
@@ -22,34 +23,54 @@
     """Strips quotes from the url."""
     return url.strip("\"'")
 
 
 class BaseRequestsTool(BaseModel):
     """Base class for requests tools."""
 
-    requests_wrapper: TextRequestsWrapper
+    requests_wrapper: GenericRequestsWrapper
+
+    allow_dangerous_requests: bool = False
+
+    def __init__(self, **kwargs: Any):
+        """Initialize the tool."""
+        if not kwargs.get("allow_dangerous_requests", False):
+            raise ValueError(
+                "You must set allow_dangerous_requests to True to use this tool. "
+                "Requests can be dangerous and can lead to security vulnerabilities. "
+                "For example, users can ask a server to make a request to an internal "
+                "server. It's recommended to use requests through a proxy server "
+                "and avoid accepting inputs from untrusted sources without proper "
+                "sandboxing."
+                "Please see: https://python.langchain.com/docs/security for "
+                "further security information."
+            )
+        super().__init__(**kwargs)
 
 
 class RequestsGetTool(BaseRequestsTool, BaseTool):
     """Tool for making a GET request to an API endpoint."""
 
     name: str = "requests_get"
-    description: str = "A portal to the internet. Use this when you need to get specific content from a website. Input should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request."
+    description: str = """A portal to the internet. Use this when you need to get specific
+    content from a website. Input should be a  url (i.e. https://www.google.com).
+    The output will be the text response of the GET request.
+    """
 
     def _run(
         self, url: str, run_manager: Optional[CallbackManagerForToolRun] = None
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool."""
         return self.requests_wrapper.get(_clean_url(url))
 
     async def _arun(
         self,
         url: str,
         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool asynchronously."""
         return await self.requests_wrapper.aget(_clean_url(url))
 
 
 class RequestsPostTool(BaseRequestsTool, BaseTool):
     """Tool for making a POST request to an API endpoint."""
 
@@ -60,27 +81,27 @@
     key-value pairs you want to POST to the url.
     Be careful to always use double quotes for strings in the json string
     The output will be the text response of the POST request.
     """
 
     def _run(
         self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool."""
         try:
             data = _parse_input(text)
             return self.requests_wrapper.post(_clean_url(data["url"]), data["data"])
         except Exception as e:
             return repr(e)
 
     async def _arun(
         self,
         text: str,
         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool asynchronously."""
         try:
             data = _parse_input(text)
             return await self.requests_wrapper.apost(
                 _clean_url(data["url"]), data["data"]
             )
         except Exception as e:
@@ -97,27 +118,27 @@
     key-value pairs you want to PATCH to the url.
     Be careful to always use double quotes for strings in the json string
     The output will be the text response of the PATCH request.
     """
 
     def _run(
         self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool."""
         try:
             data = _parse_input(text)
             return self.requests_wrapper.patch(_clean_url(data["url"]), data["data"])
         except Exception as e:
             return repr(e)
 
     async def _arun(
         self,
         text: str,
         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool asynchronously."""
         try:
             data = _parse_input(text)
             return await self.requests_wrapper.apatch(
                 _clean_url(data["url"]), data["data"]
             )
         except Exception as e:
@@ -134,51 +155,55 @@
     key-value pairs you want to PUT to the url.
     Be careful to always use double quotes for strings in the json string.
     The output will be the text response of the PUT request.
     """
 
     def _run(
         self, text: str, run_manager: Optional[CallbackManagerForToolRun] = None
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool."""
         try:
             data = _parse_input(text)
             return self.requests_wrapper.put(_clean_url(data["url"]), data["data"])
         except Exception as e:
             return repr(e)
 
     async def _arun(
         self,
         text: str,
         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool asynchronously."""
         try:
             data = _parse_input(text)
             return await self.requests_wrapper.aput(
                 _clean_url(data["url"]), data["data"]
             )
         except Exception as e:
             return repr(e)
 
 
 class RequestsDeleteTool(BaseRequestsTool, BaseTool):
     """Tool for making a DELETE request to an API endpoint."""
 
     name: str = "requests_delete"
-    description: str = "A portal to the internet. Use this when you need to make a DELETE request to a URL. Input should be a specific url, and the output will be the text response of the DELETE request."
+    description: str = """A portal to the internet.
+    Use this when you need to make a DELETE request to a URL.
+    Input should be a specific url, and the output will be the text
+    response of the DELETE request.
+    """
 
     def _run(
         self,
         url: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool."""
         return self.requests_wrapper.delete(_clean_url(url))
 
     async def _arun(
         self,
         url: str,
         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Dict[str, Any]]:
         """Run the tool asynchronously."""
         return await self.requests_wrapper.adelete(_clean_url(url))
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/scenexplain/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/scenexplain/tool.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Tool for the SceneXplain API."""
+
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 
 from langchain_community.utilities.scenexplain import SceneXplainAPIWrapper
@@ -19,14 +20,14 @@
 
     name: str = "image_explainer"
     description: str = (
         "An Image Captioning Tool: Use this tool to generate a detailed caption "
         "for an image. The input can be an image file of any format, and "
         "the output will be a text description that covers every detail of the image."
     )
-    api_wrapper: SceneXplainAPIWrapper = Field(default_factory=SceneXplainAPIWrapper)
+    api_wrapper: SceneXplainAPIWrapper = Field(default_factory=SceneXplainAPIWrapper)  # type: ignore[arg-type]
 
     def _run(
         self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None
     ) -> str:
         """Use the tool."""
         return self.api_wrapper.run(query)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/searchapi/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/searchapi/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/searx_search/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/searx_search/tool.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Tool for the SearxNG search API."""
+
 from typing import Optional
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import Extra, Field
@@ -39,15 +40,15 @@
         """Use the tool asynchronously."""
         return await self.wrapper.arun(query, **self.kwargs)
 
 
 class SearxSearchResults(BaseTool):
     """Tool that queries a Searx instance and gets back json."""
 
-    name: str = "Searx-Search-Results"
+    name: str = "searx_search_results"
     description: str = (
         "A meta search engine."
         "Useful for when you need to answer questions about current events."
         "Input should be a search query. Output is a JSON array of the query results"
     )
     wrapper: SearxSearchWrapper
     num_results: int = 4
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/shell/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/shell/tool.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,20 @@
-import asyncio
+import logging
 import platform
 import warnings
 from typing import Any, List, Optional, Type, Union
 
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 from langchain_core.tools import BaseTool
 
+logger = logging.getLogger(__name__)
+
 
 class ShellInput(BaseModel):
     """Commands for the Bash Shell tool."""
 
     commands: Union[str, List[str]] = Field(
         ...,
         description="List of shell commands to run. Deserialized using json.loads",
@@ -37,16 +38,16 @@
 def _get_default_bash_process() -> Any:
     """Get default bash process."""
     try:
         from langchain_experimental.llm_bash.bash import BashProcess
     except ImportError:
         raise ImportError(
             "BashProcess has been moved to langchain experimental."
-            "To use this tool, install langchain-experimental "
-            "with `pip install langchain-experimental`."
+            "To use this tool, install gigachain-experimental "
+            "with `pip install gigachain-experimental`."
         )
     return BashProcess(return_err_output=True)
 
 
 def _get_platform() -> str:
     """Get platform."""
     system = platform.system()
@@ -66,24 +67,36 @@
 
     description: str = f"Run shell commands on this {_get_platform()} machine."
     """Description of tool."""
 
     args_schema: Type[BaseModel] = ShellInput
     """Schema for input arguments."""
 
+    ask_human_input: bool = False
+    """
+    If True, prompts the user for confirmation (y/n) before executing 
+    a command generated by the language model in the bash shell.
+    """
+
     def _run(
         self,
         commands: Union[str, List[str]],
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Run commands and return final output."""
-        return self.process.run(commands)
 
-    async def _arun(
-        self,
-        commands: Union[str, List[str]],
-        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
-    ) -> str:
-        """Run commands asynchronously and return final output."""
-        return await asyncio.get_event_loop().run_in_executor(
-            None, self.process.run, commands
-        )
+        print(f"Executing command:\n {commands}")  # noqa: T201
+
+        try:
+            if self.ask_human_input:
+                user_input = input("Proceed with command execution? (y/n): ").lower()
+                if user_input == "y":
+                    return self.process.run(commands)
+                else:
+                    logger.info("Invalid input. User aborted command execution.")
+                    return None  # type: ignore[return-value]
+            else:
+                return self.process.run(commands)
+
+        except Exception as e:
+            logger.error(f"Error during command execution: {e}")
+            return None  # type: ignore[return-value]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/slack/get_channel.py` & `gigachain_community-0.2.0/langchain_community/tools/slack/get_channel.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,36 +1,37 @@
 import json
 import logging
-from typing import Optional
+from typing import Any, Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 
 from langchain_community.tools.slack.base import SlackBaseTool
 
 
 class SlackGetChannel(SlackBaseTool):
     """Tool that gets Slack channel information."""
 
     name: str = "get_channelid_name_dict"
-    description: str = "Use this tool to get channelid-name dict."
+    description: str = (
+        "Use this tool to get channelid-name dict. There is no input to this tool"
+    )
 
     def _run(
-        self,
-        run_manager: Optional[CallbackManagerForToolRun] = None,
+        self, *args: Any, run_manager: Optional[CallbackManagerForToolRun] = None
     ) -> str:
         try:
             logging.getLogger(__name__)
 
             result = self.client.conversations_list()
             channels = result["channels"]
             filtered_result = [
                 {key: channel[key] for key in ("id", "name", "created", "num_members")}
                 for channel in channels
                 if "id" in channel
                 and "name" in channel
                 and "created" in channel
                 and "num_members" in channel
             ]
-            return json.dumps(filtered_result)
+            return json.dumps(filtered_result, ensure_ascii=False)
 
         except Exception as e:
             return "Error creating conversation: {}".format(e)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/slack/get_message.py` & `gigachain_community-0.2.0/langchain_community/tools/slack/get_message.py`

 * *Files 6% similar despite different names*

```diff
@@ -35,10 +35,10 @@
             result = self.client.conversations_history(channel=channel_id)
             messages = result["messages"]
             filtered_messages = [
                 {key: message[key] for key in ("user", "text", "ts")}
                 for message in messages
                 if "user" in message and "text" in message and "ts" in message
             ]
-            return json.dumps(filtered_messages)
+            return json.dumps(filtered_messages, ensure_ascii=False)
         except Exception as e:
             return "Error creating conversation: {}".format(e)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/slack/schedule_message.py` & `gigachain_community-0.2.0/langchain_community/tools/slack/schedule_message.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/slack/send_message.py` & `gigachain_community-0.2.0/langchain_community/tools/slack/send_message.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/slack/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/slack/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Slack tool utils."""
+
 from __future__ import annotations
 
 import logging
 import os
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/sleep/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/sleep/tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Tool for agent to sleep."""
+
 from asyncio import sleep as asleep
 from time import sleep
 from typing import Optional, Type
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/spark_sql/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/spark_sql/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/spark_sql/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/spark_sql/tool.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 # flake8: noqa
 """Tools for interacting with Spark SQL."""
+
 from typing import Any, Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
@@ -93,15 +94,15 @@
 
     @root_validator(pre=True)
     def initialize_llm_chain(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         if "llm_chain" not in values:
             from langchain.chains.llm import LLMChain
 
             values["llm_chain"] = LLMChain(
-                llm=values.get("llm"),
+                llm=values.get("llm"),  # type: ignore[arg-type]
                 prompt=PromptTemplate(
                     template=QUERY_CHECKER, input_variables=["query"]
                 ),
             )
 
         if values["llm_chain"].prompt.input_variables != ["query"]:
             raise ValueError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/sql_database/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/sql_database/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/sql_database/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/sql_database/tool.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,15 @@
 # flake8: noqa
 """Tools for interacting with a SQL database."""
-from typing import Any, Dict, Optional
 
-from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
+from typing import Any, Dict, Optional, Sequence, Type, Union
+
+from sqlalchemy.engine import Result
+
+from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
 
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
 from langchain_core.prompts import PromptTemplate
@@ -20,89 +23,111 @@
 
     db: SQLDatabase = Field(exclude=True)
 
     class Config(BaseTool.Config):
         pass
 
 
+class _QuerySQLDataBaseToolInput(BaseModel):
+    query: str = Field(..., description="A detailed and correct SQL query.")
+
+
 class QuerySQLDataBaseTool(BaseSQLDatabaseTool, BaseTool):
     """Tool for querying a SQL database."""
 
     name: str = "sql_db_query"
     description: str = """
-    Input to this tool is a detailed and correct SQL query, output is a result from the database.
+    Execute a SQL query against the database and get back the result..
     If the query is not correct, an error message will be returned.
     If an error is returned, rewrite the query, check the query, and try again.
     """
+    args_schema: Type[BaseModel] = _QuerySQLDataBaseToolInput
 
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
-    ) -> str:
+    ) -> Union[str, Sequence[Dict[str, Any]], Result]:
         """Execute the query, return the results or an error message."""
         return self.db.run_no_throw(query)
 
 
+class _InfoSQLDatabaseToolInput(BaseModel):
+    table_names: str = Field(
+        ...,
+        description=(
+            "A comma-separated list of the table names for which to return the schema. "
+            "Example input: 'table1, table2, table3'"
+        ),
+    )
+
+
 class InfoSQLDatabaseTool(BaseSQLDatabaseTool, BaseTool):
     """Tool for getting metadata about a SQL database."""
 
     name: str = "sql_db_schema"
-    description: str = """
-    Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables.    
-
-    Example Input: "table1, table2, table3"
-    """
+    description: str = "Get the schema and sample rows for the specified SQL tables."
+    args_schema: Type[BaseModel] = _InfoSQLDatabaseToolInput
 
     def _run(
         self,
         table_names: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Get the schema for tables in a comma-separated list."""
         return self.db.get_table_info_no_throw(
             [t.strip() for t in table_names.split(",")]
         )
 
 
+class _ListSQLDataBaseToolInput(BaseModel):
+    tool_input: str = Field("", description="An empty string")
+
+
 class ListSQLDatabaseTool(BaseSQLDatabaseTool, BaseTool):
     """Tool for getting tables names."""
 
     name: str = "sql_db_list_tables"
-    description: str = "Input is an empty string, output is a comma separated list of tables in the database."
+    description: str = "Input is an empty string, output is a comma-separated list of tables in the database."
+    args_schema: Type[BaseModel] = _ListSQLDataBaseToolInput
 
     def _run(
         self,
         tool_input: str = "",
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
-        """Get the schema for a specific table."""
+        """Get a comma-separated list of table names."""
         return ", ".join(self.db.get_usable_table_names())
 
 
+class _QuerySQLCheckerToolInput(BaseModel):
+    query: str = Field(..., description="A detailed and SQL query to be checked.")
+
+
 class QuerySQLCheckerTool(BaseSQLDatabaseTool, BaseTool):
     """Use an LLM to check if a query is correct.
     Adapted from https://www.patterns.app/blog/2023/01/18/crunchbot-sql-analyst-gpt/"""
 
     template: str = QUERY_CHECKER
     llm: BaseLanguageModel
     llm_chain: Any = Field(init=False)
     name: str = "sql_db_query_checker"
     description: str = """
     Use this tool to double check if your query is correct before executing it.
     Always use this tool before executing a query with sql_db_query!
     """
+    args_schema: Type[BaseModel] = _QuerySQLCheckerToolInput
 
     @root_validator(pre=True)
     def initialize_llm_chain(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         if "llm_chain" not in values:
             from langchain.chains.llm import LLMChain
 
             values["llm_chain"] = LLMChain(
-                llm=values.get("llm"),
+                llm=values.get("llm"),  # type: ignore[arg-type]
                 prompt=PromptTemplate(
                     template=QUERY_CHECKER, input_variables=["dialect", "query"]
                 ),
             )
 
         if values["llm_chain"].prompt.input_variables != ["dialect", "query"]:
             raise ValueError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/stackexchange/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/stackexchange/tool.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from langchain_community.utilities.stackexchange import StackExchangeAPIWrapper
 
 
 class StackExchangeTool(BaseTool):
     """Tool that uses StackExchange"""
 
-    name: str = "StackExchange"
+    name: str = "stack_exchange"
     description: str = (
         "A wrapper around StackExchange. "
         "Useful for when you need to answer specific programming questions"
         "code excerpts, code examples and solutions"
         "Input should be a fully formed question."
     )
     api_wrapper: StackExchangeAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/steam/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/steam/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/steam/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/steam/tool.py`

 * *Files 10% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from langchain_community.utilities.steam import SteamWebAPIWrapper
 
 
 class SteamWebAPIQueryRun(BaseTool):
     """Tool that searches the Steam Web API."""
 
     mode: str
-    name: str = "Steam"
+    name: str = "steam"
     description: str = (
         "A wrapper around Steam Web API."
         "Steam Tool is useful for fetching User profiles and stats, Game data and more!"
         "Input should be the User or Game you want to query."
     )
 
     api_wrapper: SteamWebAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/steamship_image_generation/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/steamship_image_generation/tool.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 - Dall-E
 - Stable Diffusion
 
 To use this tool, you must first set as environment variables:
     STEAMSHIP_API_KEY
 ```
 """
+
 from __future__ import annotations
 
 from enum import Enum
 from typing import TYPE_CHECKING, Dict, Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import root_validator
@@ -37,23 +38,22 @@
 SUPPORTED_IMAGE_SIZES = {
     ModelName.DALL_E: ("256x256", "512x512", "1024x1024"),
     ModelName.STABLE_DIFFUSION: ("512x512", "768x768"),
 }
 
 
 class SteamshipImageGenerationTool(BaseTool):
-
     """Tool used to generate images from a text-prompt."""
 
     model_name: ModelName
     size: Optional[str] = "512x512"
     steamship: Steamship
     return_urls: Optional[bool] = False
 
-    name: str = "GenerateImage"
+    name: str = "generate_image"
     description: str = (
         "Useful for when you need to generate an image."
         "Input: A detailed text-2-image prompt describing an image"
         "Output: the UUID of a generated image"
     )
 
     @root_validator(pre=True)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/steamship_image_generation/utils.py` & `gigachain_community-0.2.0/langchain_community/tools/steamship_image_generation/utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Steamship Utils."""
+
 from __future__ import annotations
 
 import uuid
 from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
     from steamship import Block, Steamship
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/vectorstore/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/vectorstore/tool.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 """Tools for interacting with vectorstores."""
 
 import json
 from typing import Any, Dict, Optional
 
-from langchain_core.callbacks import CallbackManagerForToolRun
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForToolRun,
+    CallbackManagerForToolRun,
+)
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.llms.openai import OpenAI
 
@@ -47,17 +50,36 @@
     ) -> str:
         """Use the tool."""
         from langchain.chains.retrieval_qa.base import RetrievalQA
 
         chain = RetrievalQA.from_chain_type(
             self.llm, retriever=self.vectorstore.as_retriever()
         )
-        return chain.run(
-            query, callbacks=run_manager.get_child() if run_manager else None
+        return chain.invoke(
+            {chain.input_key: query},
+            config={"callbacks": run_manager.get_child() if run_manager else None},
+        )[chain.output_key]
+
+    async def _arun(
+        self,
+        query: str,
+        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
+    ) -> str:
+        """Use the tool asynchronously."""
+        from langchain.chains.retrieval_qa.base import RetrievalQA
+
+        chain = RetrievalQA.from_chain_type(
+            self.llm, retriever=self.vectorstore.as_retriever()
         )
+        return (
+            await chain.ainvoke(
+                {chain.input_key: query},
+                config={"callbacks": run_manager.get_child() if run_manager else None},
+            )
+        )[chain.output_key]
 
 
 class VectorStoreQAWithSourcesTool(BaseVectorStoreTool, BaseTool):
     """Tool for the VectorDBQAWithSources chain."""
 
     @staticmethod
     def get_description(name: str, description: str) -> str:
@@ -83,13 +105,34 @@
             RetrievalQAWithSourcesChain,
         )
 
         chain = RetrievalQAWithSourcesChain.from_chain_type(
             self.llm, retriever=self.vectorstore.as_retriever()
         )
         return json.dumps(
-            chain(
+            chain.invoke(
+                {chain.question_key: query},
+                return_only_outputs=True,
+                config={"callbacks": run_manager.get_child() if run_manager else None},
+            )
+        )
+
+    async def _arun(
+        self,
+        query: str,
+        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
+    ) -> str:
+        """Use the tool asynchronously."""
+        from langchain.chains.qa_with_sources.retrieval import (
+            RetrievalQAWithSourcesChain,
+        )
+
+        chain = RetrievalQAWithSourcesChain.from_chain_type(
+            self.llm, retriever=self.vectorstore.as_retriever()
+        )
+        return json.dumps(
+            await chain.ainvoke(
                 {chain.question_key: query},
                 return_only_outputs=True,
-                callbacks=run_manager.get_child() if run_manager else None,
+                config={"callbacks": run_manager.get_child() if run_manager else None},
             )
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/wikipedia/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/wikipedia/tool.py`

 * *Files 11% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 from langchain_community.utilities.wikipedia import WikipediaAPIWrapper
 
 
 class WikipediaQueryRun(BaseTool):
     """Tool that searches the Wikipedia API."""
 
-    name: str = "Wikipedia"
+    name: str = "wikipedia"
     description: str = (
         "A wrapper around Wikipedia. "
         "Useful for when you need to answer general questions about "
         "people, places, companies, facts, historical events, or other subjects. "
         "Input should be a search query."
     )
     api_wrapper: WikipediaAPIWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/wolfram_alpha/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/wolfram_alpha/tool.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/yahoo_finance_news.py` & `gigachain_community-0.2.0/langchain_community/tools/yahoo_finance_news.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,40 @@
-from typing import Iterable, Optional
+from typing import Iterable, Optional, Type
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.documents import Document
+from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_core.tools import BaseTool
 from requests.exceptions import HTTPError, ReadTimeout
 from urllib3.exceptions import ConnectionError
 
 from langchain_community.document_loaders.web_base import WebBaseLoader
 
 
+class YahooFinanceNewsInput(BaseModel):
+    """Input for the YahooFinanceNews tool."""
+
+    query: str = Field(description="company ticker query to look up")
+
+
 class YahooFinanceNewsTool(BaseTool):
     """Tool that searches financial news on Yahoo Finance."""
 
     name: str = "yahoo_finance_news"
     description: str = (
         "Useful for when you need to find financial news "
         "about a public company. "
         "Input should be a company ticker. "
         "For example, AAPL for Apple, MSFT for Microsoft."
     )
     top_k: int = 10
     """The number of results to return."""
 
+    args_schema: Type[BaseModel] = YahooFinanceNewsInput
+
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Use the Yahoo Finance News tool."""
         try:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/youtube/search.py` & `gigachain_community-0.2.0/langchain_community/tools/youtube/search.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,16 @@
 
 CustomYTSearchTool searches YouTube videos related to a person
 and returns a specified number of video URLs.
 Input to this tool should be a comma separated list,
  - the first part contains a person name
  - and the second(optional) a number that is the
     maximum number of video results to return
- """
+"""
+
 import json
 from typing import Optional
 
 from langchain_core.callbacks import CallbackManagerForToolRun
 
 from langchain_community.tools import BaseTool
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/zapier/prompt.py` & `gigachain_community-0.2.0/langchain_community/tools/zapier/prompt.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/tools/zapier/tool.py` & `gigachain_community-0.2.0/langchain_community/tools/zapier/tool.py`

 * *Files 3% similar despite different names*

```diff
@@ -63,14 +63,15 @@
 ## the ZapierNLAWrapper. If you do this there is no need to initialize
 ## the ZAPIER_NLA_API_KEY env variable
 # zapier = ZapierNLAWrapper(zapier_nla_oauth_access_token="TOKEN_HERE")
 toolkit = ZapierToolkit.from_zapier_nla_wrapper(zapier)
 ```
 
 """
+
 from typing import Any, Dict, Optional
 
 from langchain_core._api import warn_deprecated
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManagerForToolRun,
 )
@@ -78,27 +79,28 @@
 from langchain_core.tools import BaseTool
 
 from langchain_community.tools.zapier.prompt import BASE_ZAPIER_TOOL_PROMPT
 from langchain_community.utilities.zapier import ZapierNLAWrapper
 
 
 class ZapierNLARunAction(BaseTool):
-    """
-    Args:
+    """Tool to run a specific action from the user's exposed actions.
+
+    Params:
         action_id: a specific action ID (from list actions) of the action to execute
             (the set api_key must be associated with the action owner)
         instructions: a natural language instruction string for using the action
             (eg. "get the latest email from Mike Knoop" for "Gmail: find email" action)
         params: a dict, optional. Any params provided will *override* AI guesses
             from `instructions` (see "understanding the AI guessing flow" here:
             https://nla.zapier.com/docs/using-the-api#ai-guessing)
 
     """
 
-    api_wrapper: ZapierNLAWrapper = Field(default_factory=ZapierNLAWrapper)
+    api_wrapper: ZapierNLAWrapper = Field(default_factory=ZapierNLAWrapper)  # type: ignore[arg-type]
     action_id: str
     params: Optional[dict] = None
     base_prompt: str = BASE_ZAPIER_TOOL_PROMPT
     zapier_description: str
     params_schema: Dict[str, str] = Field(default_factory=dict)
     name: str = ""
     description: str = ""
@@ -163,25 +165,21 @@
 )
 
 
 # other useful actions
 
 
 class ZapierNLAListActions(BaseTool):
-    """
-    Args:
-        None
-
-    """
+    """Tool to list all exposed actions for the user."""
 
     name: str = "ZapierNLA_list_actions"
     description: str = BASE_ZAPIER_TOOL_PROMPT + (
         "This tool returns a list of the user's exposed actions."
     )
-    api_wrapper: ZapierNLAWrapper = Field(default_factory=ZapierNLAWrapper)
+    api_wrapper: ZapierNLAWrapper = Field(default_factory=ZapierNLAWrapper)  # type: ignore[arg-type]
 
     def _run(
         self,
         _: str = "",
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> str:
         """Use the Zapier NLA tool to return a list of all exposed user actions."""
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/anthropic.py` & `gigachain_community-0.2.0/langchain_community/utilities/anthropic.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/apify.py` & `gigachain_community-0.2.0/langchain_community/utilities/apify.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/arcee.py` & `gigachain_community-0.2.0/langchain_community/utilities/arcee.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/arxiv.py` & `gigachain_community-0.2.0/langchain_community/utilities/arxiv.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """Util that calls Arxiv."""
+
 import logging
 import os
 import re
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, Iterator, List, Optional
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 
 logger = logging.getLogger(__name__)
 
 
@@ -23,14 +24,15 @@
     corresponding to the arxiv identifier.
     It limits the Document content by doc_content_chars_max.
     Set doc_content_chars_max=None if you don't want to limit the content size.
 
     Attributes:
         top_k_results: number of the top-scored document used for the arxiv tool
         ARXIV_MAX_QUERY_LENGTH: the cut limit on the query used for the arxiv tool.
+        continue_on_failure (bool): If True, continue loading other URLs on failure.
         load_max_docs: a limit to the number of loaded documents
         load_all_available_meta:
             if True: the `metadata` of the loaded Documents contains all available
             meta info (see https://lukasschwab.me/arxiv.py/index.html#Result),
             if False: the `metadata` contains only the published date, title,
             authors and summary.
         doc_content_chars_max: an optional cut limit for the length of a document's
@@ -50,14 +52,15 @@
             arxiv.run("tree of thought llm)
     """
 
     arxiv_search: Any  #: :meta private:
     arxiv_exceptions: Any  # :meta private:
     top_k_results: int = 3
     ARXIV_MAX_QUERY_LENGTH: int = 300
+    continue_on_failure: bool = False
     load_max_docs: int = 100
     load_all_available_meta: bool = False
     doc_content_chars_max: Optional[int] = 4000
 
     def is_arxiv_identifier(self, query: str) -> bool:
         """Check if a query is an arxiv identifier."""
         arxiv_identifier_pattern = r"\d{2}(0[1-9]|1[0-2])\.\d{4,5}(v\d+|)|\d{7}.*"
@@ -173,15 +176,30 @@
         Returns: a list of documents with the document.page_content in text format
 
         Performs an arxiv search, downloads the top k results as PDFs, loads
         them as Documents, and returns them in a List.
 
         Args:
             query: a plaintext search query
-        """  # noqa: E501
+        """
+        return list(self.lazy_load(query))
+
+    def lazy_load(self, query: str) -> Iterator[Document]:
+        """
+        Run Arxiv search and get the article texts plus the article meta information.
+        See https://lukasschwab.me/arxiv.py/index.html#Search
+
+        Returns: documents with the document.page_content in text format
+
+        Performs an arxiv search, downloads the top k results as PDFs, loads
+        them as Documents, and returns them.
+
+        Args:
+            query: a plaintext search query
+        """
         try:
             import fitz
         except ImportError:
             raise ImportError(
                 "PyMuPDF package not found, please install it with "
                 "`pip install pymupdf`"
             )
@@ -196,25 +214,30 @@
                 ).results()
             else:
                 results = self.arxiv_search(  # type: ignore
                     query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.load_max_docs
                 ).results()
         except self.arxiv_exceptions as ex:
             logger.debug("Error on arxiv: %s", ex)
-            return []
+            return
 
-        docs: List[Document] = []
         for result in results:
             try:
                 doc_file_name: str = result.download_pdf()
                 with fitz.open(doc_file_name) as doc_file:
                     text: str = "".join(page.get_text() for page in doc_file)
             except (FileNotFoundError, fitz.fitz.FileDataError) as f_ex:
                 logger.debug(f_ex)
                 continue
+            except Exception as e:
+                if self.continue_on_failure:
+                    logger.error(e)
+                    continue
+                else:
+                    raise e
             if self.load_all_available_meta:
                 extra_metadata = {
                     "entry_id": result.entry_id,
                     "published_first_time": str(result.published.date()),
                     "comment": result.comment,
                     "journal_ref": result.journal_ref,
                     "doi": result.doi,
@@ -227,13 +250,11 @@
             metadata = {
                 "Published": str(result.updated.date()),
                 "Title": result.title,
                 "Authors": ", ".join(a.name for a in result.authors),
                 "Summary": result.summary,
                 **extra_metadata,
             }
-            doc = Document(
+            yield Document(
                 page_content=text[: self.doc_content_chars_max], metadata=metadata
             )
-            docs.append(doc)
             os.remove(doc_file_name)
-        return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/awslambda.py` & `gigachain_community-0.2.0/langchain_community/utilities/awslambda.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Lambda."""
+
 import json
 from typing import Any, Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 
 
 class LambdaWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/bibtex.py` & `gigachain_community-0.2.0/langchain_community/utilities/bibtex.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls bibtexparser."""
+
 import logging
 from typing import Any, Dict, List, Mapping
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 
 logger = logging.getLogger(__name__)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/bing_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/bing_search.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,25 +1,17 @@
-"""Util that calls Bing Search.
-
-In order to set this up, follow instructions at:
-https://levelup.gitconnected.com/api-tutorial-how-to-use-bing-web-search-api-in-python-4165d5592a7e
-"""
+"""Util that calls Bing Search."""
 from typing import Dict, List
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 class BingSearchAPIWrapper(BaseModel):
-    """Wrapper for Bing Search API.
-
-    In order to set this up, follow instructions at:
-    https://levelup.gitconnected.com/api-tutorial-how-to-use-bing-web-search-api-in-python-4165d5592a7e
-    """
+    """Wrapper for Bing Search API."""
 
     bing_subscription_key: str
     bing_search_url: str
     k: int = 10
     search_kwargs: dict = Field(default_factory=dict)
     """Additional keyword arguments to pass to the search request."""
 
@@ -40,15 +32,17 @@
         response = requests.get(
             self.bing_search_url,
             headers=headers,
             params=params,  # type: ignore
         )
         response.raise_for_status()
         search_results = response.json()
-        return search_results["webPages"]["value"]
+        if "webPages" in search_results:
+            return search_results["webPages"]["value"]
+        return []
 
     @root_validator(pre=True)
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and endpoint exists in environment."""
         bing_subscription_key = get_from_dict_or_env(
             values, "bing_subscription_key", "BING_SUBSCRIPTION_KEY"
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/brave_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/brave_search.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 
         Returns: The results as a list of Documents.
 
         """
         results = self._search_request(query)
         return [
             Document(
-                page_content=item.get("description"),
+                page_content=item.get("description"),  # type: ignore[arg-type]
                 metadata={"title": item.get("title"), "link": item.get("url")},
             )
             for item in results
         ]
 
     def _search_request(self, query: str) -> List[dict]:
         headers = {
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/clickup.py` & `gigachain_community-0.2.0/langchain_community/utilities/clickup.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls clickup."""
+
 import json
 import warnings
 from dataclasses import asdict, dataclass, fields
 from typing import Any, Dict, List, Mapping, Optional, Tuple, Type, Union
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
@@ -191,20 +192,23 @@
             output[attribute.name] = data[attribute.name]
     return output
 
 
 def load_query(
     query: str, fault_tolerant: bool = False
 ) -> Tuple[Optional[Dict], Optional[str]]:
-    """Attempts to parse a JSON string and return the parsed object.
+    """Parse a JSON string and return the parsed object.
 
     If parsing fails, returns an error message.
 
     :param query: The JSON string to parse.
     :return: A tuple containing the parsed object or None and an error message or None.
+
+    Exceptions:
+        json.JSONDecodeError: If the input is not a valid JSON string.
     """
     try:
         return json.loads(query), None
     except json.JSONDecodeError as e:
         if fault_tolerant:
             return (
                 None,
@@ -304,18 +308,18 @@
             "code": code,
         }
 
         response = requests.post(url, params=params)
         data = response.json()
 
         if "access_token" not in data:
-            print(f"Error: {data}")
+            print(f"Error: {data}")  # noqa: T201
             if "ECODE" in data and data["ECODE"] == "OAUTH_014":
                 url = ClickupAPIWrapper.get_access_code_url(oauth_client_id)
-                print(
+                print(  # noqa: T201
                     "You already used this code once. Generate a new one.",
                     f"Our best guess for the url to get a new code is:\n{url}",
                 )
             return None
 
         return data["access_token"]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/dalle_image_generator.py` & `gigachain_community-0.2.0/langchain_community/utilities/dalle_image_generator.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Utility that calls OpenAI's Dall-E Image Generator."""
+
 import logging
 import os
 from typing import Any, Dict, Mapping, Optional, Tuple, Union
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.utils import (
     get_from_dict_or_env,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/dataforseo_api_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/dataforseo_api_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/duckduckgo_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/duckduckgo_search.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 """Util that calls DuckDuckGo Search.
 
 No setup required. Free.
 https://pypi.org/project/duckduckgo-search/
 """
+
 from typing import Dict, List, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 
 
 class DuckDuckGoSearchAPIWrapper(BaseModel):
     """Wrapper for DuckDuckGo Search API.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/github.py` & `gigachain_community-0.2.0/langchain_community/utilities/github.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls GitHub."""
+
 from __future__ import annotations
 
 import json
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
@@ -81,15 +82,21 @@
             raise ValueError(
                 f"Please make sure to install the created github app with id "
                 f"{github_app_id} on the repo: {github_repository}"
                 "More instructions can be found at "
                 "https://docs.github.com/en/apps/using-"
                 "github-apps/installing-your-own-github-app"
             )
-        installation = installation[0]
+        try:
+            installation = installation[0]
+        except ValueError as e:
+            raise ValueError(
+                "Please make sure to give correct github parameters "
+                f"Error message: {e}"
+            )
         # create a GitHub instance:
         g = installation.get_github_for_installation()
         repo = g.get_repo(github_repository)
 
         github_base_branch = get_from_dict_or_env(
             values,
             "github_base_branch",
@@ -287,15 +294,15 @@
                 )
             except GithubException as e:
                 if e.status == 422 and "Reference already exists" in e.data["message"]:
                     i += 1
                     new_branch_name = f"{proposed_branch_name}_v{i}"
                 else:
                     # Handle any other exceptions
-                    print(f"Failed to create branch. Error: {e}")
+                    print(f"Failed to create branch. Error: {e}")  # noqa: T201
                     raise Exception(
                         "Unable to create branch name from proposed_branch_name: "
                         f"{proposed_branch_name}"
                     )
         return (
             "Unable to create branch. "
             "At least 1000 branches exist with named derived from "
@@ -304,15 +311,15 @@
 
     def list_files_in_bot_branch(self) -> str:
         """
         Fetches all files in the active branch of the repo,
         the branch the bot uses to make changes.
 
         Returns:
-            str: A plaintext list containing the the filepaths in the branch.
+            str: A plaintext list containing the filepaths in the branch.
         """
         files: List[str] = []
         try:
             contents = self.github_repo_instance.get_contents(
                 "", ref=self.active_branch
             )
             for content in contents:
@@ -417,23 +424,23 @@
                 try:
                     file_metadata_response = requests.get(file.contents_url)
                     if file_metadata_response.status_code == 200:
                         download_url = json.loads(file_metadata_response.text)[
                             "download_url"
                         ]
                     else:
-                        print(f"Failed to download file: {file.contents_url}, skipping")
+                        print(f"Failed to download file: {file.contents_url}, skipping")  # noqa: T201
                         continue
 
                     file_content_response = requests.get(download_url)
                     if file_content_response.status_code == 200:
                         # Save the content as a UTF-8 string
                         file_content = file_content_response.text
                     else:
-                        print(
+                        print(  # noqa: T201
                             "Failed downloading file content "
                             f"(Error {file_content_response.status_code}). Skipping"
                         )
                         continue
 
                     file_tokens = len(
                         tiktoken.get_encoding("cl100k_base").encode(
@@ -447,15 +454,15 @@
                                 "contents": file_content,
                                 "additions": file.additions,
                                 "deletions": file.deletions,
                             }
                         )
                         total_tokens += file_tokens
                 except Exception as e:
-                    print(f"Error when reading files from a PR on github. {e}")
+                    print(f"Error when reading files from a PR on github. {e}")  # noqa: T201
             page += 1
         return pr_files
 
     def get_pull_request(self, pr_number: int) -> Dict[str, Any]:
         """
         Fetches a specific pull request and its first 10 comments,
         limited by max_tokens.
@@ -724,15 +731,15 @@
         Parameters:
             query(str): The search query
 
         Returns:
             str: A string containing the first 5 issues and pull requests
         """
         search_result = self.github.search_issues(query, repo=self.github_repository)
-        max_items = min(5, len(search_result))
+        max_items = min(5, search_result.totalCount)
         results = [f"Top {max_items} results:"]
         for issue in search_result[:max_items]:
             results.append(
                 f"Title: {issue.title}, Number: {issue.number}, State: {issue.state}"
             )
         return "\n".join(results)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/gitlab.py` & `gigachain_community-0.2.0/langchain_community/utilities/gitlab.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls gitlab."""
+
 from __future__ import annotations
 
 import json
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
@@ -35,15 +36,15 @@
         extra = Extra.forbid
 
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
 
         gitlab_url = get_from_dict_or_env(
-            values, "gitlab_url", "GITLAB_URL", default=None
+            values, "gitlab_url", "GITLAB_URL", default="https://gitlab.com"
         )
         gitlab_repository = get_from_dict_or_env(
             values, "gitlab_repository", "GITLAB_REPOSITORY"
         )
 
         gitlab_personal_access_token = get_from_dict_or_env(
             values, "gitlab_personal_access_token", "GITLAB_PERSONAL_ACCESS_TOKEN"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/golden_query.py` & `gigachain_community-0.2.0/langchain_community/utilities/golden_query.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Golden."""
+
 import json
 from typing import Dict, Optional
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_finance.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_finance.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Finance Search."""
+
 from typing import Any, Dict, Optional, cast
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 
 class GoogleFinanceAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_jobs.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_jobs.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Scholar Search."""
+
 from typing import Any, Dict, Optional, cast
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 
 class GoogleJobsAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_lens.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_lens.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Lens Search."""
+
 from typing import Any, Dict, Optional, cast
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 
@@ -60,25 +61,29 @@
 
         responseValue = response.json()
 
         if responseValue["search_metadata"]["status"] != "Success":
             return "Google Lens search failed"
 
         xs = ""
-        if len(responseValue["knowledge_graph"]) > 0:
+        if (
+            "knowledge_graph" in responseValue
+            and len(responseValue["knowledge_graph"]) > 0
+        ):
             subject = responseValue["knowledge_graph"][0]
             xs += f"Subject:{subject['title']}({subject['subtitle']})\n"
             xs += f"Link to subject:{subject['link']}\n\n"
         xs += "Related Images:\n\n"
         for image in responseValue["visual_matches"]:
             xs += f"Title: {image['title']}\n"
             xs += f"Source({image['source']}): {image['link']}\n"
             xs += f"Image: {image['thumbnail']}\n\n"
-        xs += (
-            "Reverse Image Search"
-            + f"Link: {responseValue['reverse_image_search']['link']}\n"
-        )
-        print(xs)
+        if "reverse_image_search" in responseValue:
+            xs += (
+                "Reverse Image Search"
+                + f"Link: {responseValue['reverse_image_search']['link']}\n"
+            )
+        print(xs)  # noqa: T201
 
         docs = [xs]
 
         return "\n\n".join(docs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_places_api.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_places_api.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,22 @@
-"""Chain that calls Google Places API.
-"""
+"""Chain that calls Google Places API."""
 
 import logging
 from typing import Any, Dict, Optional
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GooglePlacesAPIWrapper",
+)
 class GooglePlacesAPIWrapper(BaseModel):
     """Wrapper around Google Places API.
 
     To use, you should have the ``googlemaps`` python package installed,
      **an API key for the google maps platform**,
      and the environment variable ''GPLACES_API_KEY''
      set with your API key , or pass 'gplaces_api_key'
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_scholar.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_scholar.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Scholar Search."""
+
 from typing import Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 class GoogleScholarAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_search.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,22 @@
 """Util that calls Google Search."""
+
 from typing import Any, Dict, List, Optional
 
+import httplib2
+from langchain_core._api.deprecation import deprecated
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
+@deprecated(
+    since="0.0.33",
+    removal="0.3.0",
+    alternative_import="langchain_google_community.GoogleSearchAPIWrapper",
+)
 class GoogleSearchAPIWrapper(BaseModel):
     """Wrapper for Google Search API.
 
     Adapted from: Instructions adapted from https://stackoverflow.com/questions/
     37083058/
     programmatically-searching-google-in-python-using-custom-search
 
@@ -38,25 +46,36 @@
 
     4. Setup Custom Search Engine so you can search the entire web
     - Create a custom search engine here: https://programmablesearchengine.google.com/.
     - In `What to search` to search, pick the `Search the entire Web` option.
     After search engine is created, you can click on it and find `Search engine ID`
       on the Overview page.
 
+    5. How to use proxy (you need socks5 proxy):
+    ```python
+    from httplib2 import socks, ProxyInfo
+
+    http = build_http()
+    http.proxy_info = ProxyInfo(socks.PROXY_TYPE_SOCKS5, "localhost", 1081)
+    search = GoogleSearchAPIWrapper(http=http)
+    ```
+
     """
 
     search_engine: Any  #: :meta private:
     google_api_key: Optional[str] = None
     google_cse_id: Optional[str] = None
     k: int = 10
     siterestrict: bool = False
+    http: Optional[httplib2.Http] = None
 
     class Config:
         """Configuration for this pydantic object."""
 
+        arbitrary_types_allowed = True
         extra = Extra.forbid
 
     def _google_search_results(self, search_term: str, **kwargs: Any) -> List[dict]:
         cse = self.search_engine.cse()
         if self.siterestrict:
             cse = cse.siterestrict()
         res = cse.list(q=search_term, cx=self.google_cse_id, **kwargs).execute()
@@ -79,15 +98,20 @@
         except ImportError:
             raise ImportError(
                 "google-api-python-client is not installed. "
                 "Please install it with `pip install google-api-python-client"
                 ">=2.100.0`"
             )
 
-        service = build("customsearch", "v1", developerKey=google_api_key)
+        service = build(
+            "customsearch",
+            "v1",
+            developerKey=google_api_key,
+            http=values.get("http", None),
+        )
         values["search_engine"] = service
 
         return values
 
     def run(self, query: str) -> str:
         """Run query through GoogleSearch and parse result."""
         snippets = []
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_serper.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_serper.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Search using the Serper.dev API."""
+
 from typing import Any, Dict, List, Optional
 
 import aiohttp
 import requests
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 from langchain_core.utils import get_from_dict_or_env
 from typing_extensions import Literal
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/google_trends.py` & `gigachain_community-0.2.0/langchain_community/utilities/google_trends.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Google Scholar Search."""
+
 from typing import Any, Dict, Optional, cast
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
 
 
 class GoogleTrendsAPIWrapper(BaseModel):
@@ -61,15 +62,20 @@
             "engine": "google_trends",
             "api_key": serpapi_api_key.get_secret_value(),
             "q": query,
         }
 
         total_results = []
         client = self.serp_search_engine(params)
-        total_results = client.get_dict()["interest_over_time"]["timeline_data"]
+        client_dict = client.get_dict()
+        total_results = (
+            client_dict["interest_over_time"]["timeline_data"]
+            if "interest_over_time" in client_dict
+            else None
+        )
 
         if not total_results:
             return "No good Trend Result was found"
 
         start_date = total_results[0]["date"].split()
         end_date = total_results[-1]["date"].split()
         values = [
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/jira.py` & `gigachain_community-0.2.0/langchain_community/utilities/jira.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Jira."""
+
 from typing import Any, Dict, List, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 # TODO: think about error handling, more specific api specs, and jql/project limits
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/max_compute.py` & `gigachain_community-0.2.0/langchain_community/utilities/max_compute.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/merriam_webster.py` & `gigachain_community-0.2.0/langchain_community/utilities/merriam_webster.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Merriam-Webster."""
+
 import json
 from typing import Dict, Iterator, List, Optional
 from urllib.parse import quote
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/metaphor_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/metaphor_search.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """Util that calls Metaphor Search API.
 
 In order to set this up, follow instructions at:
 """
+
 import json
 from typing import Dict, List, Optional
 
 import aiohttp
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/nasa.py` & `gigachain_community-0.2.0/langchain_community/utilities/nasa.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls several NASA APIs."""
+
 import json
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel
 
 IMAGE_AND_VIDEO_LIBRARY_URL = "https://images-api.nasa.gov"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/opaqueprompts.py` & `gigachain_community-0.2.0/langchain_community/utilities/opaqueprompts.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/openapi.py` & `gigachain_community-0.2.0/langchain_community/utilities/openapi.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Utility functions for parsing an OpenAPI spec."""
+
 from __future__ import annotations
 
 import copy
 import json
 import logging
 import re
 from enum import Enum
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/openweathermap.py` & `gigachain_community-0.2.0/langchain_community/utilities/openweathermap.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls OpenWeatherMap using PyOWM."""
+
 from typing import Any, Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 class OpenWeatherMapAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/outline.py` & `gigachain_community-0.2.0/langchain_community/utilities/outline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Outline."""
+
 import logging
 from typing import Any, Dict, List, Optional
 
 import requests
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/portkey.py` & `gigachain_community-0.2.0/langchain_community/utilities/portkey.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/powerbi.py` & `gigachain_community-0.2.0/langchain_community/utilities/powerbi.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around a Power BI endpoint."""
+
 from __future__ import annotations
 
 import asyncio
 import logging
 import os
 from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Union
 
@@ -247,15 +248,15 @@
                 return response_json
 
 
 def json_to_md(
     json_contents: List[Dict[str, Union[str, int, float]]],
     table_name: Optional[str] = None,
 ) -> str:
-    """Converts a JSON object to a markdown table."""
+    """Convert a JSON object to a markdown table."""
     if len(json_contents) == 0:
         return ""
     output_md = ""
     headers = json_contents[0].keys()
     for header in headers:
         header.replace("[", ".").replace("]", "")
         if table_name:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/pubmed.py` & `gigachain_community-0.2.0/langchain_community/utilities/pubmed.py`

 * *Files 1% similar despite different names*

```diff
@@ -140,15 +140,15 @@
             try:
                 result = urllib.request.urlopen(url)
                 break
             except urllib.error.HTTPError as e:
                 if e.code == 429 and retry < self.max_retry:
                     # Too Many Requests errors
                     # wait for an exponentially increasing amount of time
-                    print(
+                    print(  # noqa: T201
                         f"Too Many Requests, "
                         f"waiting for {self.sleep_time:.2f} seconds..."
                     )
                     time.sleep(self.sleep_time)
                     self.sleep_time *= 2
                     retry += 1
                 else:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/python.py` & `gigachain_community-0.2.0/langchain_community/utilities/python.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/reddit_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/reddit_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/requests.py` & `gigachain_community-0.2.0/langchain_community/utilities/requests.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 """Lightweight wrapper around requests library, with async support."""
+
 from contextlib import asynccontextmanager
-from typing import Any, AsyncGenerator, Dict, Optional
+from typing import Any, AsyncGenerator, Dict, Literal, Optional, Union
 
 import aiohttp
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra
+from requests import Response
 
 
 class Requests(BaseModel):
     """Wrapper around requests to handle auth and async.
 
     The main purpose of this wrapper is to handle authentication (by saving
     headers) and enable easy async methods on the same base object.
@@ -104,77 +106,121 @@
         self, url: str, **kwargs: Any
     ) -> AsyncGenerator[aiohttp.ClientResponse, None]:
         """DELETE the URL and return the text asynchronously."""
         async with self._arequest("DELETE", url, **kwargs) as response:
             yield response
 
 
-class TextRequestsWrapper(BaseModel):
-    """Lightweight wrapper around requests library.
-
-    The main purpose of this wrapper is to always return a text output.
-    """
+class GenericRequestsWrapper(BaseModel):
+    """Lightweight wrapper around requests library."""
 
     headers: Optional[Dict[str, str]] = None
     aiosession: Optional[aiohttp.ClientSession] = None
     auth: Optional[Any] = None
+    response_content_type: Literal["text", "json"] = "text"
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
         arbitrary_types_allowed = True
 
     @property
     def requests(self) -> Requests:
         return Requests(
             headers=self.headers, aiosession=self.aiosession, auth=self.auth
         )
 
-    def get(self, url: str, **kwargs: Any) -> str:
+    def _get_resp_content(self, response: Response) -> Union[str, Dict[str, Any]]:
+        if self.response_content_type == "text":
+            return response.text
+        elif self.response_content_type == "json":
+            return response.json()
+        else:
+            raise ValueError(f"Invalid return type: {self.response_content_type}")
+
+    async def _aget_resp_content(
+        self, response: aiohttp.ClientResponse
+    ) -> Union[str, Dict[str, Any]]:
+        if self.response_content_type == "text":
+            return await response.text()
+        elif self.response_content_type == "json":
+            return await response.json()
+        else:
+            raise ValueError(f"Invalid return type: {self.response_content_type}")
+
+    def get(self, url: str, **kwargs: Any) -> Union[str, Dict[str, Any]]:
         """GET the URL and return the text."""
-        return self.requests.get(url, **kwargs).text
+        return self._get_resp_content(self.requests.get(url, **kwargs))
 
-    def post(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    def post(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """POST to the URL and return the text."""
-        return self.requests.post(url, data, **kwargs).text
+        return self._get_resp_content(self.requests.post(url, data, **kwargs))
 
-    def patch(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    def patch(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """PATCH the URL and return the text."""
-        return self.requests.patch(url, data, **kwargs).text
+        return self._get_resp_content(self.requests.patch(url, data, **kwargs))
 
-    def put(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    def put(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """PUT the URL and return the text."""
-        return self.requests.put(url, data, **kwargs).text
+        return self._get_resp_content(self.requests.put(url, data, **kwargs))
 
-    def delete(self, url: str, **kwargs: Any) -> str:
+    def delete(self, url: str, **kwargs: Any) -> Union[str, Dict[str, Any]]:
         """DELETE the URL and return the text."""
-        return self.requests.delete(url, **kwargs).text
+        return self._get_resp_content(self.requests.delete(url, **kwargs))
 
-    async def aget(self, url: str, **kwargs: Any) -> str:
+    async def aget(self, url: str, **kwargs: Any) -> Union[str, Dict[str, Any]]:
         """GET the URL and return the text asynchronously."""
         async with self.requests.aget(url, **kwargs) as response:
-            return await response.text()
+            return await self._aget_resp_content(response)
 
-    async def apost(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    async def apost(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """POST to the URL and return the text asynchronously."""
         async with self.requests.apost(url, data, **kwargs) as response:
-            return await response.text()
+            return await self._aget_resp_content(response)
 
-    async def apatch(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    async def apatch(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """PATCH the URL and return the text asynchronously."""
         async with self.requests.apatch(url, data, **kwargs) as response:
-            return await response.text()
+            return await self._aget_resp_content(response)
 
-    async def aput(self, url: str, data: Dict[str, Any], **kwargs: Any) -> str:
+    async def aput(
+        self, url: str, data: Dict[str, Any], **kwargs: Any
+    ) -> Union[str, Dict[str, Any]]:
         """PUT the URL and return the text asynchronously."""
         async with self.requests.aput(url, data, **kwargs) as response:
-            return await response.text()
+            return await self._aget_resp_content(response)
 
-    async def adelete(self, url: str, **kwargs: Any) -> str:
+    async def adelete(self, url: str, **kwargs: Any) -> Union[str, Dict[str, Any]]:
         """DELETE the URL and return the text asynchronously."""
         async with self.requests.adelete(url, **kwargs) as response:
-            return await response.text()
+            return await self._aget_resp_content(response)
+
+
+class JsonRequestsWrapper(GenericRequestsWrapper):
+    """Lightweight wrapper around requests library, with async support.
+
+    The main purpose of this wrapper is to always return a json output."""
+
+    response_content_type: Literal["text", "json"] = "json"
+
+
+class TextRequestsWrapper(GenericRequestsWrapper):
+    """Lightweight wrapper around requests library, with async support.
+
+    The main purpose of this wrapper is to always return a text output."""
+
+    response_content_type: Literal["text", "json"] = "text"
 
 
 # For backwards compatibility
 RequestsWrapper = TextRequestsWrapper
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/scenexplain.py` & `gigachain_community-0.2.0/langchain_community/utilities/scenexplain.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 """Util that calls SceneXplain.
 
 In order to set this up, you need API key for the SceneXplain API.
 You can obtain a key by following the steps below.
 - Sign up for a free account at https://scenex.jina.ai/.
 - Navigate to the API Access page (https://scenex.jina.ai/api) and create a new API key.
 """
+
 from typing import Dict
 
 import requests
 from langchain_core.pydantic_v1 import BaseModel, BaseSettings, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/searchapi.py` & `gigachain_community-0.2.0/langchain_community/utilities/searchapi.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/searx_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/searx_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -221,15 +221,15 @@
         if v:
             # requests.urllib3.disable_warnings()
             try:
                 import urllib3
 
                 urllib3.disable_warnings()
             except ImportError as e:
-                print(e)
+                print(e)  # noqa: T201
 
         return v
 
     @root_validator()
     def validate_params(cls, values: Dict) -> Dict:
         """Validate that custom searx params are merged with default ones."""
         user_params = values["params"]
@@ -242,15 +242,15 @@
 
         categories = values.get("categories")
         if categories:
             values["params"]["categories"] = ",".join(categories)
 
         searx_host = get_from_dict_or_env(values, "searx_host", "SEARX_HOST")
         if not searx_host.startswith("http"):
-            print(
+            print(  # noqa: T201
                 f"Warning: missing the url scheme on host \
                 ! assuming secure https://{searx_host} "
             )
             searx_host = "https://" + searx_host
         elif searx_host.startswith("http://"):
             values["unsecure"] = True
             cls.disable_ssl_warnings(True)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/serpapi.py` & `gigachain_community-0.2.0/langchain_community/utilities/serpapi.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 """Chain that calls SerpAPI.
 
 Heavily borrowed from https://github.com/ofirpress/self-ask
 """
+
 import os
 import sys
 from typing import Any, Dict, Optional, Tuple
 
 import aiohttp
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
@@ -65,15 +66,15 @@
         )
         values["serpapi_api_key"] = serpapi_api_key
         try:
             from serpapi import GoogleSearch
 
             values["search_engine"] = GoogleSearch
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import serpapi python package. "
                 "Please install it with `pip install google-search-results`."
             )
         return values
 
     async def arun(self, query: str, **kwargs: Any) -> str:
         """Run query through SerpAPI and parse result async."""
@@ -207,14 +208,19 @@
             elif "rich_snippet_table" in organic_result.keys():
                 snippets.append(organic_result["rich_snippet_table"])
             elif "link" in organic_result.keys():
                 snippets.append(organic_result["link"])
 
         if "buying_guide" in res.keys():
             snippets.append(res["buying_guide"])
-        if "local_results" in res.keys() and "places" in res["local_results"].keys():
+        if "local_results" in res and isinstance(res["local_results"], list):
+            snippets += res["local_results"]
+        if (
+            "local_results" in res.keys()
+            and isinstance(res["local_results"], dict)
+            and "places" in res["local_results"].keys()
+        ):
             snippets.append(res["local_results"]["places"])
-
         if len(snippets) > 0:
             return str(snippets)
         else:
             return "No good search result found"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/spark_sql.py` & `gigachain_community-0.2.0/langchain_community/utilities/spark_sql.py`

 * *Files 2% similar despite different names*

```diff
@@ -78,15 +78,15 @@
     ) -> SparkSQL:
         """Creating a remote Spark Session via Spark connect.
         For example: SparkSQL.from_uri("sc://localhost:15002")
         """
         try:
             from pyspark.sql import SparkSession
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "pyspark is not installed. Please install it with `pip install pyspark`"
             )
 
         spark = SparkSession.builder.remote(database_uri).getOrCreate()
         return cls(spark, **kwargs)
 
     def get_usable_table_names(self) -> Iterable[str]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/sql_database.py` & `gigachain_community-0.2.0/langchain_community/utilities/sql_database.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,19 +1,28 @@
 """SQLAlchemy wrapper around a database."""
+
 from __future__ import annotations
 
-import warnings
 from typing import Any, Dict, Iterable, List, Literal, Optional, Sequence, Union
 
 import sqlalchemy
+from langchain_core._api import deprecated
 from langchain_core.utils import get_from_env
-from sqlalchemy import MetaData, Table, create_engine, inspect, select, text
-from sqlalchemy.engine import Engine
+from sqlalchemy import (
+    MetaData,
+    Table,
+    create_engine,
+    inspect,
+    select,
+    text,
+)
+from sqlalchemy.engine import Engine, Result
 from sqlalchemy.exc import ProgrammingError, SQLAlchemyError
 from sqlalchemy.schema import CreateTable
+from sqlalchemy.sql.expression import Executable
 from sqlalchemy.types import NullType
 
 
 def _format_index(index: sqlalchemy.engine.interfaces.ReflectedIndex) -> str:
     return (
         f'Name: {index["name"]}, Unique: {index["unique"]},'
         f' Columns: {str(index["column_names"])}'
@@ -46,14 +55,15 @@
         ignore_tables: Optional[List[str]] = None,
         include_tables: Optional[List[str]] = None,
         sample_rows_in_table_info: int = 3,
         indexes_in_table_info: bool = False,
         custom_table_info: Optional[dict] = None,
         view_support: bool = False,
         max_string_length: int = 300,
+        lazy_table_reflection: bool = False,
     ):
         """Create engine from database URI."""
         self._engine = engine
         self._schema = schema
         if include_tables and ignore_tables:
             raise ValueError("Cannot specify both include_tables and ignore_tables")
 
@@ -101,23 +111,25 @@
             self._custom_table_info = dict(
                 (table, self._custom_table_info[table])
                 for table in self._custom_table_info
                 if table in intersection
             )
 
         self._max_string_length = max_string_length
+        self._view_support = view_support
 
         self._metadata = metadata or MetaData()
-        # including view support if view_support = true
-        self._metadata.reflect(
-            views=view_support,
-            bind=self._engine,
-            only=list(self._usable_tables),
-            schema=self._schema,
-        )
+        if not lazy_table_reflection:
+            # including view support if view_support = true
+            self._metadata.reflect(
+                views=view_support,
+                bind=self._engine,
+                only=list(self._usable_tables),
+                schema=self._schema,
+            )
 
     @classmethod
     def from_uri(
         cls, database_uri: str, engine_args: Optional[dict] = None, **kwargs: Any
     ) -> SQLDatabase:
         """Construct a SQLAlchemy engine from URI."""
         _engine_args = engine_args or {}
@@ -174,15 +186,15 @@
                 'warehouse_id' and 'cluster_id' are provided, or if neither
                 'warehouse_id' nor 'cluster_id' are provided and it's not executing
                 inside a Databricks notebook.
         """
         try:
             from databricks import sql  # noqa: F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "databricks-sql-connector package not found, please install with"
                 " `pip install databricks-sql-connector`"
             )
         context = None
         try:
             from dbruntime.databricks_repl_context import get_context
 
@@ -252,15 +264,15 @@
         """
         try:
             from cnosdb_connector import make_cnosdb_langchain_uri
 
             uri = make_cnosdb_langchain_uri(url, user, password, tenant, database)
             return cls.from_uri(database_uri=uri)
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "cnos-connector package not found, please install with"
                 " `pip install cnos-connector`"
             )
 
     @property
     def dialect(self) -> str:
         """Return string representation of dialect to use."""
@@ -268,19 +280,17 @@
 
     def get_usable_table_names(self) -> Iterable[str]:
         """Get names of tables available."""
         if self._include_tables:
             return sorted(self._include_tables)
         return sorted(self._all_tables - self._ignore_tables)
 
+    @deprecated("0.0.1", alternative="get_usable_table_names", removal="0.3.0")
     def get_table_names(self) -> Iterable[str]:
         """Get names of tables available."""
-        warnings.warn(
-            "This method is deprecated - please use `get_usable_table_names`."
-        )
         return self.get_usable_table_names()
 
     @property
     def table_info(self) -> str:
         """Information about all tables in the database."""
         return self.get_table_info()
 
@@ -297,14 +307,24 @@
         all_table_names = self.get_usable_table_names()
         if table_names is not None:
             missing_tables = set(table_names).difference(all_table_names)
             if missing_tables:
                 raise ValueError(f"table_names {missing_tables} not found in database")
             all_table_names = table_names
 
+        metadata_table_names = [tbl.name for tbl in self._metadata.sorted_tables]
+        to_reflect = set(all_table_names) - set(metadata_table_names)
+        if to_reflect:
+            self._metadata.reflect(
+                views=self._view_support,
+                bind=self._engine,
+                only=list(to_reflect),
+                schema=self._schema,
+            )
+
         meta_tables = [
             tbl
             for tbl in self._metadata.sorted_tables
             if tbl.name in set(all_table_names)
             and not (self.dialect == "sqlite" and tbl.name.startswith("sqlite_"))
         ]
 
@@ -371,79 +391,133 @@
             f"{self._sample_rows_in_table_info} rows from {table.name} table:\n"
             f"{columns_str}\n"
             f"{sample_rows_str}"
         )
 
     def _execute(
         self,
-        command: str,
-        fetch: Union[Literal["all"], Literal["one"]] = "all",
-    ) -> Sequence[Dict[str, Any]]:
+        command: Union[str, Executable],
+        fetch: Literal["all", "one", "cursor"] = "all",
+        *,
+        parameters: Optional[Dict[str, Any]] = None,
+        execution_options: Optional[Dict[str, Any]] = None,
+    ) -> Union[Sequence[Dict[str, Any]], Result]:
         """
         Executes SQL command through underlying engine.
 
         If the statement returns no rows, an empty list is returned.
         """
-        with self._engine.begin() as connection:
+        parameters = parameters or {}
+        execution_options = execution_options or {}
+        with self._engine.begin() as connection:  # type: Connection  # type: ignore[name-defined]
             if self._schema is not None:
                 if self.dialect == "snowflake":
                     connection.exec_driver_sql(
-                        "ALTER SESSION SET search_path = %s", (self._schema,)
+                        "ALTER SESSION SET search_path = %s",
+                        (self._schema,),
+                        execution_options=execution_options,
                     )
                 elif self.dialect == "bigquery":
-                    connection.exec_driver_sql("SET @@dataset_id=?", (self._schema,))
+                    connection.exec_driver_sql(
+                        "SET @@dataset_id=?",
+                        (self._schema,),
+                        execution_options=execution_options,
+                    )
                 elif self.dialect == "mssql":
                     pass
                 elif self.dialect == "trino":
-                    connection.exec_driver_sql("USE ?", (self._schema,))
+                    connection.exec_driver_sql(
+                        "USE ?",
+                        (self._schema,),
+                        execution_options=execution_options,
+                    )
                 elif self.dialect == "duckdb":
                     # Unclear which parameterized argument syntax duckdb supports.
                     # The docs for the duckdb client say they support multiple,
                     # but `duckdb_engine` seemed to struggle with all of them:
                     # https://github.com/Mause/duckdb_engine/issues/796
-                    connection.exec_driver_sql(f"SET search_path TO {self._schema}")
+                    connection.exec_driver_sql(
+                        f"SET search_path TO {self._schema}",
+                        execution_options=execution_options,
+                    )
                 elif self.dialect == "oracle":
                     connection.exec_driver_sql(
-                        f"ALTER SESSION SET CURRENT_SCHEMA = {self._schema}"
+                        f"ALTER SESSION SET CURRENT_SCHEMA = {self._schema}",
+                        execution_options=execution_options,
                     )
                 elif self.dialect == "sqlany":
                     # If anybody using Sybase SQL anywhere database then it should not
                     # go to else condition. It should be same as mssql.
                     pass
-                else:  # postgresql and other compatible dialects
-                    connection.exec_driver_sql("SET search_path TO %s", (self._schema,))
-            cursor = connection.execute(text(command))
+                elif self.dialect == "postgresql":  # postgresql
+                    connection.exec_driver_sql(
+                        "SET search_path TO %s",
+                        (self._schema,),
+                        execution_options=execution_options,
+                    )
+
+            if isinstance(command, str):
+                command = text(command)
+            elif isinstance(command, Executable):
+                pass
+            else:
+                raise TypeError(f"Query expression has unknown type: {type(command)}")
+            cursor = connection.execute(
+                command,
+                parameters,
+                execution_options=execution_options,
+            )
+
             if cursor.returns_rows:
                 if fetch == "all":
                     result = [x._asdict() for x in cursor.fetchall()]
                 elif fetch == "one":
                     first_result = cursor.fetchone()
                     result = [] if first_result is None else [first_result._asdict()]
+                elif fetch == "cursor":
+                    return cursor
                 else:
-                    raise ValueError("Fetch parameter must be either 'one' or 'all'")
+                    raise ValueError(
+                        "Fetch parameter must be either 'one', 'all', or 'cursor'"
+                    )
                 return result
         return []
 
     def run(
         self,
-        command: str,
-        fetch: Union[Literal["all"], Literal["one"]] = "all",
-    ) -> str:
+        command: Union[str, Executable],
+        fetch: Literal["all", "one", "cursor"] = "all",
+        include_columns: bool = False,
+        *,
+        parameters: Optional[Dict[str, Any]] = None,
+        execution_options: Optional[Dict[str, Any]] = None,
+    ) -> Union[str, Sequence[Dict[str, Any]], Result[Any]]:
         """Execute a SQL command and return a string representing the results.
 
         If the statement returns rows, a string of the results is returned.
         If the statement returns no rows, an empty string is returned.
         """
-        result = self._execute(command, fetch)
-        # Convert columns values to string to avoid issues with sqlalchemy
-        # truncating text
+        result = self._execute(
+            command, fetch, parameters=parameters, execution_options=execution_options
+        )
+
+        if fetch == "cursor":
+            return result
+
         res = [
-            tuple(truncate_word(c, length=self._max_string_length) for c in r.values())
+            {
+                column: truncate_word(value, length=self._max_string_length)
+                for column, value in r.items()
+            }
             for r in result
         ]
+
+        if not include_columns:
+            res = [tuple(row.values()) for row in res]  # type: ignore[misc]
+
         if not res:
             return ""
         else:
             return str(res)
 
     def get_table_info_no_throw(self, table_names: Optional[List[str]] = None) -> str:
         """Get information about specified tables.
@@ -460,21 +534,37 @@
         except ValueError as e:
             """Format the error message"""
             return f"Error: {e}"
 
     def run_no_throw(
         self,
         command: str,
-        fetch: Union[Literal["all"], Literal["one"]] = "all",
-    ) -> str:
+        fetch: Literal["all", "one"] = "all",
+        include_columns: bool = False,
+        *,
+        parameters: Optional[Dict[str, Any]] = None,
+        execution_options: Optional[Dict[str, Any]] = None,
+    ) -> Union[str, Sequence[Dict[str, Any]], Result[Any]]:
         """Execute a SQL command and return a string representing the results.
 
         If the statement returns rows, a string of the results is returned.
         If the statement returns no rows, an empty string is returned.
 
         If the statement throws an error, the error message is returned.
         """
         try:
-            return self.run(command, fetch)
+            return self.run(
+                command,
+                fetch,
+                parameters=parameters,
+                execution_options=execution_options,
+                include_columns=include_columns,
+            )
         except SQLAlchemyError as e:
             """Format the error message"""
             return f"Error: {e}"
+
+    def get_context(self) -> Dict[str, Any]:
+        """Return db context that you may want in agent prompt."""
+        table_names = list(self.get_usable_table_names())
+        table_info = self.get_table_info_no_throw()
+        return {"table_info": table_info, "table_names": ", ".join(table_names)}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/stackexchange.py` & `gigachain_community-0.2.0/langchain_community/utilities/stackexchange.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/steam.py` & `gigachain_community-0.2.0/langchain_community/utilities/steam.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/tavily_search.py` & `gigachain_community-0.2.0/langchain_community/utilities/tavily_search.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 """Util that calls Tavily Search API.
 
 In order to set this up, follow instructions at:
 """
+
 import json
 from typing import Dict, List, Optional
 
 import aiohttp
 import requests
-from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
+from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 TAVILY_API_URL = "https://api.tavily.com"
 
 
 class TavilySearchAPIWrapper(BaseModel):
     """Wrapper for Tavily Search API."""
 
-    tavily_api_key: str
+    tavily_api_key: SecretStr
 
     class Config:
         """Configuration for this pydantic object."""
 
         extra = Extra.forbid
 
     @root_validator(pre=True)
@@ -41,15 +42,15 @@
         include_domains: Optional[List[str]] = [],
         exclude_domains: Optional[List[str]] = [],
         include_answer: Optional[bool] = False,
         include_raw_content: Optional[bool] = False,
         include_images: Optional[bool] = False,
     ) -> Dict:
         params = {
-            "api_key": self.tavily_api_key,
+            "api_key": self.tavily_api_key.get_secret_value(),
             "query": query,
             "max_results": max_results,
             "search_depth": search_depth,
             "include_domains": include_domains,
             "exclude_domains": exclude_domains,
             "include_answer": include_answer,
             "include_raw_content": include_raw_content,
@@ -122,15 +123,15 @@
         include_images: Optional[bool] = False,
     ) -> Dict:
         """Get results from the Tavily Search API asynchronously."""
 
         # Function to perform the API call
         async def fetch() -> str:
             params = {
-                "api_key": self.tavily_api_key,
+                "api_key": self.tavily_api_key.get_secret_value(),
                 "query": query,
                 "max_results": max_results,
                 "search_depth": search_depth,
                 "include_domains": include_domains,
                 "exclude_domains": exclude_domains,
                 "include_answer": include_answer,
                 "include_raw_content": include_raw_content,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/tensorflow_datasets.py` & `gigachain_community-0.2.0/langchain_community/utilities/tensorflow_datasets.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/twilio.py` & `gigachain_community-0.2.0/langchain_community/utilities/twilio.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls Twilio."""
+
 from typing import Any, Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 class TwilioAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/vertexai.py` & `gigachain_community-0.2.0/langchain_community/utilities/vertexai.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Utilities to init Vertex AI."""
+
 from importlib import metadata
 from typing import TYPE_CHECKING, Any, Callable, Optional, Union
 
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
@@ -18,15 +19,15 @@
     llm: BaseLLM,
     *,
     max_retries: int = 1,
     run_manager: Optional[
         Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]
     ] = None,
 ) -> Callable[[Any], Any]:
-    """Creates a retry decorator for Vertex / Palm LLMs."""
+    """Create a retry decorator for Vertex / Palm LLMs."""
     import google.api_core
 
     errors = [
         google.api_core.exceptions.ResourceExhausted,
         google.api_core.exceptions.ServiceUnavailable,
         google.api_core.exceptions.Aborted,
         google.api_core.exceptions.DeadlineExceeded,
@@ -53,15 +54,15 @@
 
 
 def init_vertexai(
     project: Optional[str] = None,
     location: Optional[str] = None,
     credentials: Optional["Credentials"] = None,
 ) -> None:
-    """Init vertexai.
+    """Init Vertex AI.
 
     Args:
         project: The default GCP project to use when making Vertex API calls.
         location: The default location to use when making API calls.
         credentials: The default custom
             credentials to use when making API calls. If not provided credentials
             will be ascertained from the environment.
@@ -78,15 +79,15 @@
         project=project,
         location=location,
         credentials=credentials,
     )
 
 
 def get_client_info(module: Optional[str] = None) -> "ClientInfo":
-    r"""Returns a custom user agent header.
+    r"""Return a custom user agent header.
 
     Args:
         module (Optional[str]):
             Optional. The module for a custom user agent header.
     Returns:
         google.api_core.gapic_v1.client_info.ClientInfo
     """
@@ -105,15 +106,15 @@
     return ClientInfo(
         client_library_version=client_library_version,
         user_agent=f"langchain/{client_library_version}",
     )
 
 
 def load_image_from_gcs(path: str, project: Optional[str] = None) -> "Image":
-    """Loads im Image from GCS."""
+    """Load an image from Google Cloud Storage."""
     try:
         from google.cloud import storage
     except ImportError:
         raise ImportError("Could not import google-cloud-storage python package.")
     from vertexai.preview.generative_models import Image
 
     gcs_client = storage.Client(project=project)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/wikipedia.py` & `gigachain_community-0.2.0/langchain_community/utilities/wikipedia.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """Util that calls Wikipedia."""
+
 import logging
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, Iterator, List, Optional
 
 from langchain_core.documents import Document
 from langchain_core.pydantic_v1 import BaseModel, root_validator
 
 logger = logging.getLogger(__name__)
 
 WIKIPEDIA_MAX_QUERY_LENGTH = 300
@@ -101,16 +102,24 @@
         """
         Run Wikipedia search and get the article text plus the meta information.
         See
 
         Returns: a list of documents.
 
         """
+        return list(self.lazy_load(query))
+
+    def lazy_load(self, query: str) -> Iterator[Document]:
+        """
+        Run Wikipedia search and get the article text plus the meta information.
+        See
+
+        Returns: a list of documents.
+
+        """
         page_titles = self.wiki_client.search(
             query[:WIKIPEDIA_MAX_QUERY_LENGTH], results=self.top_k_results
         )
-        docs = []
         for page_title in page_titles[: self.top_k_results]:
             if wiki_page := self._fetch_page(page_title):
                 if doc := self._page_to_document(page_title, wiki_page):
-                    docs.append(doc)
-        return docs
+                    yield doc
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/wolfram_alpha.py` & `gigachain_community-0.2.0/langchain_community/utilities/wolfram_alpha.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Util that calls WolframAlpha."""
+
 from typing import Any, Dict, Optional
 
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
 
 
 class WolframAlphaAPIWrapper(BaseModel):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utilities/zapier.py` & `gigachain_community-0.2.0/langchain_community/utilities/zapier.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 Zapier.com)
 
 For use-cases where LangChain + Zapier NLA is powering a user-facing application, and
 LangChain needs access to the end-user's connected accounts on Zapier.com, you'll need
 to use oauth. Review the full docs above and reach out to nla@zapier.com for
 developer support.
 """
+
 import json
 from typing import Any, Dict, List, Optional
 
 import aiohttp
 import requests
 from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
 from langchain_core.utils import get_from_dict_or_env
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utils/math.py` & `gigachain_community-0.2.0/langchain_community/utils/math.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Math utils."""
+
 import logging
 from typing import List, Optional, Tuple, Union
 
 import numpy as np
 
 logger = logging.getLogger(__name__)
 
@@ -25,17 +26,17 @@
         import simsimd as simd
 
         X = np.array(X, dtype=np.float32)
         Y = np.array(Y, dtype=np.float32)
         Z = 1 - simd.cdist(X, Y, metric="cosine")
         if isinstance(Z, float):
             return np.array([Z])
-        return Z
+        return np.array(Z)
     except ImportError:
-        logger.info(
+        logger.debug(
             "Unable to import simsimd, defaulting to NumPy implementation. If you want "
             "to use simsimd please install with `pip install simsimd`."
         )
         X_norm = np.linalg.norm(X, axis=1)
         Y_norm = np.linalg.norm(Y, axis=1)
         # Ignore divide by zero errors run time warnings as those are handled below.
         with np.errstate(divide="ignore", invalid="ignore"):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/utils/openai_functions.py` & `gigachain_community-0.2.0/langchain_community/utils/ernie_functions.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 from typing import Literal, Optional, Type, TypedDict
 
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.utils.json_schema import dereference_refs
 
 
 class FunctionDescription(TypedDict):
-    """Representation of a callable function to the OpenAI API."""
+    """Representation of a callable function to the Ernie API."""
 
     name: str
     """The name of the function."""
     description: str
     """A description of the function."""
     parameters: dict
     """The parameters of the function."""
 
 
 class ToolDescription(TypedDict):
-    """Representation of a callable function to the OpenAI API."""
+    """Representation of a callable function to the Ernie API."""
 
     type: Literal["function"]
     function: FunctionDescription
 
 
-def convert_pydantic_to_openai_function(
+def convert_pydantic_to_ernie_function(
     model: Type[BaseModel],
     *,
     name: Optional[str] = None,
     description: Optional[str] = None,
 ) -> FunctionDescription:
-    """Converts a Pydantic model to a function description for the OpenAI API."""
+    """Convert a Pydantic model to a function description for the Ernie API."""
     schema = dereference_refs(model.schema())
     schema.pop("definitions", None)
     return {
         "name": name or schema["title"],
         "description": description or schema["description"],
         "parameters": schema,
     }
 
 
-def convert_pydantic_to_openai_tool(
+def convert_pydantic_to_ernie_tool(
     model: Type[BaseModel],
     *,
     name: Optional[str] = None,
     description: Optional[str] = None,
 ) -> ToolDescription:
-    """Converts a Pydantic model to a function description for the OpenAI API."""
-    function = convert_pydantic_to_openai_function(
+    """Convert a Pydantic model to a function description for the Ernie API."""
+    function = convert_pydantic_to_ernie_function(
         model, name=name, description=description
     )
     return {"type": "function", "function": function}
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/alibabacloud_opensearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/alibabacloud_opensearch.py`

 * *Files 0% similar despite different names*

```diff
@@ -352,15 +352,15 @@
         items = json_result["result"]
         query_result_list: List[Document] = []
         for item in items:
             if (
                 "fields" not in item
                 or self.config.field_name_mapping["document"] not in item["fields"]
             ):
-                query_result_list.append(Document())
+                query_result_list.append(Document())  # type: ignore[call-arg]
             else:
                 fields = item["fields"]
                 query_result_list.append(
                     Document(
                         page_content=fields[self.config.field_name_mapping["document"]],
                         metadata=self.create_inverse_metadata(fields),
                     )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/analyticdb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/analyticdb.py`

 * *Files 0% similar despite different names*

```diff
@@ -153,15 +153,15 @@
             metadatas: Optional list of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         embeddings = self.embedding_function.embed_documents(list(texts))
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         # Define the table schema
@@ -343,15 +343,15 @@
         try:
             with self.engine.connect() as conn:
                 with conn.begin():
                     delete_condition = chunks_table.c.id.in_(ids)
                     conn.execute(chunks_table.delete().where(delete_condition))
                     return True
         except Exception as e:
-            print("Delete operation failed:", str(e))
+            print("Delete operation failed:", str(e))  # noqa: T201
             return False
 
     @classmethod
     def from_texts(
         cls: Type[AnalyticDB],
         texts: List[str],
         embedding: Embeddings,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/annoy.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/annoy.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,34 +6,28 @@
 from configparser import ConfigParser
 from pathlib import Path
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
+from langchain_core.utils import guard_import
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.docstore.base import Docstore
 from langchain_community.docstore.in_memory import InMemoryDocstore
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 INDEX_METRICS = frozenset(["angular", "euclidean", "manhattan", "hamming", "dot"])
 DEFAULT_METRIC = "angular"
 
 
 def dependable_annoy_import() -> Any:
     """Import annoy if available, otherwise raise error."""
-    try:
-        import annoy
-    except ImportError:
-        raise ImportError(
-            "Could not import annoy python package. "
-            "Please install it with `pip install --user annoy` "
-        )
-    return annoy
+    return guard_import("annoy")
 
 
 class Annoy(VectorStore):
     """`Annoy` vector store.
 
     To use, you should have the ``annoy`` python package installed.
 
@@ -296,15 +290,15 @@
         if metric not in INDEX_METRICS:
             raise ValueError(
                 (
                     f"Unsupported distance metric: {metric}. "
                     f"Expected one of {list(INDEX_METRICS)}"
                 )
             )
-        annoy = dependable_annoy_import()
+        annoy = guard_import("annoy")
         if not embeddings:
             raise ValueError("embeddings must be provided to build AnnoyIndex")
         f = len(embeddings[0])
         index = annoy.AnnoyIndex(f, metric=metric)
         for i, emb in enumerate(embeddings):
             index.add_item(i, emb)
         index.build(trees, n_jobs=n_jobs)
@@ -425,25 +419,45 @@
             pickle.dump((self.docstore, self.index_to_docstore_id, config_object), file)
 
     @classmethod
     def load_local(
         cls,
         folder_path: str,
         embeddings: Embeddings,
+        *,
+        allow_dangerous_deserialization: bool = False,
     ) -> Annoy:
         """Load Annoy index, docstore, and index_to_docstore_id to disk.
 
         Args:
             folder_path: folder path to load index, docstore,
                 and index_to_docstore_id from.
             embeddings: Embeddings to use when generating queries.
+            allow_dangerous_deserialization: whether to allow deserialization
+                of the data which involves loading a pickle file.
+                Pickle files can be modified by malicious actors to deliver a
+                malicious payload that results in execution of
+                arbitrary code on your machine.
         """
+        if not allow_dangerous_deserialization:
+            raise ValueError(
+                "The de-serialization relies loading a pickle file. "
+                "Pickle files can be modified to deliver a malicious payload that "
+                "results in execution of arbitrary code on your machine."
+                "You will need to set `allow_dangerous_deserialization` to `True` to "
+                "enable deserialization. If you do this, make sure that you "
+                "trust the source of the data. For example, if you are loading a "
+                "file that you created, and know that no one else has modified the "
+                "file, then this is safe to do. Do not set this to `True` if you are "
+                "loading a file from an untrusted source (e.g., some random site on "
+                "the internet.)."
+            )
         path = Path(folder_path)
         # load index separately since it is not picklable
-        annoy = dependable_annoy_import()
+        annoy = guard_import("annoy")
         # load docstore and index_to_docstore_id
         with open(path / "index.pkl", "rb") as file:
             docstore, index_to_docstore_id, config_object = pickle.load(file)
 
         f = int(config_object["ANNOY"]["f"])
         metric = config_object["ANNOY"]["metric"]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/astradb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/oraclevs.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,776 +1,930 @@
 from __future__ import annotations
 
+import array
+import functools
+import hashlib
+import json
+import logging
+import os
 import uuid
-import warnings
-from concurrent.futures import ThreadPoolExecutor
 from typing import (
+    TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Iterable,
     List,
     Optional,
-    Set,
     Tuple,
     Type,
     TypeVar,
+    Union,
+    cast,
 )
 
+if TYPE_CHECKING:
+    from oracledb import Connection
+
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
-from langchain_core.utils.iter import batch_iterate
 from langchain_core.vectorstores import VectorStore
 
-from langchain_community.vectorstores.utils import maximal_marginal_relevance
+from langchain_community.vectorstores.utils import (
+    DistanceStrategy,
+    maximal_marginal_relevance,
+)
+
+logger = logging.getLogger(__name__)
+log_level = os.getenv("LOG_LEVEL", "ERROR").upper()
+logging.basicConfig(
+    level=getattr(logging, log_level),
+    format="%(asctime)s - %(levelname)s - %(message)s",
+)
 
-ADBVST = TypeVar("ADBVST", bound="AstraDB")
-T = TypeVar("T")
-U = TypeVar("U")
-DocDict = Dict[str, Any]  # dicts expressing entries to insert
-
-# Batch/concurrency default values (if parameters not provided):
-# Size of batches for bulk insertions:
-#   (20 is the max batch size for the HTTP API at the time of writing)
-DEFAULT_BATCH_SIZE = 20
-# Number of threads to insert batches concurrently:
-DEFAULT_BULK_INSERT_BATCH_CONCURRENCY = 16
-# Number of threads in a batch to insert pre-existing entries:
-DEFAULT_BULK_INSERT_OVERWRITE_CONCURRENCY = 10
-# Number of threads (for deleting multiple rows concurrently):
-DEFAULT_BULK_DELETE_CONCURRENCY = 20
-
-
-def _unique_list(lst: List[T], key: Callable[[T], U]) -> List[T]:
-    visited_keys: Set[U] = set()
-    new_lst = []
-    for item in lst:
-        item_key = key(item)
-        if item_key not in visited_keys:
-            visited_keys.add(item_key)
-            new_lst.append(item)
-    return new_lst
 
+# Define a type variable that can be any kind of function
+T = TypeVar("T", bound=Callable[..., Any])
 
-class AstraDB(VectorStore):
-    """Wrapper around DataStax Astra DB for vector-store workloads.
 
-    To use it, you need a recent installation of the `astrapy` library
-    and an Astra DB cloud database.
+def _handle_exceptions(func: T) -> T:
+    @functools.wraps(func)
+    def wrapper(*args: Any, **kwargs: Any) -> Any:
+        try:
+            return func(*args, **kwargs)
+        except RuntimeError as db_err:
+            # Handle a known type of error (e.g., DB-related) specifically
+            logger.exception("DB-related error occurred.")
+            raise RuntimeError(
+                "Failed due to a DB issue: {}".format(db_err)
+            ) from db_err
+        except ValueError as val_err:
+            # Handle another known type of error specifically
+            logger.exception("Validation error.")
+            raise ValueError("Validation failed: {}".format(val_err)) from val_err
+        except Exception as e:
+            # Generic handler for all other exceptions
+            logger.exception("An unexpected error occurred: {}".format(e))
+            raise RuntimeError("Unexpected error: {}".format(e)) from e
+
+    return cast(T, wrapper)
+
+
+def _table_exists(client: Connection, table_name: str) -> bool:
+    try:
+        import oracledb
+    except ImportError as e:
+        raise ImportError(
+            "Unable to import oracledb, please install with "
+            "`pip install -U oracledb`."
+        ) from e
+
+    try:
+        with client.cursor() as cursor:
+            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
+            return True
+    except oracledb.DatabaseError as ex:
+        err_obj = ex.args
+        if err_obj[0].code == 942:
+            return False
+        raise
+
+
+@_handle_exceptions
+def _index_exists(client: Connection, index_name: str) -> bool:
+    # Check if the index exists
+    query = """
+        SELECT index_name 
+        FROM all_indexes 
+        WHERE upper(index_name) = upper(:idx_name)
+        """
+
+    with client.cursor() as cursor:
+        # Execute the query
+        cursor.execute(query, idx_name=index_name.upper())
+        result = cursor.fetchone()
+
+        # Check if the index exists
+    return result is not None
+
+
+def _get_distance_function(distance_strategy: DistanceStrategy) -> str:
+    # Dictionary to map distance strategies to their corresponding function
+    # names
+    distance_strategy2function = {
+        DistanceStrategy.EUCLIDEAN_DISTANCE: "EUCLIDEAN",
+        DistanceStrategy.DOT_PRODUCT: "DOT",
+        DistanceStrategy.COSINE: "COSINE",
+    }
+
+    # Attempt to return the corresponding distance function
+    if distance_strategy in distance_strategy2function:
+        return distance_strategy2function[distance_strategy]
+
+    # If it's an unsupported distance strategy, raise an error
+    raise ValueError(f"Unsupported distance strategy: {distance_strategy}")
+
+
+def _get_index_name(base_name: str) -> str:
+    unique_id = str(uuid.uuid4()).replace("-", "")
+    return f"{base_name}_{unique_id}"
+
+
+@_handle_exceptions
+def _create_table(client: Connection, table_name: str, embedding_dim: int) -> None:
+    cols_dict = {
+        "id": "RAW(16) DEFAULT SYS_GUID() PRIMARY KEY",
+        "text": "CLOB",
+        "metadata": "CLOB",
+        "embedding": f"vector({embedding_dim}, FLOAT32)",
+    }
+
+    if not _table_exists(client, table_name):
+        with client.cursor() as cursor:
+            ddl_body = ", ".join(
+                f"{col_name} {col_type}" for col_name, col_type in cols_dict.items()
+            )
+            ddl = f"CREATE TABLE {table_name} ({ddl_body})"
+            cursor.execute(ddl)
+        logger.info("Table created successfully...")
+    else:
+        logger.info("Table already exists...")
+
+
+@_handle_exceptions
+def create_index(
+    client: Connection,
+    vector_store: OracleVS,
+    params: Optional[dict[str, Any]] = None,
+) -> None:
+    if params:
+        if params["idx_type"] == "HNSW":
+            _create_hnsw_index(
+                client, vector_store.table_name, vector_store.distance_strategy, params
+            )
+        elif params["idx_type"] == "IVF":
+            _create_ivf_index(
+                client, vector_store.table_name, vector_store.distance_strategy, params
+            )
+        else:
+            _create_hnsw_index(
+                client, vector_store.table_name, vector_store.distance_strategy, params
+            )
+    else:
+        _create_hnsw_index(
+            client, vector_store.table_name, vector_store.distance_strategy, params
+        )
+    return
+
+
+@_handle_exceptions
+def _create_hnsw_index(
+    client: Connection,
+    table_name: str,
+    distance_strategy: DistanceStrategy,
+    params: Optional[dict[str, Any]] = None,
+) -> None:
+    defaults = {
+        "idx_name": "HNSW",
+        "idx_type": "HNSW",
+        "neighbors": 32,
+        "efConstruction": 200,
+        "accuracy": 90,
+        "parallel": 8,
+    }
+
+    if params:
+        config = params.copy()
+        # Ensure compulsory parts are included
+        for compulsory_key in ["idx_name", "parallel"]:
+            if compulsory_key not in config:
+                if compulsory_key == "idx_name":
+                    config[compulsory_key] = _get_index_name(
+                        str(defaults[compulsory_key])
+                    )
+                else:
+                    config[compulsory_key] = defaults[compulsory_key]
+
+        # Validate keys in config against defaults
+        for key in config:
+            if key not in defaults:
+                raise ValueError(f"Invalid parameter: {key}")
+    else:
+        config = defaults
+
+    # Base SQL statement
+    idx_name = config["idx_name"]
+    base_sql = (
+        f"create vector index {idx_name} on {table_name}(embedding) "
+        f"ORGANIZATION INMEMORY NEIGHBOR GRAPH"
+    )
+
+    # Optional parts depending on parameters
+    accuracy_part = " WITH TARGET ACCURACY {accuracy}" if ("accuracy" in config) else ""
+    distance_part = f" DISTANCE {_get_distance_function(distance_strategy)}"
+
+    parameters_part = ""
+    if "neighbors" in config and "efConstruction" in config:
+        parameters_part = (
+            " parameters (type {idx_type}, neighbors {"
+            "neighbors}, efConstruction {efConstruction})"
+        )
+    elif "neighbors" in config and "efConstruction" not in config:
+        config["efConstruction"] = defaults["efConstruction"]
+        parameters_part = (
+            " parameters (type {idx_type}, neighbors {"
+            "neighbors}, efConstruction {efConstruction})"
+        )
+    elif "neighbors" not in config and "efConstruction" in config:
+        config["neighbors"] = defaults["neighbors"]
+        parameters_part = (
+            " parameters (type {idx_type}, neighbors {"
+            "neighbors}, efConstruction {efConstruction})"
+        )
+
+    # Always included part for parallel
+    parallel_part = " parallel {parallel}"
+
+    # Combine all parts
+    ddl_assembly = (
+        base_sql + accuracy_part + distance_part + parameters_part + parallel_part
+    )
+    # Format the SQL with values from the params dictionary
+    ddl = ddl_assembly.format(**config)
+
+    # Check if the index exists
+    if not _index_exists(client, config["idx_name"]):
+        with client.cursor() as cursor:
+            cursor.execute(ddl)
+            logger.info("Index created successfully...")
+    else:
+        logger.info("Index already exists...")
+
+
+@_handle_exceptions
+def _create_ivf_index(
+    client: Connection,
+    table_name: str,
+    distance_strategy: DistanceStrategy,
+    params: Optional[dict[str, Any]] = None,
+) -> None:
+    # Default configuration
+    defaults = {
+        "idx_name": "IVF",
+        "idx_type": "IVF",
+        "neighbor_part": 32,
+        "accuracy": 90,
+        "parallel": 8,
+    }
+
+    if params:
+        config = params.copy()
+        # Ensure compulsory parts are included
+        for compulsory_key in ["idx_name", "parallel"]:
+            if compulsory_key not in config:
+                if compulsory_key == "idx_name":
+                    config[compulsory_key] = _get_index_name(
+                        str(defaults[compulsory_key])
+                    )
+                else:
+                    config[compulsory_key] = defaults[compulsory_key]
 
-    For quickstart and details, visit:
-        docs.datastax.com/en/astra/home/astra.html
+        # Validate keys in config against defaults
+        for key in config:
+            if key not in defaults:
+                raise ValueError(f"Invalid parameter: {key}")
+    else:
+        config = defaults
+
+    # Base SQL statement
+    idx_name = config["idx_name"]
+    base_sql = (
+        f"CREATE VECTOR INDEX {idx_name} ON {table_name}(embedding) "
+        f"ORGANIZATION NEIGHBOR PARTITIONS"
+    )
+
+    # Optional parts depending on parameters
+    accuracy_part = " WITH TARGET ACCURACY {accuracy}" if ("accuracy" in config) else ""
+    distance_part = f" DISTANCE {_get_distance_function(distance_strategy)}"
+
+    parameters_part = ""
+    if "idx_type" in config and "neighbor_part" in config:
+        parameters_part = (
+            f" PARAMETERS (type {config['idx_type']}, neighbor"
+            f" partitions {config['neighbor_part']})"
+        )
+
+    # Always included part for parallel
+    parallel_part = f" PARALLEL {config['parallel']}"
+
+    # Combine all parts
+    ddl_assembly = (
+        base_sql + accuracy_part + distance_part + parameters_part + parallel_part
+    )
+    # Format the SQL with values from the params dictionary
+    ddl = ddl_assembly.format(**config)
+
+    # Check if the index exists
+    if not _index_exists(client, config["idx_name"]):
+        with client.cursor() as cursor:
+            cursor.execute(ddl)
+        logger.info("Index created successfully...")
+    else:
+        logger.info("Index already exists...")
+
+
+@_handle_exceptions
+def drop_table_purge(client: Connection, table_name: str) -> None:
+    if _table_exists(client, table_name):
+        cursor = client.cursor()
+        with cursor:
+            ddl = f"DROP TABLE {table_name} PURGE"
+            cursor.execute(ddl)
+        logger.info("Table dropped successfully...")
+    else:
+        logger.info("Table not found...")
+    return
+
+
+@_handle_exceptions
+def drop_index_if_exists(client: Connection, index_name: str) -> None:
+    if _index_exists(client, index_name):
+        drop_query = f"DROP INDEX {index_name}"
+        with client.cursor() as cursor:
+            cursor.execute(drop_query)
+            logger.info(f"Index {index_name} has been dropped.")
+    else:
+        logger.exception(f"Index {index_name} does not exist.")
+    return
+
+
+class OracleVS(VectorStore):
+    """`OracleVS` vector store.
+
+    To use, you should have both:
+    - the ``oracledb`` python package installed
+    - a connection string associated with a OracleDBCluster having deployed an
+       Search index
 
     Example:
         .. code-block:: python
 
-                from langchain_community.vectorstores import AstraDB
-                from langchain_community.embeddings.openai import OpenAIEmbeddings
-
+            from langchain.vectorstores import OracleVS
+            from langchain.embeddings.openai import OpenAIEmbeddings
+            import oracledb
+
+            with oracledb.connect(user = user, passwd = pwd, dsn = dsn) as
+            connection:
+                print ("Database version:", connection.version)
                 embeddings = OpenAIEmbeddings()
-                vectorstore = AstraDB(
-                  embedding=embeddings,
-                  collection_name="my_store",
-                  token="AstraCS:...",
-                  api_endpoint="https://<DB-ID>-us-east1.apps.astra.datastax.com"
-                )
-
-                vectorstore.add_texts(["Giraffes", "All good here"])
-                results = vectorstore.similarity_search("Everything's ok", k=1)
-
-      Constructor Args (only keyword-arguments accepted):
-          embedding (Embeddings): embedding function to use.
-          collection_name (str): name of the Astra DB collection to create/use.
-          token (Optional[str]): API token for Astra DB usage.
-          api_endpoint (Optional[str]): full URL to the API endpoint,
-              such as "https://<DB-ID>-us-east1.apps.astra.datastax.com".
-          astra_db_client (Optional[Any]): *alternative to token+api_endpoint*,
-              you can pass an already-created 'astrapy.db.AstraDB' instance.
-          namespace (Optional[str]): namespace (aka keyspace) where the
-              collection is created. Defaults to the database's "default namespace".
-          metric (Optional[str]): similarity function to use out of those
-              available in Astra DB. If left out, it will use Astra DB API's
-              defaults (i.e. "cosine" - but, for performance reasons,
-              "dot_product" is suggested if embeddings are normalized to one).
-
-      Advanced arguments (coming with sensible defaults):
-          batch_size (Optional[int]): Size of batches for bulk insertions.
-          bulk_insert_batch_concurrency (Optional[int]): Number of threads
-              to insert batches concurrently.
-          bulk_insert_overwrite_concurrency (Optional[int]): Number of
-              threads in a batch to insert pre-existing entries.
-          bulk_delete_concurrency (Optional[int]): Number of threads
-              (for deleting multiple rows concurrently).
-          pre_delete_collection (Optional[bool]): whether to delete the collection
-              before creating it. If False and the collection already exists,
-              the collection will be used as is.
-
-      A note on concurrency: as a rule of thumb, on a typical client machine
-      it is suggested to keep the quantity
-          bulk_insert_batch_concurrency * bulk_insert_overwrite_concurrency
-      much below 1000 to avoid exhausting the client multithreading/networking
-      resources. The hardcoded defaults are somewhat conservative to meet
-      most machines' specs, but a sensible choice to test may be:
-          bulk_insert_batch_concurrency = 80
-          bulk_insert_overwrite_concurrency = 10
-      A bit of experimentation is required to nail the best results here,
-      depending on both the machine/network specs and the expected workload
-      (specifically, how often a write is an update of an existing id).
-      Remember you can pass concurrency settings to individual calls to
-      add_texts and add_documents as well.
+                query = ""
+                vectors = OracleVS(connection, table_name, embeddings, query)
     """
 
-    @staticmethod
-    def _filter_to_metadata(filter_dict: Optional[Dict[str, str]]) -> Dict[str, Any]:
-        if filter_dict is None:
-            return {}
-        else:
-            return {f"metadata.{mdk}": mdv for mdk, mdv in filter_dict.items()}
-
     def __init__(
         self,
-        *,
-        embedding: Embeddings,
-        collection_name: str,
-        token: Optional[str] = None,
-        api_endpoint: Optional[str] = None,
-        astra_db_client: Optional[Any] = None,  # 'astrapy.db.AstraDB' if passed
-        namespace: Optional[str] = None,
-        metric: Optional[str] = None,
-        batch_size: Optional[int] = None,
-        bulk_insert_batch_concurrency: Optional[int] = None,
-        bulk_insert_overwrite_concurrency: Optional[int] = None,
-        bulk_delete_concurrency: Optional[int] = None,
-        pre_delete_collection: bool = False,
-    ) -> None:
-        """
-        Create an AstraDB vector store object. See class docstring for help.
-        """
+        client: Connection,
+        embedding_function: Union[
+            Callable[[str], List[float]],
+            Embeddings,
+        ],
+        table_name: str,
+        distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,
+        query: Optional[str] = "What is a Oracle database",
+        params: Optional[Dict[str, Any]] = None,
+    ):
         try:
-            from astrapy.db import (
-                AstraDB as LibAstraDB,
-            )
-            from astrapy.db import (
-                AstraDBCollection as LibAstraDBCollection,
-            )
-        except (ImportError, ModuleNotFoundError):
+            import oracledb
+        except ImportError as e:
             raise ImportError(
-                "Could not import a recent astrapy python package. "
-                "Please install it with `pip install --upgrade astrapy`."
-            )
+                "Unable to import oracledb, please install with "
+                "`pip install -U oracledb`."
+            ) from e
 
-        # Conflicting-arg checks:
-        if astra_db_client is not None:
-            if token is not None or api_endpoint is not None:
-                raise ValueError(
-                    "You cannot pass 'astra_db_client' to AstraDB if passing "
-                    "'token' and 'api_endpoint'."
+        try:
+            """Initialize with oracledb client."""
+            self.client = client
+            """Initialize with necessary components."""
+            if not isinstance(embedding_function, Embeddings):
+                logger.warning(
+                    "`embedding_function` is expected to be an Embeddings "
+                    "object, support "
+                    "for passing in a function will soon be removed."
                 )
-
-        self.embedding = embedding
-        self.collection_name = collection_name
-        self.token = token
-        self.api_endpoint = api_endpoint
-        self.namespace = namespace
-        # Concurrency settings
-        self.batch_size: int = batch_size or DEFAULT_BATCH_SIZE
-        self.bulk_insert_batch_concurrency: int = (
-            bulk_insert_batch_concurrency or DEFAULT_BULK_INSERT_BATCH_CONCURRENCY
-        )
-        self.bulk_insert_overwrite_concurrency: int = (
-            bulk_insert_overwrite_concurrency
-            or DEFAULT_BULK_INSERT_OVERWRITE_CONCURRENCY
-        )
-        self.bulk_delete_concurrency: int = (
-            bulk_delete_concurrency or DEFAULT_BULK_DELETE_CONCURRENCY
-        )
-        # "vector-related" settings
-        self._embedding_dimension: Optional[int] = None
-        self.metric = metric
-
-        if astra_db_client is not None:
-            self.astra_db = astra_db_client
-        else:
-            self.astra_db = LibAstraDB(
-                token=self.token,
-                api_endpoint=self.api_endpoint,
-                namespace=self.namespace,
-            )
-        if not pre_delete_collection:
-            self._provision_collection()
-        else:
-            self.clear()
-
-        self.collection = LibAstraDBCollection(
-            collection_name=self.collection_name,
-            astra_db=self.astra_db,
-        )
-
-    def _get_embedding_dimension(self) -> int:
-        if self._embedding_dimension is None:
-            self._embedding_dimension = len(
-                self.embedding.embed_query("This is a sample sentence.")
-            )
-        return self._embedding_dimension
-
-    def _drop_collection(self) -> None:
-        """
-        Drop the collection from storage.
-
-        This is meant as an internal-usage method, no members
-        are set other than actual deletion on the backend.
-        """
-        _ = self.astra_db.delete_collection(
-            collection_name=self.collection_name,
-        )
-        return None
-
-    def _provision_collection(self) -> None:
-        """
-        Run the API invocation to create the collection on the backend.
-
-        Internal-usage method, no object members are set,
-        other than working on the underlying actual storage.
-        """
-        _ = self.astra_db.create_collection(
-            dimension=self._get_embedding_dimension(),
-            collection_name=self.collection_name,
-            metric=self.metric,
-        )
-        return None
+            self.embedding_function = embedding_function
+            self.query = query
+            embedding_dim = self.get_embedding_dimension()
+
+            self.table_name = table_name
+            self.distance_strategy = distance_strategy
+            self.params = params
+
+            _create_table(client, table_name, embedding_dim)
+        except oracledb.DatabaseError as db_err:
+            logger.exception(f"Database error occurred while create table: {db_err}")
+            raise RuntimeError(
+                "Failed to create table due to a database error."
+            ) from db_err
+        except ValueError as val_err:
+            logger.exception(f"Validation error: {val_err}")
+            raise RuntimeError(
+                "Failed to create table due to a validation error."
+            ) from val_err
+        except Exception as ex:
+            logger.exception("An unexpected error occurred while creating the index.")
+            raise RuntimeError(
+                "Failed to create table due to an unexpected error."
+            ) from ex
 
     @property
-    def embeddings(self) -> Embeddings:
-        return self.embedding
-
-    @staticmethod
-    def _dont_flip_the_cos_score(similarity0to1: float) -> float:
-        """Keep similarity from client unchanged ad it's in [0:1] already."""
-        return similarity0to1
-
-    def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        """
-        The underlying API calls already returns a "score proper",
-        i.e. one in [0, 1] where higher means more *similar*,
-        so here the final score transformation is not reversing the interval:
+    def embeddings(self) -> Optional[Embeddings]:
         """
-        return self._dont_flip_the_cos_score
-
-    def clear(self) -> None:
-        """Empty the collection of all its stored entries."""
-        self._drop_collection()
-        self._provision_collection()
-        return None
-
-    def delete_by_document_id(self, document_id: str) -> bool:
-        """
-        Remove a single document from the store, given its document_id (str).
-        Return True if a document has indeed been deleted, False if ID not found.
-        """
-        deletion_response = self.collection.delete(document_id)
-        return ((deletion_response or {}).get("status") or {}).get(
-            "deletedCount", 0
-        ) == 1
-
-    def delete(
-        self,
-        ids: Optional[List[str]] = None,
-        concurrency: Optional[int] = None,
-        **kwargs: Any,
-    ) -> Optional[bool]:
-        """Delete by vector ids.
-
-        Args:
-            ids (Optional[List[str]]): List of ids to delete.
-            concurrency (Optional[int]): max number of threads issuing
-                single-doc delete requests. Defaults to instance-level setting.
+        A property that returns an Embeddings instance embedding_function
+        is an instance of Embeddings, otherwise returns None.
 
         Returns:
-            Optional[bool]: True if deletion is successful,
-                False otherwise, None if not implemented.
+            Optional[Embeddings]: The embedding function if it's an instance of
+            Embeddings, otherwise None.
         """
+        return (
+            self.embedding_function
+            if isinstance(self.embedding_function, Embeddings)
+            else None
+        )
 
-        if kwargs:
-            warnings.warn(
-                "Method 'delete' of AstraDB vector store invoked with "
-                f"unsupported arguments ({', '.join(sorted(kwargs.keys()))}), "
-                "which will be ignored."
-            )
+    def get_embedding_dimension(self) -> int:
+        # Embed the single document by wrapping it in a list
+        embedded_document = self._embed_documents(
+            [self.query if self.query is not None else ""]
+        )
 
-        if ids is None:
-            raise ValueError("No ids provided to delete.")
+        # Get the first (and only) embedding's dimension
+        return len(embedded_document[0])
 
-        _max_workers = concurrency or self.bulk_delete_concurrency
-        with ThreadPoolExecutor(max_workers=_max_workers) as tpe:
-            _ = list(
-                tpe.map(
-                    self.delete_by_document_id,
-                    ids,
-                )
+    def _embed_documents(self, texts: List[str]) -> List[List[float]]:
+        if isinstance(self.embedding_function, Embeddings):
+            return self.embedding_function.embed_documents(texts)
+        elif callable(self.embedding_function):
+            return [self.embedding_function(text) for text in texts]
+        else:
+            raise TypeError(
+                "The embedding_function is neither Embeddings nor callable."
             )
-        return True
 
-    def delete_collection(self) -> None:
-        """
-        Completely delete the collection from the database (as opposed
-        to 'clear()', which empties it only).
-        Stored data is lost and unrecoverable, resources are freed.
-        Use with caution.
-        """
-        self._drop_collection()
-        return None
+    def _embed_query(self, text: str) -> List[float]:
+        if isinstance(self.embedding_function, Embeddings):
+            return self.embedding_function.embed_query(text)
+        else:
+            return self.embedding_function(text)
 
+    @_handle_exceptions
     def add_texts(
         self,
         texts: Iterable[str],
-        metadatas: Optional[List[dict]] = None,
+        metadatas: Optional[List[Dict[Any, Any]]] = None,
         ids: Optional[List[str]] = None,
-        *,
-        batch_size: Optional[int] = None,
-        batch_concurrency: Optional[int] = None,
-        overwrite_concurrency: Optional[int] = None,
         **kwargs: Any,
     ) -> List[str]:
-        """Run texts through the embeddings and add them to the vectorstore.
-
-        If passing explicit ids, those entries whose id is in the store already
-        will be replaced.
-
+        """Add more texts to the vectorstore index.
         Args:
-            texts (Iterable[str]): Texts to add to the vectorstore.
-            metadatas (Optional[List[dict]], optional): Optional list of metadatas.
-            ids (Optional[List[str]], optional): Optional list of ids.
-            batch_size (Optional[int]): Number of documents in each API call.
-                Check the underlying Astra DB HTTP API specs for the max value
-                (20 at the time of writing this). If not provided, defaults
-                to the instance-level setting.
-            batch_concurrency (Optional[int]): number of threads to process
-                insertion batches concurrently. Defaults to instance-level
-                setting if not provided.
-            overwrite_concurrency (Optional[int]):  number of threads to process
-                pre-existing documents in each batch (which require individual
-                API calls). Defaults to instance-level setting if not provided.
-
-        A note on metadata: there are constraints on the allowed field names
-        in this dictionary, coming from the underlying Astra DB API.
-        For instance, the `$` (dollar sign) cannot be used in the dict keys.
-        See this document for details:
-            docs.datastax.com/en/astra-serverless/docs/develop/dev-with-json.html
-
-        Returns:
-            List[str]: List of ids of the added texts.
+          texts: Iterable of strings to add to the vectorstore.
+          metadatas: Optional list of metadatas associated with the texts.
+          ids: Optional list of ids for the texts that are being added to
+          the vector store.
+          kwargs: vectorstore specific parameters
         """
 
-        if kwargs:
-            warnings.warn(
-                "Method 'add_texts' of AstraDB vector store invoked with "
-                f"unsupported arguments ({', '.join(sorted(kwargs.keys()))}), "
-                "which will be ignored."
-            )
-
-        _texts = list(texts)
-        if ids is None:
-            ids = [uuid.uuid4().hex for _ in _texts]
-        if metadatas is None:
-            metadatas = [{} for _ in _texts]
-        #
-        embedding_vectors = self.embedding.embed_documents(_texts)
-
-        documents_to_insert = [
-            {
-                "content": b_txt,
-                "_id": b_id,
-                "$vector": b_emb,
-                "metadata": b_md,
-            }
-            for b_txt, b_emb, b_id, b_md in zip(
-                _texts,
-                embedding_vectors,
-                ids,
-                metadatas,
-            )
-        ]
-        # make unique by id, keeping the last
-        uniqued_documents_to_insert = _unique_list(
-            documents_to_insert[::-1],
-            lambda document: document["_id"],
-        )[::-1]
-
-        all_ids = []
-
-        def _handle_batch(document_batch: List[DocDict]) -> List[str]:
-            im_result = self.collection.insert_many(
-                documents=document_batch,
-                options={"ordered": False},
-                partial_failures_allowed=True,
-            )
-            if "status" not in im_result:
-                raise ValueError(
-                    f"API Exception while running bulk insertion: {str(im_result)}"
-                )
-
-            batch_inserted = im_result["status"]["insertedIds"]
-            # estimation of the preexisting documents that failed
-            missed_inserted_ids = {
-                document["_id"] for document in document_batch
-            } - set(batch_inserted)
-            errors = im_result.get("errors", [])
-            # careful for other sources of error other than "doc already exists"
-            num_errors = len(errors)
-            unexpected_errors = any(
-                error.get("errorCode") != "DOCUMENT_ALREADY_EXISTS" for error in errors
-            )
-            if num_errors != len(missed_inserted_ids) or unexpected_errors:
-                raise ValueError(
-                    f"API Exception while running bulk insertion: {str(errors)}"
-                )
-
-            # deal with the missing insertions as upserts
-            missing_from_batch = [
-                document
-                for document in document_batch
-                if document["_id"] in missed_inserted_ids
+        texts = list(texts)
+        if ids:
+            # If ids are provided, hash them to maintain consistency
+            processed_ids = [
+                hashlib.sha256(_id.encode()).hexdigest()[:16].upper() for _id in ids
+            ]
+        elif metadatas and all("id" in metadata for metadata in metadatas):
+            # If no ids are provided but metadatas with ids are, generate
+            # ids from metadatas
+            processed_ids = [
+                hashlib.sha256(metadata["id"].encode()).hexdigest()[:16].upper()
+                for metadata in metadatas
+            ]
+        else:
+            # Generate new ids if none are provided
+            generated_ids = [
+                str(uuid.uuid4()) for _ in texts
+            ]  # uuid4 is more standard for random UUIDs
+            processed_ids = [
+                hashlib.sha256(_id.encode()).hexdigest()[:16].upper()
+                for _id in generated_ids
             ]
 
-            def _handle_missing_document(missing_document: DocDict) -> str:
-                replacement_result = self.collection.find_one_and_replace(
-                    filter={"_id": missing_document["_id"]},
-                    replacement=missing_document,
-                )
-                return replacement_result["data"]["document"]["_id"]
-
-            _u_max_workers = (
-                overwrite_concurrency or self.bulk_insert_overwrite_concurrency
+        embeddings = self._embed_documents(texts)
+        if not metadatas:
+            metadatas = [{} for _ in texts]
+        docs = [
+            (id_, text, json.dumps(metadata), array.array("f", embedding))
+            for id_, text, metadata, embedding in zip(
+                processed_ids, texts, metadatas, embeddings
             )
-            with ThreadPoolExecutor(max_workers=_u_max_workers) as tpe2:
-                batch_replaced = list(
-                    tpe2.map(
-                        _handle_missing_document,
-                        missing_from_batch,
-                    )
-                )
-
-            upsert_ids = batch_inserted + batch_replaced
-            return upsert_ids
+        ]
 
-        _b_max_workers = batch_concurrency or self.bulk_insert_batch_concurrency
-        with ThreadPoolExecutor(max_workers=_b_max_workers) as tpe:
-            all_ids_nested = tpe.map(
-                _handle_batch,
-                batch_iterate(
-                    batch_size or self.batch_size,
-                    uniqued_documents_to_insert,
-                ),
+        with self.client.cursor() as cursor:
+            cursor.executemany(
+                f"INSERT INTO {self.table_name} (id, text, metadata, "
+                f"embedding) VALUES (:1, :2, :3, :4)",
+                docs,
             )
+            self.client.commit()
+        return processed_ids
 
-        all_ids = [iid for id_list in all_ids_nested for iid in id_list]
-
-        return all_ids
+    def similarity_search(
+        self,
+        query: str,
+        k: int = 4,
+        filter: Optional[Dict[str, Any]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs most similar to query."""
+        if isinstance(self.embedding_function, Embeddings):
+            embedding = self.embedding_function.embed_query(query)
+        documents = self.similarity_search_by_vector(
+            embedding=embedding, k=k, filter=filter, **kwargs
+        )
+        return documents
 
-    def similarity_search_with_score_id_by_vector(
+    def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Tuple[Document, float, str]]:
-        """Return docs most similar to embedding vector.
-
-        Args:
-            embedding (str): Embedding to look up documents similar to.
-            k (int): Number of Documents to return. Defaults to 4.
-        Returns:
-            List of (Document, score, id), the most similar to the query vector.
-        """
-        metadata_parameter = self._filter_to_metadata(filter)
-        #
-        hits = list(
-            self.collection.paginated_find(
-                filter=metadata_parameter,
-                sort={"$vector": embedding},
-                options={"limit": k, "includeSimilarity": True},
-                projection={
-                    "_id": 1,
-                    "content": 1,
-                    "metadata": 1,
-                },
-            )
+        filter: Optional[dict[str, Any]] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        docs_and_scores = self.similarity_search_by_vector_with_relevance_scores(
+            embedding=embedding, k=k, filter=filter, **kwargs
         )
-        #
-        return [
-            (
-                Document(
-                    page_content=hit["content"],
-                    metadata=hit["metadata"],
-                ),
-                hit["$similarity"],
-                hit["_id"],
-            )
-            for hit in hits
-        ]
+        return [doc for doc, _ in docs_and_scores]
 
-    def similarity_search_with_score_id(
+    def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Tuple[Document, float, str]]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_with_score_id_by_vector(
-            embedding=embedding_vector,
-            k=k,
-            filter=filter,
+        filter: Optional[dict[str, Any]] = None,
+        **kwargs: Any,
+    ) -> List[Tuple[Document, float]]:
+        """Return docs most similar to query."""
+        if isinstance(self.embedding_function, Embeddings):
+            embedding = self.embedding_function.embed_query(query)
+        docs_and_scores = self.similarity_search_by_vector_with_relevance_scores(
+            embedding=embedding, k=k, filter=filter, **kwargs
         )
+        return docs_and_scores
 
-    def similarity_search_with_score_by_vector(
+    @_handle_exceptions
+    def _get_clob_value(self, result: Any) -> str:
+        try:
+            import oracledb
+        except ImportError as e:
+            raise ImportError(
+                "Unable to import oracledb, please install with "
+                "`pip install -U oracledb`."
+            ) from e
+
+        clob_value = ""
+        if result:
+            if isinstance(result, oracledb.LOB):
+                raw_data = result.read()
+                if isinstance(raw_data, bytes):
+                    clob_value = raw_data.decode(
+                        "utf-8"
+                    )  # Specify the correct encoding
+                else:
+                    clob_value = raw_data
+            elif isinstance(result, str):
+                clob_value = result
+            else:
+                raise Exception("Unexpected type:", type(result))
+        return clob_value
+
+    @_handle_exceptions
+    def similarity_search_by_vector_with_relevance_scores(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        filter: Optional[dict[str, Any]] = None,
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Return docs most similar to embedding vector.
+        docs_and_scores = []
+        embedding_arr = array.array("f", embedding)
 
-        Args:
-            embedding (str): Embedding to look up documents similar to.
-            k (int): Number of Documents to return. Defaults to 4.
-        Returns:
-            List of (Document, score), the most similar to the query vector.
-        """
-        return [
-            (doc, score)
-            for (doc, score, doc_id) in self.similarity_search_with_score_id_by_vector(
-                embedding=embedding,
-                k=k,
-                filter=filter,
-            )
-        ]
+        query = f"""
+        SELECT id,
+          text,
+          metadata,
+          vector_distance(embedding, :embedding,
+          {_get_distance_function(self.distance_strategy)}) as distance
+        FROM {self.table_name}
+        ORDER BY distance
+        FETCH APPROX FIRST {k} ROWS ONLY
+        """
+        # Execute the query
+        with self.client.cursor() as cursor:
+            cursor.execute(query, embedding=embedding_arr)
+            results = cursor.fetchall()
+
+            # Filter results if filter is provided
+            for result in results:
+                metadata = json.loads(
+                    self._get_clob_value(result[2]) if result[2] is not None else "{}"
+                )
 
-    def similarity_search(
-        self,
-        query: str,
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-        **kwargs: Any,
-    ) -> List[Document]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_by_vector(
-            embedding_vector,
-            k,
-            filter=filter,
-        )
+                # Apply filtering based on the 'filter' dictionary
+                if filter:
+                    if all(metadata.get(key) in value for key, value in filter.items()):
+                        doc = Document(
+                            page_content=(
+                                self._get_clob_value(result[1])
+                                if result[1] is not None
+                                else ""
+                            ),
+                            metadata=metadata,
+                        )
+                        distance = result[3]
+                        docs_and_scores.append((doc, distance))
+                else:
+                    doc = Document(
+                        page_content=(
+                            self._get_clob_value(result[1])
+                            if result[1] is not None
+                            else ""
+                        ),
+                        metadata=metadata,
+                    )
+                    distance = result[3]
+                    docs_and_scores.append((doc, distance))
 
-    def similarity_search_by_vector(
+        return docs_and_scores
+
+    @_handle_exceptions
+    def similarity_search_by_vector_returning_embeddings(
         self,
         embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        k: int,
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
-    ) -> List[Document]:
-        return [
-            doc
-            for doc, _ in self.similarity_search_with_score_by_vector(
-                embedding,
-                k,
-                filter=filter,
-            )
-        ]
+    ) -> List[Tuple[Document, float, np.ndarray[np.float32, Any]]]:
+        documents = []
+        embedding_arr = array.array("f", embedding)
+
+        query = f"""
+        SELECT id,
+          text,
+          metadata,
+          vector_distance(embedding, :embedding, {_get_distance_function(
+            self.distance_strategy)}) as distance,
+          embedding
+        FROM {self.table_name}
+        ORDER BY distance
+        FETCH APPROX FIRST {k} ROWS ONLY
+        """
+
+        # Execute the query
+        with self.client.cursor() as cursor:
+            cursor.execute(query, embedding=embedding_arr)
+            results = cursor.fetchall()
+
+            for result in results:
+                page_content_str = self._get_clob_value(result[1])
+                metadata_str = self._get_clob_value(result[2])
+                metadata = json.loads(metadata_str)
+
+                # Apply filter if provided and matches; otherwise, add all
+                # documents
+                if not filter or all(
+                    metadata.get(key) in value for key, value in filter.items()
+                ):
+                    document = Document(
+                        page_content=page_content_str, metadata=metadata
+                    )
+                    distance = result[3]
+                    # Assuming result[4] is already in the correct format;
+                    # adjust if necessary
+                    current_embedding = (
+                        np.array(result[4], dtype=np.float32)
+                        if result[4]
+                        else np.empty(0, dtype=np.float32)
+                    )
+                    documents.append((document, distance, current_embedding))
+        return documents  # type: ignore
 
-    def similarity_search_with_score(
+    @_handle_exceptions
+    def max_marginal_relevance_search_with_score_by_vector(
         self,
-        query: str,
+        embedding: List[float],
+        *,
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        filter: Optional[Dict[str, Any]] = None,
     ) -> List[Tuple[Document, float]]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_with_score_by_vector(
-            embedding_vector,
-            k,
-            filter=filter,
+        """Return docs and their similarity scores selected using the
+        maximal marginal
+            relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND
+        diversity
+        among selected documents.
+
+        Args:
+          self: An instance of the class
+          embedding: Embedding to look up documents similar to.
+          k: Number of Documents to return. Defaults to 4.
+          fetch_k: Number of Documents to fetch before filtering to
+                   pass to MMR algorithm.
+          filter: (Optional[Dict[str, str]]): Filter by metadata. Defaults
+          to None.
+          lambda_mult: Number between 0 and 1 that determines the degree
+                       of diversity among the results with 0 corresponding
+                       to maximum diversity and 1 to minimum diversity.
+                       Defaults to 0.5.
+        Returns:
+            List of Documents and similarity scores selected by maximal
+            marginal
+            relevance and score for each.
+        """
+
+        # Fetch documents and their scores
+        docs_scores_embeddings = self.similarity_search_by_vector_returning_embeddings(
+            embedding, fetch_k, filter=filter
+        )
+        # Assuming documents_with_scores is a list of tuples (Document, score)
+
+        # If you need to split documents and scores for processing (e.g.,
+        # for MMR calculation)
+        documents, scores, embeddings = (
+            zip(*docs_scores_embeddings) if docs_scores_embeddings else ([], [], [])
         )
 
+        # Assume maximal_marginal_relevance method accepts embeddings and
+        # scores, and returns indices of selected docs
+        mmr_selected_indices = maximal_marginal_relevance(
+            np.array(embedding, dtype=np.float32),
+            list(embeddings),
+            k=k,
+            lambda_mult=lambda_mult,
+        )
+
+        # Filter documents based on MMR-selected indices and map scores
+        mmr_selected_documents_with_scores = [
+            (documents[i], scores[i]) for i in mmr_selected_indices
+        ]
+
+        return mmr_selected_documents_with_scores
+
+    @_handle_exceptions
     def max_marginal_relevance_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, str]] = None,
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
-        Maximal marginal relevance optimizes for similarity to query AND diversity
+
+        Maximal marginal relevance optimizes for similarity to query AND
+        diversity
         among selected documents.
+
         Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
+          self: An instance of the class
+          embedding: Embedding to look up documents similar to.
+          k: Number of Documents to return. Defaults to 4.
+          fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+          lambda_mult: Number between 0 and 1 that determines the degree
+                       of diversity among the results with 0 corresponding
+                       to maximum diversity and 1 to minimum diversity.
+                       Defaults to 0.5.
+          filter: Optional[Dict[str, Any]]
+          **kwargs: Any
         Returns:
-            List of Documents selected by maximal marginal relevance.
+          List of Documents selected by maximal marginal relevance.
         """
-        metadata_parameter = self._filter_to_metadata(filter)
-
-        prefetch_hits = list(
-            self.collection.paginated_find(
-                filter=metadata_parameter,
-                sort={"$vector": embedding},
-                options={"limit": fetch_k, "includeSimilarity": True},
-                projection={
-                    "_id": 1,
-                    "content": 1,
-                    "metadata": 1,
-                    "$vector": 1,
-                },
-            )
-        )
-
-        mmr_chosen_indices = maximal_marginal_relevance(
-            np.array(embedding, dtype=np.float32),
-            [prefetch_hit["$vector"] for prefetch_hit in prefetch_hits],
-            k=k,
-            lambda_mult=lambda_mult,
+        docs_and_scores = self.max_marginal_relevance_search_with_score_by_vector(
+            embedding, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, filter=filter
         )
-        mmr_hits = [
-            prefetch_hit
-            for prefetch_index, prefetch_hit in enumerate(prefetch_hits)
-            if prefetch_index in mmr_chosen_indices
-        ]
-        return [
-            Document(
-                page_content=hit["content"],
-                metadata=hit["metadata"],
-            )
-            for hit in mmr_hits
-        ]
+        return [doc for doc, _ in docs_and_scores]
 
+    @_handle_exceptions
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, str]] = None,
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
-        Maximal marginal relevance optimizes for similarity to query AND diversity
+
+        Maximal marginal relevance optimizes for similarity to query AND
+        diversity
         among selected documents.
+
         Args:
-            query (str): Text to look up documents similar to.
-            k (int = 4): Number of Documents to return.
-            fetch_k (int = 20): Number of Documents to fetch to pass to MMR algorithm.
-            lambda_mult (float = 0.5): Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
-                        Optional.
+          self: An instance of the class
+          query: Text to look up documents similar to.
+          k: Number of Documents to return. Defaults to 4.
+          fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+          lambda_mult: Number between 0 and 1 that determines the degree
+                       of diversity among the results with 0 corresponding
+                       to maximum diversity and 1 to minimum diversity.
+                       Defaults to 0.5.
+          filter: Optional[Dict[str, Any]]
+          **kwargs
         Returns:
-            List of Documents selected by maximal marginal relevance.
+          List of Documents selected by maximal marginal relevance.
+
+        `max_marginal_relevance_search` requires that `query` returns matched
+        embeddings alongside the match documents.
         """
-        embedding_vector = self.embedding.embed_query(query)
-        return self.max_marginal_relevance_search_by_vector(
-            embedding_vector,
-            k,
-            fetch_k,
+        embedding = self._embed_query(query)
+        documents = self.max_marginal_relevance_search_by_vector(
+            embedding,
+            k=k,
+            fetch_k=fetch_k,
             lambda_mult=lambda_mult,
             filter=filter,
+            **kwargs,
         )
+        return documents
 
-    @classmethod
-    def from_texts(
-        cls: Type[ADBVST],
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        ids: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> ADBVST:
-        """Create an Astra DB vectorstore from raw texts.
-
+    @_handle_exceptions
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:
+        """Delete by vector IDs.
         Args:
-            texts (List[str]): the texts to insert.
-            embedding (Embeddings): the embedding function to use in the store.
-            metadatas (Optional[List[dict]]): metadata dicts for the texts.
-            ids (Optional[List[str]]): ids to associate to the texts.
-            *Additional arguments*: you can pass any argument that you would
-                to 'add_texts' and/or to the 'AstraDB' class constructor
-                (see these methods for details). These arguments will be
-                routed to the respective methods as they are.
-
-        Returns:
-            an `AstraDb` vectorstore.
+          self: An instance of the class
+          ids: List of ids to delete.
+          **kwargs
         """
 
-        known_kwargs = {
-            "collection_name",
-            "token",
-            "api_endpoint",
-            "astra_db_client",
-            "namespace",
-            "metric",
-            "batch_size",
-            "bulk_insert_batch_concurrency",
-            "bulk_insert_overwrite_concurrency",
-            "bulk_delete_concurrency",
-            "batch_concurrency",
-            "overwrite_concurrency",
+        if ids is None:
+            raise ValueError("No ids provided to delete.")
+
+        # Compute SHA-256 hashes of the ids and truncate them
+        hashed_ids = [
+            hashlib.sha256(_id.encode()).hexdigest()[:16].upper() for _id in ids
+        ]
+
+        # Constructing the SQL statement with individual placeholders
+        placeholders = ", ".join([":id" + str(i + 1) for i in range(len(hashed_ids))])
+
+        ddl = f"DELETE FROM {self.table_name} WHERE id IN ({placeholders})"
+
+        # Preparing bind variables
+        bind_vars = {
+            f"id{i}": hashed_id for i, hashed_id in enumerate(hashed_ids, start=1)
         }
-        if kwargs:
-            unknown_kwargs = set(kwargs.keys()) - known_kwargs
-            if unknown_kwargs:
-                warnings.warn(
-                    "Method 'from_texts' of AstraDB vector store invoked with "
-                    f"unsupported arguments ({', '.join(sorted(unknown_kwargs))}), "
-                    "which will be ignored."
-                )
 
-        collection_name: str = kwargs["collection_name"]
-        token = kwargs.get("token")
-        api_endpoint = kwargs.get("api_endpoint")
-        astra_db_client = kwargs.get("astra_db_client")
-        namespace = kwargs.get("namespace")
-        metric = kwargs.get("metric")
-
-        astra_db_store = cls(
-            embedding=embedding,
-            collection_name=collection_name,
-            token=token,
-            api_endpoint=api_endpoint,
-            astra_db_client=astra_db_client,
-            namespace=namespace,
-            metric=metric,
-            batch_size=kwargs.get("batch_size"),
-            bulk_insert_batch_concurrency=kwargs.get("bulk_insert_batch_concurrency"),
-            bulk_insert_overwrite_concurrency=kwargs.get(
-                "bulk_insert_overwrite_concurrency"
-            ),
-            bulk_delete_concurrency=kwargs.get("bulk_delete_concurrency"),
-        )
-        astra_db_store.add_texts(
-            texts=texts,
-            metadatas=metadatas,
-            ids=ids,
-            batch_size=kwargs.get("batch_size"),
-            batch_concurrency=kwargs.get("batch_concurrency"),
-            overwrite_concurrency=kwargs.get("overwrite_concurrency"),
-        )
-        return astra_db_store
+        with self.client.cursor() as cursor:
+            cursor.execute(ddl, bind_vars)
+            self.client.commit()
 
     @classmethod
-    def from_documents(
-        cls: Type[ADBVST],
-        documents: List[Document],
+    @_handle_exceptions
+    def from_texts(
+        cls: Type[OracleVS],
+        texts: Iterable[str],
         embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
-    ) -> ADBVST:
-        """Create an Astra DB vectorstore from a document list.
+    ) -> OracleVS:
+        """Return VectorStore initialized from texts and embeddings."""
+        client = kwargs.get("client")
+        if client is None:
+            raise ValueError("client parameter is required...")
+        params = kwargs.get("params", {})
 
-        Utility method that defers to 'from_texts' (see that one).
+        table_name = str(kwargs.get("table_name", "langchain"))
 
-        Args: see 'from_texts', except here you have to supply 'documents'
-            in place of 'texts' and 'metadatas'.
+        distance_strategy = cast(
+            DistanceStrategy, kwargs.get("distance_strategy", None)
+        )
+        if not isinstance(distance_strategy, DistanceStrategy):
+            raise TypeError(
+                f"Expected DistanceStrategy got " f"{type(distance_strategy).__name__} "
+            )
 
-        Returns:
-            an `AstraDB` vectorstore.
-        """
-        return super().from_documents(documents, embedding, **kwargs)
+        query = kwargs.get("query", "What is a Oracle database")
+
+        drop_table_purge(client, table_name)
+
+        vss = cls(
+            client=client,
+            embedding_function=embedding,
+            table_name=table_name,
+            distance_strategy=distance_strategy,
+            query=query,
+            params=params,
+        )
+        vss.add_texts(texts=list(texts), metadatas=metadatas)
+        return vss
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/atlas.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/atlas.py`

 * *Files 0% similar despite different names*

```diff
@@ -115,15 +115,15 @@
             and len(metadatas) > 0
             and "text" in metadatas[0].keys()
         ):
             raise ValueError("Cannot accept key text in metadata!")
 
         texts = list(texts)
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         # Embedding upload case
         if self._embedding_function is not None:
             _embeddings = self._embedding_function.embed_documents(texts)
             embeddings = np.stack(_embeddings)
             if metadatas is None:
                 data = [
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/awadb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/awadb.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/azure_cosmos_db.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/rocksetdb.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,425 +1,418 @@
 from __future__ import annotations
 
 import logging
+from copy import deepcopy
 from enum import Enum
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Dict,
-    Generator,
-    Iterable,
-    List,
-    Optional,
-    Tuple,
-    TypeVar,
-    Union,
-)
+from typing import Any, Iterable, List, Optional, Tuple
 
 import numpy as np
 from langchain_core.documents import Document
+from langchain_core.embeddings import Embeddings
+from langchain_core.runnables import run_in_executor
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
-if TYPE_CHECKING:
-    from langchain_core.embeddings import Embeddings
-    from pymongo.collection import Collection
-
-
-# Before Python 3.11 native StrEnum is not available
-class CosmosDBSimilarityType(str, Enum):
-    """Cosmos DB Similarity Type as enumerator."""
-
-    COS = "COS"
-    """CosineSimilarity"""
-    IP = "IP"
-    """inner - product"""
-    L2 = "L2"
-    """Euclidean distance"""
-
-
-CosmosDBDocumentType = TypeVar("CosmosDBDocumentType", bound=Dict[str, Any])
-
 logger = logging.getLogger(__name__)
 
-DEFAULT_INSERT_BATCH_SIZE = 128
 
+class Rockset(VectorStore):
+    """`Rockset` vector store.
 
-class AzureCosmosDBVectorSearch(VectorStore):
-    """`Azure Cosmos DB for MongoDB vCore` vector store.
+    To use, you should have the `rockset` python package installed. Note that to use
+    this, the collection being used must already exist in your Rockset instance.
+    You must also ensure you use a Rockset ingest transformation to apply
+    `VECTOR_ENFORCE` on the column being used to store `embedding_key` in the
+    collection.
+    See: https://rockset.com/blog/introducing-vector-search-on-rockset/ for more details
 
-    To use, you should have both:
-    - the ``pymongo`` python package installed
-    - a connection string associated with a MongoDB VCore Cluster
+    Everything below assumes `commons` Rockset workspace.
 
     Example:
-        . code-block:: python
+        .. code-block:: python
 
-            from langchain_community.vectorstores import AzureCosmosDBVectorSearch
+            from langchain_community.vectorstores import Rockset
             from langchain_community.embeddings.openai import OpenAIEmbeddings
-            from pymongo import MongoClient
+            import rockset
 
-            mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
-            collection = mongo_client["<db_name>"]["<collection_name>"]
+            # Make sure you use the right host (region) for your Rockset instance
+            # and APIKEY has both read-write access to your collection.
+
+            rs = rockset.RocksetClient(host=rockset.Regions.use1a1, api_key="***")
+            collection_name = "langchain_demo"
             embeddings = OpenAIEmbeddings()
-            vectorstore = AzureCosmosDBVectorSearch(collection, embeddings)
+            vectorstore = Rockset(rs, collection_name, embeddings,
+                "description", "description_embedding")
+
     """
 
     def __init__(
         self,
-        collection: Collection[CosmosDBDocumentType],
-        embedding: Embeddings,
-        *,
-        index_name: str = "vectorSearchIndex",
-        text_key: str = "textContent",
-        embedding_key: str = "vectorContent",
+        client: Any,
+        embeddings: Embeddings,
+        collection_name: str,
+        text_key: str,
+        embedding_key: str,
+        workspace: str = "commons",
     ):
-        """Constructor for AzureCosmosDBVectorSearch
-
-        Args:
-            collection: MongoDB collection to add the texts to.
-            embedding: Text embedding model to use.
-            index_name: Name of the Atlas Search index.
-            text_key: MongoDB field that will contain the text
-                for each document.
-            embedding_key: MongoDB field that will contain the embedding
-                for each document.
-        """
-        self._collection = collection
-        self._embedding = embedding
-        self._index_name = index_name
-        self._text_key = text_key
-        self._embedding_key = embedding_key
-
-    @property
-    def embeddings(self) -> Embeddings:
-        return self._embedding
-
-    def get_index_name(self) -> str:
-        """Returns the index name
-
-        Returns:
-            Returns the index name
-
-        """
-        return self._index_name
-
-    @classmethod
-    def from_connection_string(
-        cls,
-        connection_string: str,
-        namespace: str,
-        embedding: Embeddings,
-        **kwargs: Any,
-    ) -> AzureCosmosDBVectorSearch:
-        """Creates an Instance of AzureCosmosDBVectorSearch from a Connection String
-
+        """Initialize with Rockset client.
         Args:
-            connection_string: The MongoDB vCore instance connection string
-            namespace: The namespace (database.collection)
-            embedding: The embedding utility
-            **kwargs: Dynamic keyword arguments
-
-        Returns:
-            an instance of the vector store
+            client: Rockset client object
+            collection: Rockset collection to insert docs / query
+            embeddings: Langchain Embeddings object to use to generate
+                        embedding for given text.
+            text_key: column in Rockset collection to use to store the text
+            embedding_key: column in Rockset collection to use to store the embedding.
+                           Note: We must apply `VECTOR_ENFORCE()` on this column via
+                           Rockset ingest transformation.
 
         """
         try:
-            from pymongo import MongoClient
+            from rockset import RocksetClient
         except ImportError:
             raise ImportError(
-                "Could not import pymongo, please install it with "
-                "`pip install pymongo`."
+                "Could not import rockset client python package. "
+                "Please install it with `pip install rockset`."
             )
-        client: MongoClient = MongoClient(connection_string)
-        db_name, collection_name = namespace.split(".")
-        collection = client[db_name][collection_name]
-        return cls(collection, embedding, **kwargs)
-
-    def index_exists(self) -> bool:
-        """Verifies if the specified index name during instance
-            construction exists on the collection
-
-        Returns:
-          Returns True on success and False if no such index exists
-            on the collection
-        """
-        cursor = self._collection.list_indexes()
-        index_name = self._index_name
-
-        for res in cursor:
-            current_index_name = res.pop("name")
-            if current_index_name == index_name:
-                return True
-
-        return False
-
-    def delete_index(self) -> None:
-        """Deletes the index specified during instance construction if it exists"""
-        if self.index_exists():
-            self._collection.drop_index(self._index_name)
-            # Raises OperationFailure on an error (e.g. trying to drop
-            # an index that does not exist)
-
-    def create_index(
-        self,
-        num_lists: int = 100,
-        dimensions: int = 1536,
-        similarity: CosmosDBSimilarityType = CosmosDBSimilarityType.COS,
-    ) -> dict[str, Any]:
-        """Creates an index using the index name specified at
-            instance construction
-
-        Setting the numLists parameter correctly is important for achieving
-            good accuracy and performance.
-            Since the vector store uses IVF as the indexing strategy,
-            you should create the index only after you
-            have loaded a large enough sample documents to ensure that the
-            centroids for the respective buckets are
-            faily distributed.
-
-        We recommend that numLists is set to documentCount/1000 for up
-            to 1 million documents
-            and to sqrt(documentCount) for more than 1 million documents.
-            As the number of items in your database grows, you should
-            tune numLists to be larger
-            in order to achieve good latency performance for vector search.
-
-            If you're experimenting with a new scenario or creating a
-            small demo, you can start with numLists
-            set to 1 to perform a brute-force search across all vectors.
-            This should provide you with the most
-            accurate results from the vector search, however be aware that
-            the search speed and latency will be slow.
-            After your initial setup, you should go ahead and tune
-            the numLists parameter using the above guidance.
-
-        Args:
-            num_lists: This integer is the number of clusters that the
-                inverted file (IVF) index uses to group the vector data.
-                We recommend that numLists is set to documentCount/1000
-                for up to 1 million documents and to sqrt(documentCount)
-                for more than 1 million documents.
-                Using a numLists value of 1 is akin to performing
-                brute-force search, which has limited performance
-            dimensions: Number of dimensions for vector similarity.
-                The maximum number of supported dimensions is 2000
-            similarity: Similarity metric to use with the IVF index.
-
-                Possible options are:
-                    - CosmosDBSimilarityType.COS (cosine distance),
-                    - CosmosDBSimilarityType.L2 (Euclidean distance), and
-                    - CosmosDBSimilarityType.IP (inner product).
 
-        Returns:
-            An object describing the created index
+        if not isinstance(client, RocksetClient):
+            raise ValueError(
+                f"client should be an instance of rockset.RocksetClient, "
+                f"got {type(client)}"
+            )
+        # TODO: check that `collection_name` exists in rockset. Create if not.
+        self._client = client
+        self._collection_name = collection_name
+        self._embeddings = embeddings
+        self._text_key = text_key
+        self._embedding_key = embedding_key
+        self._workspace = workspace
 
-        """
-        # prepare the command
-        create_index_commands = {
-            "createIndexes": self._collection.name,
-            "indexes": [
-                {
-                    "name": self._index_name,
-                    "key": {self._embedding_key: "cosmosSearch"},
-                    "cosmosSearchOptions": {
-                        "kind": "vector-ivf",
-                        "numLists": num_lists,
-                        "similarity": similarity,
-                        "dimensions": dimensions,
-                    },
-                }
-            ],
-        }
-
-        # retrieve the database object
-        current_database = self._collection.database
-
-        # invoke the command from the database object
-        create_index_responses: dict[str, Any] = current_database.command(
-            create_index_commands
-        )
+        try:
+            self._client.set_application("langchain")
+        except AttributeError:
+            # ignore
+            pass
 
-        return create_index_responses
+    @property
+    def embeddings(self) -> Embeddings:
+        return self._embeddings
 
     def add_texts(
         self,
         texts: Iterable[str],
-        metadatas: Optional[List[Dict[str, Any]]] = None,
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        batch_size: int = 32,
         **kwargs: Any,
-    ) -> List:
-        batch_size = kwargs.get("batch_size", DEFAULT_INSERT_BATCH_SIZE)
-        _metadatas: Union[List, Generator] = metadatas or ({} for _ in texts)
-        texts_batch = []
-        metadatas_batch = []
-        result_ids = []
-        for i, (text, metadata) in enumerate(zip(texts, _metadatas)):
-            texts_batch.append(text)
-            metadatas_batch.append(metadata)
-            if (i + 1) % batch_size == 0:
-                result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
-                texts_batch = []
-                metadatas_batch = []
-        if texts_batch:
-            result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
-        return result_ids
-
-    def _insert_texts(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> List:
-        """Used to Load Documents into the collection
+    ) -> List[str]:
+        """Run more texts through the embeddings and add to the vectorstore
 
-        Args:
-            texts: The list of documents strings to load
-            metadatas: The list of metadata objects associated with each document
+                Args:
+            texts: Iterable of strings to add to the vectorstore.
+            metadatas: Optional list of metadatas associated with the texts.
+            ids: Optional list of ids to associate with the texts.
+            batch_size: Send documents in batches to rockset.
 
         Returns:
+            List of ids from adding the texts into the vectorstore.
 
         """
-        # If the text is empty, then exit early
-        if not texts:
-            return []
+        batch: list[dict] = []
+        stored_ids = []
 
-        # Embed and create the documents
-        embeddings = self._embedding.embed_documents(texts)
-        to_insert = [
-            {self._text_key: t, self._embedding_key: embedding, **m}
-            for t, m, embedding in zip(texts, metadatas, embeddings)
-        ]
-        # insert the documents in Cosmos DB
-        insert_result = self._collection.insert_many(to_insert)  # type: ignore
-        return insert_result.inserted_ids
+        for i, text in enumerate(texts):
+            if len(batch) == batch_size:
+                stored_ids += self._write_documents_to_rockset(batch)
+                batch = []
+            doc = {}
+            if metadatas and len(metadatas) > i:
+                doc = deepcopy(metadatas[i])
+            if ids and len(ids) > i:
+                doc["_id"] = ids[i]
+            doc[self._text_key] = text
+            doc[self._embedding_key] = self._embeddings.embed_query(text)
+            batch.append(doc)
+        if len(batch) > 0:
+            stored_ids += self._write_documents_to_rockset(batch)
+            batch = []
+        return stored_ids
 
     @classmethod
     def from_texts(
         cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
-        collection: Optional[Collection[CosmosDBDocumentType]] = None,
+        client: Any = None,
+        collection_name: str = "",
+        text_key: str = "",
+        embedding_key: str = "",
+        ids: Optional[List[str]] = None,
+        batch_size: int = 32,
         **kwargs: Any,
-    ) -> AzureCosmosDBVectorSearch:
-        if collection is None:
-            raise ValueError("Must provide 'collection' named parameter.")
-        vectorstore = cls(collection, embedding, **kwargs)
-        vectorstore.add_texts(texts, metadatas=metadatas)
-        return vectorstore
-
-    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
-        if ids is None:
-            raise ValueError("No document ids provided to delete.")
-
-        for document_id in ids:
-            self.delete_document_by_id(document_id)
-        return True
-
-    def delete_document_by_id(self, document_id: Optional[str] = None) -> None:
-        """Removes a Specific Document by Id
-
-        Args:
-            document_id: The document identifier
+    ) -> Rockset:
+        """Create Rockset wrapper with existing texts.
+        This is intended as a quicker way to get started.
         """
-        try:
-            from bson.objectid import ObjectId
-        except ImportError as e:
-            raise ImportError(
-                "Unable to import bson, please install with `pip install bson`."
-            ) from e
-        if document_id is None:
-            raise ValueError("No document id provided to delete.")
 
-        self._collection.delete_one({"_id": ObjectId(document_id)})
+        # Sanitize inputs
+        assert client is not None, "Rockset Client cannot be None"
+        assert collection_name, "Collection name cannot be empty"
+        assert text_key, "Text key name cannot be empty"
+        assert embedding_key, "Embedding key cannot be empty"
+
+        rockset = cls(client, embedding, collection_name, text_key, embedding_key)
+        rockset.add_texts(texts, metadatas, ids, batch_size)
+        return rockset
+
+    # Rockset supports these vector distance functions.
+    class DistanceFunction(Enum):
+        COSINE_SIM = "COSINE_SIM"
+        EUCLIDEAN_DIST = "EUCLIDEAN_DIST"
+        DOT_PRODUCT = "DOT_PRODUCT"
+
+        # how to sort results for "similarity"
+        def order_by(self) -> str:
+            if self.value == "EUCLIDEAN_DIST":
+                return "ASC"
+            return "DESC"
 
-    def _similarity_search_with_score(
-        self, embeddings: List[float], k: int = 4
+    def similarity_search_with_relevance_scores(
+        self,
+        query: str,
+        k: int = 4,
+        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
+        where_str: Optional[str] = None,
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Returns a list of documents with their scores
+        """Perform a similarity search with Rockset
 
         Args:
-            embeddings: The query vector
-            k: the number of documents to return
+            query (str): Text to look up documents similar to.
+            distance_func (DistanceFunction): how to compute distance between two
+                vectors in Rockset.
+            k (int, optional): Top K neighbors to retrieve. Defaults to 4.
+            where_str (Optional[str], optional): Metadata filters supplied as a
+                SQL `where` condition string. Defaults to None.
+                eg. "price<=70.0 AND brand='Nintendo'"
+
+            NOTE: Please do not let end-user to fill this and always be aware
+                  of SQL injection.
 
         Returns:
-            A list of documents closest to the query vector
+            List[Tuple[Document, float]]: List of documents with their relevance score
         """
-        pipeline: List[dict[str, Any]] = [
-            {
-                "$search": {
-                    "cosmosSearch": {
-                        "vector": embeddings,
-                        "path": self._embedding_key,
-                        "k": k,
-                    },
-                    "returnStoredSource": True,
-                }
-            },
-            {
-                "$project": {
-                    "similarityScore": {"$meta": "searchScore"},
-                    "document": "$$ROOT",
-                }
-            },
-        ]
-
-        cursor = self._collection.aggregate(pipeline)
-
-        docs = []
-
-        for res in cursor:
-            score = res.pop("similarityScore")
-            document_object_field = res.pop("document")
-            text = document_object_field.pop(self._text_key)
-            docs.append(
-                (Document(page_content=text, metadata=document_object_field), score)
-            )
-
-        return docs
-
-    def similarity_search_with_score(
-        self, query: str, k: int = 4
-    ) -> List[Tuple[Document, float]]:
-        embeddings = self._embedding.embed_query(query)
-        docs = self._similarity_search_with_score(embeddings=embeddings, k=k)
-        return docs
+        return self.similarity_search_by_vector_with_relevance_scores(
+            self._embeddings.embed_query(query),
+            k,
+            distance_func,
+            where_str,
+            **kwargs,
+        )
 
     def similarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
+        self,
+        query: str,
+        k: int = 4,
+        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
+        where_str: Optional[str] = None,
+        **kwargs: Any,
     ) -> List[Document]:
-        docs_and_scores = self.similarity_search_with_score(query, k=k)
-        return [doc for doc, _ in docs_and_scores]
+        """Same as `similarity_search_with_relevance_scores` but
+        doesn't return the scores.
+        """
+        return self.similarity_search_by_vector(
+            self._embeddings.embed_query(query),
+            k,
+            distance_func,
+            where_str,
+            **kwargs,
+        )
 
-    def max_marginal_relevance_search_by_vector(
+    def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
+        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
+        where_str: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        # Retrieves the docs with similarity scores
-        # sorted by similarity scores in DESC order
-        docs = self._similarity_search_with_score(embedding, k=fetch_k)
-
-        # Re-ranks the docs using MMR
-        mmr_doc_indexes = maximal_marginal_relevance(
-            np.array(embedding),
-            [doc.metadata[self._embedding_key] for doc, _ in docs],
-            k=k,
-            lambda_mult=lambda_mult,
+        """Accepts a query_embedding (vector), and returns documents with
+        similar embeddings."""
+
+        docs_and_scores = self.similarity_search_by_vector_with_relevance_scores(
+            embedding, k, distance_func, where_str, **kwargs
+        )
+        return [doc for doc, _ in docs_and_scores]
+
+    def similarity_search_by_vector_with_relevance_scores(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
+        where_str: Optional[str] = None,
+        **kwargs: Any,
+    ) -> List[Tuple[Document, float]]:
+        """Accepts a query_embedding (vector), and returns documents with
+        similar embeddings along with their relevance scores."""
+
+        exclude_embeddings = True
+        if "exclude_embeddings" in kwargs:
+            exclude_embeddings = kwargs["exclude_embeddings"]
+        q_str = self._build_query_sql(
+            embedding, distance_func, k, where_str, exclude_embeddings
         )
-        mmr_docs = [docs[i][0] for i in mmr_doc_indexes]
-        return mmr_docs
+        try:
+            query_response = self._client.Queries.query(sql={"query": q_str})
+        except Exception as e:
+            logger.error("Exception when querying Rockset: %s\n", e)
+            return []
+        finalResult: list[Tuple[Document, float]] = []
+        for document in query_response.results:
+            metadata = {}
+            assert isinstance(
+                document, dict
+            ), "document should be of type `dict[str,Any]`. But found: `{}`".format(
+                type(document)
+            )
+            for k, v in document.items():
+                if k == self._text_key:
+                    assert isinstance(v, str), (
+                        "page content stored in column `{}` must be of type `str`. "
+                        "But found: `{}`"
+                    ).format(self._text_key, type(v))
+                    page_content = v
+                elif k == "dist":
+                    assert isinstance(v, float), (
+                        "Computed distance between vectors must of type `float`. "
+                        "But found {}"
+                    ).format(type(v))
+                    score = v
+                elif k not in ["_id", "_event_time", "_meta"]:
+                    # These columns are populated by Rockset when documents are
+                    # inserted. No need to return them in metadata dict.
+                    metadata[k] = v
+            finalResult.append(
+                (Document(page_content=page_content, metadata=metadata), score)
+            )
+        return finalResult
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
+        *,
+        where_str: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        # compute the embeddings vector from the query string
-        embeddings = self._embedding.embed_query(query)
+        """Return docs selected using the maximal marginal relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            distance_func (DistanceFunction): how to compute distance between two
+                vectors in Rockset.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+            where_str: where clause for the sql query
+        Returns:
+            List of Documents selected by maximal marginal relevance.
+        """
+        query_embedding = self._embeddings.embed_query(query)
+        initial_docs = self.similarity_search_by_vector(
+            query_embedding,
+            k=fetch_k,
+            where_str=where_str,
+            exclude_embeddings=False,
+            **kwargs,
+        )
+
+        embeddings = [doc.metadata[self._embedding_key] for doc in initial_docs]
 
-        docs = self.max_marginal_relevance_search_by_vector(
-            embeddings, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult
+        selected_indices = maximal_marginal_relevance(
+            np.array(query_embedding),
+            embeddings,
+            lambda_mult=lambda_mult,
+            k=k,
         )
-        return docs
+
+        # remove embeddings key before returning for cleanup to be consistent with
+        #   other search functions
+        for i in selected_indices:
+            del initial_docs[i].metadata[self._embedding_key]
+
+        return [initial_docs[i] for i in selected_indices]
+
+    # Helper functions
+
+    def _build_query_sql(
+        self,
+        query_embedding: List[float],
+        distance_func: DistanceFunction,
+        k: int = 4,
+        where_str: Optional[str] = None,
+        exclude_embeddings: bool = True,
+    ) -> str:
+        """Builds Rockset SQL query to query similar vectors to query_vector"""
+
+        q_embedding_str = ",".join(map(str, query_embedding))
+        distance_str = f"""{distance_func.value}({self._embedding_key}, \
+[{q_embedding_str}]) as dist"""
+        where_str = f"WHERE {where_str}\n" if where_str else ""
+        select_embedding = (
+            f" EXCEPT({self._embedding_key})," if exclude_embeddings else ","
+        )
+        return f"""\
+SELECT *{select_embedding} {distance_str}
+FROM {self._workspace}.{self._collection_name}
+{where_str}\
+ORDER BY dist {distance_func.order_by()}
+LIMIT {str(k)}
+"""
+
+    def _write_documents_to_rockset(self, batch: List[dict]) -> List[str]:
+        add_doc_res = self._client.Documents.add_documents(
+            collection=self._collection_name, data=batch, workspace=self._workspace
+        )
+        return [doc_status._id for doc_status in add_doc_res.data]
+
+    def delete_texts(self, ids: List[str]) -> None:
+        """Delete a list of docs from the Rockset collection"""
+        try:
+            from rockset.models import DeleteDocumentsRequestData
+        except ImportError:
+            raise ImportError(
+                "Could not import rockset client python package. "
+                "Please install it with `pip install rockset`."
+            )
+
+        self._client.Documents.delete_documents(
+            collection=self._collection_name,
+            data=[DeleteDocumentsRequestData(id=i) for i in ids],
+            workspace=self._workspace,
+        )
+
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
+        try:
+            if ids is None:
+                ids = []
+            self.delete_texts(ids)
+        except Exception as e:
+            logger.error("Exception when deleting docs from Rockset: %s\n", e)
+            return False
+
+        return True
+
+    async def adelete(
+        self, ids: Optional[List[str]] = None, **kwargs: Any
+    ) -> Optional[bool]:
+        return await run_in_executor(None, self.delete, ids, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/azuresearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/azuresearch.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,17 +4,20 @@
 import json
 import logging
 import uuid
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
+    ClassVar,
+    Collection,
     Dict,
     Iterable,
     List,
+    Literal,
     Optional,
     Tuple,
     Type,
     Union,
 )
 
 import numpy as np
@@ -33,22 +36,18 @@
 
 if TYPE_CHECKING:
     from azure.search.documents import SearchClient
     from azure.search.documents.indexes.models import (
         CorsOptions,
         ScoringProfile,
         SearchField,
+        SemanticConfiguration,
         VectorSearch,
     )
 
-    try:
-        from azure.search.documents.indexes.models import SemanticSearch
-    except ImportError:
-        from azure.search.documents.indexes.models import SemanticSettings  # <11.4.0
-
 # Allow overriding field names for Azure Search
 FIELDS_ID = get_from_env(
     key="AZURESEARCH_FIELDS_ID", env_key="AZURESEARCH_FIELDS_ID", default="id"
 )
 FIELDS_CONTENT = get_from_env(
     key="AZURESEARCH_FIELDS_CONTENT",
     env_key="AZURESEARCH_FIELDS_CONTENT",
@@ -69,51 +68,44 @@
 def _get_search_client(
     endpoint: str,
     key: str,
     index_name: str,
     semantic_configuration_name: Optional[str] = None,
     fields: Optional[List[SearchField]] = None,
     vector_search: Optional[VectorSearch] = None,
-    semantic_settings: Optional[Union[SemanticSearch, SemanticSettings]] = None,
+    semantic_configurations: Optional[
+        Union[SemanticConfiguration, List[SemanticConfiguration]]
+    ] = None,
     scoring_profiles: Optional[List[ScoringProfile]] = None,
     default_scoring_profile: Optional[str] = None,
     default_fields: Optional[List[SearchField]] = None,
     user_agent: Optional[str] = "langchain",
     cors_options: Optional[CorsOptions] = None,
 ) -> SearchClient:
     from azure.core.credentials import AzureKeyCredential
     from azure.core.exceptions import ResourceNotFoundError
     from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential
     from azure.search.documents import SearchClient
     from azure.search.documents.indexes import SearchIndexClient
     from azure.search.documents.indexes.models import (
+        ExhaustiveKnnAlgorithmConfiguration,
+        ExhaustiveKnnParameters,
+        HnswAlgorithmConfiguration,
+        HnswParameters,
         SearchIndex,
         SemanticConfiguration,
         SemanticField,
+        SemanticPrioritizedFields,
+        SemanticSearch,
         VectorSearch,
+        VectorSearchAlgorithmKind,
+        VectorSearchAlgorithmMetric,
+        VectorSearchProfile,
     )
 
-    # class names changed for versions >= 11.4.0
-    try:
-        from azure.search.documents.indexes.models import (
-            HnswAlgorithmConfiguration,  # HnswVectorSearchAlgorithmConfiguration is old
-            SemanticPrioritizedFields,  # PrioritizedFields outdated
-            SemanticSearch,  # SemanticSettings outdated
-        )
-
-        NEW_VERSION = True
-    except ImportError:
-        from azure.search.documents.indexes.models import (
-            HnswVectorSearchAlgorithmConfiguration,
-            PrioritizedFields,
-            SemanticSettings,
-        )
-
-        NEW_VERSION = False
-
     default_fields = default_fields or []
     if key is None:
         credential = DefaultAzureCredential()
     elif key.upper() == "INTERACTIVE":
         credential = InteractiveBrowserCredential()
         credential.get_token("https://search.azure.com/.default")
     else:
@@ -151,85 +143,73 @@
                     f"{missing_fields} or provide alternative field names in the env "
                     f"variables.\n\n{error}"
                 )
         else:
             fields = default_fields
         # Vector search configuration
         if vector_search is None:
-            if NEW_VERSION:
-                # >= 11.4.0:
-                #   VectorSearch(algorithm_configuration) --> VectorSearch(algorithms)
-                # HnswVectorSearchAlgorithmConfiguration --> HnswAlgorithmConfiguration
-                vector_search = VectorSearch(
-                    algorithms=[
-                        HnswAlgorithmConfiguration(
-                            name="default",
-                            kind="hnsw",
-                            parameters={  # type: ignore
-                                "m": 4,
-                                "efConstruction": 400,
-                                "efSearch": 500,
-                                "metric": "cosine",
-                            },
-                        )
-                    ]
-                )
-            else:  # < 11.4.0
-                vector_search = VectorSearch(
-                    algorithm_configurations=[
-                        HnswVectorSearchAlgorithmConfiguration(
-                            name="default",
-                            kind="hnsw",
-                            parameters={  # type: ignore
-                                "m": 4,
-                                "efConstruction": 400,
-                                "efSearch": 500,
-                                "metric": "cosine",
-                            },
-                        )
-                    ]
-                )
+            vector_search = VectorSearch(
+                algorithms=[
+                    HnswAlgorithmConfiguration(
+                        name="default",
+                        kind=VectorSearchAlgorithmKind.HNSW,
+                        parameters=HnswParameters(
+                            m=4,
+                            ef_construction=400,
+                            ef_search=500,
+                            metric=VectorSearchAlgorithmMetric.COSINE,
+                        ),
+                    ),
+                    ExhaustiveKnnAlgorithmConfiguration(
+                        name="default_exhaustive_knn",
+                        kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,
+                        parameters=ExhaustiveKnnParameters(
+                            metric=VectorSearchAlgorithmMetric.COSINE
+                        ),
+                    ),
+                ],
+                profiles=[
+                    VectorSearchProfile(
+                        name="myHnswProfile",
+                        algorithm_configuration_name="default",
+                    ),
+                    VectorSearchProfile(
+                        name="myExhaustiveKnnProfile",
+                        algorithm_configuration_name="default_exhaustive_knn",
+                    ),
+                ],
+            )
 
         # Create the semantic settings with the configuration
-        if semantic_settings is None and semantic_configuration_name is not None:
-            if NEW_VERSION:
-                # <=11.4.0: SemanticSettings --> SemanticSearch
-                # PrioritizedFields(prioritized_content_fields)
-                #   --> SemanticPrioritizedFields(content_fields)
-                semantic_settings = SemanticSearch(
-                    configurations=[
-                        SemanticConfiguration(
-                            name=semantic_configuration_name,
-                            prioritized_fields=SemanticPrioritizedFields(
-                                content_fields=[
-                                    SemanticField(field_name=FIELDS_CONTENT)
-                                ],
-                            ),
-                        )
-                    ]
-                )
-            else:  # < 11.4.0
-                semantic_settings = SemanticSettings(
-                    configurations=[
-                        SemanticConfiguration(
-                            name=semantic_configuration_name,
-                            prioritized_fields=PrioritizedFields(
-                                prioritized_content_fields=[
-                                    SemanticField(field_name=FIELDS_CONTENT)
-                                ],
-                            ),
-                        )
-                    ]
-                )
+        if semantic_configurations:
+            if not isinstance(semantic_configurations, list):
+                semantic_configurations = [semantic_configurations]
+            semantic_search = SemanticSearch(
+                configurations=semantic_configurations,
+                default_configuration_name=semantic_configuration_name,
+            )
+        elif semantic_configuration_name:
+            # use default semantic configuration
+            semantic_configuration = SemanticConfiguration(
+                name=semantic_configuration_name,
+                prioritized_fields=SemanticPrioritizedFields(
+                    content_fields=[SemanticField(field_name=FIELDS_CONTENT)],
+                ),
+            )
+            semantic_search = SemanticSearch(configurations=[semantic_configuration])
+        else:
+            # don't use semantic search
+            semantic_search = None
+
         # Create the search index with the semantic settings and vector search
         index = SearchIndex(
             name=index_name,
             fields=fields,
             vector_search=vector_search,
-            semantic_settings=semantic_settings,
+            semantic_search=semantic_search,
             scoring_profiles=scoring_profiles,
             default_scoring_profile=default_scoring_profile,
             cors_options=cors_options,
         )
         index_client.create_index(index)
     # Create the search client
     return SearchClient(
@@ -244,21 +224,22 @@
     """`Azure Cognitive Search` vector store."""
 
     def __init__(
         self,
         azure_search_endpoint: str,
         azure_search_key: str,
         index_name: str,
-        embedding_function: Callable,
+        embedding_function: Union[Callable, Embeddings],
         search_type: str = "hybrid",
         semantic_configuration_name: Optional[str] = None,
-        semantic_query_language: str = "en-us",
         fields: Optional[List[SearchField]] = None,
         vector_search: Optional[VectorSearch] = None,
-        semantic_settings: Optional[Union[SemanticSearch, SemanticSettings]] = None,
+        semantic_configurations: Optional[
+            Union[SemanticConfiguration, List[SemanticConfiguration]]
+        ] = None,
         scoring_profiles: Optional[List[ScoringProfile]] = None,
         default_scoring_profile: Optional[str] = None,
         cors_options: Optional[CorsOptions] = None,
         **kwargs: Any,
     ):
         from azure.search.documents.indexes.models import (
             SearchableField,
@@ -266,14 +247,20 @@
             SearchFieldDataType,
             SimpleField,
         )
 
         """Initialize with necessary components."""
         # Initialize base class
         self.embedding_function = embedding_function
+
+        if isinstance(self.embedding_function, Embeddings):
+            self.embed_query = self.embedding_function.embed_query
+        else:
+            self.embed_query = self.embedding_function
+
         default_fields = [
             SimpleField(
                 name=FIELDS_ID,
                 type=SearchFieldDataType.String,
                 key=True,
                 filterable=True,
             ),
@@ -281,16 +268,16 @@
                 name=FIELDS_CONTENT,
                 type=SearchFieldDataType.String,
             ),
             SearchField(
                 name=FIELDS_CONTENT_VECTOR,
                 type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                 searchable=True,
-                vector_search_dimensions=len(embedding_function("Text")),
-                vector_search_configuration="default",
+                vector_search_dimensions=len(self.embed_query("Text")),
+                vector_search_profile_name="myHnswProfile",
             ),
             SearchableField(
                 name=FIELDS_METADATA,
                 type=SearchFieldDataType.String,
             ),
         ]
         user_agent = "langchain"
@@ -299,24 +286,23 @@
         self.client = _get_search_client(
             azure_search_endpoint,
             azure_search_key,
             index_name,
             semantic_configuration_name=semantic_configuration_name,
             fields=fields,
             vector_search=vector_search,
-            semantic_settings=semantic_settings,
+            semantic_configurations=semantic_configurations,
             scoring_profiles=scoring_profiles,
             default_scoring_profile=default_scoring_profile,
             default_fields=default_fields,
             user_agent=user_agent,
             cors_options=cors_options,
         )
         self.search_type = search_type
         self.semantic_configuration_name = semantic_configuration_name
-        self.semantic_query_language = semantic_query_language
         self.fields = fields if fields else default_fields
 
     @property
     def embeddings(self) -> Optional[Embeddings]:
         # TODO: Support embedding object directly
         return None
 
@@ -325,14 +311,28 @@
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """Add texts data to an existing index."""
         keys = kwargs.get("keys")
         ids = []
+
+        # batching support if embedding function is an Embeddings object
+        if isinstance(self.embedding_function, Embeddings):
+            try:
+                embeddings = self.embedding_function.embed_documents(texts)  # type: ignore[arg-type]
+            except NotImplementedError:
+                embeddings = [self.embedding_function.embed_query(x) for x in texts]
+        else:
+            embeddings = [self.embedding_function(x) for x in texts]
+
+        if len(embeddings) == 0:
+            logger.debug("Nothing to insert, skipping.")
+            return []
+
         # Write data to index
         data = []
         for i, text in enumerate(texts):
             # Use provided key otherwise use default key
             key = keys[i] if keys else str(uuid.uuid4())
             # Encoding key for Azure Search valid characters
             key = base64.urlsafe_b64encode(bytes(key, "utf-8")).decode("ascii")
@@ -340,15 +340,15 @@
             # Add data to index
             # Additional metadata to fields mapping
             doc = {
                 "@search.action": "upload",
                 FIELDS_ID: key,
                 FIELDS_CONTENT: text,
                 FIELDS_CONTENT_VECTOR: np.array(
-                    self.embedding_function(text), dtype=np.float32
+                    embeddings[i], dtype=np.float32
                 ).tolist(),
                 FIELDS_METADATA: json.dumps(metadata),
             }
             if metadata:
                 additional_fields = {
                     k: v
                     for k, v in metadata.items()
@@ -374,14 +374,30 @@
         response = self.client.upload_documents(documents=data)
         # Check if all documents were successfully uploaded
         if all([r.succeeded for r in response]):
             return ids
         else:
             raise Exception(response)
 
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> bool:
+        """Delete by vector ID.
+
+        Args:
+            ids: List of ids to delete.
+
+        Returns:
+            bool: True if deletion is successful,
+            False otherwise.
+        """
+        if ids:
+            res = self.client.delete_documents([{"id": i} for i in ids])
+            return len(res) > 0
+        else:
+            return False
+
     def similarity_search(
         self, query: str, k: int = 4, **kwargs: Any
     ) -> List[Document]:
         search_type = kwargs.get("search_type", self.search_type)
         if search_type == "similarity":
             docs = self.vector_search(query, k=k, **kwargs)
         elif search_type == "hybrid":
@@ -427,49 +443,38 @@
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
-        from azure.search.documents.models import Vector
+
+        from azure.search.documents.models import VectorizedQuery
 
         results = self.client.search(
             search_text="",
-            vectors=[
-                Vector(
-                    value=np.array(
-                        self.embedding_function(query), dtype=np.float32
-                    ).tolist(),
-                    k=k,
+            vector_queries=[
+                VectorizedQuery(
+                    vector=np.array(self.embed_query(query), dtype=np.float32).tolist(),
+                    k_nearest_neighbors=k,
                     fields=FIELDS_CONTENT_VECTOR,
                 )
             ],
             filter=filters,
+            top=k,
         )
         # Convert results to Document objects
         docs = [
             (
                 Document(
                     page_content=result.pop(FIELDS_CONTENT),
-                    metadata={
-                        **(
-                            {FIELDS_ID: result.pop(FIELDS_ID)}
-                            if FIELDS_ID in result
-                            else {}
-                        ),
-                        **(
-                            json.loads(result[FIELDS_METADATA])
-                            if FIELDS_METADATA in result
-                            else {
-                                k: v
-                                for k, v in result.items()
-                                if k != FIELDS_CONTENT_VECTOR
-                            }
-                        ),
+                    metadata=json.loads(result[FIELDS_METADATA])
+                    if FIELDS_METADATA in result
+                    else {
+                        k: v for k, v in result.items() if k != FIELDS_CONTENT_VECTOR
                     },
                 ),
                 float(result["@search.score"]),
             )
             for result in results
         ]
         return docs
@@ -489,67 +494,65 @@
             query, k=k, filters=kwargs.get("filters", None)
         )
         return [doc for doc, _ in docs_and_scores]
 
     def hybrid_search_with_score(
         self, query: str, k: int = 4, filters: Optional[str] = None
     ) -> List[Tuple[Document, float]]:
-        """Return docs most similar to query with an hybrid query.
+        """Return docs most similar to query with a hybrid query.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
-        from azure.search.documents.models import Vector
+        from azure.search.documents.models import VectorizedQuery
 
         results = self.client.search(
             search_text=query,
-            vectors=[
-                Vector(
-                    value=np.array(
-                        self.embedding_function(query), dtype=np.float32
-                    ).tolist(),
-                    k=k,
+            vector_queries=[
+                VectorizedQuery(
+                    vector=np.array(self.embed_query(query), dtype=np.float32).tolist(),
+                    k_nearest_neighbors=k,
                     fields=FIELDS_CONTENT_VECTOR,
                 )
             ],
             filter=filters,
             top=k,
         )
         # Convert results to Document objects
         docs = [
             (
                 Document(
                     page_content=result.pop(FIELDS_CONTENT),
-                    metadata={
-                        **(
-                            {FIELDS_ID: result.pop(FIELDS_ID)}
-                            if FIELDS_ID in result
-                            else {}
-                        ),
-                        **(
-                            json.loads(result[FIELDS_METADATA])
-                            if FIELDS_METADATA in result
-                            else {
-                                k: v
-                                for k, v in result.items()
-                                if k != FIELDS_CONTENT_VECTOR
-                            }
-                        ),
+                    metadata=json.loads(result[FIELDS_METADATA])
+                    if FIELDS_METADATA in result
+                    else {
+                        k: v for k, v in result.items() if k != FIELDS_CONTENT_VECTOR
                     },
                 ),
                 float(result["@search.score"]),
             )
             for result in results
         ]
         return docs
 
+    def hybrid_search_with_relevance_scores(
+        self, query: str, k: int = 4, **kwargs: Any
+    ) -> List[Tuple[Document, float]]:
+        score_threshold = kwargs.pop("score_threshold", None)
+        result = self.hybrid_search_with_score(query, k=k, **kwargs)
+        return (
+            result
+            if score_threshold is None
+            else [r for r in result if r[1] >= score_threshold]
+        )
+
     def semantic_hybrid_search(
         self, query: str, k: int = 4, **kwargs: Any
     ) -> List[Document]:
         """
         Returns the most similar indexed documents to the query text.
 
         Args:
@@ -561,59 +564,75 @@
         """
         docs_and_scores = self.semantic_hybrid_search_with_score_and_rerank(
             query, k=k, filters=kwargs.get("filters", None)
         )
         return [doc for doc, _, _ in docs_and_scores]
 
     def semantic_hybrid_search_with_score(
-        self, query: str, k: int = 4, **kwargs: Any
+        self,
+        query: str,
+        k: int = 4,
+        score_type: Literal["score", "reranker_score"] = "score",
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """
         Returns the most similar indexed documents to the query text.
 
         Args:
             query (str): The query text for which to find similar documents.
             k (int): The number of documents to return. Default is 4.
+            score_type: Must either be "score" or "reranker_score".
+                Defaulted to "score".
 
         Returns:
-            List[Document]: A list of documents that are most similar to the query text.
+            List[Tuple[Document, float]]: A list of documents and their
+                corresponding scores.
         """
+        score_threshold = kwargs.pop("score_threshold", None)
         docs_and_scores = self.semantic_hybrid_search_with_score_and_rerank(
             query, k=k, filters=kwargs.get("filters", None)
         )
-        return [(doc, score) for doc, score, _ in docs_and_scores]
+        if score_type == "score":
+            return [
+                (doc, score)
+                for doc, score, _ in docs_and_scores
+                if score_threshold is None or score >= score_threshold
+            ]
+        elif score_type == "reranker_score":
+            return [
+                (doc, reranker_score)
+                for doc, _, reranker_score in docs_and_scores
+                if score_threshold is None or reranker_score >= score_threshold
+            ]
 
     def semantic_hybrid_search_with_score_and_rerank(
         self, query: str, k: int = 4, filters: Optional[str] = None
     ) -> List[Tuple[Document, float, float]]:
-        """Return docs most similar to query with an hybrid query.
+        """Return docs most similar to query with a hybrid query.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
-        from azure.search.documents.models import Vector
+        from azure.search.documents.models import VectorizedQuery
 
         results = self.client.search(
             search_text=query,
-            vectors=[
-                Vector(
-                    value=np.array(
-                        self.embedding_function(query), dtype=np.float32
-                    ).tolist(),
-                    k=50,
+            vector_queries=[
+                VectorizedQuery(
+                    vector=np.array(self.embed_query(query), dtype=np.float32).tolist(),
+                    k_nearest_neighbors=k,
                     fields=FIELDS_CONTENT_VECTOR,
                 )
             ],
             filter=filters,
             query_type="semantic",
-            query_language=self.semantic_query_language,
             semantic_configuration_name=self.semantic_configuration_name,
             query_caption="extractive",
             query_answer="extractive",
             top=k,
         )
         # Get Semantic Answers
         semantic_answers = results.get_answers() or []
@@ -626,19 +645,14 @@
         # Convert results to Document objects
         docs = [
             (
                 Document(
                     page_content=result.pop(FIELDS_CONTENT),
                     metadata={
                         **(
-                            {FIELDS_ID: result.pop(FIELDS_ID)}
-                            if FIELDS_ID in result
-                            else {}
-                        ),
-                        **(
                             json.loads(result[FIELDS_METADATA])
                             if FIELDS_METADATA in result
                             else {
                                 k: v
                                 for k, v in result.items()
                                 if k != FIELDS_CONTENT_VECTOR
                             }
@@ -649,15 +663,16 @@
                                 "highlights": result.get("@search.captions", [{}])[
                                     0
                                 ].highlights,
                             }
                             if result.get("@search.captions")
                             else {},
                             "answers": semantic_answers_dict.get(
-                                json.loads(result["metadata"]).get("key"), ""
+                                result.get(FIELDS_ID, ""),
+                                "",
                             ),
                         },
                     },
                 ),
                 float(result["@search.score"]),
                 float(result["@search.reranker_score"]),
             )
@@ -670,64 +685,124 @@
         cls: Type[AzureSearch],
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         azure_search_endpoint: str = "",
         azure_search_key: str = "",
         index_name: str = "langchain-index",
+        fields: Optional[List[SearchField]] = None,
         **kwargs: Any,
     ) -> AzureSearch:
         # Creating a new Azure Search instance
         azure_search = cls(
             azure_search_endpoint,
             azure_search_key,
             index_name,
-            embedding.embed_query,
+            embedding,
+            fields=fields,
         )
         azure_search.add_texts(texts, metadatas, **kwargs)
         return azure_search
 
+    def as_retriever(self, **kwargs: Any) -> AzureSearchVectorStoreRetriever:  # type: ignore
+        """Return AzureSearchVectorStoreRetriever initialized from this VectorStore.
+
+        Args:
+            search_type (Optional[str]): Defines the type of search that
+                the Retriever should perform.
+                Can be "similarity" (default), "hybrid", or
+                    "semantic_hybrid".
+            search_kwargs (Optional[Dict]): Keyword arguments to pass to the
+                search function. Can include things like:
+                    k: Amount of documents to return (Default: 4)
+                    score_threshold: Minimum relevance threshold
+                        for similarity_score_threshold
+                    fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)
+                    lambda_mult: Diversity of results returned by MMR;
+                        1 for minimum diversity and 0 for maximum. (Default: 0.5)
+                    filter: Filter by document metadata
+
+        Returns:
+            AzureSearchVectorStoreRetriever: Retriever class for VectorStore.
+        """
+        tags = kwargs.pop("tags", None) or []
+        tags.extend(self._get_retriever_tags())
+        return AzureSearchVectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)
+
 
 class AzureSearchVectorStoreRetriever(BaseRetriever):
     """Retriever that uses `Azure Cognitive Search`."""
 
     vectorstore: AzureSearch
     """Azure Search instance used to find similar documents."""
     search_type: str = "hybrid"
     """Type of search to perform. Options are "similarity", "hybrid",
-    "semantic_hybrid"."""
+    "semantic_hybrid", "similarity_score_threshold", "hybrid_score_threshold", 
+    or "semantic_hybrid_score_threshold"."""
     k: int = 4
     """Number of documents to return."""
+    allowed_search_types: ClassVar[Collection[str]] = (
+        "similarity",
+        "similarity_score_threshold",
+        "hybrid",
+        "hybrid_score_threshold",
+        "semantic_hybrid",
+        "semantic_hybrid_score_threshold",
+    )
 
     class Config:
         """Configuration for this pydantic object."""
 
         arbitrary_types_allowed = True
 
     @root_validator()
     def validate_search_type(cls, values: Dict) -> Dict:
         """Validate search type."""
         if "search_type" in values:
             search_type = values["search_type"]
-            if search_type not in ("similarity", "hybrid", "semantic_hybrid"):
-                raise ValueError(f"search_type of {search_type} not allowed.")
+            if search_type not in cls.allowed_search_types:
+                raise ValueError(
+                    f"search_type of {search_type} not allowed. Valid values are: "
+                    f"{cls.allowed_search_types}"
+                )
         return values
 
     def _get_relevant_documents(
         self,
         query: str,
         run_manager: CallbackManagerForRetrieverRun,
         **kwargs: Any,
     ) -> List[Document]:
         if self.search_type == "similarity":
             docs = self.vectorstore.vector_search(query, k=self.k, **kwargs)
+        elif self.search_type == "similarity_score_threshold":
+            docs = [
+                doc
+                for doc, _ in self.vectorstore.similarity_search_with_relevance_scores(
+                    query, k=self.k, **kwargs
+                )
+            ]
         elif self.search_type == "hybrid":
             docs = self.vectorstore.hybrid_search(query, k=self.k, **kwargs)
+        elif self.search_type == "hybrid_score_threshold":
+            docs = [
+                doc
+                for doc, _ in self.vectorstore.hybrid_search_with_relevance_scores(
+                    query, k=self.k, **kwargs
+                )
+            ]
         elif self.search_type == "semantic_hybrid":
             docs = self.vectorstore.semantic_hybrid_search(query, k=self.k, **kwargs)
+        elif self.search_type == "semantic_hybrid_score_threshold":
+            docs = [
+                doc
+                for doc, _ in self.vectorstore.semantic_hybrid_search_with_score(
+                    query, k=self.k, **kwargs
+                )
+            ]
         else:
             raise ValueError(f"search_type of {self.search_type} not allowed.")
         return docs
 
     async def _aget_relevant_documents(
         self,
         query: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/bageldb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/bagel.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,17 +38,17 @@
             results["metadatas"][0],
             results["distances"][0],
         )
     ]
 
 
 class Bagel(VectorStore):
-    """``BagelDB.ai`` vector store.
+    """``Bagel.net`` Inference platform.
 
-    To use, you should have the ``betabageldb`` python package installed.
+    To use, you should have the ``bagelML`` python package installed.
 
     Example:
         .. code-block:: python
 
                 from langchain_community.vectorstores import Bagel
                 vectorstore = Bagel(cluster_name="langchain_store")
     """
@@ -65,15 +65,15 @@
         relevance_score_fn: Optional[Callable[[float], float]] = None,
     ) -> None:
         """Initialize with bagel client"""
         try:
             import bagel
             import bagel.config
         except ImportError:
-            raise ImportError("Please install bagel `pip install betabageldb`.")
+            raise ImportError("Please install bagel `pip install bagelML`.")
         if client is not None:
             self._client_settings = client_settings
             self._client = client
         else:
             if client_settings:
                 _client_settings = client_settings
             else:
@@ -100,19 +100,25 @@
         self,
         query_texts: Optional[List[str]] = None,
         query_embeddings: Optional[List[List[float]]] = None,
         n_results: int = 4,
         where: Optional[Dict[str, str]] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        """Query the BagelDB cluster based on the provided parameters."""
+        """Query the Bagel cluster based on the provided parameters."""
         try:
             import bagel  # noqa: F401
         except ImportError:
-            raise ImportError("Please install bagel `pip install betabageldb`.")
+            raise ImportError("Please install bagel `pip install bagelML`.")
+
+        if self._embedding_function and query_embeddings is None and query_texts:
+            texts = list(query_texts)
+            query_embeddings = self._embedding_function.embed_documents(texts)
+            query_texts = None
+
         return self._cluster.find(
             query_texts=query_texts,
             query_embeddings=query_embeddings,
             n_results=n_results,
             where=where,
             **kwargs,
         )
@@ -123,28 +129,28 @@
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         embeddings: Optional[List[List[float]]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """
         Add texts along with their corresponding embeddings and optional
-        metadata to the BagelDB cluster.
+        metadata to the Bagel cluster.
 
         Args:
             texts (Iterable[str]): Texts to be added.
             embeddings (Optional[List[float]]): List of embeddingvectors
             metadatas (Optional[List[dict]]): Optional list of metadatas.
             ids (Optional[List[str]]): List of unique ID for the texts.
 
         Returns:
             List[str]: List of unique ID representing the added texts.
         """
         # creating unique ids if None
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         texts = list(texts)
         if self._embedding_function and embeddings is None and texts:
             embeddings = self._embedding_function.embed_documents(texts)
         if metadatas:
             length_diff = len(texts) - len(metadatas)
             if length_diff:
@@ -194,15 +200,15 @@
         self,
         query: str,
         k: int = DEFAULT_K,
         where: Optional[Dict[str, str]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """
-        Run a similarity search with BagelDB.
+        Run a similarity search with Bagel.
 
         Args:
             query (str): The query text to search for similar documents/texts.
             k (int): The number of results to return.
             where (Optional[Dict[str, str]]): Metadata filters to narrow down.
 
         Returns:
@@ -216,15 +222,15 @@
         self,
         query: str,
         k: int = DEFAULT_K,
         where: Optional[Dict[str, str]] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """
-        Run a similarity search with BagelDB and return documents with their
+        Run a similarity search with Bagel and return documents with their
         corresponding similarity scores.
 
         Args:
             query (str): The query text to search for similar documents.
             k (int): The number of results to return.
             where (Optional[Dict[str, str]]): Filter using metadata.
 
@@ -252,15 +258,15 @@
         **kwargs: Any,
     ) -> Bagel:
         """
         Create and initialize a Bagel instance from list of texts.
 
         Args:
             texts (List[str]): List of text content to be added.
-            cluster_name (str): The name of the BagelDB cluster.
+            cluster_name (str): The name of the Bagel cluster.
             client_settings (Optional[bagel.config.Settings]): Client settings.
             cluster_metadata (Optional[Dict]): Metadata of the cluster.
             embeddings (Optional[Embeddings]): List of embedding.
             metadatas (Optional[List[dict]]): List of metadata.
             ids (Optional[List[str]]): List of unique ID. Defaults to None.
             client (Optional[bagel.Client]): Bagel client instance.
 
@@ -311,15 +317,15 @@
             query_embeddings=embedding, n_results=k, where=where
         )
         return _results_to_docs(results)
 
     def _select_relevance_score_fn(self) -> Callable[[float], float]:
         """
         Select and return the appropriate relevance score function based
-        on the distance metric used in the BagelDB cluster.
+        on the distance metric used in the Bagel cluster.
         """
         if self.override_relevance_score_fn:
             return self.override_relevance_score_fn
 
         distance = "l2"
         distance_key = "hnsw:space"
         metadata = self._cluster.metadata
@@ -356,15 +362,15 @@
         Create a Bagel vectorstore from a list of documents.
 
         Args:
             documents (List[Document]): List of Document objects to add to the
                                         Bagel vectorstore.
             embedding (Optional[List[float]]): List of embedding.
             ids (Optional[List[str]]): List of IDs. Defaults to None.
-            cluster_name (str): The name of the BagelDB cluster.
+            cluster_name (str): The name of the Bagel cluster.
             client_settings (Optional[bagel.config.Settings]): Client settings.
             client (Optional[bagel.Client]): Bagel client instance.
             cluster_metadata (Optional[Dict]): Metadata associated with the
                                                Bagel cluster. Defaults to None.
 
         Returns:
             Bagel: Bagel vectorstore.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/baiducloud_vector_search.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/baiducloud_vector_search.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/cassandra.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/mongodb_atlas.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,457 +1,376 @@
 from __future__ import annotations
 
-import typing
-import uuid
+import logging
 from typing import (
+    TYPE_CHECKING,
     Any,
     Callable,
     Dict,
+    Generator,
     Iterable,
     List,
     Optional,
     Tuple,
-    Type,
     TypeVar,
     Union,
 )
 
 import numpy as np
-
-if typing.TYPE_CHECKING:
-    from cassandra.cluster import Session
-
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
-CVST = TypeVar("CVST", bound="Cassandra")
+if TYPE_CHECKING:
+    from pymongo.collection import Collection
 
+MongoDBDocumentType = TypeVar("MongoDBDocumentType", bound=Dict[str, Any])
 
-class Cassandra(VectorStore):
-    """Wrapper around Apache Cassandra(R) for vector-store workloads.
+logger = logging.getLogger(__name__)
 
-    To use it, you need a recent installation of the `cassio` library
-    and a Cassandra cluster / Astra DB instance supporting vector capabilities.
+DEFAULT_INSERT_BATCH_SIZE = 100
 
-    Visit the cassio.org website for extensive quickstarts and code examples.
+
+@deprecated(
+    since="0.0.25",
+    removal="0.3.0",
+    alternative_import="langchain_mongodb.MongoDBAtlasVectorSearch",
+)
+class MongoDBAtlasVectorSearch(VectorStore):
+    """`MongoDB Atlas Vector Search` vector store.
+
+    To use, you should have both:
+    - the ``pymongo`` python package installed
+    - a connection string associated with a MongoDB Atlas Cluster having deployed an
+        Atlas Search index
 
     Example:
         .. code-block:: python
 
-                from langchain_community.vectorstores import Cassandra
-                from langchain_community.embeddings.openai import OpenAIEmbeddings
-
-                embeddings = OpenAIEmbeddings()
-                session = ...             # create your Cassandra session object
-                keyspace = 'my_keyspace'  # the keyspace should exist already
-                table_name = 'my_vector_store'
-                vectorstore = Cassandra(embeddings, session, keyspace, table_name)
+            from langchain_community.vectorstores import MongoDBAtlasVectorSearch
+            from langchain_community.embeddings.openai import OpenAIEmbeddings
+            from pymongo import MongoClient
+
+            mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
+            collection = mongo_client["<db_name>"]["<collection_name>"]
+            embeddings = OpenAIEmbeddings()
+            vectorstore = MongoDBAtlasVectorSearch(collection, embeddings)
     """
 
-    _embedding_dimension: Union[int, None]
-
-    @staticmethod
-    def _filter_to_metadata(filter_dict: Optional[Dict[str, str]]) -> Dict[str, Any]:
-        if filter_dict is None:
-            return {}
-        else:
-            return filter_dict
-
-    def _get_embedding_dimension(self) -> int:
-        if self._embedding_dimension is None:
-            self._embedding_dimension = len(
-                self.embedding.embed_query("This is a sample sentence.")
-            )
-        return self._embedding_dimension
-
     def __init__(
         self,
+        collection: Collection[MongoDBDocumentType],
         embedding: Embeddings,
-        session: Session,
-        keyspace: str,
-        table_name: str,
-        ttl_seconds: Optional[int] = None,
-    ) -> None:
-        try:
-            from cassio.vector import VectorTable
-        except (ImportError, ModuleNotFoundError):
-            raise ImportError(
-                "Could not import cassio python package. "
-                "Please install it with `pip install cassio`."
-            )
-        """Create a vector table."""
-        self.embedding = embedding
-        self.session = session
-        self.keyspace = keyspace
-        self.table_name = table_name
-        self.ttl_seconds = ttl_seconds
-        #
-        self._embedding_dimension = None
-        #
-        self.table = VectorTable(
-            session=session,
-            keyspace=keyspace,
-            table=table_name,
-            embedding_dimension=self._get_embedding_dimension(),
-            primary_key_type="TEXT",
-        )
+        *,
+        index_name: str = "default",
+        text_key: str = "text",
+        embedding_key: str = "embedding",
+        relevance_score_fn: str = "cosine",
+    ):
+        """
+        Args:
+            collection: MongoDB collection to add the texts to.
+            embedding: Text embedding model to use.
+            text_key: MongoDB field that will contain the text for each
+                document.
+            embedding_key: MongoDB field that will contain the embedding for
+                each document.
+            index_name: Name of the Atlas Search index.
+            relevance_score_fn: The similarity score used for the index.
+            Currently supported: Euclidean, cosine, and dot product.
+        """
+        self._collection = collection
+        self._embedding = embedding
+        self._index_name = index_name
+        self._text_key = text_key
+        self._embedding_key = embedding_key
+        self._relevance_score_fn = relevance_score_fn
 
     @property
     def embeddings(self) -> Embeddings:
-        return self.embedding
-
-    @staticmethod
-    def _dont_flip_the_cos_score(distance: float) -> float:
-        # the identity
-        return distance
+        return self._embedding
 
     def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        """
-        The underlying VectorTable already returns a "score proper",
-        i.e. one in [0, 1] where higher means more *similar*,
-        so here the final score transformation is not reversing the interval:
-        """
-        return self._dont_flip_the_cos_score
-
-    def delete_collection(self) -> None:
-        """
-        Just an alias for `clear`
-        (to better align with other VectorStore implementations).
-        """
-        self.clear()
-
-    def clear(self) -> None:
-        """Empty the collection."""
-        self.table.clear()
-
-    def delete_by_document_id(self, document_id: str) -> None:
-        return self.table.delete(document_id)
-
-    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
-        """Delete by vector IDs.
+        if self._relevance_score_fn == "euclidean":
+            return self._euclidean_relevance_score_fn
+        elif self._relevance_score_fn == "dotProduct":
+            return self._max_inner_product_relevance_score_fn
+        elif self._relevance_score_fn == "cosine":
+            return self._cosine_relevance_score_fn
+        else:
+            raise NotImplementedError(
+                f"No relevance score function for ${self._relevance_score_fn}"
+            )
 
+    @classmethod
+    def from_connection_string(
+        cls,
+        connection_string: str,
+        namespace: str,
+        embedding: Embeddings,
+        **kwargs: Any,
+    ) -> MongoDBAtlasVectorSearch:
+        """Construct a `MongoDB Atlas Vector Search` vector store
+        from a MongoDB connection URI.
 
         Args:
-            ids: List of ids to delete.
+            connection_string: A valid MongoDB connection URI.
+            namespace: A valid MongoDB namespace (database and collection).
+            embedding: The text embedding model to use for the vector store.
 
         Returns:
-            Optional[bool]: True if deletion is successful,
-            False otherwise, None if not implemented.
-        """
+            A new MongoDBAtlasVectorSearch instance.
 
-        if ids is None:
-            raise ValueError("No ids provided to delete.")
+        """
+        try:
+            from importlib.metadata import version
 
-        for document_id in ids:
-            self.delete_by_document_id(document_id)
-        return True
+            from pymongo import MongoClient
+            from pymongo.driver_info import DriverInfo
+        except ImportError:
+            raise ImportError(
+                "Could not import pymongo, please install it with "
+                "`pip install pymongo`."
+            )
+        client: MongoClient = MongoClient(
+            connection_string,
+            driver=DriverInfo(name="Langchain", version=version("langchain")),
+        )
+        db_name, collection_name = namespace.split(".")
+        collection = client[db_name][collection_name]
+        return cls(collection, embedding, **kwargs)
 
     def add_texts(
         self,
         texts: Iterable[str],
-        metadatas: Optional[List[dict]] = None,
-        ids: Optional[List[str]] = None,
-        batch_size: int = 16,
-        ttl_seconds: Optional[int] = None,
+        metadatas: Optional[List[Dict[str, Any]]] = None,
         **kwargs: Any,
-    ) -> List[str]:
+    ) -> List:
         """Run more texts through the embeddings and add to the vectorstore.
 
         Args:
-            texts (Iterable[str]): Texts to add to the vectorstore.
-            metadatas (Optional[List[dict]], optional): Optional list of metadatas.
-            ids (Optional[List[str]], optional): Optional list of IDs.
-            batch_size (int): Number of concurrent requests to send to the server.
-            ttl_seconds (Optional[int], optional): Optional time-to-live
-                for the added texts.
+            texts: Iterable of strings to add to the vectorstore.
+            metadatas: Optional list of metadatas associated with the texts.
 
         Returns:
-            List[str]: List of IDs of the added texts.
+            List of ids from adding the texts into the vectorstore.
         """
-        _texts = list(texts)  # lest it be a generator or something
-        if ids is None:
-            ids = [uuid.uuid4().hex for _ in _texts]
-        if metadatas is None:
-            metadatas = [{} for _ in _texts]
-        #
-        ttl_seconds = ttl_seconds or self.ttl_seconds
-        #
-        embedding_vectors = self.embedding.embed_documents(_texts)
-        #
-        for i in range(0, len(_texts), batch_size):
-            batch_texts = _texts[i : i + batch_size]
-            batch_embedding_vectors = embedding_vectors[i : i + batch_size]
-            batch_ids = ids[i : i + batch_size]
-            batch_metadatas = metadatas[i : i + batch_size]
-
-            futures = [
-                self.table.put_async(
-                    text, embedding_vector, text_id, metadata, ttl_seconds
-                )
-                for text, embedding_vector, text_id, metadata in zip(
-                    batch_texts, batch_embedding_vectors, batch_ids, batch_metadatas
-                )
-            ]
-            for future in futures:
-                future.result()
-        return ids
+        batch_size = kwargs.get("batch_size", DEFAULT_INSERT_BATCH_SIZE)
+        _metadatas: Union[List, Generator] = metadatas or ({} for _ in texts)
+        texts_batch = []
+        metadatas_batch = []
+        result_ids = []
+        for i, (text, metadata) in enumerate(zip(texts, _metadatas)):
+            texts_batch.append(text)
+            metadatas_batch.append(metadata)
+            if (i + 1) % batch_size == 0:
+                result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
+                texts_batch = []
+                metadatas_batch = []
+        if texts_batch:
+            result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
+        return result_ids
+
+    def _insert_texts(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> List:
+        if not texts:
+            return []
+        # Embed and create the documents
+        embeddings = self._embedding.embed_documents(texts)
+        to_insert = [
+            {self._text_key: t, self._embedding_key: embedding, **m}
+            for t, m, embedding in zip(texts, metadatas, embeddings)
+        ]
+        # insert the documents in MongoDB Atlas
+        insert_result = self._collection.insert_many(to_insert)  # type: ignore
+        return insert_result.inserted_ids
 
-    # id-returning search facilities
-    def similarity_search_with_score_id_by_vector(
+    def _similarity_search_with_score(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Tuple[Document, float, str]]:
-        """Return docs most similar to embedding vector.
-
-        Args:
-            embedding (str): Embedding to look up documents similar to.
-            k (int): Number of Documents to return. Defaults to 4.
-        Returns:
-            List of (Document, score, id), the most similar to the query vector.
-        """
-        search_metadata = self._filter_to_metadata(filter)
-        #
-        hits = self.table.search(
-            embedding_vector=embedding,
-            top_k=k,
-            metric="cos",
-            metric_threshold=None,
-            metadata=search_metadata,
-        )
-        # We stick to 'cos' distance as it can be normalized on a 0-1 axis
-        # (1=most relevant), as required by this class' contract.
-        return [
-            (
-                Document(
-                    page_content=hit["document"],
-                    metadata=hit["metadata"],
-                ),
-                0.5 + 0.5 * hit["distance"],
-                hit["document_id"],
-            )
-            for hit in hits
+        pre_filter: Optional[Dict] = None,
+        post_filter_pipeline: Optional[List[Dict]] = None,
+    ) -> List[Tuple[Document, float]]:
+        params = {
+            "queryVector": embedding,
+            "path": self._embedding_key,
+            "numCandidates": k * 10,
+            "limit": k,
+            "index": self._index_name,
+        }
+        if pre_filter:
+            params["filter"] = pre_filter
+        query = {"$vectorSearch": params}
+
+        pipeline = [
+            query,
+            {"$set": {"score": {"$meta": "vectorSearchScore"}}},
         ]
+        if post_filter_pipeline is not None:
+            pipeline.extend(post_filter_pipeline)
+        cursor = self._collection.aggregate(pipeline)  # type: ignore[arg-type]
+        docs = []
+        for res in cursor:
+            text = res.pop(self._text_key)
+            score = res.pop("score")
+            docs.append((Document(page_content=text, metadata=res), score))
+        return docs
 
-    def similarity_search_with_score_id(
+    def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Tuple[Document, float, str]]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_with_score_id_by_vector(
-            embedding=embedding_vector,
-            k=k,
-            filter=filter,
-        )
-
-    # id-unaware search facilities
-    def similarity_search_with_score_by_vector(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        pre_filter: Optional[Dict] = None,
+        post_filter_pipeline: Optional[List[Dict]] = None,
     ) -> List[Tuple[Document, float]]:
-        """Return docs most similar to embedding vector.
+        """Return MongoDB documents most similar to the given query and their scores.
+
+        Uses the vectorSearch operator available in MongoDB Atlas Search.
+        For more: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/
 
         Args:
-            embedding (str): Embedding to look up documents similar to.
-            k (int): Number of Documents to return. Defaults to 4.
+            query: Text to look up documents similar to.
+            k: (Optional) number of documents to return. Defaults to 4.
+            pre_filter: (Optional) dictionary of argument(s) to prefilter document
+                fields on.
+            post_filter_pipeline: (Optional) Pipeline of MongoDB aggregation stages
+                following the vectorSearch stage.
+
         Returns:
-            List of (Document, score), the most similar to the query vector.
+            List of documents most similar to the query and their scores.
         """
-        return [
-            (doc, score)
-            for (doc, score, docId) in self.similarity_search_with_score_id_by_vector(
-                embedding=embedding,
-                k=k,
-                filter=filter,
-            )
-        ]
+        embedding = self._embedding.embed_query(query)
+        docs = self._similarity_search_with_score(
+            embedding,
+            k=k,
+            pre_filter=pre_filter,
+            post_filter_pipeline=post_filter_pipeline,
+        )
+        return docs
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-        **kwargs: Any,
-    ) -> List[Document]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_by_vector(
-            embedding_vector,
-            k,
-            filter=filter,
-        )
-
-    def similarity_search_by_vector(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        pre_filter: Optional[Dict] = None,
+        post_filter_pipeline: Optional[List[Dict]] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        return [
-            doc
-            for doc, _ in self.similarity_search_with_score_by_vector(
-                embedding,
-                k,
-                filter=filter,
-            )
-        ]
+        """Return MongoDB documents most similar to the given query.
 
-    def similarity_search_with_score(
-        self,
-        query: str,
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Tuple[Document, float]]:
-        embedding_vector = self.embedding.embed_query(query)
-        return self.similarity_search_with_score_by_vector(
-            embedding_vector,
-            k,
-            filter=filter,
-        )
+        Uses the vectorSearch operator available in MongoDB Atlas Search.
+        For more: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/
 
-    def max_marginal_relevance_search_by_vector(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, str]] = None,
-        **kwargs: Any,
-    ) -> List[Document]:
-        """Return docs selected using the maximal marginal relevance.
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
         Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
+            query: Text to look up documents similar to.
+            k: (Optional) number of documents to return. Defaults to 4.
+            pre_filter: (Optional) dictionary of argument(s) to prefilter document
+                fields on.
+            post_filter_pipeline: (Optional) Pipeline of MongoDB aggregation stages
+                following the vectorSearch stage.
+
         Returns:
-            List of Documents selected by maximal marginal relevance.
+            List of documents most similar to the query and their scores.
         """
-        search_metadata = self._filter_to_metadata(filter)
-
-        prefetchHits = self.table.search(
-            embedding_vector=embedding,
-            top_k=fetch_k,
-            metric="cos",
-            metric_threshold=None,
-            metadata=search_metadata,
-        )
-        # let the mmr utility pick the *indices* in the above array
-        mmrChosenIndices = maximal_marginal_relevance(
-            np.array(embedding, dtype=np.float32),
-            [pfHit["embedding_vector"] for pfHit in prefetchHits],
+        additional = kwargs.get("additional")
+        docs_and_scores = self.similarity_search_with_score(
+            query,
             k=k,
-            lambda_mult=lambda_mult,
+            pre_filter=pre_filter,
+            post_filter_pipeline=post_filter_pipeline,
         )
-        mmrHits = [
-            pfHit
-            for pfIndex, pfHit in enumerate(prefetchHits)
-            if pfIndex in mmrChosenIndices
-        ]
-        return [
-            Document(
-                page_content=hit["document"],
-                metadata=hit["metadata"],
-            )
-            for hit in mmrHits
-        ]
+
+        if additional and "similarity_score" in additional:
+            for doc, score in docs_and_scores:
+                doc.metadata["score"] = score
+        return [doc for doc, _ in docs_and_scores]
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, str]] = None,
+        pre_filter: Optional[Dict] = None,
+        post_filter_pipeline: Optional[List[Dict]] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        """Return docs selected using the maximal marginal relevance.
+        """Return documents selected using the maximal marginal relevance.
+
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
+
         Args:
             query: Text to look up documents similar to.
-            k: Number of Documents to return.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            k: (Optional) number of documents to return. Defaults to 4.
+            fetch_k: (Optional) number of documents to fetch before passing to MMR
+                algorithm. Defaults to 20.
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
-                        Optional.
+                        Defaults to 0.5.
+            pre_filter: (Optional) dictionary of argument(s) to prefilter on document
+                fields.
+            post_filter_pipeline: (Optional) pipeline of MongoDB aggregation stages
+                following the vectorSearch stage.
         Returns:
-            List of Documents selected by maximal marginal relevance.
+            List of documents selected by maximal marginal relevance.
         """
-        embedding_vector = self.embedding.embed_query(query)
-        return self.max_marginal_relevance_search_by_vector(
-            embedding_vector,
-            k,
-            fetch_k,
+        query_embedding = self._embedding.embed_query(query)
+        docs = self._similarity_search_with_score(
+            query_embedding,
+            k=fetch_k,
+            pre_filter=pre_filter,
+            post_filter_pipeline=post_filter_pipeline,
+        )
+        mmr_doc_indexes = maximal_marginal_relevance(
+            np.array(query_embedding),
+            [doc.metadata[self._embedding_key] for doc, _ in docs],
+            k=k,
             lambda_mult=lambda_mult,
-            filter=filter,
         )
+        mmr_docs = [docs[i][0] for i in mmr_doc_indexes]
+        return mmr_docs
 
     @classmethod
     def from_texts(
-        cls: Type[CVST],
+        cls,
         texts: List[str],
         embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        batch_size: int = 16,
+        metadatas: Optional[List[Dict]] = None,
+        collection: Optional[Collection[MongoDBDocumentType]] = None,
         **kwargs: Any,
-    ) -> CVST:
-        """Create a Cassandra vectorstore from raw texts.
+    ) -> MongoDBAtlasVectorSearch:
+        """Construct a `MongoDB Atlas Vector Search` vector store from raw documents.
 
-        No support for specifying text IDs
+        This is a user-friendly interface that:
+            1. Embeds documents.
+            2. Adds the documents to a provided MongoDB Atlas Vector Search index
+                (Lucene)
 
-        Returns:
-            a Cassandra vectorstore.
-        """
-        session: Session = kwargs["session"]
-        keyspace: str = kwargs["keyspace"]
-        table_name: str = kwargs["table_name"]
-        cassandraStore = cls(
-            embedding=embedding,
-            session=session,
-            keyspace=keyspace,
-            table_name=table_name,
-        )
-        cassandraStore.add_texts(texts=texts, metadatas=metadatas)
-        return cassandraStore
+        This is intended to be a quick way to get started.
 
-    @classmethod
-    def from_documents(
-        cls: Type[CVST],
-        documents: List[Document],
-        embedding: Embeddings,
-        batch_size: int = 16,
-        **kwargs: Any,
-    ) -> CVST:
-        """Create a Cassandra vectorstore from a document list.
+        Example:
+            .. code-block:: python
+                from pymongo import MongoClient
 
-        No support for specifying text IDs
+                from langchain_community.vectorstores import MongoDBAtlasVectorSearch
+                from langchain_community.embeddings import OpenAIEmbeddings
 
-        Returns:
-            a Cassandra vectorstore.
+                mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
+                collection = mongo_client["<db_name>"]["<collection_name>"]
+                embeddings = OpenAIEmbeddings()
+                vectorstore = MongoDBAtlasVectorSearch.from_texts(
+                    texts,
+                    embeddings,
+                    metadatas=metadatas,
+                    collection=collection
+                )
         """
-        texts = [doc.page_content for doc in documents]
-        metadatas = [doc.metadata for doc in documents]
-        session: Session = kwargs["session"]
-        keyspace: str = kwargs["keyspace"]
-        table_name: str = kwargs["table_name"]
-        return cls.from_texts(
-            texts=texts,
-            metadatas=metadatas,
-            embedding=embedding,
-            session=session,
-            keyspace=keyspace,
-            table_name=table_name,
-        )
+        if collection is None:
+            raise ValueError("Must provide 'collection' named parameter.")
+        vectorstore = cls(collection, embedding, **kwargs)
+        vectorstore.add_texts(texts, metadatas=metadatas)
+        return vectorstore
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/chroma.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/chroma.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,14 +12,15 @@
     List,
     Optional,
     Tuple,
     Type,
 )
 
 import numpy as np
+from langchain_core._api import deprecated
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.utils import xor_args
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
@@ -144,15 +145,15 @@
         where_document: Optional[Dict[str, str]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Query the chroma collection."""
         try:
             import chromadb  # noqa: F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import chromadb python package. "
                 "Please install it with `pip install chromadb`."
             )
         return self._collection.query(
             query_texts=query_texts,
             query_embeddings=query_embeddings,
             n_results=n_results,
@@ -183,15 +184,15 @@
         Returns:
             List[str]: List of IDs of the added images.
         """
         # Map from uris to b64 encoded strings
         b64_texts = [self.encode_image(uri=uri) for uri in uris]
         # Populate IDs
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in uris]
+            ids = [str(uuid.uuid4()) for _ in uris]
         embeddings = None
         # Set embeddings
         if self._embedding_function is not None and hasattr(
             self._embedding_function, "embed_image"
         ):
             embeddings = self._embedding_function.embed_image(uris=uris)
         if metadatas:
@@ -205,15 +206,15 @@
             for idx, m in enumerate(metadatas):
                 if m:
                     non_empty_ids.append(idx)
                 else:
                     empty_ids.append(idx)
             if non_empty_ids:
                 metadatas = [metadatas[idx] for idx in non_empty_ids]
-                images_with_metadatas = [uris[idx] for idx in non_empty_ids]
+                images_with_metadatas = [b64_texts[idx] for idx in non_empty_ids]
                 embeddings_with_metadatas = (
                     [embeddings[idx] for idx in non_empty_ids] if embeddings else None
                 )
                 ids_with_metadata = [ids[idx] for idx in non_empty_ids]
                 try:
                     self._collection.upsert(
                         metadatas=metadatas,
@@ -221,21 +222,21 @@
                         documents=images_with_metadatas,
                         ids=ids_with_metadata,
                     )
                 except ValueError as e:
                     if "Expected metadata value to be" in str(e):
                         msg = (
                             "Try filtering complex metadata using "
-                            "langchain.vectorstores.utils.filter_complex_metadata."
+                            "langchain_community.vectorstores.utils.filter_complex_metadata."
                         )
                         raise ValueError(e.args[0] + "\n\n" + msg)
                     else:
                         raise e
             if empty_ids:
-                images_without_metadatas = [uris[j] for j in empty_ids]
+                images_without_metadatas = [b64_texts[j] for j in empty_ids]
                 embeddings_without_metadatas = (
                     [embeddings[j] for j in empty_ids] if embeddings else None
                 )
                 ids_without_metadatas = [ids[j] for j in empty_ids]
                 self._collection.upsert(
                     embeddings=embeddings_without_metadatas,
                     documents=images_without_metadatas,
@@ -264,15 +265,15 @@
             ids (Optional[List[str]], optional): Optional list of IDs.
 
         Returns:
             List[str]: List of IDs of the added texts.
         """
         # TODO: Handle the case where the user doesn't provide ids on the Collection
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
         embeddings = None
         texts = list(texts)
         if self._embedding_function is not None:
             embeddings = self._embedding_function.embed_documents(texts)
         if metadatas:
             # fill metadatas with empty dicts if somebody
             # did not specify metadata for all texts
@@ -300,15 +301,15 @@
                         documents=texts_with_metadatas,
                         ids=ids_with_metadata,
                     )
                 except ValueError as e:
                     if "Expected metadata value to be" in str(e):
                         msg = (
                             "Try filtering complex metadata from the document using "
-                            "langchain.vectorstores.utils.filter_complex_metadata."
+                            "langchain_community.vectorstores.utils.filter_complex_metadata."
                         )
                         raise ValueError(e.args[0] + "\n\n" + msg)
                     else:
                         raise e
             if empty_ids:
                 texts_without_metadatas = [texts[j] for j in empty_ids]
                 embeddings_without_metadatas = (
@@ -341,15 +342,17 @@
             query (str): Query text to search for.
             k (int): Number of results to return. Defaults to 4.
             filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
 
         Returns:
             List[Document]: List of documents most similar to the query text.
         """
-        docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)
+        docs_and_scores = self.similarity_search_with_score(
+            query, k, filter=filter, **kwargs
+        )
         return [doc for doc, _ in docs_and_scores]
 
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = DEFAULT_K,
         filter: Optional[Dict[str, str]] = None,
@@ -365,14 +368,15 @@
             List of Documents most similar to the query vector.
         """
         results = self.__query_collection(
             query_embeddings=embedding,
             n_results=k,
             where=filter,
             where_document=where_document,
+            **kwargs,
         )
         return _results_to_docs(results)
 
     def similarity_search_by_vector_with_relevance_scores(
         self,
         embedding: List[float],
         k: int = DEFAULT_K,
@@ -394,14 +398,15 @@
             Lower score represents more similarity.
         """
         results = self.__query_collection(
             query_embeddings=embedding,
             n_results=k,
             where=filter,
             where_document=where_document,
+            **kwargs,
         )
         return _results_to_docs_and_scores(results)
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = DEFAULT_K,
@@ -423,22 +428,24 @@
         """
         if self._embedding_function is None:
             results = self.__query_collection(
                 query_texts=[query],
                 n_results=k,
                 where=filter,
                 where_document=where_document,
+                **kwargs,
             )
         else:
             query_embedding = self._embedding_function.embed_query(query)
             results = self.__query_collection(
                 query_embeddings=[query_embedding],
                 n_results=k,
                 where=filter,
                 where_document=where_document,
+                **kwargs,
             )
 
         return _results_to_docs_and_scores(results)
 
     def _select_relevance_score_fn(self) -> Callable[[float], float]:
         """
         The 'correct' relevance function
@@ -501,14 +508,15 @@
 
         results = self.__query_collection(
             query_embeddings=embedding,
             n_results=fetch_k,
             where=filter,
             where_document=where_document,
             include=["metadatas", "documents", "distances", "embeddings"],
+            **kwargs,
         )
         mmr_selected = maximal_marginal_relevance(
             np.array(embedding, dtype=np.float32),
             results["embeddings"][0],
             k=k,
             lambda_mult=lambda_mult,
         )
@@ -599,19 +607,30 @@
         }
 
         if include is not None:
             kwargs["include"] = include
 
         return self._collection.get(**kwargs)
 
+    @deprecated(
+        since="0.1.17",
+        message=(
+            "Since Chroma 0.4.x the manual persistence method is no longer "
+            "supported as docs are automatically persisted."
+        ),
+        removal="0.3.0",
+    )
     def persist(self) -> None:
         """Persist the collection.
 
         This can be used to explicitly persist the data to disk.
         It will also be called automatically when the object is destroyed.
+
+        Since Chroma 0.4.x the manual persistence method is no longer
+        supported as docs are automatically persisted.
         """
         if self._persist_directory is None:
             raise ValueError(
                 "You must specify a persist_directory on"
                 "creation to persist the collection."
             )
         import chromadb
@@ -710,15 +729,15 @@
             persist_directory=persist_directory,
             client_settings=client_settings,
             client=client,
             collection_metadata=collection_metadata,
             **kwargs,
         )
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
         if hasattr(
             chroma_collection._client, "max_batch_size"
         ):  # for Chroma 0.4.10 and above
             from chromadb.utils.batch_utils import create_batches
 
             for batch in create_batches(
                 api=chroma_collection._client,
@@ -784,7 +803,11 @@
     def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:
         """Delete by vector IDs.
 
         Args:
             ids: List of ids to delete.
         """
         self._collection.delete(ids=ids)
+
+    def __len__(self) -> int:
+        """Count the number of documents in the collection."""
+        return self._collection.count()
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/clarifai.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/clarifai.py`

 * *Files 14% similar despite different names*

```diff
@@ -20,55 +20,72 @@
 
     To use, you should have the ``clarifai`` python SDK package installed.
 
     Example:
         .. code-block:: python
 
                 from langchain_community.vectorstores import Clarifai
-                from langchain_community.embeddings.openai import OpenAIEmbeddings
 
-                embeddings = OpenAIEmbeddings()
-                vectorstore = Clarifai("langchain_store", embeddings.embed_query)
+                clarifai_vector_db = Clarifai(
+                        user_id=USER_ID,
+                        app_id=APP_ID,
+                        number_of_docs=NUMBER_OF_DOCS,
+                        )
     """
 
     def __init__(
         self,
         user_id: Optional[str] = None,
         app_id: Optional[str] = None,
-        number_of_docs: Optional[int] = None,
+        number_of_docs: Optional[int] = 4,
         pat: Optional[str] = None,
+        token: Optional[str] = None,
+        api_base: Optional[str] = "https://api.clarifai.com",
     ) -> None:
         """Initialize with Clarifai client.
 
         Args:
             user_id (Optional[str], optional): User ID. Defaults to None.
             app_id (Optional[str], optional): App ID. Defaults to None.
             pat (Optional[str], optional): Personal access token. Defaults to None.
+            token (Optional[str], optional): Session token. Defaults to None.
             number_of_docs (Optional[int], optional): Number of documents to return
             during vector search. Defaults to None.
             api_base (Optional[str], optional): API base. Defaults to None.
 
         Raises:
             ValueError: If user ID, app ID or personal access token is not provided.
         """
-        self._user_id = user_id or os.environ.get("CLARIFAI_USER_ID")
-        self._app_id = app_id or os.environ.get("CLARIFAI_APP_ID")
-        if pat:
-            os.environ["CLARIFAI_PAT"] = pat
-        self._pat = os.environ.get("CLARIFAI_PAT")
-        if self._user_id is None or self._app_id is None or self._pat is None:
+        _user_id = user_id or os.environ.get("CLARIFAI_USER_ID")
+        _app_id = app_id or os.environ.get("CLARIFAI_APP_ID")
+        if _user_id is None or _app_id is None:
             raise ValueError(
-                "Could not find CLARIFAI_USER_ID, CLARIFAI_APP_ID or\
-                CLARIFAI_PAT in your environment. "
-                "Please set those env variables with a valid user ID, \
-                app ID and personal access token \
-                from https://clarifai.com/settings/security."
+                "Could not find CLARIFAI_USER_ID "
+                "or CLARIFAI_APP_ID in your environment. "
+                "Please set those env variables with a valid user ID, app ID"
             )
         self._number_of_docs = number_of_docs
 
+        try:
+            from clarifai.client.search import Search
+        except ImportError as e:
+            raise ImportError(
+                "Could not import clarifai python package. "
+                "Please install it with `pip install clarifai`."
+            ) from e
+
+        self._auth = Search(
+            user_id=_user_id,
+            app_id=_app_id,
+            top_k=number_of_docs,
+            pat=pat,
+            token=token,
+            base_url=api_base,
+        ).auth_helper
+
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[str]:
@@ -103,15 +120,15 @@
             ), "Number of texts and metadatas should be the same."
 
         if ids is not None:
             assert len(ltexts) == len(
                 ids
             ), "Number of text inputs and input ids should be the same."
 
-        input_obj = Inputs(app_id=self._app_id, user_id=self._user_id)
+        input_obj = Inputs.from_auth_helper(auth=self._auth)
         batch_size = 32
         input_job_ids = []
         for idx in range(0, length, batch_size):
             try:
                 batch_texts = ltexts[idx : idx + batch_size]
                 batch_metadatas = (
                     metadatas[idx : idx + batch_size] if metadatas else None
@@ -143,23 +160,24 @@
                 traceback.print_exc()
 
         return input_job_ids
 
     def similarity_search_with_score(
         self,
         query: str,
-        k: int = 4,
+        k: Optional[int] = None,
         filters: Optional[dict] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Run similarity search with score using Clarifai.
 
         Args:
             query (str): Query text to search for.
-            k (int): Number of results to return. Defaults to 4.
+            k (Optional[int]): Number of results to return. If not set,
+            it'll take _number_of_docs. Defaults to None.
             filter (Optional[Dict[str, str]]): Filter by metadata.
             Defaults to None.
 
         Returns:
             List[Document]: List of documents most similar to the query text.
         """
         try:
@@ -169,33 +187,32 @@
         except ImportError as e:
             raise ImportError(
                 "Could not import clarifai python package. "
                 "Please install it with `pip install clarifai`."
             ) from e
 
         # Get number of docs to return
-        if self._number_of_docs is not None:
-            k = self._number_of_docs
+        top_k = k or self._number_of_docs
 
-        search_obj = Search(user_id=self._user_id, app_id=self._app_id, top_k=k)
+        search_obj = Search.from_auth_helper(auth=self._auth, top_k=top_k)
         rank = [{"text_raw": query}]
         # Add filter by metadata if provided.
         if filters is not None:
             search_metadata = {"metadata": filters}
             search_response = search_obj.query(ranks=rank, filters=[search_metadata])
         else:
             search_response = search_obj.query(ranks=rank)
 
         # Retrieve hits
         hits = [hit for data in search_response for hit in data.hits]
         executor = ThreadPoolExecutor(max_workers=10)
 
         def hit_to_document(hit: resources_pb2.Hit) -> Tuple[Document, float]:
             metadata = json_format.MessageToDict(hit.input.data.metadata)
-            h = {"Authorization": f"Key {self._pat}"}
+            h = dict(self._auth.metadata)
             request = requests.get(hit.input.data.text.url, headers=h)
 
             # override encoding by real educated guess as provided by chardet
             request.encoding = request.apparent_encoding
             requested_text = request.text
 
             logger.debug(
@@ -209,90 +226,104 @@
         docs_and_scores = [future.result() for future in futures]
 
         return docs_and_scores
 
     def similarity_search(
         self,
         query: str,
-        k: int = 4,
+        k: Optional[int] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Run similarity search using Clarifai.
 
         Args:
             query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
+            k: Number of Documents to return.
+            If not set, it'll take _number_of_docs. Defaults to None.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
-        docs_and_scores = self.similarity_search_with_score(query, **kwargs)
+        docs_and_scores = self.similarity_search_with_score(query, k=k, **kwargs)
         return [doc for doc, _ in docs_and_scores]
 
     @classmethod
     def from_texts(
         cls,
         texts: List[str],
         embedding: Optional[Embeddings] = None,
         metadatas: Optional[List[dict]] = None,
         user_id: Optional[str] = None,
         app_id: Optional[str] = None,
         number_of_docs: Optional[int] = None,
         pat: Optional[str] = None,
+        token: Optional[str] = None,
         **kwargs: Any,
     ) -> Clarifai:
         """Create a Clarifai vectorstore from a list of texts.
 
         Args:
             user_id (str): User ID.
             app_id (str): App ID.
             texts (List[str]): List of texts to add.
-            number_of_docs (Optional[int]): Number of documents to return
-            during vector search. Defaults to None.
-            metadatas (Optional[List[dict]]): Optional list of metadatas.
+            number_of_docs (Optional[int]): Number of documents
+            to return during vector search. Defaults to None.
+            pat (Optional[str], optional): Personal access token.
             Defaults to None.
+            token (Optional[str], optional): Session token. Defaults to None.
+            metadatas (Optional[List[dict]]): Optional list
+            of metadatas. Defaults to None.
+            **kwargs: Additional keyword arguments to be passed to the Search.
 
         Returns:
             Clarifai: Clarifai vectorstore.
         """
         clarifai_vector_db = cls(
             user_id=user_id,
             app_id=app_id,
             number_of_docs=number_of_docs,
             pat=pat,
+            token=token,
+            **kwargs,
         )
         clarifai_vector_db.add_texts(texts=texts, metadatas=metadatas)
         return clarifai_vector_db
 
     @classmethod
     def from_documents(
         cls,
         documents: List[Document],
         embedding: Optional[Embeddings] = None,
         user_id: Optional[str] = None,
         app_id: Optional[str] = None,
         number_of_docs: Optional[int] = None,
         pat: Optional[str] = None,
+        token: Optional[str] = None,
         **kwargs: Any,
     ) -> Clarifai:
         """Create a Clarifai vectorstore from a list of documents.
 
         Args:
             user_id (str): User ID.
             app_id (str): App ID.
             documents (List[Document]): List of documents to add.
-            number_of_docs (Optional[int]): Number of documents to return
-            during vector search. Defaults to None.
+            number_of_docs (Optional[int]): Number of documents
+            to return during vector search. Defaults to None.
+            pat (Optional[str], optional): Personal access token. Defaults to None.
+            token (Optional[str], optional): Session token. Defaults to None.
+            **kwargs: Additional keyword arguments to be passed to the Search.
 
         Returns:
             Clarifai: Clarifai vectorstore.
         """
         texts = [doc.page_content for doc in documents]
         metadatas = [doc.metadata for doc in documents]
         return cls.from_texts(
             user_id=user_id,
             app_id=app_id,
             texts=texts,
             number_of_docs=number_of_docs,
             pat=pat,
             metadatas=metadatas,
+            token=token,
+            **kwargs,
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/clickhouse.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/clickhouse.py`

 * *Files 14% similar despite different names*

```diff
@@ -68,15 +68,15 @@
 
     host: str = "localhost"
     port: int = 8123
 
     username: Optional[str] = None
     password: Optional[str] = None
 
-    index_type: str = "annoy"
+    index_type: Optional[str] = "annoy"
     # Annoy supports L2Distance and cosineDistance.
     index_param: Optional[Union[List, Dict]] = ["'L2Distance'", 100]
     index_query_params: Dict[str, str] = {}
 
     column_map: Dict[str, str] = {
         "id": "id",
         "uuid": "uuid",
@@ -168,31 +168,23 @@
         index_params = (
             (
                 ",".join([f"'{k}={v}'" for k, v in self.config.index_param.items()])
                 if self.config.index_param
                 else ""
             )
             if isinstance(self.config.index_param, Dict)
-            else ",".join([str(p) for p in self.config.index_param])
-            if isinstance(self.config.index_param, List)
-            else self.config.index_param
+            else (
+                ",".join([str(p) for p in self.config.index_param])
+                if isinstance(self.config.index_param, List)
+                else self.config.index_param
+            )
         )
 
-        self.schema = f"""\
-CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.table}(
-    {self.config.column_map['id']} Nullable(String),
-    {self.config.column_map['document']} Nullable(String),
-    {self.config.column_map['embedding']} Array(Float32),
-    {self.config.column_map['metadata']} JSON,
-    {self.config.column_map['uuid']} UUID DEFAULT generateUUIDv4(),
-    CONSTRAINT cons_vec_len CHECK length({self.config.column_map['embedding']}) = {dim},
-    INDEX vec_idx {self.config.column_map['embedding']} TYPE \
-{self.config.index_type}({index_params}) GRANULARITY 1000
-) ENGINE = MergeTree ORDER BY uuid SETTINGS index_granularity = 8192\
-"""
+        self.schema = self._schema(dim, index_params)
+
         self.dim = dim
         self.BS = "\\"
         self.must_escape = ("\\", "'")
         self.embedding_function = embedding
         self.dist_order = "ASC"  # Only support ConsingDistance and L2Distance
 
         # Create a connection to clickhouse
@@ -201,40 +193,130 @@
             port=self.config.port,
             username=self.config.username,
             password=self.config.password,
             **kwargs,
         )
         # Enable JSON type
         self.client.command("SET allow_experimental_object_type=1")
-        # Enable Annoy index
-        self.client.command("SET allow_experimental_annoy_index=1")
+        if self.config.index_type:
+            # Enable index
+            self.client.command(
+                f"SET allow_experimental_{self.config.index_type}_index=1"
+            )
         self.client.command(self.schema)
 
+    def _schema(self, dim: int, index_params: Optional[str] = "") -> str:
+        """Create table schema
+        :param dim: dimension of embeddings
+        :param index_params: parameters used for index
+
+        This function returns a `CREATE TABLE` statement based on the value of
+        `self.config.index_type`.
+        If an index type is specified that index will be created, otherwise
+        no index will be created.
+        In the case of there being no index, a linear scan will be performed
+        when the embedding field is queried.
+        """
+
+        if self.config.index_type:
+            return f"""\
+        CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.table}(
+            {self.config.column_map['id']} Nullable(String),
+            {self.config.column_map['document']} Nullable(String),
+            {self.config.column_map['embedding']} Array(Float32),
+            {self.config.column_map['metadata']} JSON,
+            {self.config.column_map['uuid']} UUID DEFAULT generateUUIDv4(),
+            CONSTRAINT cons_vec_len CHECK length(
+                {self.config.column_map['embedding']}) = {dim},
+            INDEX vec_idx {self.config.column_map['embedding']} TYPE \
+        {self.config.index_type}({index_params}) GRANULARITY 1000
+        ) ENGINE = MergeTree ORDER BY uuid SETTINGS index_granularity = 8192\
+        """
+        else:
+            return f"""\
+                CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.table}(
+                    {self.config.column_map['id']} Nullable(String),
+                    {self.config.column_map['document']} Nullable(String),
+                    {self.config.column_map['embedding']} Array(Float32),
+                    {self.config.column_map['metadata']} JSON,
+                    {self.config.column_map['uuid']} UUID DEFAULT generateUUIDv4(),
+                    CONSTRAINT cons_vec_len CHECK length({
+                        self.config.column_map['embedding']}) = {dim}
+                ) ENGINE = MergeTree ORDER BY uuid
+                """
+
     @property
     def embeddings(self) -> Embeddings:
+        """Provides access to the embedding mechanism used by the Clickhouse instance.
+
+        This property allows direct access to the embedding function or model being
+        used by the Clickhouse instance to convert text documents into embedding vectors
+        for vector similarity search.
+
+        Returns:
+            The `Embeddings` instance associated with this Clickhouse instance.
+        """
         return self.embedding_function
 
     def escape_str(self, value: str) -> str:
+        """Escape special characters in a string for Clickhouse SQL queries.
+
+        This method is used internally to prepare strings for safe insertion
+        into SQL queries by escaping special characters that might otherwise
+        interfere with the query syntax.
+
+        Args:
+            value: The string to be escaped.
+
+        Returns:
+            The escaped string, safe for insertion into SQL queries.
+        """
         return "".join(f"{self.BS}{c}" if c in self.must_escape else c for c in value)
 
     def _build_insert_sql(self, transac: Iterable, column_names: Iterable[str]) -> str:
+        """Construct an SQL query for inserting data into the Clickhouse database.
+
+        This method formats and constructs an SQL `INSERT` query string using the
+        provided transaction data and column names. It is utilized internally during
+        the process of batch insertion of documents and their embeddings into the
+        database.
+
+        Args:
+            transac: iterable of tuples, representing a row of data to be inserted.
+            column_names: iterable of strings representing the names of the columns
+                into which data will be inserted.
+
+        Returns:
+            A string containing the constructed SQL `INSERT` query.
+        """
         ks = ",".join(column_names)
         _data = []
         for n in transac:
             n = ",".join([f"'{self.escape_str(str(_n))}'" for _n in n])
             _data.append(f"({n})")
         i_str = f"""
                 INSERT INTO TABLE 
                     {self.config.database}.{self.config.table}({ks})
                 VALUES
                 {','.join(_data)}
                 """
         return i_str
 
     def _insert(self, transac: Iterable, column_names: Iterable[str]) -> None:
+        """Execute an SQL query to insert data into the Clickhouse database.
+
+        This method performs the actual insertion of data into the database by
+        executing the SQL query constructed by `_build_insert_sql`. It's a critical
+        step in adding new documents and their associated data into the vector store.
+
+        Args:
+            transac:iterable of tuples, representing a row of data to be inserted.
+            column_names: An iterable of strings representing the names of the columns
+                into which data will be inserted.
+        """
         _insert_query = self._build_insert_sql(transac, column_names)
         self.client.command(_insert_query)
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
@@ -341,14 +423,29 @@
             )
         _repr += "-" * 51 + "\n"
         return _repr
 
     def _build_query_sql(
         self, q_emb: List[float], topk: int, where_str: Optional[str] = None
     ) -> str:
+        """Construct an SQL query for performing a similarity search.
+
+        This internal method generates an SQL query for finding the top-k most similar
+        vectors in the database to a given query vector.It allows for optional filtering
+        conditions to be applied via a WHERE clause.
+
+        Args:
+            q_emb: The query vector as a list of floats.
+            topk: The number of top similar items to retrieve.
+            where_str: opt str representing additional WHERE conditions for the query
+                Defaults to None.
+
+        Returns:
+            A string containing the SQL query for the similarity search.
+        """
         q_emb_str = ",".join(map(str, q_emb))
         if where_str:
             where_str = f"PREWHERE {where_str}"
         else:
             where_str = ""
 
         settings_strs = []
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/dashvector.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/dashvector.py`

 * *Files 12% similar despite different names*

```diff
@@ -47,39 +47,46 @@
         text_field: str,
     ):
         """Initialize with DashVector collection."""
 
         try:
             import dashvector
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import dashvector python package. "
                 "Please install it with `pip install dashvector`."
             )
 
         if not isinstance(collection, dashvector.Collection):
             raise ValueError(
                 f"collection should be an instance of dashvector.Collection, "
                 f"bug got {type(collection)}"
             )
 
         self._collection = collection
         self._embedding = embedding
         self._text_field = text_field
 
+    def _create_partition_if_not_exists(self, partition: str) -> None:
+        """Create a Partition in current Collection."""
+        self._collection.create_partition(partition)
+
     def _similarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[str] = None,
+        partition: str = "default",
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query vector, along with scores"""
 
         # query by vector
-        ret = self._collection.query(embedding, topk=k, filter=filter)
+        ret = self._collection.query(
+            embedding, topk=k, filter=filter, partition=partition
+        )
         if not ret:
             raise ValueError(
                 f"Fail to query docs by vector, error {self._collection.message}"
             )
 
         docs = []
         for doc in ret:
@@ -91,28 +98,31 @@
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         batch_size: int = 25,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[str]:
         """Run more texts through the embeddings and add to the vectorstore.
 
         Args:
             texts: Iterable of strings to add to the vectorstore.
             metadatas: Optional list of metadatas associated with the texts.
             ids: Optional list of ids associated with the texts.
             batch_size: Optional batch size to upsert docs.
+            partition: a partition name in collection. [optional].
             kwargs: vectorstore specific parameters
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
+        self._create_partition_if_not_exists(partition)
         ids = ids or [str(uuid.uuid4().hex) for _ in texts]
         text_list = list(texts)
         for i in range(0, len(text_list), batch_size):
             # batch end
             end = min(i + batch_size, len(text_list))
 
             batch_texts = text_list[i:end]
@@ -125,112 +135,124 @@
             else:
                 batch_metadatas = [{} for _ in range(i, end)]
             for metadata, text in zip(batch_metadatas, batch_texts):
                 metadata[self._text_field] = text
 
             # batch upsert to collection
             docs = list(zip(batch_ids, batch_embeddings, batch_metadatas))
-            ret = self._collection.upsert(docs)
+            ret = self._collection.upsert(docs, partition=partition)
             if not ret:
                 raise ValueError(
                     f"Fail to upsert docs to dashvector vector database,"
                     f"Error: {ret.message}"
                 )
         return ids
 
-    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> bool:
+    def delete(
+        self, ids: Optional[List[str]] = None, partition: str = "default", **kwargs: Any
+    ) -> bool:
         """Delete by vector ID.
 
         Args:
             ids: List of ids to delete.
+            partition: a partition name in collection. [optional].
 
         Returns:
             True if deletion is successful,
             False otherwise.
         """
-        return bool(self._collection.delete(ids))
+        return bool(self._collection.delete(ids, partition=partition))
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
         filter: Optional[str] = None,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to query.
 
         Args:
             query: Text to search documents similar to.
             k: Number of documents to return. Default to 4.
             filter: Doc fields filter conditions that meet the SQL where clause
                     specification.
+            partition: a partition name in collection. [optional].
 
         Returns:
             List of Documents most similar to the query text.
         """
 
-        docs_and_scores = self.similarity_search_with_relevance_scores(query, k, filter)
+        docs_and_scores = self.similarity_search_with_relevance_scores(
+            query, k, filter, partition
+        )
         return [doc for doc, _ in docs_and_scores]
 
     def similarity_search_with_relevance_scores(
         self,
         query: str,
         k: int = 4,
         filter: Optional[str] = None,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query text , alone with relevance scores.
 
         Less is more similar, more is more dissimilar.
 
         Args:
             query: input text
             k: Number of Documents to return. Defaults to 4.
             filter: Doc fields filter conditions that meet the SQL where clause
                     specification.
+            partition: a partition name in collection. [optional].
 
         Returns:
             List of Tuples of (doc, similarity_score)
         """
 
         embedding = self._embedding.embed_query(query)
         return self._similarity_search_with_score_by_vector(
-            embedding, k=k, filter=filter
+            embedding, k=k, filter=filter, partition=partition
         )
 
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[str] = None,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to embedding vector.
 
         Args:
             embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
             filter: Doc fields filter conditions that meet the SQL where clause
                     specification.
+            partition: a partition name in collection. [optional].
 
         Returns:
             List of Documents most similar to the query vector.
         """
         docs_and_scores = self._similarity_search_with_score_by_vector(
-            embedding, k, filter
+            embedding, k, filter, partition
         )
         return [doc for doc, _ in docs_and_scores]
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
         filter: Optional[dict] = None,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -240,30 +262,32 @@
             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5.
             filter: Doc fields filter conditions that meet the SQL where clause
                     specification.
+            partition: a partition name in collection. [optional].
 
         Returns:
             List of Documents selected by maximal marginal relevance.
         """
         embedding = self._embedding.embed_query(query)
         return self.max_marginal_relevance_search_by_vector(
-            embedding, k, fetch_k, lambda_mult, filter
+            embedding, k, fetch_k, lambda_mult, filter, partition
         )
 
     def max_marginal_relevance_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
         filter: Optional[dict] = None,
+        partition: str = "default",
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -273,22 +297,27 @@
             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5.
             filter: Doc fields filter conditions that meet the SQL where clause
                     specification.
+            partition: a partition name in collection. [optional].
 
         Returns:
             List of Documents selected by maximal marginal relevance.
         """
 
         # query by vector
         ret = self._collection.query(
-            embedding, topk=fetch_k, filter=filter, include_vector=True
+            embedding,
+            topk=fetch_k,
+            filter=filter,
+            partition=partition,
+            include_vector=True,
         )
         if not ret:
             raise ValueError(
                 f"Fail to query docs by vector, error {self._collection.message}"
             )
 
         candidate_embeddings = [doc.vector for doc in ret]
@@ -333,15 +362,15 @@
                 embeddings,
                 dashvector_api_key="{DASHVECTOR_API_KEY}"
             )
         """
         try:
             import dashvector
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import dashvector python package. "
                 "Please install it with `pip install dashvector`."
             )
 
         dashvector_api_key = dashvector_api_key or get_from_env(
             "dashvector_api_key", "DASHVECTOR_API_KEY"
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/databricks_vector_search.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/databricks_vector_search.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,18 +1,21 @@
 from __future__ import annotations
 
 import json
 import logging
 import uuid
-from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Type
+from typing import TYPE_CHECKING, Any, Callable, Iterable, List, Optional, Tuple, Type
 
+import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VST, VectorStore
 
+from langchain_community.vectorstores.utils import maximal_marginal_relevance
+
 if TYPE_CHECKING:
     from databricks.vector_search.client import VectorSearchIndex
 
 logger = logging.getLogger(__name__)
 
 
 class DatabricksVectorSearch(VectorStore):
@@ -317,14 +320,134 @@
             query_text=query_text,
             query_vector=query_vector,
             filters=filters,
             num_results=k,
         )
         return self._parse_search_response(search_resp)
 
+    @staticmethod
+    def _identity_fn(score: float) -> float:
+        return score
+
+    def _select_relevance_score_fn(self) -> Callable[[float], float]:
+        """
+        Databricks Vector search uses a normalized score 1/(1+d) where d
+        is the L2 distance. Hence, we simply return the identity function.
+        """
+
+        return self._identity_fn
+
+    def max_marginal_relevance_search(
+        self,
+        query: str,
+        k: int = 4,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        filters: Optional[Any] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+            filters: Filters to apply to the query. Defaults to None.
+        Returns:
+            List of Documents selected by maximal marginal relevance.
+        """
+        if not self._is_databricks_managed_embeddings():
+            assert self.embeddings is not None, "embedding model is required."
+            query_vector = self.embeddings.embed_query(query)
+        else:
+            raise ValueError(
+                "`max_marginal_relevance_search` is not supported for index with "
+                "Databricks-managed embeddings."
+            )
+
+        docs = self.max_marginal_relevance_search_by_vector(
+            query_vector,
+            k,
+            fetch_k,
+            lambda_mult=lambda_mult,
+            filters=filters,
+        )
+        return docs
+
+    def max_marginal_relevance_search_by_vector(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        filters: Optional[Any] = None,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            embedding: Embedding to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+            filters: Filters to apply to the query. Defaults to None.
+        Returns:
+            List of Documents selected by maximal marginal relevance.
+        """
+        if not self._is_databricks_managed_embeddings():
+            embedding_column = self._embedding_vector_column_name()
+        else:
+            raise ValueError(
+                "`max_marginal_relevance_search` is not supported for index with "
+                "Databricks-managed embeddings."
+            )
+
+        search_resp = self.index.similarity_search(
+            columns=list(set(self.columns + [embedding_column])),
+            query_text=None,
+            query_vector=embedding,
+            filters=filters,
+            num_results=fetch_k,
+        )
+
+        embeddings_result_index = (
+            search_resp.get("manifest").get("columns").index({"name": embedding_column})
+        )
+        embeddings = [
+            doc[embeddings_result_index]
+            for doc in search_resp.get("result").get("data_array")
+        ]
+
+        mmr_selected = maximal_marginal_relevance(
+            np.array(embedding, dtype=np.float32),
+            embeddings,
+            k=k,
+            lambda_mult=lambda_mult,
+        )
+
+        ignore_cols: List = (
+            [embedding_column] if embedding_column not in self.columns else []
+        )
+        candidates = self._parse_search_response(search_resp, ignore_cols=ignore_cols)
+        selected_results = [r[0] for i, r in enumerate(candidates) if i in mmr_selected]
+        return selected_results
+
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filters: Optional[Any] = None,
         **kwargs: Any,
     ) -> List[Document]:
@@ -369,28 +492,33 @@
             columns=self.columns,
             query_vector=embedding,
             filters=filters,
             num_results=k,
         )
         return self._parse_search_response(search_resp)
 
-    def _parse_search_response(self, search_resp: dict) -> List[Tuple[Document, float]]:
+    def _parse_search_response(
+        self, search_resp: dict, ignore_cols: Optional[List[str]] = None
+    ) -> List[Tuple[Document, float]]:
         """Parse the search response into a list of Documents with score."""
+        if ignore_cols is None:
+            ignore_cols = []
+
         columns = [
             col["name"]
             for col in search_resp.get("manifest", dict()).get("columns", [])
         ]
         docs_with_score = []
         for result in search_resp.get("result", dict()).get("data_array", []):
             doc_id = result[columns.index(self.primary_key)]
             text_content = result[columns.index(self.text_column)]
             metadata = {
                 col: value
                 for col, value in zip(columns[:-1], result[:-1])
-                if col not in [self.primary_key, self.text_column]
+                if col not in ([self.primary_key, self.text_column] + ignore_cols)
             }
             metadata[self.primary_key] = doc_id
             score = result[-1]
             doc = Document(page_content=text_content, metadata=metadata)
             docs_with_score.append((doc, score))
         return docs_with_score
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/deeplake.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/deeplake.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 
 import numpy as np
 
 try:
     import deeplake
     from deeplake import VectorStore as DeepLakeVectorStore
     from deeplake.core.fast_forwarding import version_compare
+    from deeplake.util.exceptions import SampleExtendError
 
     _DEEPLAKE_INSTALLED = True
 except ImportError:
     _DEEPLAKE_INSTALLED = False
 
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
@@ -47,23 +48,24 @@
                 from langchain_community.embeddings.openai import OpenAIEmbeddings
 
                 embeddings = OpenAIEmbeddings()
                 vectorstore = DeepLake("langchain_store", embeddings.embed_query)
     """
 
     _LANGCHAIN_DEFAULT_DEEPLAKE_PATH = "./deeplake/"
+    _valid_search_kwargs = ["lambda_mult"]
 
     def __init__(
         self,
         dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,
         token: Optional[str] = None,
         embedding: Optional[Embeddings] = None,
         embedding_function: Optional[Embeddings] = None,
         read_only: bool = False,
-        ingestion_batch_size: int = 1000,
+        ingestion_batch_size: int = 1024,
         num_workers: int = 0,
         verbose: bool = True,
         exec_option: Optional[str] = None,
         runtime: Optional[Dict] = None,
         index_params: Optional[Dict[str, Union[int, str]]] = None,
         **kwargs: Any,
     ) -> None:
@@ -80,46 +82,61 @@
             >>> # Create a vector store in the Deep Lake Managed Tensor Database
             >>> data = DeepLake(
             ...        path = "hub://org_id/dataset_name",
             ...        runtime = {"tensor_db": True},
             ... )
 
         Args:
-            dataset_path (str): Path to existing dataset or where to create
-                a new one. Defaults to _LANGCHAIN_DEFAULT_DEEPLAKE_PATH.
+            dataset_path (str): The full path for storing to the Deep Lake
+                Vector Store. It can be:
+                - a Deep Lake cloud path of the form ``hub://org_id/dataset_name``.
+                    Requires registration with Deep Lake.
+                - an s3 path of the form ``s3://bucketname/path/to/dataset``.
+                    Credentials are required in either the environment or passed to
+                    the creds argument.
+                - a local file system path of the form ``./path/to/dataset``
+                    or ``~/path/to/dataset`` or ``path/to/dataset``.
+                - a memory path of the form ``mem://path/to/dataset`` which doesn't
+                    save the dataset but keeps it in memory instead.
+                    Should be used only for testing as it does not persist.
+                    Defaults to _LANGCHAIN_DEFAULT_DEEPLAKE_PATH.
             token (str, optional):  Activeloop token, for fetching credentials
                 to the dataset at path if it is a Deep Lake dataset.
                 Tokens are normally autogenerated. Optional.
             embedding (Embeddings, optional): Function to convert
                 either documents or query. Optional.
             embedding_function (Embeddings, optional): Function to convert
                 either documents or query. Optional. Deprecated: keeping this
                 parameter for backwards compatibility.
             read_only (bool): Open dataset in read-only mode. Default is False.
             ingestion_batch_size (int): During data ingestion, data is divided
                 into batches. Batch size is the size of each batch.
-                Default is 1000.
+                Default is 1024.
             num_workers (int): Number of workers to use during data ingestion.
                 Default is 0.
             verbose (bool): Print dataset summary after each operation.
                 Default is True.
-            exec_option (str, optional): DeepLakeVectorStore supports 3 ways to perform
-                searching - "python", "compute_engine", "tensor_db" and auto.
-                Default is None.
+            exec_option (str, optional): Default method for search execution.
+                It could be either ``"auto"``, ``"python"``, ``"compute_engine"``
+                or ``"tensor_db"``. Defaults to ``"auto"``.
+                If None, it's set to "auto".
                 - ``auto``- Selects the best execution method based on the storage
                     location of the Vector Store. It is the default option.
-                - ``python`` - Pure-python implementation that runs on the client.
-                    WARNING: using this with big datasets can lead to memory
-                    issues. Data can be stored anywhere.
-                - ``compute_engine`` - C++ implementation of the Deep Lake Compute
-                    Engine that runs on the client. Can be used for any data stored in
-                    or connected to Deep Lake. Not for in-memory or local datasets.
-                - ``tensor_db`` - Hosted Managed Tensor Database that is
-                    responsible for storage and query execution. Only for data stored in
-                    the Deep Lake Managed Database. Use runtime = {"db_engine": True}
+                - ``python`` - Pure-python implementation that runs on the client and
+                    can be used for data stored anywhere. WARNING: using this option
+                    with big datasets is discouraged because it can lead to
+                    memory issues.
+                - ``compute_engine`` - Performant C++ implementation of the Deep Lake
+                    Compute Engine that runs on the client and can be used for any data
+                    stored in or connected to Deep Lake. It cannot be used with
+                    in-memory or local datasets.
+                - ``tensor_db`` - Performant and fully-hosted Managed Tensor Database
+                    that is responsible for storage and query execution. Only available
+                    for data stored in the Deep Lake Managed Database. Store datasets
+                    in this database by specifying runtime = {"tensor_db": True}
                     during dataset creation.
             runtime (Dict, optional): Parameters for creating the Vector Store in
                 Deep Lake's Managed Tensor Database. Not applicable when loading an
                 existing Vector Store. To create a Vector Store in the Managed Tensor
                 Database, set `runtime = {"tensor_db": True}`.
             index_params (Optional[Dict[str, Union[int, str]]], optional): Dictionary
                 containing information about vector index that will be created. Defaults
@@ -215,19 +232,15 @@
                 to use to convert the text into embeddings.
             **kwargs (Any): Any additional keyword arguments passed is not supported
                 by this method.
 
         Returns:
             List[str]: List of IDs of the added texts.
         """
-        if kwargs:
-            unsupported_items = "`, `".join(set(kwargs.keys()))
-            raise TypeError(
-                f"`{unsupported_items}` is/are not a valid argument to add_text method"
-            )
+        self._validate_kwargs(kwargs, "add_texts")
 
         kwargs = {}
         if ids:
             if self._id_tensor_name == "ids":  # for backwards compatibility
                 kwargs["ids"] = ids
             else:
                 kwargs["id"] = ids
@@ -239,23 +252,33 @@
             texts = list(texts)
 
         if texts is None:
             raise ValueError("`texts` parameter shouldn't be None.")
         elif len(texts) == 0:
             raise ValueError("`texts` parameter shouldn't be empty.")
 
-        return self.vectorstore.add(
-            text=texts,
-            metadata=metadatas,
-            embedding_data=texts,
-            embedding_tensor="embedding",
-            embedding_function=self._embedding_function.embed_documents,  # type: ignore
-            return_ids=True,
-            **kwargs,
-        )
+        try:
+            return self.vectorstore.add(
+                text=texts,
+                metadata=metadatas,
+                embedding_data=texts,
+                embedding_tensor="embedding",
+                embedding_function=self._embedding_function.embed_documents,  # type: ignore
+                return_ids=True,
+                **kwargs,
+            )
+        except SampleExtendError as e:
+            if "Failed to append a sample to the tensor 'metadata'" in str(e):
+                msg = (
+                    "**Hint: You might be using invalid type of argument in "
+                    "document loader (e.g. 'pathlib.PosixPath' instead of 'str')"
+                )
+                raise ValueError(e.args[0] + "\n\n" + msg)
+            else:
+                raise e
 
     def _search_tql(
         self,
         tql: Optional[str],
         exec_option: Optional[str] = None,
         **kwargs: Any,
     ) -> List[Document]:
@@ -367,27 +390,32 @@
         Returns:
             List of Documents by the specified distance metric,
             if return_score True, return a tuple of (Document, score)
 
         Raises:
             ValueError: if both `embedding` and `embedding_function` are not specified.
         """
+        if kwargs.get("tql_query"):
+            logger.warning("`tql_query` is deprecated. Please use `tql` instead.")
+            kwargs["tql"] = kwargs.pop("tql_query")
 
         if kwargs.get("tql"):
             return self._search_tql(
                 tql=kwargs["tql"],
                 exec_option=exec_option,
                 return_score=return_score,
                 embedding=embedding,
                 embedding_function=embedding_function,
                 distance_metric=distance_metric,
                 use_maximal_marginal_relevance=use_maximal_marginal_relevance,
                 filter=filter,
             )
 
+        self._validate_kwargs(kwargs, "search")
+
         if embedding_function:
             if isinstance(embedding_function, Embeddings):
                 _embedding_function = embedding_function.embed_query
             else:
                 _embedding_function = embedding_function
         elif self._embedding_function:
             _embedding_function = self._embedding_function.embed_query
@@ -413,15 +441,14 @@
             k=fetch_k if use_maximal_marginal_relevance else k,
             distance_metric=distance_metric,
             filter=filter,
             exec_option=exec_option,
             return_tensors=["embedding", "metadata", "text", self._id_tensor_name],
             deep_memory=deep_memory,
         )
-
         scores = result["score"]
         embeddings = result["embedding"]
         metadatas = result["metadata"]
         texts = result["text"]
 
         if use_maximal_marginal_relevance:
             lambda_mult = kwargs.get("lambda_mult", 0.5)
@@ -441,14 +468,17 @@
                 page_content=text,
                 metadata=metadata,
             )
             for text, metadata in zip(texts, metadatas)
         ]
 
         if return_score:
+            if not isinstance(scores, list):
+                scores = [scores]
+
             return [(doc, score) for doc, score in zip(docs, scores)]
 
         return docs
 
     def similarity_search(
         self,
         query: str,
@@ -879,15 +909,15 @@
         Raises:
             ValueError: if deeplake is not installed.
         """
 
         try:
             import deeplake
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import deeplake python package. "
                 "Please install it with `pip install deeplake`."
             )
         deeplake.delete(path, large_ok=True, force=True)
 
     def delete_dataset(self) -> None:
         """Delete the collection."""
@@ -895,7 +925,34 @@
 
     def ds(self) -> Any:
         logger.warning(
             "this method is deprecated and will be removed, "
             "better to use `db.vectorstore.dataset` instead."
         )
         return self.vectorstore.dataset
+
+    @classmethod
+    def _validate_kwargs(cls, kwargs, method_name):  # type: ignore[no-untyped-def]
+        if kwargs:
+            valid_items = cls._get_valid_args(method_name)
+            unsupported_items = cls._get_unsupported_items(kwargs, valid_items)
+
+            if unsupported_items:
+                raise TypeError(
+                    f"`{unsupported_items}` are not a valid "
+                    f"argument to {method_name} method"
+                )
+
+    @classmethod
+    def _get_valid_args(cls, method_name):  # type: ignore[no-untyped-def]
+        if method_name == "search":
+            return cls._valid_search_kwargs
+        else:
+            return []
+
+    @staticmethod
+    def _get_unsupported_items(kwargs, valid_items):  # type: ignore[no-untyped-def]
+        kwargs = {k: v for k, v in kwargs.items() if k not in valid_items}
+        unsupported_items = None
+        if kwargs:
+            unsupported_items = "`, `".join(set(kwargs.keys()))
+        return unsupported_items
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/dingo.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/dingo.py`

 * *Files 2% similar despite different names*

```diff
@@ -103,15 +103,15 @@
 
         Returns:
             List of ids from adding the texts into the vectorstore.
 
         """
 
         # Embed and create the documents
-        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]
+        ids = ids or [str(uuid.uuid4().int)[:13] for _ in texts]
         metadatas_list = []
         texts = list(texts)
         embeds = self._embedding.embed_documents(texts)
         for i, text in enumerate(texts):
             metadata = metadatas[i] if metadatas else {}
             metadata[self._text_key] = text
             metadatas_list.append(metadata)
@@ -141,15 +141,15 @@
             k: Number of Documents to return. Defaults to 4.
             search_params: Dictionary of argument(s) to filter on metadata
 
         Returns:
             List of Documents most similar to the query and score for each
         """
         docs_and_scores = self.similarity_search_with_score(
-            query, k=k, search_params=search_params
+            query, k=k, search_params=search_params, **kwargs
         )
         return [doc for doc, _ in docs_and_scores]
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
@@ -173,17 +173,23 @@
             self._index_name, xq=query_obj, top_k=k, search_params=search_params
         )
 
         if not results:
             return []
 
         for res in results[0]["vectorWithDistances"]:
+            score = res["distance"]
+            if (
+                "score_threshold" in kwargs
+                and kwargs.get("score_threshold") is not None
+            ):
+                if score > kwargs.get("score_threshold"):
+                    continue
             metadatas = res["scalarData"]
             id = res["id"]
-            score = res["distance"]
             text = metadatas[self._text_key]["fields"][0]["data"]
             metadata = {"id": id, "text": text, "score": score}
             for meta_key in metadatas.keys():
                 metadata[meta_key] = metadatas[meta_key]["fields"][0]["data"]
             docs.append((Document(page_content=text, metadata=metadata), score))
 
         return docs
@@ -337,15 +343,15 @@
                 and index_name not in dingo_client.get_index()
                 and index_name.upper() not in dingo_client.get_index()
             ):
                 dingo_client.create_index(index_name, dimension=dimension)
 
         # Embed and create the documents
 
-        ids = ids or [str(uuid.uuid1().int)[:13] for _ in texts]
+        ids = ids or [str(uuid.uuid4().int)[:13] for _ in texts]
         metadatas_list = []
         texts = list(texts)
         embeds = embedding.embed_documents(texts)
         for i, text in enumerate(texts):
             metadata = metadatas[i] if metadatas else {}
             metadata[text_key] = text
             metadatas_list.append(metadata)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/base.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/docarray/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,17 +47,17 @@
     @staticmethod
     def _get_doc_cls(**embeddings_params: Any) -> Type["BaseDoc"]:
         """Get docarray Document class describing the schema of DocIndex."""
         from docarray import BaseDoc
         from docarray.typing import NdArray
 
         class DocArrayDoc(BaseDoc):
-            text: Optional[str]
+            text: Optional[str] = Field(default=None, required=False)
             embedding: Optional[NdArray] = Field(**embeddings_params)
-            metadata: Optional[dict]
+            metadata: Optional[dict] = Field(default=None, required=False)
 
         return DocArrayDoc
 
     @property
     def doc_cls(self) -> Type["BaseDoc"]:
         if self.doc_index._schema is None:
             raise ValueError("doc_index expected to have non-null _schema attribute.")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/hnsw.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/docarray/hnsw.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 )
 
 
 class DocArrayHnswSearch(DocArrayIndex):
     """`HnswLib` storage using `DocArray` package.
 
     To use it, you should have the ``docarray`` package with version >=0.32.0 installed.
-    You can install it with `pip install "langchain[docarray]"`.
+    You can install it with `pip install "docarray[hnswlib]"`.
     """
 
     @classmethod
     def from_params(
         cls,
         embedding: Embeddings,
         work_dir: str,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/docarray/in_memory.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/docarray/in_memory.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around in-memory storage."""
+
 from __future__ import annotations
 
 from typing import Any, Dict, List, Literal, Optional
 
 from langchain_core.embeddings import Embeddings
 
 from langchain_community.vectorstores.docarray.base import (
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/elastic_vector_search.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/elastic_vector_search.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,14 +46,19 @@
                 "source": "cosineSimilarity(params.query_vector, 'vector') + 1.0",
                 "params": {"query_vector": query_vector},
             },
         }
     }
 
 
+@deprecated(
+    "0.0.27",
+    alternative="Use ElasticsearchStore class in langchain-elasticsearch package",
+    pending=True,
+)
 class ElasticVectorSearch(VectorStore):
     """
 
     ElasticVectorSearch uses the brute force method of searching on vectors.
 
     Recommended to use ElasticsearchStore instead, which gives you the option
     to uses the approx  HNSW algorithm which performs better on large datasets.
@@ -354,15 +359,19 @@
             raise ValueError("No ids provided to delete.")
 
         # TODO: Check if this can be done in bulk
         for id in ids:
             self.client.delete(index=self.index_name, id=id)
 
 
-@deprecated("0.0.265", alternative="ElasticsearchStore class.", pending=True)
+@deprecated(
+    "0.0.1",
+    alternative="Use ElasticsearchStore class in langchain-elasticsearch package",
+    pending=True,
+)
 class ElasticKnnSearch(VectorStore):
     """[DEPRECATED] `Elasticsearch` with k-nearest neighbor search
     (`k-NN`) vector store.
 
     Recommended to use ElasticsearchStore instead, which supports
     metadata filtering, customising the query retriever and much more!
 
@@ -560,17 +569,19 @@
             fields=fields,
         )
 
         hits = [hit for hit in response["hits"]["hits"]]
         docs_and_scores = [
             (
                 Document(
-                    page_content=hit["_source"][page_content]
-                    if source
-                    else hit["fields"][page_content][0],
+                    page_content=(
+                        hit["_source"][page_content]
+                        if source
+                        else hit["fields"][page_content][0]
+                    ),
                     metadata=hit["fields"] if fields else {},
                 ),
                 hit["_score"],
             )
             for hit in hits
         ]
 
@@ -643,17 +654,19 @@
             source=source,
         )
 
         hits = [hit for hit in response["hits"]["hits"]]
         docs_and_scores = [
             (
                 Document(
-                    page_content=hit["_source"][page_content]
-                    if source
-                    else hit["fields"][page_content][0],
+                    page_content=(
+                        hit["_source"][page_content]
+                        if source
+                        else hit["fields"][page_content][0]
+                    ),
                     metadata=hit["fields"] if fields else {},
                 ),
                 hit["_score"],
             )
             for hit in hits
         ]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/elasticsearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/elasticsearch.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,14 +11,15 @@
     Literal,
     Optional,
     Tuple,
     Union,
 )
 
 import numpy as np
+from langchain_core._api import deprecated
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import (
     DistanceStrategy,
     maximal_marginal_relevance,
@@ -109,14 +110,17 @@
         Returns:
             bool: Whether or not the strategy requires inference
             to be performed on the text before it is added to the index.
         """
         return True
 
 
+@deprecated(
+    "0.0.27", alternative="Use class in langchain-elasticsearch package", pending=True
+)
 class ApproxRetrievalStrategy(BaseRetrievalStrategy):
     """Approximate retrieval strategy using the `HNSW` algorithm."""
 
     def __init__(
         self,
         query_model_id: Optional[str] = None,
         hybrid: Optional[bool] = False,
@@ -210,14 +214,16 @@
 
         if similarity is DistanceStrategy.COSINE:
             similarityAlgo = "cosine"
         elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:
             similarityAlgo = "l2_norm"
         elif similarity is DistanceStrategy.DOT_PRODUCT:
             similarityAlgo = "dot_product"
+        elif similarity is DistanceStrategy.MAX_INNER_PRODUCT:
+            similarityAlgo = "max_inner_product"
         else:
             raise ValueError(f"Similarity {similarity} not supported.")
 
         return {
             "mappings": {
                 "properties": {
                     vector_query_field: {
@@ -227,14 +233,17 @@
                         "similarity": similarityAlgo,
                     },
                 }
             }
         }
 
 
+@deprecated(
+    "0.0.27", alternative="Use class in langchain-elasticsearch package", pending=True
+)
 class ExactRetrievalStrategy(BaseRetrievalStrategy):
     """Exact retrieval strategy using the `script_score` query."""
 
     def query(
         self,
         query_vector: Union[List[float], None],
         query: Union[str, None],
@@ -252,15 +261,15 @@
         elif similarity is DistanceStrategy.EUCLIDEAN_DISTANCE:
             similarityAlgo = (
                 f"1 / (1 + l2norm(params.query_vector, '{vector_query_field}'))"
             )
         elif similarity is DistanceStrategy.DOT_PRODUCT:
             similarityAlgo = f"""
             double value = dotProduct(params.query_vector, '{vector_query_field}');
-            return sigmoid(1, Math.E, -value); 
+            return sigmoid(1, Math.E, -value);
             """
         else:
             raise ValueError(f"Similarity {similarity} not supported.")
 
         queryBool: Dict = {"match_all": {}}
         if filter:
             queryBool = {"bool": {"filter": filter}}
@@ -294,14 +303,17 @@
                         "index": False,
                     },
                 }
             }
         }
 
 
+@deprecated(
+    "0.0.27", alternative="Use class in langchain-elasticsearch package", pending=True
+)
 class SparseRetrievalStrategy(BaseRetrievalStrategy):
     """Sparse retrieval strategy using the `text_expansion` processor."""
 
     def __init__(self, model_id: Optional[str] = None):
         self.model_id = model_id or ".elser_model_1"
 
     def query(
@@ -375,24 +387,26 @@
             "settings": {"default_pipeline": self._get_pipeline_name()},
         }
 
     def require_inference(self) -> bool:
         return False
 
 
+@deprecated(
+    "0.0.27", alternative="Use class in langchain-elasticsearch package", pending=True
+)
 class ElasticsearchStore(VectorStore):
     """`Elasticsearch` vector store.
 
     Example:
         .. code-block:: python
 
             from langchain_community.vectorstores import ElasticsearchStore
             from langchain_community.embeddings.openai import OpenAIEmbeddings
 
-            embeddings = OpenAIEmbeddings()
             vectorstore = ElasticsearchStore(
                 embedding=OpenAIEmbeddings(),
                 index_name="langchain-demo",
                 es_url="http://localhost:9200"
             )
 
     Args:
@@ -409,15 +423,15 @@
         strategy: Optional. Retrieval strategy to use when searching the index.
                  Defaults to ApproxRetrievalStrategy. Can be one of
                  ExactRetrievalStrategy, ApproxRetrievalStrategy,
                  or SparseRetrievalStrategy.
         distance_strategy: Optional. Distance strategy to use when
                             searching the index.
                             Defaults to COSINE. Can be one of COSINE,
-                            EUCLIDEAN_DISTANCE, or DOT_PRODUCT.
+                            EUCLIDEAN_DISTANCE, MAX_INNER_PRODUCT or DOT_PRODUCT.
 
     If you want to use a cloud hosted Elasticsearch instance, you can pass in the
     cloud_id argument instead of the es_url argument.
 
     Example:
         .. code-block:: python
 
@@ -479,16 +493,16 @@
         .. code-block:: python
 
             from langchain_community.vectorstores import ElasticsearchStore
             from langchain_community.embeddings.openai import OpenAIEmbeddings
             from langchain_community.vectorstores.utils import DistanceStrategy
 
             vectorstore = ElasticsearchStore(
+                "langchain-demo",
                 embedding=OpenAIEmbeddings(),
-                index_name="langchain-demo",
                 es_url="http://localhost:9200",
                 distance_strategy="DOT_PRODUCT"
             )
 
     """
 
     def __init__(
@@ -505,40 +519,43 @@
         vector_query_field: str = "vector",
         query_field: str = "text",
         distance_strategy: Optional[
             Literal[
                 DistanceStrategy.COSINE,
                 DistanceStrategy.DOT_PRODUCT,
                 DistanceStrategy.EUCLIDEAN_DISTANCE,
+                DistanceStrategy.MAX_INNER_PRODUCT,
             ]
         ] = None,
         strategy: BaseRetrievalStrategy = ApproxRetrievalStrategy(),
+        es_params: Optional[Dict[str, Any]] = None,
     ):
         self.embedding = embedding
         self.index_name = index_name
         self.query_field = query_field
         self.vector_query_field = vector_query_field
         self.distance_strategy = (
             DistanceStrategy.COSINE
             if distance_strategy is None
             else DistanceStrategy[distance_strategy]
         )
         self.strategy = strategy
 
         if es_connection is not None:
-            self.client = es_connection.options(
-                headers={"user-agent": self.get_user_agent()}
-            )
+            headers = dict(es_connection._headers)
+            headers.update({"user-agent": self.get_user_agent()})
+            self.client = es_connection.options(headers=headers)
         elif es_url is not None or es_cloud_id is not None:
             self.client = ElasticsearchStore.connect_to_elasticsearch(
                 es_url=es_url,
                 username=es_user,
                 password=es_password,
                 cloud_id=es_cloud_id,
                 api_key=es_api_key,
+                es_params=es_params,
             )
         else:
             raise ValueError(
                 """Either provide a pre-existing Elasticsearch connection, \
                 or valid credentials for creating a new connection."""
             )
 
@@ -552,14 +569,15 @@
     def connect_to_elasticsearch(
         *,
         es_url: Optional[str] = None,
         cloud_id: Optional[str] = None,
         api_key: Optional[str] = None,
         username: Optional[str] = None,
         password: Optional[str] = None,
+        es_params: Optional[Dict[str, Any]] = None,
     ) -> "Elasticsearch":
         try:
             import elasticsearch
         except ImportError:
             raise ImportError(
                 "Could not import elasticsearch python package. "
                 "Please install it with `pip install elasticsearch`."
@@ -580,14 +598,17 @@
             raise ValueError("Please provide either elasticsearch_url or cloud_id.")
 
         if api_key:
             connection_params["api_key"] = api_key
         elif username and password:
             connection_params["basic_auth"] = (username, password)
 
+        if es_params is not None:
+            connection_params.update(es_params)
+
         es_client = elasticsearch.Elasticsearch(
             **connection_params,
             headers={"user-agent": ElasticsearchStore.get_user_agent()},
         )
         try:
             es_client.info()
         except Exception as e:
@@ -683,27 +704,49 @@
 
         if remove_vector_query_field_from_metadata:
             for doc in selected_docs:
                 del doc.metadata[self.vector_query_field]
 
         return selected_docs
 
+    @staticmethod
+    def _identity_fn(score: float) -> float:
+        return score
+
+    def _select_relevance_score_fn(self) -> Callable[[float], float]:
+        """
+        The 'correct' relevance function
+        may differ depending on a few things, including:
+        - the distance / similarity metric used by the VectorStore
+        - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)
+        - embedding dimensionality
+        - etc.
+
+        Vectorstores should define their own selection based method of relevance.
+        """
+        # All scores from Elasticsearch are already normalized similarities:
+        # https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html#dense-vector-params
+        return self._identity_fn
+
     def similarity_search_with_score(
         self, query: str, k: int = 4, filter: Optional[List[dict]] = None, **kwargs: Any
     ) -> List[Tuple[Document, float]]:
         """Return Elasticsearch documents most similar to query, along with scores.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
             filter: Array of Elasticsearch filter clauses to apply to the query.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
+        if isinstance(self.strategy, ApproxRetrievalStrategy) and self.strategy.hybrid:
+            raise ValueError("scores are currently not supported in hybrid mode")
+
         return self._search(query=query, k=k, filter=filter, **kwargs)
 
     def similarity_search_by_vector_with_relevance_scores(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[List[Dict]] = None,
@@ -715,14 +758,17 @@
             embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
             filter: Array of Elasticsearch filter clauses to apply to the query.
 
         Returns:
             List of Documents most similar to the embedding and score for each
         """
+        if isinstance(self.strategy, ApproxRetrievalStrategy) and self.strategy.hybrid:
+            raise ValueError("scores are currently not supported in hybrid mode")
+
         return self._search(query_vector=embedding, k=k, filter=filter, **kwargs)
 
     def _search(
         self,
         query: Optional[str] = None,
         k: int = 4,
         query_vector: Union[List[float], None] = None,
@@ -1094,15 +1140,16 @@
             es_connection: Optional pre-existing Elasticsearch connection.
             vector_query_field: Optional. Name of the field to
                                 store the embedding vectors in.
             query_field: Optional. Name of the field to store the texts in.
             distance_strategy: Optional. Name of the distance
                                 strategy to use. Defaults to "COSINE".
                                 can be one of "COSINE",
-                                "EUCLIDEAN_DISTANCE", "DOT_PRODUCT".
+                                "EUCLIDEAN_DISTANCE", "DOT_PRODUCT",
+                                "MAX_INNER_PRODUCT".
             bulk_kwargs: Optional. Additional arguments to pass to
                         Elasticsearch bulk.
         """
 
         elasticsearchStore = ElasticsearchStore._create_cls_from_kwargs(
             embedding=embedding, **kwargs
         )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/epsilla.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/epsilla.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around Epsilla vector database."""
+
 from __future__ import annotations
 
 import logging
 import uuid
 from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Type
 
 from langchain_core.documents import Document
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/faiss.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/faiss.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,15 @@
 from __future__ import annotations
 
-import asyncio
 import logging
 import operator
 import os
 import pickle
 import uuid
 import warnings
-from functools import partial
 from pathlib import Path
 from typing import (
     Any,
     Callable,
     Dict,
     Iterable,
     List,
@@ -20,14 +18,15 @@
     Tuple,
     Union,
 )
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
+from langchain_core.runnables.config import run_in_executor
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.docstore.base import AddableMixin, Docstore
 from langchain_community.docstore.in_memory import InMemoryDocstore
 from langchain_community.vectorstores.utils import (
     DistanceStrategy,
     maximal_marginal_relevance,
@@ -116,17 +115,16 @@
         self.override_relevance_score_fn = relevance_score_fn
         self._normalize_L2 = normalize_L2
         if (
             self.distance_strategy != DistanceStrategy.EUCLIDEAN_DISTANCE
             and self._normalize_L2
         ):
             warnings.warn(
-                "Normalizing L2 is not applicable for metric type: {strategy}".format(
-                    strategy=self.distance_strategy
-                )
+                "Normalizing L2 is not applicable for "
+                f"metric type: {self.distance_strategy}"
             )
 
     @property
     def embeddings(self) -> Optional[Embeddings]:
         return (
             self.embedding_function
             if isinstance(self.embedding_function, Embeddings)
@@ -187,14 +185,17 @@
         documents = [
             Document(page_content=t, metadata=m) for t, m in zip(texts, _metadatas)
         ]
 
         _len_check_if_sized(documents, embeddings, "documents", "embeddings")
         _len_check_if_sized(documents, ids, "documents", "ids")
 
+        if ids and len(ids) != len(set(ids)):
+            raise ValueError("Duplicate ids found in the ids list.")
+
         # Add to the index.
         vector = np.array(embeddings, dtype=np.float32)
         if self._normalize_L2:
             faiss.normalize_L2(vector)
         self.index.add(vector)
 
         # Add information to docstore and index.
@@ -270,24 +271,26 @@
         texts, embeddings = zip(*text_embeddings)
         return self.__add(texts, embeddings, metadatas=metadatas, ids=ids)
 
     def similarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query.
 
         Args:
             embedding: Embedding vector to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, Any]]): Filter by metadata. Defaults to None.
+            filter (Optional[Union[Callable, Dict[str, Any]]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
             **kwargs: kwargs to be passed to similarity search. Can include:
                 score_threshold: Optional, a floating point value between 0 to 1 to
                     filter the resulting set of retrieved docs
 
         Returns:
@@ -296,28 +299,28 @@
         """
         faiss = dependable_faiss_import()
         vector = np.array([embedding], dtype=np.float32)
         if self._normalize_L2:
             faiss.normalize_L2(vector)
         scores, indices = self.index.search(vector, k if filter is None else fetch_k)
         docs = []
+
+        if filter is not None:
+            filter_func = self._create_filter_func(filter)
+
         for j, i in enumerate(indices[0]):
             if i == -1:
                 # This happens when not enough docs are returned.
                 continue
             _id = self.index_to_docstore_id[i]
             doc = self.docstore.search(_id)
             if not isinstance(doc, Document):
                 raise ValueError(f"Could not find document for id {_id}, got {doc}")
             if filter is not None:
-                filter = {
-                    key: [value] if not isinstance(value, list) else value
-                    for key, value in filter.items()
-                }
-                if all(doc.metadata.get(key) in value for key, value in filter.items()):
+                if filter_func(doc.metadata):
                     docs.append((doc, scores[0][j]))
             else:
                 docs.append((doc, scores[0][j]))
 
         score_threshold = kwargs.get("score_threshold")
         if score_threshold is not None:
             cmp = (
@@ -333,60 +336,66 @@
             ]
         return docs[:k]
 
     async def asimilarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query asynchronously.
 
         Args:
             embedding: Embedding vector to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, Any]]): Filter by metadata. Defaults to None.
+            filter (Optional[Dict[str, Any]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
+
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
             **kwargs: kwargs to be passed to similarity search. Can include:
                 score_threshold: Optional, a floating point value between 0 to 1 to
                     filter the resulting set of retrieved docs
 
         Returns:
             List of documents most similar to the query text and L2 distance
             in float for each. Lower score represents more similarity.
         """
 
         # This is a temporary workaround to make the similarity search asynchronous.
-        func = partial(
+        return await run_in_executor(
+            None,
             self.similarity_search_with_score_by_vector,
             embedding,
             k=k,
             filter=filter,
             fetch_k=fetch_k,
             **kwargs,
         )
-        return await asyncio.get_event_loop().run_in_executor(None, func)
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
+
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
 
         Returns:
             List of documents most similar to the query text with
             L2 distance in float. Lower score represents more similarity.
         """
@@ -400,24 +409,27 @@
         )
         return docs
 
     async def asimilarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query asynchronously.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
+
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
 
         Returns:
             List of documents most similar to the query text with
             L2 distance in float. Lower score represents more similarity.
         """
@@ -440,15 +452,18 @@
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to embedding vector.
 
         Args:
             embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
+
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
 
         Returns:
             List of Documents most similar to the embedding.
         """
         docs_and_scores = self.similarity_search_with_score_by_vector(
@@ -460,24 +475,27 @@
         )
         return [doc for doc, _ in docs_and_scores]
 
     async def asimilarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to embedding vector asynchronously.
 
         Args:
             embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None. If a callable, it must take as input the
+                metadata dict of Document and return a bool.
+
             fetch_k: (Optional[int]) Number of Documents to fetch before filtering.
                       Defaults to 20.
 
         Returns:
             List of Documents most similar to the embedding.
         """
         docs_and_scores = await self.asimilarity_search_with_score_by_vector(
@@ -489,15 +507,15 @@
         )
         return [doc for doc, _ in docs_and_scores]
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to query.
 
         Args:
             query: Text to look up documents similar to.
@@ -514,15 +532,15 @@
         )
         return [doc for doc, _ in docs_and_scores]
 
     async def asimilarity_search(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to query asynchronously.
 
         Args:
             query: Text to look up documents similar to.
@@ -542,15 +560,15 @@
     def max_marginal_relevance_search_with_score_by_vector(
         self,
         embedding: List[float],
         *,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
     ) -> List[Tuple[Document, float]]:
         """Return docs and their similarity scores selected using the maximal marginal
             relevance.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -568,61 +586,57 @@
                 relevance and score for each.
         """
         scores, indices = self.index.search(
             np.array([embedding], dtype=np.float32),
             fetch_k if filter is None else fetch_k * 2,
         )
         if filter is not None:
+            filter_func = self._create_filter_func(filter)
             filtered_indices = []
             for i in indices[0]:
                 if i == -1:
                     # This happens when not enough docs are returned.
                     continue
                 _id = self.index_to_docstore_id[i]
                 doc = self.docstore.search(_id)
                 if not isinstance(doc, Document):
                     raise ValueError(f"Could not find document for id {_id}, got {doc}")
-                if all(
-                    doc.metadata.get(key) in value
-                    if isinstance(value, list)
-                    else doc.metadata.get(key) == value
-                    for key, value in filter.items()
-                ):
+                if filter_func(doc.metadata):
                     filtered_indices.append(i)
             indices = np.array([filtered_indices])
         # -1 happens when not enough docs are returned.
         embeddings = [self.index.reconstruct(int(i)) for i in indices[0] if i != -1]
         mmr_selected = maximal_marginal_relevance(
             np.array([embedding], dtype=np.float32),
             embeddings,
             k=k,
             lambda_mult=lambda_mult,
         )
-        selected_indices = [indices[0][i] for i in mmr_selected]
-        selected_scores = [scores[0][i] for i in mmr_selected]
+
         docs_and_scores = []
-        for i, score in zip(selected_indices, selected_scores):
-            if i == -1:
+        for i in mmr_selected:
+            if indices[0][i] == -1:
                 # This happens when not enough docs are returned.
                 continue
-            _id = self.index_to_docstore_id[i]
+            _id = self.index_to_docstore_id[indices[0][i]]
             doc = self.docstore.search(_id)
             if not isinstance(doc, Document):
                 raise ValueError(f"Could not find document for id {_id}, got {doc}")
-            docs_and_scores.append((doc, score))
+            docs_and_scores.append((doc, scores[0][i]))
+
         return docs_and_scores
 
     async def amax_marginal_relevance_search_with_score_by_vector(
         self,
         embedding: List[float],
         *,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
     ) -> List[Tuple[Document, float]]:
         """Return docs and their similarity scores selected using the maximal marginal
             relevance asynchronously.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -636,31 +650,31 @@
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5.
         Returns:
             List of Documents and similarity scores selected by maximal marginal
                 relevance and score for each.
         """
         # This is a temporary workaround to make the similarity search asynchronous.
-        func = partial(
+        return await run_in_executor(
+            None,
             self.max_marginal_relevance_search_with_score_by_vector,
             embedding,
             k=k,
             fetch_k=fetch_k,
             lambda_mult=lambda_mult,
             filter=filter,
         )
-        return await asyncio.get_event_loop().run_in_executor(None, func)
 
     def max_marginal_relevance_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -683,15 +697,15 @@
 
     async def amax_marginal_relevance_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance asynchronously.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -716,15 +730,15 @@
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -753,15 +767,15 @@
 
     async def amax_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance asynchronously.
 
         Maximal marginal relevance optimizes for similarity to query AND diversity
         among selected documents.
 
@@ -804,17 +818,17 @@
         if missing_ids:
             raise ValueError(
                 f"Some specified ids do not exist in the current store. Ids not found: "
                 f"{missing_ids}"
             )
 
         reversed_index = {id_: idx for idx, id_ in self.index_to_docstore_id.items()}
-        index_to_delete = [reversed_index[id_] for id_ in ids]
+        index_to_delete = {reversed_index[id_] for id_ in ids}
 
-        self.index.remove_ids(np.array(index_to_delete, dtype=np.int64))
+        self.index.remove_ids(np.fromiter(index_to_delete, dtype=np.int64))
         self.docstore.delete(ids)
 
         remaining_ids = [
             id_
             for i, id_ in sorted(self.index_to_docstore_id.items())
             if i not in index_to_delete
         ]
@@ -868,19 +882,21 @@
     ) -> FAISS:
         faiss = dependable_faiss_import()
         if distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
             index = faiss.IndexFlatIP(len(embeddings[0]))
         else:
             # Default to L2, currently other metric types not initialized.
             index = faiss.IndexFlatL2(len(embeddings[0]))
+        docstore = kwargs.pop("docstore", InMemoryDocstore())
+        index_to_docstore_id = kwargs.pop("index_to_docstore_id", {})
         vecstore = cls(
             embedding,
             index,
-            InMemoryDocstore(),
-            {},
+            docstore,
+            index_to_docstore_id,
             normalize_L2=normalize_L2,
             distance_strategy=distance_strategy,
             **kwargs,
         )
         vecstore.__add(texts, embeddings, metadatas=metadatas, ids=ids)
         return vecstore
 
@@ -983,19 +999,18 @@
                 from langchain_community.embeddings import OpenAIEmbeddings
 
                 embeddings = OpenAIEmbeddings()
                 text_embeddings = embeddings.embed_documents(texts)
                 text_embedding_pairs = zip(texts, text_embeddings)
                 faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)
         """
-        texts = [t[0] for t in text_embeddings]
-        embeddings = [t[1] for t in text_embeddings]
+        texts, embeddings = zip(*text_embeddings)
         return cls.__from(
-            texts,
-            embeddings,
+            list(texts),
+            list(embeddings),
             embedding,
             metadatas=metadatas,
             ids=ids,
             **kwargs,
         )
 
     @classmethod
@@ -1025,48 +1040,64 @@
             index_name: for saving with a specific index file name
         """
         path = Path(folder_path)
         path.mkdir(exist_ok=True, parents=True)
 
         # save index separately since it is not picklable
         faiss = dependable_faiss_import()
-        faiss.write_index(
-            self.index, str(path / "{index_name}.faiss".format(index_name=index_name))
-        )
+        faiss.write_index(self.index, str(path / f"{index_name}.faiss"))
 
         # save docstore and index_to_docstore_id
-        with open(path / "{index_name}.pkl".format(index_name=index_name), "wb") as f:
+        with open(path / f"{index_name}.pkl", "wb") as f:
             pickle.dump((self.docstore, self.index_to_docstore_id), f)
 
     @classmethod
     def load_local(
         cls,
         folder_path: str,
         embeddings: Embeddings,
         index_name: str = "index",
+        *,
+        allow_dangerous_deserialization: bool = False,
         **kwargs: Any,
     ) -> FAISS:
         """Load FAISS index, docstore, and index_to_docstore_id from disk.
 
         Args:
             folder_path: folder path to load index, docstore,
                 and index_to_docstore_id from.
             embeddings: Embeddings to use when generating queries
             index_name: for saving with a specific index file name
+            allow_dangerous_deserialization: whether to allow deserialization
+                of the data which involves loading a pickle file.
+                Pickle files can be modified by malicious actors to deliver a
+                malicious payload that results in execution of
+                arbitrary code on your machine.
             asynchronous: whether to use async version or not
         """
+        if not allow_dangerous_deserialization:
+            raise ValueError(
+                "The de-serialization relies loading a pickle file. "
+                "Pickle files can be modified to deliver a malicious payload that "
+                "results in execution of arbitrary code on your machine."
+                "You will need to set `allow_dangerous_deserialization` to `True` to "
+                "enable deserialization. If you do this, make sure that you "
+                "trust the source of the data. For example, if you are loading a "
+                "file that you created, and know that no one else has modified the "
+                "file, then this is safe to do. Do not set this to `True` if you are "
+                "loading a file from an untrusted source (e.g., some random site on "
+                "the internet.)."
+            )
         path = Path(folder_path)
         # load index separately since it is not picklable
         faiss = dependable_faiss_import()
-        index = faiss.read_index(
-            str(path / "{index_name}.faiss".format(index_name=index_name))
-        )
+        index = faiss.read_index(str(path / f"{index_name}.faiss"))
 
         # load docstore and index_to_docstore_id
-        with open(path / "{index_name}.pkl".format(index_name=index_name), "rb") as f:
+        with open(path / f"{index_name}.pkl", "rb") as f:
             docstore, index_to_docstore_id = pickle.load(f)
         return cls(embeddings, index, docstore, index_to_docstore_id, **kwargs)
 
     def serialize_to_bytes(self) -> bytes:
         """Serialize FAISS index, docstore, and index_to_docstore_id to bytes."""
         return pickle.dumps((self.index, self.docstore, self.index_to_docstore_id))
 
@@ -1108,15 +1139,15 @@
                 " or euclidean"
             )
 
     def _similarity_search_with_relevance_scores(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs and their similarity scores on a scale from 0 to 1."""
         # Pop score threshold so that only relevancy scores, not raw scores, are
         # filtered.
         relevance_score_fn = self._select_relevance_score_fn()
@@ -1137,15 +1168,15 @@
         ]
         return docs_and_rel_scores
 
     async def _asimilarity_search_with_relevance_scores(
         self,
         query: str,
         k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
+        filter: Optional[Union[Callable, Dict[str, Any]]] = None,
         fetch_k: int = 20,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs and their similarity scores on a scale from 0 to 1."""
         # Pop score threshold so that only relevancy scores, not raw scores, are
         # filtered.
         relevance_score_fn = self._select_relevance_score_fn()
@@ -1161,7 +1192,40 @@
             fetch_k=fetch_k,
             **kwargs,
         )
         docs_and_rel_scores = [
             (doc, relevance_score_fn(score)) for doc, score in docs_and_scores
         ]
         return docs_and_rel_scores
+
+    @staticmethod
+    def _create_filter_func(
+        filter: Optional[Union[Callable, Dict[str, Any]]],
+    ) -> Callable[[Dict[str, Any]], bool]:
+        """
+        Create a filter function based on the provided filter.
+
+        Args:
+            filter: A callable or a dictionary representing the filter
+            conditions for documents.
+
+        Returns:
+            Callable[[Dict[str, Any]], bool]: A function that takes Document's metadata
+            and returns True if it satisfies the filter conditions, otherwise False.
+        """
+        if callable(filter):
+            return filter
+
+        if not isinstance(filter, dict):
+            raise ValueError(
+                f"filter must be a dict of metadata or a callable, not {type(filter)}"
+            )
+
+        def filter_func(metadata: Dict[str, Any]) -> bool:
+            return all(
+                metadata.get(key) in value
+                if isinstance(value, list)
+                else metadata.get(key) == value
+                for key, value in filter.items()  # type: ignore
+            )
+
+        return filter_func
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/hippo.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/hippo.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/hologres.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/hologres.py`

 * *Files 0% similar despite different names*

```diff
@@ -76,15 +76,15 @@
         ids: Optional[List[str]] = None,
         ndims: int = ADA_TOKEN_COUNT,
         table_name: str = _LANGCHAIN_DEFAULT_TABLE_NAME,
         pre_delete_table: bool = False,
         **kwargs: Any,
     ) -> Hologres:
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         connection_string = cls.get_connection_string(kwargs)
 
         store = cls(
@@ -137,15 +137,15 @@
             metadatas: Optional list of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         embeddings = self.embedding_function.embed_documents(list(texts))
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         self.add_embeddings(texts, embeddings, metadatas, ids, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/jaguar.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/jaguar.py`

 * *Files 4% similar despite different names*

```diff
@@ -49,15 +49,15 @@
         self._vector_type = vector_type
         self._vector_dimension = vector_dimension
 
         self._embedding = embedding
         try:
             from jaguardb_http_client.JaguarHttpClient import JaguarHttpClient
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import jaguardb-http-client python package. "
                 "Please install it with `pip install -U jaguardb-http-client`"
             )
 
         self._jag = JaguarHttpClient(url)
         self._token = ""
 
@@ -131,15 +131,15 @@
         except Exception:
             return {}
 
     @property
     def embeddings(self) -> Optional[Embeddings]:
         return self._embedding
 
-    def add_texts(
+    def add_texts(  # type: ignore[override]
         self,
         texts: List[str],
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """
         Add  texts through the embeddings and add to the vectorstore.
@@ -154,33 +154,41 @@
                   file_column=name_of_file_column
 
         Returns:
             List of ids from adding the texts into the vectorstore
         """
         vcol = self._vector_index
         filecol = kwargs.get("file_column", "")
+        text_tag = kwargs.get("text_tag", "")
         podstorevcol = self._pod + "." + self._store + "." + vcol
         q = "textcol " + podstorevcol
         js = self.run(q)
         if js == "":
             return []
         textcol = js["data"]
 
+        if text_tag != "":
+            tag_texts = []
+            for t in texts:
+                tag_texts.append(text_tag + " " + t)
+            texts = tag_texts
+
         embeddings = self._embedding.embed_documents(list(texts))
         ids = []
         if metadatas is None:
             ### no meta and no files to upload
             i = 0
             for vec in embeddings:
                 str_vec = [str(x) for x in vec]
                 values_comma = ",".join(str_vec)
                 podstore = self._pod + "." + self._store
                 q = "insert into " + podstore + " ("
                 q += vcol + "," + textcol + ") values ('" + values_comma
-                q += "','" + texts[i] + "')"
+                txt = texts[i].replace("'", "\\'")
+                q += "','" + txt + "')"
                 js = self.run(q, False)
                 ids.append(js["zid"])
                 i += 1
         else:
             i = 0
             for vec in embeddings:
                 str_vec = [str(x) for x in vec]
@@ -195,15 +203,16 @@
                 values_comma = "'" + "','".join(vvec) + "'"
                 ### 'va1','val2','val3'
                 values_comma += ",'" + ",".join(str_vec) + "'"
                 ### 'v1,v2,v3'
                 podstore = self._pod + "." + self._store
                 q = "insert into " + podstore + " ("
                 q += names_comma + "," + textcol + ") values (" + values_comma
-                q += ",'" + texts[i] + "')"
+                txt = texts[i].replace("'", "\\'")
+                q += ",'" + txt + "')"
                 if filecol != "":
                     js = self.run(q, True)
                 else:
                     js = self.run(q, False)
                 ids.append(js["zid"])
                 i += 1
 
@@ -211,29 +220,27 @@
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 3,
         fetch_k: int = -1,
         where: Optional[str] = None,
-        score_threshold: Optional[float] = -1.0,
+        args: Optional[str] = None,
         metadatas: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """
         Return Jaguar documents most similar to query, along with scores.
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 3.
             lambda_val: lexical match parameter for hybrid search.
             where: the where clause in select similarity. For example a
                 where can be "rating > 3.0 and (state = 'NV' or state = 'CA')"
-            score_threshold: minimal score threshold for the result.
-                If defined, results with score less than this value will be
-                filtered out.
+            args: extra options passed to select similarity
             kwargs:  vector_index=vcol, vector_type=cosine_fraction_float
         Returns:
             List of Documents most similar to the query and score for each.
             List of Tuples of (doc, similarity_score):
                 [ (doc, score), (doc, score), ...]
         """
         vcol = self._vector_index
@@ -250,15 +257,17 @@
             + "','topk="
             + str(k)
             + ",fetch_k="
             + str(fetch_k)
             + ",type="
             + vtype
         )
-        q += ",with_score=yes,with_text=yes,score_threshold=" + str(score_threshold)
+        q += ",with_score=yes,with_text=yes"
+        if args is not None:
+            q += "," + args
 
         if metadatas is not None:
             meta = "&".join(metadatas)
             q += ",metadata=" + meta
 
         q += "') from " + podstore
 
@@ -338,15 +347,15 @@
             return False
         jd = json.loads(js[0])
         if jd["anomalous"] == "YES":
             return True
         return False
 
     @classmethod
-    def from_texts(
+    def from_texts(  # type: ignore[override]
         cls,
         texts: List[str],
         embedding: Embeddings,
         url: str,
         pod: str,
         store: str,
         vector_index: str,
@@ -370,15 +379,15 @@
         Args: No args
         Returns: None
         """
         podstore = self._pod + "." + self._store
         q = "truncate store " + podstore
         self.run(q)
 
-    def delete(self, zids: List[str], **kwargs: Any) -> None:
+    def delete(self, zids: List[str], **kwargs: Any) -> None:  # type: ignore[override]
         """
         Delete records in jaguardb by a list of zero-ids
         Args:
             pod (str):  name of a Pod
             ids (List[str]):  a list of zid as string
         Returns:
             Do not return anything
@@ -418,15 +427,15 @@
         Args: no args
         Returns: None
         """
         self._jag.logout(self._token)
 
     def prt(self, msg: str) -> None:
         with open("/tmp/debugjaguar.log", "a") as file:
-            print(f"msg={msg}", file=file, flush=True)
+            print(f"msg={msg}", file=file, flush=True)  # noqa: T201
 
     def _parseMeta(self, nvmap: dict, filecol: str) -> Tuple[List[str], List[str], str]:
         filepath = ""
         if filecol == "":
             nvec = list(nvmap.keys())
             vvec = list(nvmap.values())
         else:
@@ -438,8 +447,9 @@
                 filepath = nvmap[filecol]
 
             for k, v in nvmap.items():
                 if k != filecol:
                     nvec.append(k)
                     vvec.append(v)
 
-        return nvec, vvec, filepath
+        vvec_s = [str(e) for e in vvec]
+        return nvec, vvec_s, filepath
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/llm_rails.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/llm_rails.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Wrapper around LLMRails vector database."""
+
 from __future__ import annotations
 
 import json
 import logging
 import os
 import uuid
 from typing import Any, Iterable, List, Optional, Tuple
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/marqo.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/marqo.py`

 * *Files 1% similar despite different names*

```diff
@@ -433,18 +433,18 @@
             index_name = str(uuid.uuid4())
 
         client = marqo.Client(url=url, api_key=api_key)
 
         try:
             client.create_index(index_name, settings_dict=index_settings or {})
             if verbose:
-                print(f"Created {index_name} successfully.")
+                print(f"Created {index_name} successfully.")  # noqa: T201
         except Exception:
             if verbose:
-                print(f"Index {index_name} exists.")
+                print(f"Index {index_name} exists.")  # noqa: T201
 
         instance: Marqo = cls(
             client,
             index_name,
             searchable_attributes=searchable_attributes,
             add_documents_settings=add_documents_settings or {},
             page_content_builder=page_content_builder,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/matching_engine.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/matching_engine.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 import json
 import logging
 import time
 import uuid
 from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Type
 
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.utilities.vertexai import get_client_info
 
 if TYPE_CHECKING:
@@ -21,14 +22,19 @@
     from google.oauth2.service_account import Credentials
 
     from langchain_community.embeddings import TensorflowHubEmbeddings
 
 logger = logging.getLogger(__name__)
 
 
+@deprecated(
+    since="0.0.12",
+    removal="0.3.0",
+    alternative_import="langchain_google_vertexai.VectorSearchVectorStore",
+)
 class MatchingEngine(VectorStore):
     """`Google Vertex AI Vector Search` (previously Matching Engine) vector store.
 
     While the embeddings are stored in the Matching Engine, the embedded
     documents will be stored in GCS.
 
     An existing Index and corresponding Endpoint are preconditions for
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/meilisearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/documentdb.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,311 +1,361 @@
 from __future__ import annotations
 
-import uuid
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Type
+import logging
+from enum import Enum
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Dict,
+    Generator,
+    Iterable,
+    List,
+    Optional,
+    TypeVar,
+    Union,
+)
 
 from langchain_core.documents import Document
-from langchain_core.embeddings import Embeddings
-from langchain_core.utils import get_from_env
 from langchain_core.vectorstores import VectorStore
 
 if TYPE_CHECKING:
-    from meilisearch import Client
+    from langchain_core.embeddings import Embeddings
+    from pymongo.collection import Collection
 
 
-def _create_client(
-    client: Optional[Client] = None,
-    url: Optional[str] = None,
-    api_key: Optional[str] = None,
-) -> Client:
-    try:
-        import meilisearch
-    except ImportError:
-        raise ImportError(
-            "Could not import meilisearch python package. "
-            "Please install it with `pip install meilisearch`."
-        )
-    if not client:
-        url = url or get_from_env("url", "MEILI_HTTP_ADDR")
-        try:
-            api_key = api_key or get_from_env("api_key", "MEILI_MASTER_KEY")
-        except Exception:
-            pass
-        client = meilisearch.Client(url=url, api_key=api_key)
-    elif not isinstance(client, meilisearch.Client):
-        raise ValueError(
-            f"client should be an instance of meilisearch.Client, "
-            f"got {type(client)}"
-        )
-    try:
-        client.version()
-    except ValueError as e:
-        raise ValueError(f"Failed to connect to Meilisearch: {e}")
-    return client
+# Before Python 3.11 native StrEnum is not available
+class DocumentDBSimilarityType(str, Enum):
+    """DocumentDB Similarity Type as enumerator."""
+
+    COS = "cosine"
+    """Cosine similarity"""
+    DOT = "dotProduct"
+    """Dot product"""
+    EUC = "euclidean"
+    """Euclidean distance"""
 
 
-class Meilisearch(VectorStore):
-    """`Meilisearch` vector store.
+DocumentDBDocumentType = TypeVar("DocumentDBDocumentType", bound=Dict[str, Any])
 
-    To use this, you need to have `meilisearch` python package installed,
-    and a running Meilisearch instance.
+logger = logging.getLogger(__name__)
 
-    To learn more about Meilisearch Python, refer to the in-depth
-    Meilisearch Python documentation: https://meilisearch.github.io/meilisearch-python/.
+DEFAULT_INSERT_BATCH_SIZE = 128
 
-    See the following documentation for how to run a Meilisearch instance:
-    https://www.meilisearch.com/docs/learn/getting_started/quick_start.
+
+class DocumentDBVectorSearch(VectorStore):
+    """`Amazon DocumentDB (with MongoDB compatibility)` vector store.
+    Please refer to the official Vector Search documentation for more details:
+    https://docs.aws.amazon.com/documentdb/latest/developerguide/vector-search.html
+
+    To use, you should have both:
+    - the ``pymongo`` python package installed
+    - a connection string and credentials associated with a DocumentDB cluster
 
     Example:
-        .. code-block:: python
+        . code-block:: python
 
-            from langchain_community.vectorstores import Meilisearch
+            from langchain_community.vectorstores import DocumentDBVectorSearch
             from langchain_community.embeddings.openai import OpenAIEmbeddings
-            import meilisearch
+            from pymongo import MongoClient
 
-            # api_key is optional; provide it if your meilisearch instance requires it
-            client = meilisearch.Client(url='http://127.0.0.1:7700', api_key='***')
+            mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
+            collection = mongo_client["<db_name>"]["<collection_name>"]
             embeddings = OpenAIEmbeddings()
-            vectorstore = Meilisearch(
-                embedding=embeddings,
-                client=client,
-                index_name='langchain_demo',
-                text_key='text')
+            vectorstore = DocumentDBVectorSearch(collection, embeddings)
     """
 
     def __init__(
         self,
+        collection: Collection[DocumentDBDocumentType],
         embedding: Embeddings,
-        client: Optional[Client] = None,
-        url: Optional[str] = None,
-        api_key: Optional[str] = None,
-        index_name: str = "langchain-demo",
-        text_key: str = "text",
-        metadata_key: str = "metadata",
+        *,
+        index_name: str = "vectorSearchIndex",
+        text_key: str = "textContent",
+        embedding_key: str = "vectorContent",
     ):
-        """Initialize with Meilisearch client."""
-        client = _create_client(client=client, url=url, api_key=api_key)
+        """Constructor for DocumentDBVectorSearch
 
-        self._client = client
-        self._index_name = index_name
+        Args:
+            collection: MongoDB collection to add the texts to.
+            embedding: Text embedding model to use.
+            index_name: Name of the Vector Search index.
+            text_key: MongoDB field that will contain the text
+                for each document.
+            embedding_key: MongoDB field that will contain the embedding
+                for each document.
+        """
+        self._collection = collection
         self._embedding = embedding
+        self._index_name = index_name
         self._text_key = text_key
-        self._metadata_key = metadata_key
+        self._embedding_key = embedding_key
+        self._similarity_type = DocumentDBSimilarityType.COS
 
-    def add_texts(
-        self,
-        texts: Iterable[str],
-        metadatas: Optional[List[dict]] = None,
-        ids: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> List[str]:
-        """Run more texts through the embedding and add them to the vector store.
+    @property
+    def embeddings(self) -> Embeddings:
+        return self._embedding
 
-        Args:
-            texts (Iterable[str]): Iterable of strings/text to add to the vectorstore.
-            metadatas (Optional[List[dict]]): Optional list of metadata.
-                Defaults to None.
-            ids Optional[List[str]]: Optional list of IDs.
-                Defaults to None.
+    def get_index_name(self) -> str:
+        """Returns the index name
 
         Returns:
-            List[str]: List of IDs of the texts added to the vectorstore.
-        """
-        texts = list(texts)
-
-        # Embed and create the documents
-        docs = []
-        if ids is None:
-            ids = [uuid.uuid4().hex for _ in texts]
-        if metadatas is None:
-            metadatas = [{} for _ in texts]
-        embedding_vectors = self._embedding.embed_documents(texts)
-
-        for i, text in enumerate(texts):
-            id = ids[i]
-            metadata = metadatas[i]
-            metadata[self._text_key] = text
-            embedding = embedding_vectors[i]
-            docs.append(
-                {
-                    "id": id,
-                    "_vectors": embedding,
-                    f"{self._metadata_key}": metadata,
-                }
-            )
+            Returns the index name
 
-        # Send to Meilisearch
-        self._client.index(str(self._index_name)).add_documents(docs)
-        return ids
+        """
+        return self._index_name
 
-    def similarity_search(
-        self,
-        query: str,
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+    @classmethod
+    def from_connection_string(
+        cls,
+        connection_string: str,
+        namespace: str,
+        embedding: Embeddings,
         **kwargs: Any,
-    ) -> List[Document]:
-        """Return meilisearch documents most similar to the query.
+    ) -> DocumentDBVectorSearch:
+        """Creates an Instance of DocumentDBVectorSearch from a Connection String
 
         Args:
-            query (str): Query text for which to find similar documents.
-            k (int): Number of documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata.
-                Defaults to None.
+            connection_string: The DocumentDB cluster endpoint connection string
+            namespace: The namespace (database.collection)
+            embedding: The embedding utility
+            **kwargs: Dynamic keyword arguments
 
         Returns:
-            List[Document]: List of Documents most similar to the query
-            text and score for each.
-        """
-        docs_and_scores = self.similarity_search_with_score(
-            query=query,
-            k=k,
-            filter=filter,
-            kwargs=kwargs,
-        )
-        return [doc for doc, _ in docs_and_scores]
-
-    def similarity_search_with_score(
-        self,
-        query: str,
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-        **kwargs: Any,
-    ) -> List[Tuple[Document, float]]:
-        """Return meilisearch documents most similar to the query, along with scores.
+            an instance of the vector store
 
-        Args:
-            query (str): Query text for which to find similar documents.
-            k (int): Number of documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata.
-                Defaults to None.
+        """
+        try:
+            from pymongo import MongoClient
+        except ImportError:
+            raise ImportError(
+                "Could not import pymongo, please install it with "
+                "`pip install pymongo`."
+            )
+        client: MongoClient = MongoClient(connection_string)
+        db_name, collection_name = namespace.split(".")
+        collection = client[db_name][collection_name]
+        return cls(collection, embedding, **kwargs)
+
+    def index_exists(self) -> bool:
+        """Verifies if the specified index name during instance
+            construction exists on the collection
 
         Returns:
-            List[Document]: List of Documents most similar to the query
-            text and score for each.
+          Returns True on success and False if no such index exists
+            on the collection
         """
-        _query = self._embedding.embed_query(query)
+        cursor = self._collection.list_indexes()
+        index_name = self._index_name
 
-        docs = self.similarity_search_by_vector_with_scores(
-            embedding=_query,
-            k=k,
-            filter=filter,
-            kwargs=kwargs,
-        )
-        return docs
+        for res in cursor:
+            current_index_name = res.pop("name")
+            if current_index_name == index_name:
+                return True
+
+        return False
+
+    def delete_index(self) -> None:
+        """Deletes the index specified during instance construction if it exists"""
+        if self.index_exists():
+            self._collection.drop_index(self._index_name)
+            # Raises OperationFailure on an error (e.g. trying to drop
+            # an index that does not exist)
 
-    def similarity_search_by_vector_with_scores(
+    def create_index(
         self,
-        embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, Any]] = None,
-        **kwargs: Any,
-    ) -> List[Tuple[Document, float]]:
-        """Return meilisearch documents most similar to embedding vector.
+        dimensions: int = 1536,
+        similarity: DocumentDBSimilarityType = DocumentDBSimilarityType.COS,
+        m: int = 16,
+        ef_construction: int = 64,
+    ) -> dict[str, Any]:
+        """Creates an index using the index name specified at
+            instance construction
 
         Args:
-            embedding (List[float]): Embedding to look up similar documents.
-            k (int): Number of documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata.
-                Defaults to None.
+            dimensions: Number of dimensions for vector similarity.
+                The maximum number of supported dimensions is 2000
+
+            similarity: Similarity algorithm to use with the HNSW index.
+
+            m: Specifies the max number of connections for an HNSW index.
+                Large impact on memory consumption.
+
+            ef_construction: Specifies the size of the dynamic candidate list
+                for constructing the graph for HNSW index. Higher values lead
+                to more accurate results but slower indexing speed.
+
+                Possible options are:
+                    - DocumentDBSimilarityType.COS (cosine distance),
+                    - DocumentDBSimilarityType.EUC (Euclidean distance), and
+                    - DocumentDBSimilarityType.DOT (dot product).
 
         Returns:
-            List[Document]: List of Documents most similar to the query
-                vector and score for each.
+            An object describing the created index
+
         """
-        docs = []
-        results = self._client.index(str(self._index_name)).search(
-            "", {"vector": embedding, "limit": k, "filter": filter}
-        )
+        self._similarity_type = similarity
 
-        for result in results["hits"]:
-            metadata = result[self._metadata_key]
-            if self._text_key in metadata:
-                text = metadata.pop(self._text_key)
-                semantic_score = result["_semanticScore"]
-                docs.append(
-                    (Document(page_content=text, metadata=metadata), semantic_score)
-                )
+        # prepare the command
+        create_index_commands = {
+            "createIndexes": self._collection.name,
+            "indexes": [
+                {
+                    "name": self._index_name,
+                    "key": {self._embedding_key: "vector"},
+                    "vectorOptions": {
+                        "type": "hnsw",
+                        "similarity": similarity,
+                        "dimensions": dimensions,
+                        "m": m,
+                        "efConstruction": ef_construction,
+                    },
+                }
+            ],
+        }
 
-        return docs
+        # retrieve the database object
+        current_database = self._collection.database
 
-    def similarity_search_by_vector(
+        # invoke the command from the database object
+        create_index_responses: dict[str, Any] = current_database.command(
+            create_index_commands
+        )
+
+        return create_index_responses
+
+    def add_texts(
         self,
-        embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
+        texts: Iterable[str],
+        metadatas: Optional[List[Dict[str, Any]]] = None,
         **kwargs: Any,
-    ) -> List[Document]:
-        """Return meilisearch documents most similar to embedding vector.
+    ) -> List:
+        batch_size = kwargs.get("batch_size", DEFAULT_INSERT_BATCH_SIZE)
+        _metadatas: Union[List, Generator] = metadatas or ({} for _ in texts)
+        texts_batch = []
+        metadatas_batch = []
+        result_ids = []
+        for i, (text, metadata) in enumerate(zip(texts, _metadatas)):
+            texts_batch.append(text)
+            metadatas_batch.append(metadata)
+            if (i + 1) % batch_size == 0:
+                result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
+                texts_batch = []
+                metadatas_batch = []
+        if texts_batch:
+            result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
+        return result_ids
+
+    def _insert_texts(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> List:
+        """Used to Load Documents into the collection
 
         Args:
-            embedding (List[float]): Embedding to look up similar documents.
-            k (int): Number of documents to return. Defaults to 4.
-            filter (Optional[Dict[str, str]]): Filter by metadata.
-                Defaults to None.
+            texts: The list of documents strings to load
+            metadatas: The list of metadata objects associated with each document
 
         Returns:
-            List[Document]: List of Documents most similar to the query
-                vector and score for each.
+
         """
-        docs = self.similarity_search_by_vector_with_scores(
-            embedding=embedding,
-            k=k,
-            filter=filter,
-            kwargs=kwargs,
-        )
-        return [doc for doc, _ in docs]
+        # If the text is empty, then exit early
+        if not texts:
+            return []
+
+        # Embed and create the documents
+        embeddings = self._embedding.embed_documents(texts)
+        to_insert = [
+            {self._text_key: t, self._embedding_key: embedding, **m}
+            for t, m, embedding in zip(texts, metadatas, embeddings)
+        ]
+        # insert the documents in DocumentDB
+        insert_result = self._collection.insert_many(to_insert)  # type: ignore
+        return insert_result.inserted_ids
 
     @classmethod
     def from_texts(
-        cls: Type[Meilisearch],
+        cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
-        client: Optional[Client] = None,
-        url: Optional[str] = None,
-        api_key: Optional[str] = None,
-        index_name: str = "langchain-demo",
-        ids: Optional[List[str]] = None,
-        text_key: Optional[str] = "text",
-        metadata_key: Optional[str] = "metadata",
+        collection: Optional[Collection[DocumentDBDocumentType]] = None,
         **kwargs: Any,
-    ) -> Meilisearch:
-        """Construct Meilisearch wrapper from raw documents.
+    ) -> DocumentDBVectorSearch:
+        if collection is None:
+            raise ValueError("Must provide 'collection' named parameter.")
+        vectorstore = cls(collection, embedding, **kwargs)
+        vectorstore.add_texts(texts, metadatas=metadatas)
+        return vectorstore
+
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
+        if ids is None:
+            raise ValueError("No document ids provided to delete.")
 
-        This is a user-friendly interface that:
-            1. Embeds documents.
-            2. Adds the documents to a provided Meilisearch index.
-
-        This is intended to be a quick way to get started.
-
-        Example:
-            .. code-block:: python
-
-                from langchain_community.vectorstores import Meilisearch
-                from langchain_community.embeddings import OpenAIEmbeddings
-                import meilisearch
-
-                # The environment should be the one specified next to the API key
-                # in your Meilisearch console
-                client = meilisearch.Client(url='http://127.0.0.1:7700', api_key='***')
-                embeddings = OpenAIEmbeddings()
-                docsearch = Meilisearch.from_texts(
-                    client=client,
-                    embeddings=embeddings,
-                )
+        for document_id in ids:
+            self.delete_document_by_id(document_id)
+        return True
+
+    def delete_document_by_id(self, document_id: Optional[str] = None) -> None:
+        """Removes a Specific Document by Id
+
+        Args:
+            document_id: The document identifier
         """
-        client = _create_client(client=client, url=url, api_key=api_key)
+        try:
+            from bson.objectid import ObjectId
+        except ImportError as e:
+            raise ImportError(
+                "Unable to import bson, please install with `pip install bson`."
+            ) from e
+        if document_id is None:
+            raise ValueError("No document id provided to delete.")
 
-        vectorstore = cls(
-            embedding=embedding,
-            client=client,
-            index_name=index_name,
-        )
-        vectorstore.add_texts(
-            texts=texts,
-            metadatas=metadatas,
-            ids=ids,
-            text_key=text_key,
-            metadata_key=metadata_key,
+        self._collection.delete_one({"_id": ObjectId(document_id)})
+
+    def _similarity_search_without_score(
+        self, embeddings: List[float], k: int = 4, ef_search: int = 40
+    ) -> List[Document]:
+        """Returns a list of documents.
+
+        Args:
+            embeddings: The query vector
+            k: the number of documents to return
+            ef_search: Specifies the size of the dynamic candidate list
+                that HNSW index uses during search. A higher value of
+                efSearch provides better recall at cost of speed.
+
+        Returns:
+            A list of documents closest to the query vector
+        """
+        pipeline: List[dict[str, Any]] = [
+            {
+                "$search": {
+                    "vectorSearch": {
+                        "vector": embeddings,
+                        "path": self._embedding_key,
+                        "similarity": self._similarity_type,
+                        "k": k,
+                        "efSearch": ef_search,
+                    }
+                }
+            }
+        ]
+
+        cursor = self._collection.aggregate(pipeline)
+
+        docs = []
+
+        for res in cursor:
+            text = res.pop(self._text_key)
+            docs.append(Document(page_content=text, metadata=res))
+
+        return docs
+
+    def similarity_search(
+        self,
+        query: str,
+        k: int = 4,
+        ef_search: int = 40,
+        **kwargs: Any,
+    ) -> List[Document]:
+        embeddings = self._embedding.embed_query(query)
+        docs = self._similarity_search_without_score(
+            embeddings=embeddings, k=k, ef_search=ef_search
         )
-        return vectorstore
+        return [doc for doc in docs]
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/milvus.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/milvus.py`

 * *Files 18% similar despite different names*

```diff
@@ -36,27 +36,39 @@
 
     IF USING L2/IP metric, IT IS HIGHLY SUGGESTED TO NORMALIZE YOUR DATA.
 
     Args:
         embedding_function (Embeddings): Function used to embed the text.
         collection_name (str): Which Milvus collection to use. Defaults to
             "LangChainCollection".
+        collection_description (str): The description of the collection. Defaults to
+            "".
+        collection_properties (Optional[dict[str, any]]): The collection properties.
+            Defaults to None.
+            If set, will override collection existing properties.
+            For example: {"collection.ttl.seconds": 60}.
         connection_args (Optional[dict[str, any]]): The connection args used for
             this class comes in the form of a dict.
         consistency_level (str): The consistency level to use for a collection.
             Defaults to "Session".
         index_params (Optional[dict]): Which index params to use. Defaults to
             HNSW/AUTOINDEX depending on service.
         search_params (Optional[dict]): Which search params to use. Defaults to
             default of index.
         drop_old (Optional[bool]): Whether to drop the current collection. Defaults
             to False.
+        auto_id (bool): Whether to enable auto id for primary key. Defaults to False.
+            If False, you needs to provide text ids (string less than 65535 bytes).
+            If True, Milvus will generate unique integers as primary keys.
         primary_field (str): Name of the primary key field. Defaults to "pk".
         text_field (str): Name of the text field. Defaults to "text".
         vector_field (str): Name of the vector field. Defaults to "vector".
+        metadata_field (str): Name of the metadta field. Defaults to None.
+            When metadata_field is specified,
+            the document's metadata will store as json.
 
     The connection args used for this class comes in the form of a dict,
     here are a few of the options:
         address (str): The actual address of Milvus
             instance. Example address: "localhost:19530"
         uri (str): The uri of Milvus instance. Example uri:
             "http://randomwebsite:19530",
@@ -89,39 +101,49 @@
 
         embedding = OpenAIEmbeddings()
         # Connect to a milvus instance on localhost
         milvus_store = Milvus(
             embedding_function = Embeddings,
             collection_name = "LangChainCollection",
             drop_old = True,
+            auto_id = True
         )
 
     Raises:
         ValueError: If the pymilvus python package is not installed.
     """
 
     def __init__(
         self,
         embedding_function: Embeddings,
         collection_name: str = "LangChainCollection",
+        collection_description: str = "",
+        collection_properties: Optional[dict[str, Any]] = None,
         connection_args: Optional[dict[str, Any]] = None,
         consistency_level: str = "Session",
         index_params: Optional[dict] = None,
         search_params: Optional[dict] = None,
         drop_old: Optional[bool] = False,
+        auto_id: bool = False,
         *,
         primary_field: str = "pk",
         text_field: str = "text",
         vector_field: str = "vector",
+        metadata_field: Optional[str] = None,
+        partition_key_field: Optional[str] = None,
+        partition_names: Optional[list] = None,
+        replica_number: int = 1,
+        timeout: Optional[float] = None,
+        num_shards: Optional[int] = None,
     ):
         """Initialize the Milvus vector store."""
         try:
             from pymilvus import Collection, utility
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import pymilvus python package. "
                 "Please install it with `pip install pymilvus`."
             )
 
         # Default search params when one is not provided.
         self.default_search_params = {
             "IVF_FLAT": {"metric_type": "L2", "params": {"nprobe": 10}},
@@ -129,49 +151,78 @@
             "IVF_PQ": {"metric_type": "L2", "params": {"nprobe": 10}},
             "HNSW": {"metric_type": "L2", "params": {"ef": 10}},
             "RHNSW_FLAT": {"metric_type": "L2", "params": {"ef": 10}},
             "RHNSW_SQ": {"metric_type": "L2", "params": {"ef": 10}},
             "RHNSW_PQ": {"metric_type": "L2", "params": {"ef": 10}},
             "IVF_HNSW": {"metric_type": "L2", "params": {"nprobe": 10, "ef": 10}},
             "ANNOY": {"metric_type": "L2", "params": {"search_k": 10}},
+            "SCANN": {"metric_type": "L2", "params": {"search_k": 10}},
             "AUTOINDEX": {"metric_type": "L2", "params": {}},
+            "GPU_CAGRA": {
+                "metric_type": "L2",
+                "params": {
+                    "itopk_size": 128,
+                    "search_width": 4,
+                    "min_iterations": 0,
+                    "max_iterations": 0,
+                    "team_size": 0,
+                },
+            },
+            "GPU_IVF_FLAT": {"metric_type": "L2", "params": {"nprobe": 10}},
+            "GPU_IVF_PQ": {"metric_type": "L2", "params": {"nprobe": 10}},
         }
 
         self.embedding_func = embedding_function
         self.collection_name = collection_name
+        self.collection_description = collection_description
+        self.collection_properties = collection_properties
         self.index_params = index_params
         self.search_params = search_params
         self.consistency_level = consistency_level
+        self.auto_id = auto_id
 
-        # In order for a collection to be compatible, pk needs to be auto'id and int
+        # In order for a collection to be compatible, pk needs to be varchar
         self._primary_field = primary_field
         # In order for compatibility, the text field will need to be called "text"
         self._text_field = text_field
         # In order for compatibility, the vector field needs to be called "vector"
         self._vector_field = vector_field
+        self._metadata_field = metadata_field
+        self._partition_key_field = partition_key_field
         self.fields: list[str] = []
+        self.partition_names = partition_names
+        self.replica_number = replica_number
+        self.timeout = timeout
+        self.num_shards = num_shards
+
         # Create the connection to the server
         if connection_args is None:
             connection_args = DEFAULT_MILVUS_CONNECTION
         self.alias = self._create_connection_alias(connection_args)
         self.col: Optional[Collection] = None
 
         # Grab the existing collection if it exists
         if utility.has_collection(self.collection_name, using=self.alias):
             self.col = Collection(
                 self.collection_name,
                 using=self.alias,
             )
+            if self.collection_properties is not None:
+                self.col.set_properties(self.collection_properties)
         # If need to drop old, drop it
         if drop_old and isinstance(self.col, Collection):
             self.col.drop()
             self.col = None
 
         # Initialize the vector store
-        self._init()
+        self._init(
+            partition_names=partition_names,
+            replica_number=replica_number,
+            timeout=timeout,
+        )
 
     @property
     def embeddings(self) -> Embeddings:
         return self.embedding_func
 
     def _create_connection_alias(self, connection_args: dict) -> str:
         """Create the connection to the Milvus server."""
@@ -184,15 +235,21 @@
         uri: str = connection_args.get("uri", None)
         user = connection_args.get("user", None)
 
         # Order of use is host/port, uri, address
         if host is not None and port is not None:
             given_address = str(host) + ":" + str(port)
         elif uri is not None:
-            given_address = uri.split("https://")[1]
+            if uri.startswith("https://"):
+                given_address = uri.split("https://")[1]
+            elif uri.startswith("http://"):
+                given_address = uri.split("http://")[1]
+            else:
+                logger.error("Invalid Milvus URI: %s", uri)
+                raise ValueError("Invalid Milvus URI: %s", uri)
         elif address is not None:
             given_address = address
         else:
             given_address = None
             logger.debug("Missing standard address type for reuse attempt")
 
         # User defaults to empty string when getting connection info
@@ -222,22 +279,31 @@
             logger.debug("Created new connection using: %s", alias)
             return alias
         except MilvusException as e:
             logger.error("Failed to create new connection using: %s", alias)
             raise e
 
     def _init(
-        self, embeddings: Optional[list] = None, metadatas: Optional[list[dict]] = None
+        self,
+        embeddings: Optional[list] = None,
+        metadatas: Optional[list[dict]] = None,
+        partition_names: Optional[list] = None,
+        replica_number: int = 1,
+        timeout: Optional[float] = None,
     ) -> None:
         if embeddings is not None:
             self._create_collection(embeddings, metadatas)
         self._extract_fields()
         self._create_index()
         self._create_search_params()
-        self._load()
+        self._load(
+            partition_names=partition_names,
+            replica_number=replica_number,
+            timeout=timeout,
+        )
 
     def _create_collection(
         self, embeddings: list, metadatas: Optional[list[dict]] = None
     ) -> None:
         from pymilvus import (
             Collection,
             CollectionSchema,
@@ -246,75 +312,110 @@
             MilvusException,
         )
         from pymilvus.orm.types import infer_dtype_bydata
 
         # Determine embedding dim
         dim = len(embeddings[0])
         fields = []
-        # Determine metadata schema
-        if metadatas:
-            # Create FieldSchema for each entry in metadata.
-            for key, value in metadatas[0].items():
-                # Infer the corresponding datatype of the metadata
-                dtype = infer_dtype_bydata(value)
-                # Datatype isn't compatible
-                if dtype == DataType.UNKNOWN or dtype == DataType.NONE:
-                    logger.error(
-                        "Failure to create collection, unrecognized dtype for key: %s",
-                        key,
-                    )
-                    raise ValueError(f"Unrecognized datatype for {key}.")
-                # Dataype is a string/varchar equivalent
-                elif dtype == DataType.VARCHAR:
-                    fields.append(FieldSchema(key, DataType.VARCHAR, max_length=65_535))
-                else:
-                    fields.append(FieldSchema(key, dtype))
+        if self._metadata_field is not None:
+            fields.append(FieldSchema(self._metadata_field, DataType.JSON))
+        else:
+            # Determine metadata schema
+            if metadatas:
+                # Create FieldSchema for each entry in metadata.
+                for key, value in metadatas[0].items():
+                    # Infer the corresponding datatype of the metadata
+                    dtype = infer_dtype_bydata(value)
+                    # Datatype isn't compatible
+                    if dtype == DataType.UNKNOWN or dtype == DataType.NONE:
+                        logger.error(
+                            (
+                                "Failure to create collection, "
+                                "unrecognized dtype for key: %s"
+                            ),
+                            key,
+                        )
+                        raise ValueError(f"Unrecognized datatype for {key}.")
+                    # Dataype is a string/varchar equivalent
+                    elif dtype == DataType.VARCHAR:
+                        fields.append(
+                            FieldSchema(key, DataType.VARCHAR, max_length=65_535)
+                        )
+                    else:
+                        fields.append(FieldSchema(key, dtype))
 
         # Create the text field
         fields.append(
             FieldSchema(self._text_field, DataType.VARCHAR, max_length=65_535)
         )
         # Create the primary key field
-        fields.append(
-            FieldSchema(
-                self._primary_field, DataType.INT64, is_primary=True, auto_id=True
+        if self.auto_id:
+            fields.append(
+                FieldSchema(
+                    self._primary_field, DataType.INT64, is_primary=True, auto_id=True
+                )
+            )
+        else:
+            fields.append(
+                FieldSchema(
+                    self._primary_field,
+                    DataType.VARCHAR,
+                    is_primary=True,
+                    auto_id=False,
+                    max_length=65_535,
+                )
             )
-        )
         # Create the vector field, supports binary or float vectors
         fields.append(
             FieldSchema(self._vector_field, infer_dtype_bydata(embeddings[0]), dim=dim)
         )
 
         # Create the schema for the collection
-        schema = CollectionSchema(fields)
+        schema = CollectionSchema(
+            fields,
+            description=self.collection_description,
+            partition_key_field=self._partition_key_field,
+        )
 
         # Create the collection
         try:
-            self.col = Collection(
-                name=self.collection_name,
-                schema=schema,
-                consistency_level=self.consistency_level,
-                using=self.alias,
-            )
+            if self.num_shards is not None:
+                # Issue with defaults:
+                # https://github.com/milvus-io/pymilvus/blob/59bf5e811ad56e20946559317fed855330758d9c/pymilvus/client/prepare.py#L82-L85
+                self.col = Collection(
+                    name=self.collection_name,
+                    schema=schema,
+                    consistency_level=self.consistency_level,
+                    using=self.alias,
+                    num_shards=self.num_shards,
+                )
+            else:
+                self.col = Collection(
+                    name=self.collection_name,
+                    schema=schema,
+                    consistency_level=self.consistency_level,
+                    using=self.alias,
+                )
+            # Set the collection properties if they exist
+            if self.collection_properties is not None:
+                self.col.set_properties(self.collection_properties)
         except MilvusException as e:
             logger.error(
                 "Failed to create collection: %s error: %s", self.collection_name, e
             )
             raise e
 
     def _extract_fields(self) -> None:
         """Grab the existing fields from the Collection"""
         from pymilvus import Collection
 
         if isinstance(self.col, Collection):
             schema = self.col.schema
             for x in schema.fields:
                 self.fields.append(x.name)
-            # Since primary field is auto-id, no need to track it
-            self.fields.remove(self._primary_field)
 
     def _get_index(self) -> Optional[dict[str, Any]]:
         """Return the vector index information if it exists"""
         from pymilvus import Collection
 
         if isinstance(self.col, Collection):
             for x in self.col.indexes:
@@ -375,99 +476,151 @@
             index = self._get_index()
             if index is not None:
                 index_type: str = index["index_param"]["index_type"]
                 metric_type: str = index["index_param"]["metric_type"]
                 self.search_params = self.default_search_params[index_type]
                 self.search_params["metric_type"] = metric_type
 
-    def _load(self) -> None:
+    def _load(
+        self,
+        partition_names: Optional[list] = None,
+        replica_number: int = 1,
+        timeout: Optional[float] = None,
+    ) -> None:
         """Load the collection if available."""
-        from pymilvus import Collection
+        from pymilvus import Collection, utility
+        from pymilvus.client.types import LoadState
 
-        if isinstance(self.col, Collection) and self._get_index() is not None:
-            self.col.load()
+        timeout = self.timeout or timeout
+        if (
+            isinstance(self.col, Collection)
+            and self._get_index() is not None
+            and utility.load_state(self.collection_name, using=self.alias)
+            == LoadState.NotLoad
+        ):
+            self.col.load(
+                partition_names=partition_names,
+                replica_number=replica_number,
+                timeout=timeout,
+            )
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         batch_size: int = 1000,
+        *,
+        ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """Insert text data into Milvus.
 
         Inserting data when the collection has not be made yet will result
         in creating a new Collection. The data of the first entity decides
         the schema of the new collection, the dim is extracted from the first
         embedding and the columns are decided by the first metadata dict.
-        Metada keys will need to be present for all inserted values. At
+        Metadata keys will need to be present for all inserted values. At
         the moment there is no None equivalent in Milvus.
 
         Args:
             texts (Iterable[str]): The texts to embed, it is assumed
                 that they all fit in memory.
             metadatas (Optional[List[dict]]): Metadata dicts attached to each of
                 the texts. Defaults to None.
-            timeout (Optional[int]): Timeout for each batch insert. Defaults
+            should be less than 65535 bytes. Required and work when auto_id is False.
+            timeout (Optional[float]): Timeout for each batch insert. Defaults
                 to None.
             batch_size (int, optional): Batch size to use for insertion.
                 Defaults to 1000.
+            ids (Optional[List[str]]): List of text ids. The length of each item
 
         Raises:
             MilvusException: Failure to add texts
 
         Returns:
             List[str]: The resulting keys for each inserted element.
         """
         from pymilvus import Collection, MilvusException
 
         texts = list(texts)
+        if not self.auto_id:
+            assert isinstance(
+                ids, list
+            ), "A list of valid ids are required when auto_id is False."
+            assert len(set(ids)) == len(
+                texts
+            ), "Different lengths of texts and unique ids are provided."
+            assert all(
+                len(x.encode()) <= 65_535 for x in ids
+            ), "Each id should be a string less than 65535 bytes."
 
         try:
             embeddings = self.embedding_func.embed_documents(texts)
         except NotImplementedError:
             embeddings = [self.embedding_func.embed_query(x) for x in texts]
 
         if len(embeddings) == 0:
             logger.debug("Nothing to insert, skipping.")
             return []
 
         # If the collection hasn't been initialized yet, perform all steps to do so
         if not isinstance(self.col, Collection):
-            self._init(embeddings, metadatas)
+            kwargs = {"embeddings": embeddings, "metadatas": metadatas}
+            if self.partition_names:
+                kwargs["partition_names"] = self.partition_names
+            if self.replica_number:
+                kwargs["replica_number"] = self.replica_number
+            if self.timeout:
+                kwargs["timeout"] = self.timeout
+            self._init(**kwargs)
 
         # Dict to hold all insert columns
         insert_dict: dict[str, list] = {
             self._text_field: texts,
             self._vector_field: embeddings,
         }
 
-        # Collect the metadata into the insert dict.
-        if metadatas is not None:
-            for d in metadatas:
-                for key, value in d.items():
-                    if key in self.fields:
-                        insert_dict.setdefault(key, []).append(value)
+        if not self.auto_id:
+            insert_dict[self._primary_field] = ids  # type: ignore[assignment]
+
+        if self._metadata_field is not None:
+            for d in metadatas:  # type: ignore[union-attr]
+                insert_dict.setdefault(self._metadata_field, []).append(d)
+        else:
+            # Collect the metadata into the insert dict.
+            if metadatas is not None:
+                for d in metadatas:
+                    for key, value in d.items():
+                        keys = (
+                            [x for x in self.fields if x != self._primary_field]
+                            if self.auto_id
+                            else [x for x in self.fields]
+                        )
+                        if key in keys:
+                            insert_dict.setdefault(key, []).append(value)
 
         # Total insert count
         vectors: list = insert_dict[self._vector_field]
         total_count = len(vectors)
 
         pks: list[str] = []
 
         assert isinstance(self.col, Collection)
         for i in range(0, total_count, batch_size):
             # Grab end index
             end = min(i + batch_size, total_count)
             # Convert dict to list of lists batch for insertion
-            insert_list = [insert_dict[x][i:end] for x in self.fields]
+            insert_list = [
+                insert_dict[x][i:end] for x in self.fields if x in insert_dict
+            ]
             # Insert into the collection.
             try:
                 res: Collection
+                timeout = self.timeout or timeout
                 res = self.col.insert(insert_list, timeout=timeout, **kwargs)
                 pks.extend(res.primary_keys)
             except MilvusException as e:
                 logger.error(
                     "Failed to insert batch starting at entity: %s/%s", i, total_count
                 )
                 raise e
@@ -475,15 +628,15 @@
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Perform a similarity search against the query string.
 
         Args:
             query (str): The text to search.
             k (int, optional): How many results to return. Defaults to 4.
@@ -496,26 +649,27 @@
 
         Returns:
             List[Document]: Document results for search.
         """
         if self.col is None:
             logger.debug("No existing collection to search.")
             return []
+        timeout = self.timeout or timeout
         res = self.similarity_search_with_score(
             query=query, k=k, param=param, expr=expr, timeout=timeout, **kwargs
         )
         return [doc for doc, _ in res]
 
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Perform a similarity search against the query string.
 
         Args:
             embedding (List[float]): The embedding vector to search.
             k (int, optional): How many results to return. Defaults to 4.
@@ -528,128 +682,129 @@
 
         Returns:
             List[Document]: Document results for search.
         """
         if self.col is None:
             logger.debug("No existing collection to search.")
             return []
+        timeout = self.timeout or timeout
         res = self.similarity_search_with_score_by_vector(
             embedding=embedding, k=k, param=param, expr=expr, timeout=timeout, **kwargs
         )
         return [doc for doc, _ in res]
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Perform a search on a query string and return results with score.
 
         For more information about the search parameters, take a look at the pymilvus
         documentation found here:
         https://milvus.io/api-reference/pymilvus/v2.2.6/Collection/search().md
 
         Args:
             query (str): The text being searched.
             k (int, optional): The amount of results to return. Defaults to 4.
             param (dict): The search params for the specified index.
                 Defaults to None.
             expr (str, optional): Filtering expression. Defaults to None.
-            timeout (int, optional): How long to wait before timeout error.
+            timeout (float, optional): How long to wait before timeout error.
                 Defaults to None.
             kwargs: Collection.search() keyword arguments.
 
         Returns:
             List[float], List[Tuple[Document, any, any]]:
         """
         if self.col is None:
             logger.debug("No existing collection to search.")
             return []
 
         # Embed the query text.
         embedding = self.embedding_func.embed_query(query)
-
+        timeout = self.timeout or timeout
         res = self.similarity_search_with_score_by_vector(
             embedding=embedding, k=k, param=param, expr=expr, timeout=timeout, **kwargs
         )
         return res
 
     def similarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Perform a search on a query string and return results with score.
 
         For more information about the search parameters, take a look at the pymilvus
         documentation found here:
         https://milvus.io/api-reference/pymilvus/v2.2.6/Collection/search().md
 
         Args:
             embedding (List[float]): The embedding vector being searched.
             k (int, optional): The amount of results to return. Defaults to 4.
             param (dict): The search params for the specified index.
                 Defaults to None.
             expr (str, optional): Filtering expression. Defaults to None.
-            timeout (int, optional): How long to wait before timeout error.
+            timeout (float, optional): How long to wait before timeout error.
                 Defaults to None.
             kwargs: Collection.search() keyword arguments.
 
         Returns:
             List[Tuple[Document, float]]: Result doc and score.
         """
         if self.col is None:
             logger.debug("No existing collection to search.")
             return []
 
         if param is None:
             param = self.search_params
 
-        # Determine result metadata fields.
+        # Determine result metadata fields with PK.
         output_fields = self.fields[:]
         output_fields.remove(self._vector_field)
-
+        timeout = self.timeout or timeout
         # Perform the search.
         res = self.col.search(
             data=[embedding],
             anns_field=self._vector_field,
             param=param,
             limit=k,
             expr=expr,
             output_fields=output_fields,
             timeout=timeout,
             **kwargs,
         )
         # Organize results.
         ret = []
         for result in res[0]:
-            meta = {x: result.entity.get(x) for x in output_fields}
-            doc = Document(page_content=meta.pop(self._text_field), metadata=meta)
+            data = {x: result.entity.get(x) for x in output_fields}
+            doc = self._parse_document(data)
             pair = (doc, result.score)
             ret.append(pair)
 
         return ret
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Perform a search and return results that are reordered by MMR.
 
         Args:
             query (str): The text being searched.
             k (int, optional): How many results to give. Defaults to 4.
@@ -658,28 +813,28 @@
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5
             param (dict, optional): The search params for the specified index.
                 Defaults to None.
             expr (str, optional): Filtering expression. Defaults to None.
-            timeout (int, optional): How long to wait before timeout error.
+            timeout (float, optional): How long to wait before timeout error.
                 Defaults to None.
             kwargs: Collection.search() keyword arguments.
 
 
         Returns:
             List[Document]: Document results for search.
         """
         if self.col is None:
             logger.debug("No existing collection to search.")
             return []
 
         embedding = self.embedding_func.embed_query(query)
-
+        timeout = self.timeout or timeout
         return self.max_marginal_relevance_search_by_vector(
             embedding=embedding,
             k=k,
             fetch_k=fetch_k,
             lambda_mult=lambda_mult,
             param=param,
             expr=expr,
@@ -691,15 +846,15 @@
         self,
         embedding: list[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
         param: Optional[dict] = None,
         expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        timeout: Optional[float] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Perform a search and return results that are reordered by MMR.
 
         Args:
             embedding (str): The embedding vector being searched.
             k (int, optional): How many results to give. Defaults to 4.
@@ -708,15 +863,15 @@
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5
             param (dict, optional): The search params for the specified index.
                 Defaults to None.
             expr (str, optional): Filtering expression. Defaults to None.
-            timeout (int, optional): How long to wait before timeout error.
+            timeout (float, optional): How long to wait before timeout error.
                 Defaults to None.
             kwargs: Collection.search() keyword arguments.
 
         Returns:
             List[Document]: Document results for search.
         """
         if self.col is None:
@@ -725,15 +880,15 @@
 
         if param is None:
             param = self.search_params
 
         # Determine result metadata fields.
         output_fields = self.fields[:]
         output_fields.remove(self._vector_field)
-
+        timeout = self.timeout or timeout
         # Perform the search.
         res = self.col.search(
             data=[embedding],
             anns_field=self._vector_field,
             param=param,
             limit=fetch_k,
             expr=expr,
@@ -742,16 +897,16 @@
             **kwargs,
         )
         # Organize results.
         ids = []
         documents = []
         scores = []
         for result in res[0]:
-            meta = {x: result.entity.get(x) for x in output_fields}
-            doc = Document(page_content=meta.pop(self._text_field), metadata=meta)
+            data = {x: result.entity.get(x) for x in output_fields}
+            doc = self._parse_document(data)
             documents.append(doc)
             scores.append(result.score)
             ids.append(result.id)
 
         vectors = self.col.query(
             expr=f"{self._primary_field} in {ids}",
             output_fields=[self._primary_field, self._vector_field],
@@ -773,26 +928,52 @@
             # Function can return -1 index
             if x == -1:
                 break
             else:
                 ret.append(documents[x])
         return ret
 
+    def delete(  # type: ignore[no-untyped-def]
+        self, ids: Optional[List[str]] = None, expr: Optional[str] = None, **kwargs: str
+    ):
+        """Delete by vector ID or boolean expression.
+        Refer to [Milvus documentation](https://milvus.io/docs/delete_data.md)
+        for notes and examples of expressions.
+
+        Args:
+            ids: List of ids to delete.
+            expr: Boolean expression that specifies the entities to delete.
+            kwargs: Other parameters in Milvus delete api.
+        """
+        if isinstance(ids, list) and len(ids) > 0:
+            if expr is not None:
+                logger.warning(
+                    "Both ids and expr are provided. " "Ignore expr and delete by ids."
+                )
+            expr = f"{self._primary_field} in {ids}"
+        else:
+            assert isinstance(
+                expr, str
+            ), "Either ids list or expr string must be provided."
+        return self.col.delete(expr=expr, **kwargs)  # type: ignore[union-attr]
+
     @classmethod
     def from_texts(
         cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         collection_name: str = "LangChainCollection",
         connection_args: dict[str, Any] = DEFAULT_MILVUS_CONNECTION,
         consistency_level: str = "Session",
         index_params: Optional[dict] = None,
         search_params: Optional[dict] = None,
         drop_old: bool = False,
+        *,
+        ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Milvus:
         """Create a Milvus collection, indexes it with HNSW, and insert data.
 
         Args:
             texts (List[str]): Text data.
             embedding (Embeddings): Embedding function.
@@ -806,23 +987,97 @@
                 to "Session".
             index_params (Optional[dict], optional): Which index_params to use. Defaults
                 to None.
             search_params (Optional[dict], optional): Which search params to use.
                 Defaults to None.
             drop_old (Optional[bool], optional): Whether to drop the collection with
                 that name if it exists. Defaults to False.
+            ids (Optional[List[str]]): List of text ids. Defaults to None.
 
         Returns:
             Milvus: Milvus Vector Store
         """
+        if isinstance(ids, list) and len(ids) > 0:
+            auto_id = False
+        else:
+            auto_id = True
+
         vector_db = cls(
             embedding_function=embedding,
             collection_name=collection_name,
             connection_args=connection_args,
             consistency_level=consistency_level,
             index_params=index_params,
             search_params=search_params,
             drop_old=drop_old,
+            auto_id=auto_id,
             **kwargs,
         )
-        vector_db.add_texts(texts=texts, metadatas=metadatas)
+        vector_db.add_texts(texts=texts, metadatas=metadatas, ids=ids)
         return vector_db
+
+    def _parse_document(self, data: dict) -> Document:
+        return Document(
+            page_content=data.pop(self._text_field),
+            metadata=data.pop(self._metadata_field) if self._metadata_field else data,
+        )
+
+    def get_pks(self, expr: str, **kwargs: Any) -> List[int] | None:
+        """Get primary keys with expression
+
+        Args:
+            expr: Expression - E.g: "id in [1, 2]", or "title LIKE 'Abc%'"
+
+        Returns:
+            List[int]: List of IDs (Primary Keys)
+        """
+
+        from pymilvus import MilvusException
+
+        if self.col is None:
+            logger.debug("No existing collection to get pk.")
+            return None
+
+        try:
+            query_result = self.col.query(
+                expr=expr, output_fields=[self._primary_field]
+            )
+        except MilvusException as exc:
+            logger.error("Failed to get ids: %s error: %s", self.collection_name, exc)
+            raise exc
+        pks = [item.get(self._primary_field) for item in query_result]
+        return pks
+
+    def upsert(
+        self,
+        ids: Optional[List[str]] = None,
+        documents: List[Document] | None = None,
+        **kwargs: Any,
+    ) -> List[str] | None:
+        """Update/Insert documents to the vectorstore.
+
+        Args:
+            ids: IDs to update - Let's call get_pks to get ids with expression \n
+            documents (List[Document]): Documents to add to the vectorstore.
+
+        Returns:
+            List[str]: IDs of the added texts.
+        """
+
+        from pymilvus import MilvusException
+
+        if documents is None or len(documents) == 0:
+            logger.debug("No documents to upsert.")
+            return None
+
+        if ids is not None and len(ids):
+            try:
+                self.delete(ids=ids)
+            except MilvusException:
+                pass
+        try:
+            return self.add_documents(documents=documents, **kwargs)
+        except MilvusException as exc:
+            logger.error(
+                "Failed to upsert entities: %s error: %s", self.collection_name, exc
+            )
+            raise exc
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/momento_vector_index.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/momento_vector_index.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/mongodb_atlas.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/azure_cosmos_db.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,160 +1,320 @@
 from __future__ import annotations
 
 import logging
+from enum import Enum
 from typing import (
     TYPE_CHECKING,
     Any,
-    Callable,
     Dict,
     Generator,
     Iterable,
     List,
     Optional,
     Tuple,
     TypeVar,
     Union,
 )
 
 import numpy as np
 from langchain_core.documents import Document
-from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 if TYPE_CHECKING:
+    from langchain_core.embeddings import Embeddings
     from pymongo.collection import Collection
 
-MongoDBDocumentType = TypeVar("MongoDBDocumentType", bound=Dict[str, Any])
+
+# Before Python 3.11 native StrEnum is not available
+class CosmosDBSimilarityType(str, Enum):
+    """Cosmos DB Similarity Type as enumerator."""
+
+    COS = "COS"
+    """CosineSimilarity"""
+    IP = "IP"
+    """inner - product"""
+    L2 = "L2"
+    """Euclidean distance"""
+
+
+class CosmosDBVectorSearchType(str, Enum):
+    """Cosmos DB Vector Search Type as enumerator."""
+
+    VECTOR_IVF = "vector-ivf"
+    """IVF vector index"""
+    VECTOR_HNSW = "vector-hnsw"
+    """HNSW vector index"""
+
+
+CosmosDBDocumentType = TypeVar("CosmosDBDocumentType", bound=Dict[str, Any])
 
 logger = logging.getLogger(__name__)
 
-DEFAULT_INSERT_BATCH_SIZE = 100
+DEFAULT_INSERT_BATCH_SIZE = 128
 
 
-class MongoDBAtlasVectorSearch(VectorStore):
-    """`MongoDB Atlas Vector Search` vector store.
+class AzureCosmosDBVectorSearch(VectorStore):
+    """`Azure Cosmos DB for MongoDB vCore` vector store.
 
     To use, you should have both:
     - the ``pymongo`` python package installed
-    - a connection string associated with a MongoDB Atlas Cluster having deployed an
-        Atlas Search index
+    - a connection string associated with a MongoDB VCore Cluster
 
     Example:
-        .. code-block:: python
+        . code-block:: python
 
-            from langchain_community.vectorstores import MongoDBAtlasVectorSearch
+            from langchain_community.vectorstores import AzureCosmosDBVectorSearch
             from langchain_community.embeddings.openai import OpenAIEmbeddings
             from pymongo import MongoClient
 
             mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
             collection = mongo_client["<db_name>"]["<collection_name>"]
             embeddings = OpenAIEmbeddings()
-            vectorstore = MongoDBAtlasVectorSearch(collection, embeddings)
+            vectorstore = AzureCosmosDBVectorSearch(collection, embeddings)
     """
 
     def __init__(
         self,
-        collection: Collection[MongoDBDocumentType],
+        collection: Collection[CosmosDBDocumentType],
         embedding: Embeddings,
         *,
-        index_name: str = "default",
-        text_key: str = "text",
-        embedding_key: str = "embedding",
-        relevance_score_fn: str = "cosine",
+        index_name: str = "vectorSearchIndex",
+        text_key: str = "textContent",
+        embedding_key: str = "vectorContent",
     ):
-        """
+        """Constructor for AzureCosmosDBVectorSearch
+
         Args:
             collection: MongoDB collection to add the texts to.
             embedding: Text embedding model to use.
-            text_key: MongoDB field that will contain the text for each
-                document.
-            embedding_key: MongoDB field that will contain the embedding for
-                each document.
             index_name: Name of the Atlas Search index.
-            relevance_score_fn: The similarity score used for the index.
-            Currently supported: Euclidean, cosine, and dot product.
+            text_key: MongoDB field that will contain the text
+                for each document.
+            embedding_key: MongoDB field that will contain the embedding
+                for each document.
         """
         self._collection = collection
         self._embedding = embedding
         self._index_name = index_name
         self._text_key = text_key
         self._embedding_key = embedding_key
-        self._relevance_score_fn = relevance_score_fn
 
     @property
     def embeddings(self) -> Embeddings:
         return self._embedding
 
-    def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        if self._relevance_score_fn == "euclidean":
-            return self._euclidean_relevance_score_fn
-        elif self._relevance_score_fn == "dotProduct":
-            return self._max_inner_product_relevance_score_fn
-        elif self._relevance_score_fn == "cosine":
-            return self._cosine_relevance_score_fn
-        else:
-            raise NotImplementedError(
-                f"No relevance score function for ${self._relevance_score_fn}"
-            )
+    def get_index_name(self) -> str:
+        """Returns the index name
+
+        Returns:
+            Returns the index name
+
+        """
+        return self._index_name
 
     @classmethod
     def from_connection_string(
         cls,
         connection_string: str,
         namespace: str,
         embedding: Embeddings,
+        application_name: str = "LANGCHAIN_PYTHON",
         **kwargs: Any,
-    ) -> MongoDBAtlasVectorSearch:
-        """Construct a `MongoDB Atlas Vector Search` vector store
-        from a MongoDB connection URI.
+    ) -> AzureCosmosDBVectorSearch:
+        """Creates an Instance of AzureCosmosDBVectorSearch from a Connection String
 
         Args:
-            connection_string: A valid MongoDB connection URI.
-            namespace: A valid MongoDB namespace (database and collection).
-            embedding: The text embedding model to use for the vector store.
+            connection_string: The MongoDB vCore instance connection string
+            namespace: The namespace (database.collection)
+            embedding: The embedding utility
+            **kwargs: Dynamic keyword arguments
 
         Returns:
-            A new MongoDBAtlasVectorSearch instance.
+            an instance of the vector store
 
         """
         try:
-            from importlib.metadata import version
-
             from pymongo import MongoClient
-            from pymongo.driver_info import DriverInfo
         except ImportError:
             raise ImportError(
                 "Could not import pymongo, please install it with "
                 "`pip install pymongo`."
             )
-        client: MongoClient = MongoClient(
-            connection_string,
-            driver=DriverInfo(name="Langchain", version=version("langchain")),
-        )
+        appname = application_name
+        client: MongoClient = MongoClient(connection_string, appname=appname)
         db_name, collection_name = namespace.split(".")
         collection = client[db_name][collection_name]
         return cls(collection, embedding, **kwargs)
 
+    def index_exists(self) -> bool:
+        """Verifies if the specified index name during instance
+            construction exists on the collection
+
+        Returns:
+          Returns True on success and False if no such index exists
+            on the collection
+        """
+        cursor = self._collection.list_indexes()
+        index_name = self._index_name
+
+        for res in cursor:
+            current_index_name = res.pop("name")
+            if current_index_name == index_name:
+                return True
+
+        return False
+
+    def delete_index(self) -> None:
+        """Deletes the index specified during instance construction if it exists"""
+        if self.index_exists():
+            self._collection.drop_index(self._index_name)
+            # Raises OperationFailure on an error (e.g. trying to drop
+            # an index that does not exist)
+
+    def create_index(
+        self,
+        num_lists: int = 100,
+        dimensions: int = 1536,
+        similarity: CosmosDBSimilarityType = CosmosDBSimilarityType.COS,
+        kind: str = "vector-ivf",
+        m: int = 16,
+        ef_construction: int = 64,
+    ) -> dict[str, Any]:
+        """Creates an index using the index name specified at
+            instance construction
+
+        Setting the numLists parameter correctly is important for achieving
+            good accuracy and performance.
+            Since the vector store uses IVF as the indexing strategy,
+            you should create the index only after you
+            have loaded a large enough sample documents to ensure that the
+            centroids for the respective buckets are
+            faily distributed.
+
+        We recommend that numLists is set to documentCount/1000 for up
+            to 1 million documents
+            and to sqrt(documentCount) for more than 1 million documents.
+            As the number of items in your database grows, you should
+            tune numLists to be larger
+            in order to achieve good latency performance for vector search.
+
+            If you're experimenting with a new scenario or creating a
+            small demo, you can start with numLists
+            set to 1 to perform a brute-force search across all vectors.
+            This should provide you with the most
+            accurate results from the vector search, however be aware that
+            the search speed and latency will be slow.
+            After your initial setup, you should go ahead and tune
+            the numLists parameter using the above guidance.
+
+        Args:
+            kind: Type of vector index to create.
+                Possible options are:
+                    - vector-ivf
+                    - vector-hnsw: available as a preview feature only,
+                                   to enable visit https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/preview-features
+            num_lists: This integer is the number of clusters that the
+                inverted file (IVF) index uses to group the vector data.
+                We recommend that numLists is set to documentCount/1000
+                for up to 1 million documents and to sqrt(documentCount)
+                for more than 1 million documents.
+                Using a numLists value of 1 is akin to performing
+                brute-force search, which has limited performance
+            dimensions: Number of dimensions for vector similarity.
+                The maximum number of supported dimensions is 2000
+            similarity: Similarity metric to use with the IVF index.
+
+                Possible options are:
+                    - CosmosDBSimilarityType.COS (cosine distance),
+                    - CosmosDBSimilarityType.L2 (Euclidean distance), and
+                    - CosmosDBSimilarityType.IP (inner product).
+            m: The max number of connections per layer (16 by default, minimum
+               value is 2, maximum value is 100). Higher m is suitable for datasets
+               with high dimensionality and/or high accuracy requirements.
+            ef_construction: the size of the dynamic candidate list for constructing
+                            the graph (64 by default, minimum value is 4, maximum
+                            value is 1000). Higher ef_construction will result in
+                            better index quality and higher accuracy, but it will
+                            also increase the time required to build the index.
+                            ef_construction has to be at least 2 * m
+        Returns:
+            An object describing the created index
+
+        """
+        # check the kind of vector search to be performed
+        # prepare the command accordingly
+        create_index_commands = {}
+        if kind == CosmosDBVectorSearchType.VECTOR_IVF:
+            create_index_commands = self._get_vector_index_ivf(
+                kind, num_lists, similarity, dimensions
+            )
+        elif kind == CosmosDBVectorSearchType.VECTOR_HNSW:
+            create_index_commands = self._get_vector_index_hnsw(
+                kind, m, ef_construction, similarity, dimensions
+            )
+
+        # retrieve the database object
+        current_database = self._collection.database
+
+        # invoke the command from the database object
+        create_index_responses: dict[str, Any] = current_database.command(
+            create_index_commands
+        )
+
+        return create_index_responses
+
+    def _get_vector_index_ivf(
+        self, kind: str, num_lists: int, similarity: str, dimensions: int
+    ) -> Dict[str, Any]:
+        command = {
+            "createIndexes": self._collection.name,
+            "indexes": [
+                {
+                    "name": self._index_name,
+                    "key": {self._embedding_key: "cosmosSearch"},
+                    "cosmosSearchOptions": {
+                        "kind": kind,
+                        "numLists": num_lists,
+                        "similarity": similarity,
+                        "dimensions": dimensions,
+                    },
+                }
+            ],
+        }
+        return command
+
+    def _get_vector_index_hnsw(
+        self, kind: str, m: int, ef_construction: int, similarity: str, dimensions: int
+    ) -> Dict[str, Any]:
+        command = {
+            "createIndexes": self._collection.name,
+            "indexes": [
+                {
+                    "name": self._index_name,
+                    "key": {self._embedding_key: "cosmosSearch"},
+                    "cosmosSearchOptions": {
+                        "kind": kind,
+                        "m": m,
+                        "efConstruction": ef_construction,
+                        "similarity": similarity,
+                        "dimensions": dimensions,
+                    },
+                }
+            ],
+        }
+        return command
+
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[Dict[str, Any]]] = None,
         **kwargs: Any,
     ) -> List:
-        """Run more texts through the embeddings and add to the vectorstore.
-
-        Args:
-            texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
-
-        Returns:
-            List of ids from adding the texts into the vectorstore.
-        """
         batch_size = kwargs.get("batch_size", DEFAULT_INSERT_BATCH_SIZE)
         _metadatas: Union[List, Generator] = metadatas or ({} for _ in texts)
         texts_batch = []
         metadatas_batch = []
         result_ids = []
         for i, (text, metadata) in enumerate(zip(texts, _metadatas)):
             texts_batch.append(text)
@@ -164,210 +324,262 @@
                 texts_batch = []
                 metadatas_batch = []
         if texts_batch:
             result_ids.extend(self._insert_texts(texts_batch, metadatas_batch))
         return result_ids
 
     def _insert_texts(self, texts: List[str], metadatas: List[Dict[str, Any]]) -> List:
+        """Used to Load Documents into the collection
+
+        Args:
+            texts: The list of documents strings to load
+            metadatas: The list of metadata objects associated with each document
+
+        Returns:
+
+        """
+        # If the text is empty, then exit early
         if not texts:
             return []
+
         # Embed and create the documents
         embeddings = self._embedding.embed_documents(texts)
         to_insert = [
             {self._text_key: t, self._embedding_key: embedding, **m}
             for t, m, embedding in zip(texts, metadatas, embeddings)
         ]
-        # insert the documents in MongoDB Atlas
+        # insert the documents in Cosmos DB
         insert_result = self._collection.insert_many(to_insert)  # type: ignore
         return insert_result.inserted_ids
 
+    @classmethod
+    def from_texts(
+        cls,
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        collection: Optional[Collection[CosmosDBDocumentType]] = None,
+        **kwargs: Any,
+    ) -> AzureCosmosDBVectorSearch:
+        if collection is None:
+            raise ValueError("Must provide 'collection' named parameter.")
+        vectorstore = cls(collection, embedding, **kwargs)
+        vectorstore.add_texts(texts, metadatas=metadatas)
+        return vectorstore
+
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
+        if ids is None:
+            raise ValueError("No document ids provided to delete.")
+
+        for document_id in ids:
+            self.delete_document_by_id(document_id)
+        return True
+
+    def delete_document_by_id(self, document_id: Optional[str] = None) -> None:
+        """Removes a Specific Document by Id
+
+        Args:
+            document_id: The document identifier
+        """
+        try:
+            from bson.objectid import ObjectId
+        except ImportError as e:
+            raise ImportError(
+                "Unable to import bson, please install with `pip install bson`."
+            ) from e
+        if document_id is None:
+            raise ValueError("No document id provided to delete.")
+
+        self._collection.delete_one({"_id": ObjectId(document_id)})
+
     def _similarity_search_with_score(
         self,
-        embedding: List[float],
+        embeddings: List[float],
         k: int = 4,
-        pre_filter: Optional[Dict] = None,
-        post_filter_pipeline: Optional[List[Dict]] = None,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        ef_search: int = 40,
+        score_threshold: float = 0.0,
     ) -> List[Tuple[Document, float]]:
-        params = {
-            "queryVector": embedding,
-            "path": self._embedding_key,
-            "numCandidates": k * 10,
-            "limit": k,
-            "index": self._index_name,
-        }
-        if pre_filter:
-            params["filter"] = pre_filter
-        query = {"$vectorSearch": params}
+        """Returns a list of documents with their scores
+
+        Args:
+            embeddings: The query vector
+            k: the number of documents to return
+            kind: Type of vector index to create.
+                Possible options are:
+                    - vector-ivf
+                    - vector-hnsw: available as a preview feature only,
+                                   to enable visit https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/preview-features
+            ef_search: The size of the dynamic candidate list for search
+                       (40 by default). A higher value provides better
+                       recall at the cost of speed.
+            score_threshold: (Optional[float], optional): Maximum vector distance
+                between selected documents and the query vector. Defaults to None.
+                Only vector-ivf search supports this for now.
+
+        Returns:
+            A list of documents closest to the query vector
+        """
+        pipeline: List[dict[str, Any]] = []
+        if kind == CosmosDBVectorSearchType.VECTOR_IVF:
+            pipeline = self._get_pipeline_vector_ivf(embeddings, k)
+        elif kind == CosmosDBVectorSearchType.VECTOR_HNSW:
+            pipeline = self._get_pipeline_vector_hnsw(embeddings, k, ef_search)
+
+        cursor = self._collection.aggregate(pipeline)
 
-        pipeline = [
-            query,
-            {"$set": {"score": {"$meta": "vectorSearchScore"}}},
-        ]
-        if post_filter_pipeline is not None:
-            pipeline.extend(post_filter_pipeline)
-        cursor = self._collection.aggregate(pipeline)  # type: ignore[arg-type]
         docs = []
         for res in cursor:
-            text = res.pop(self._text_key)
-            score = res.pop("score")
-            docs.append((Document(page_content=text, metadata=res), score))
+            score = res.pop("similarityScore")
+            if score < score_threshold:
+                continue
+            document_object_field = (
+                res.pop("document")
+                if kind == CosmosDBVectorSearchType.VECTOR_IVF
+                else res
+            )
+            text = document_object_field.pop(self._text_key)
+            docs.append(
+                (Document(page_content=text, metadata=document_object_field), score)
+            )
         return docs
 
+    def _get_pipeline_vector_ivf(
+        self, embeddings: List[float], k: int = 4
+    ) -> List[dict[str, Any]]:
+        pipeline: List[dict[str, Any]] = [
+            {
+                "$search": {
+                    "cosmosSearch": {
+                        "vector": embeddings,
+                        "path": self._embedding_key,
+                        "k": k,
+                    },
+                    "returnStoredSource": True,
+                }
+            },
+            {
+                "$project": {
+                    "similarityScore": {"$meta": "searchScore"},
+                    "document": "$$ROOT",
+                }
+            },
+        ]
+        return pipeline
+
+    def _get_pipeline_vector_hnsw(
+        self, embeddings: List[float], k: int = 4, ef_search: int = 40
+    ) -> List[dict[str, Any]]:
+        pipeline: List[dict[str, Any]] = [
+            {
+                "$search": {
+                    "cosmosSearch": {
+                        "vector": embeddings,
+                        "path": self._embedding_key,
+                        "k": k,
+                        "efSearch": ef_search,
+                    },
+                }
+            },
+            {
+                "$project": {
+                    "similarityScore": {"$meta": "searchScore"},
+                    "document": "$$ROOT",
+                }
+            },
+        ]
+        return pipeline
+
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        pre_filter: Optional[Dict] = None,
-        post_filter_pipeline: Optional[List[Dict]] = None,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        ef_search: int = 40,
+        score_threshold: float = 0.0,
     ) -> List[Tuple[Document, float]]:
-        """Return MongoDB documents most similar to the given query and their scores.
-
-        Uses the knnBeta Operator available in MongoDB Atlas Search.
-        This feature is in early access and available only for evaluation purposes, to
-        validate functionality, and to gather feedback from a small closed group of
-        early access users. It is not recommended for production deployments as we
-        may introduce breaking changes.
-        For more: https://www.mongodb.com/docs/atlas/atlas-search/knn-beta
-
-        Args:
-            query: Text to look up documents similar to.
-            k: (Optional) number of documents to return. Defaults to 4.
-            pre_filter: (Optional) dictionary of argument(s) to prefilter document
-                fields on.
-            post_filter_pipeline: (Optional) Pipeline of MongoDB aggregation stages
-                following the knnBeta vector search.
-
-        Returns:
-            List of documents most similar to the query and their scores.
-        """
-        embedding = self._embedding.embed_query(query)
+        embeddings = self._embedding.embed_query(query)
         docs = self._similarity_search_with_score(
-            embedding,
+            embeddings=embeddings,
             k=k,
-            pre_filter=pre_filter,
-            post_filter_pipeline=post_filter_pipeline,
+            kind=kind,
+            ef_search=ef_search,
+            score_threshold=score_threshold,
         )
         return docs
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
-        pre_filter: Optional[Dict] = None,
-        post_filter_pipeline: Optional[List[Dict]] = None,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        ef_search: int = 40,
+        score_threshold: float = 0.0,
         **kwargs: Any,
     ) -> List[Document]:
-        """Return MongoDB documents most similar to the given query.
-
-        Uses the knnBeta Operator available in MongoDB Atlas Search.
-        This feature is in early access and available only for evaluation purposes, to
-        validate functionality, and to gather feedback from a small closed group of
-        early access users. It is not recommended for production deployments as we
-        may introduce breaking changes.
-        For more: https://www.mongodb.com/docs/atlas/atlas-search/knn-beta
-
-        Args:
-            query: Text to look up documents similar to.
-            k: (Optional) number of documents to return. Defaults to 4.
-            pre_filter: (Optional) dictionary of argument(s) to prefilter document
-                fields on.
-            post_filter_pipeline: (Optional) Pipeline of MongoDB aggregation stages
-                following the knnBeta vector search.
-
-        Returns:
-            List of documents most similar to the query and their scores.
-        """
         docs_and_scores = self.similarity_search_with_score(
             query,
             k=k,
-            pre_filter=pre_filter,
-            post_filter_pipeline=post_filter_pipeline,
+            kind=kind,
+            ef_search=ef_search,
+            score_threshold=score_threshold,
         )
         return [doc for doc, _ in docs_and_scores]
 
-    def max_marginal_relevance_search(
+    def max_marginal_relevance_search_by_vector(
         self,
-        query: str,
+        embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
-        pre_filter: Optional[Dict] = None,
-        post_filter_pipeline: Optional[List[Dict]] = None,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        ef_search: int = 40,
+        score_threshold: float = 0.0,
         **kwargs: Any,
     ) -> List[Document]:
-        """Return documents selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-
-        Args:
-            query: Text to look up documents similar to.
-            k: (Optional) number of documents to return. Defaults to 4.
-            fetch_k: (Optional) number of documents to fetch before passing to MMR
-                algorithm. Defaults to 20.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
-                        Defaults to 0.5.
-            pre_filter: (Optional) dictionary of argument(s) to prefilter on document
-                fields.
-            post_filter_pipeline: (Optional) pipeline of MongoDB aggregation stages
-                following the knnBeta vector search.
-        Returns:
-            List of documents selected by maximal marginal relevance.
-        """
-        query_embedding = self._embedding.embed_query(query)
+        # Retrieves the docs with similarity scores
+        # sorted by similarity scores in DESC order
         docs = self._similarity_search_with_score(
-            query_embedding,
+            embedding,
             k=fetch_k,
-            pre_filter=pre_filter,
-            post_filter_pipeline=post_filter_pipeline,
+            kind=kind,
+            ef_search=ef_search,
+            score_threshold=score_threshold,
         )
+
+        # Re-ranks the docs using MMR
         mmr_doc_indexes = maximal_marginal_relevance(
-            np.array(query_embedding),
+            np.array(embedding),
             [doc.metadata[self._embedding_key] for doc, _ in docs],
             k=k,
             lambda_mult=lambda_mult,
         )
         mmr_docs = [docs[i][0] for i in mmr_doc_indexes]
         return mmr_docs
 
-    @classmethod
-    def from_texts(
-        cls,
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[Dict]] = None,
-        collection: Optional[Collection[MongoDBDocumentType]] = None,
+    def max_marginal_relevance_search(
+        self,
+        query: str,
+        k: int = 4,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        kind: CosmosDBVectorSearchType = CosmosDBVectorSearchType.VECTOR_IVF,
+        ef_search: int = 40,
+        score_threshold: float = 0.0,
         **kwargs: Any,
-    ) -> MongoDBAtlasVectorSearch:
-        """Construct a `MongoDB Atlas Vector Search` vector store from raw documents.
+    ) -> List[Document]:
+        # compute the embeddings vector from the query string
+        embeddings = self._embedding.embed_query(query)
 
-        This is a user-friendly interface that:
-            1. Embeds documents.
-            2. Adds the documents to a provided MongoDB Atlas Vector Search index
-                (Lucene)
-
-        This is intended to be a quick way to get started.
-
-        Example:
-            .. code-block:: python
-                from pymongo import MongoClient
-
-                from langchain_community.vectorstores import MongoDBAtlasVectorSearch
-                from langchain_community.embeddings import OpenAIEmbeddings
-
-                mongo_client = MongoClient("<YOUR-CONNECTION-STRING>")
-                collection = mongo_client["<db_name>"]["<collection_name>"]
-                embeddings = OpenAIEmbeddings()
-                vectorstore = MongoDBAtlasVectorSearch.from_texts(
-                    texts,
-                    embeddings,
-                    metadatas=metadatas,
-                    collection=collection
-                )
-        """
-        if collection is None:
-            raise ValueError("Must provide 'collection' named parameter.")
-        vectorstore = cls(collection, embedding, **kwargs)
-        vectorstore.add_texts(texts, metadatas=metadatas)
-        return vectorstore
+        docs = self.max_marginal_relevance_search_by_vector(
+            embeddings,
+            k=k,
+            fetch_k=fetch_k,
+            lambda_mult=lambda_mult,
+            kind=kind,
+            ef_search=ef_search,
+            score_threshold=score_threshold,
+        )
+        return docs
+
+    def get_collection(self) -> Collection[CosmosDBDocumentType]:
+        return self._collection
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/myscale.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/myscale.py`

 * *Files 0% similar despite different names*

```diff
@@ -466,16 +466,17 @@
             Optional[bool]: True if deletion is successful,
             False otherwise, None if not implemented.
         """
         assert not (
             ids is None and where_str is None
         ), "You need to specify where to be deleted! Either with `ids` or `where_str`"
         conds = []
-        if ids:
-            conds.extend([f"{self.config.column_map['id']} = '{id}'" for id in ids])
+        if ids and len(ids) > 0:
+            id_list = ", ".join([f"'{id}'" for id in ids])
+            conds.append(f"{self.config.column_map['id']} IN ({id_list})")
         if where_str:
             conds.append(where_str)
         assert len(conds) > 0
         where_str_final = " AND ".join(conds)
         qstr = (
             f"DELETE FROM {self.config.database}.{self.config.table} "
             f"WHERE {where_str_final}"
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/neo4j_vector.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/neo4j_vector.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,87 +1,136 @@
 from __future__ import annotations
 
 import enum
 import logging
 import os
-import uuid
+from hashlib import md5
 from typing import (
     Any,
     Callable,
     Dict,
     Iterable,
     List,
     Optional,
     Tuple,
     Type,
 )
 
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
-from langchain_core.utils import get_from_env
+from langchain_core.utils import get_from_dict_or_env
 from langchain_core.vectorstores import VectorStore
 
+from langchain_community.graphs import Neo4jGraph
 from langchain_community.vectorstores.utils import DistanceStrategy
 
 DEFAULT_DISTANCE_STRATEGY = DistanceStrategy.COSINE
 DISTANCE_MAPPING = {
     DistanceStrategy.EUCLIDEAN_DISTANCE: "euclidean",
     DistanceStrategy.COSINE: "cosine",
 }
 
+COMPARISONS_TO_NATIVE = {
+    "$eq": "=",
+    "$ne": "<>",
+    "$lt": "<",
+    "$lte": "<=",
+    "$gt": ">",
+    "$gte": ">=",
+}
+
+SPECIAL_CASED_OPERATORS = {
+    "$in",
+    "$nin",
+    "$between",
+}
+
+TEXT_OPERATORS = {
+    "$like",
+    "$ilike",
+}
+
+LOGICAL_OPERATORS = {"$and", "$or"}
+
+SUPPORTED_OPERATORS = (
+    set(COMPARISONS_TO_NATIVE)
+    .union(TEXT_OPERATORS)
+    .union(LOGICAL_OPERATORS)
+    .union(SPECIAL_CASED_OPERATORS)
+)
+
 
 class SearchType(str, enum.Enum):
     """Enumerator of the Distance strategies."""
 
     VECTOR = "vector"
     HYBRID = "hybrid"
 
 
 DEFAULT_SEARCH_TYPE = SearchType.VECTOR
 
 
-def _get_search_index_query(search_type: SearchType) -> str:
-    type_to_query_map = {
-        SearchType.VECTOR: (
-            "CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score "
-        ),
-        SearchType.HYBRID: (
-            "CALL { "
-            "CALL db.index.vector.queryNodes($index, $k, $embedding) "
-            "YIELD node, score "
-            "WITH collect({node:node, score:score}) AS nodes, max(score) AS max "
-            "UNWIND nodes AS n "
-            # We use 0 as min
-            "RETURN n.node AS node, (n.score / max) AS score UNION "
-            "CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) "
-            "YIELD node, score "
-            "WITH collect({node:node, score:score}) AS nodes, max(score) AS max "
-            "UNWIND nodes AS n "
-            # We use 0 as min
-            "RETURN n.node AS node, (n.score / max) AS score "
-            "} "
-            # dedup
-            "WITH node, max(score) AS score ORDER BY score DESC LIMIT $k "
-        ),
-    }
-    return type_to_query_map[search_type]
+class IndexType(str, enum.Enum):
+    """Enumerator of the index types."""
+
+    NODE = "NODE"
+    RELATIONSHIP = "RELATIONSHIP"
+
+
+DEFAULT_INDEX_TYPE = IndexType.NODE
+
+
+def _get_search_index_query(
+    search_type: SearchType, index_type: IndexType = DEFAULT_INDEX_TYPE
+) -> str:
+    if index_type == IndexType.NODE:
+        type_to_query_map = {
+            SearchType.VECTOR: (
+                "CALL db.index.vector.queryNodes($index, $k, $embedding) "
+                "YIELD node, score "
+            ),
+            SearchType.HYBRID: (
+                "CALL { "
+                "CALL db.index.vector.queryNodes($index, $k, $embedding) "
+                "YIELD node, score "
+                "WITH collect({node:node, score:score}) AS nodes, max(score) AS max "
+                "UNWIND nodes AS n "
+                # We use 0 as min
+                "RETURN n.node AS node, (n.score / max) AS score UNION "
+                "CALL db.index.fulltext.queryNodes($keyword_index, $query, "
+                "{limit: $k}) YIELD node, score "
+                "WITH collect({node:node, score:score}) AS nodes, max(score) AS max "
+                "UNWIND nodes AS n "
+                # We use 0 as min
+                "RETURN n.node AS node, (n.score / max) AS score "
+                "} "
+                # dedup
+                "WITH node, max(score) AS score ORDER BY score DESC LIMIT $k "
+            ),
+        }
+        return type_to_query_map[search_type]
+    else:
+        return (
+            "CALL db.index.vector.queryRelationships($index, $k, $embedding) "
+            "YIELD relationship, score "
+        )
 
 
 def check_if_not_null(props: List[str], values: List[Any]) -> None:
     """Check if the values are not None or empty string"""
     for prop, value in zip(props, values):
         if not value:
             raise ValueError(f"Parameter `{prop}` must not be None or empty string")
 
 
 def sort_by_index_name(
     lst: List[Dict[str, Any]], index_name: str
 ) -> List[Dict[str, Any]]:
     """Sort first element to match the index_name if exists"""
-    return sorted(lst, key=lambda x: x.get("index_name") != index_name)
+    return sorted(lst, key=lambda x: x.get("name") != index_name)
 
 
 def remove_lucene_chars(text: str) -> str:
     """Remove Lucene special characters"""
     special_chars = [
         "+",
         "-",
@@ -104,14 +153,283 @@
     ]
     for char in special_chars:
         if char in text:
             text = text.replace(char, " ")
     return text.strip()
 
 
+def dict_to_yaml_str(input_dict: Dict, indent: int = 0) -> str:
+    """
+    Convert a dictionary to a YAML-like string without using external libraries.
+
+    Parameters:
+    - input_dict (dict): The dictionary to convert.
+    - indent (int): The current indentation level.
+
+    Returns:
+    - str: The YAML-like string representation of the input dictionary.
+    """
+    yaml_str = ""
+    for key, value in input_dict.items():
+        padding = "  " * indent
+        if isinstance(value, dict):
+            yaml_str += f"{padding}{key}:\n{dict_to_yaml_str(value, indent + 1)}"
+        elif isinstance(value, list):
+            yaml_str += f"{padding}{key}:\n"
+            for item in value:
+                yaml_str += f"{padding}- {item}\n"
+        else:
+            yaml_str += f"{padding}{key}: {value}\n"
+    return yaml_str
+
+
+def combine_queries(
+    input_queries: List[Tuple[str, Dict[str, Any]]], operator: str
+) -> Tuple[str, Dict[str, Any]]:
+    """Combine multiple queries with an operator."""
+
+    # Initialize variables to hold the combined query and parameters
+    combined_query: str = ""
+    combined_params: Dict = {}
+    param_counter: Dict = {}
+
+    for query, params in input_queries:
+        # Process each query fragment and its parameters
+        new_query = query
+        for param, value in params.items():
+            # Update the parameter name to ensure uniqueness
+            if param in param_counter:
+                param_counter[param] += 1
+            else:
+                param_counter[param] = 1
+            new_param_name = f"{param}_{param_counter[param]}"
+
+            # Replace the parameter in the query fragment
+            new_query = new_query.replace(f"${param}", f"${new_param_name}")
+            # Add the parameter to the combined parameters dictionary
+            combined_params[new_param_name] = value
+
+        # Combine the query fragments with an AND operator
+        if combined_query:
+            combined_query += f" {operator} "
+        combined_query += f"({new_query})"
+
+    return combined_query, combined_params
+
+
+def collect_params(
+    input_data: List[Tuple[str, Dict[str, str]]],
+) -> Tuple[List[str], Dict[str, Any]]:
+    """Transform the input data into the desired format.
+
+    Args:
+    - input_data (list of tuples): Input data to transform.
+      Each tuple contains a string and a dictionary.
+
+    Returns:
+    - tuple: A tuple containing a list of strings and a dictionary.
+    """
+    # Initialize variables to hold the output parts
+    query_parts = []
+    params = {}
+
+    # Loop through each item in the input data
+    for query_part, param in input_data:
+        # Append the query part to the list
+        query_parts.append(query_part)
+        # Update the params dictionary with the param dictionary
+        params.update(param)
+
+    # Return the transformed data
+    return (query_parts, params)
+
+
+def _handle_field_filter(
+    field: str, value: Any, param_number: int = 1
+) -> Tuple[str, Dict]:
+    """Create a filter for a specific field.
+
+    Args:
+        field: name of field
+        value: value to filter
+            If provided as is then this will be an equality filter
+            If provided as a dictionary then this will be a filter, the key
+            will be the operator and the value will be the value to filter by
+        param_number: sequence number of parameters used to map between param
+           dict and Cypher snippet
+
+    Returns a tuple of
+        - Cypher filter snippet
+        - Dictionary with parameters used in filter snippet
+    """
+    if not isinstance(field, str):
+        raise ValueError(
+            f"field should be a string but got: {type(field)} with value: {field}"
+        )
+
+    if field.startswith("$"):
+        raise ValueError(
+            f"Invalid filter condition. Expected a field but got an operator: "
+            f"{field}"
+        )
+
+    # Allow [a-zA-Z0-9_], disallow $ for now until we support escape characters
+    if not field.isidentifier():
+        raise ValueError(f"Invalid field name: {field}. Expected a valid identifier.")
+
+    if isinstance(value, dict):
+        # This is a filter specification
+        if len(value) != 1:
+            raise ValueError(
+                "Invalid filter condition. Expected a value which "
+                "is a dictionary with a single key that corresponds to an operator "
+                f"but got a dictionary with {len(value)} keys. The first few "
+                f"keys are: {list(value.keys())[:3]}"
+            )
+        operator, filter_value = list(value.items())[0]
+        # Verify that that operator is an operator
+        if operator not in SUPPORTED_OPERATORS:
+            raise ValueError(
+                f"Invalid operator: {operator}. "
+                f"Expected one of {SUPPORTED_OPERATORS}"
+            )
+    else:  # Then we assume an equality operator
+        operator = "$eq"
+        filter_value = value
+
+    if operator in COMPARISONS_TO_NATIVE:
+        # Then we implement an equality filter
+        # native is trusted input
+        native = COMPARISONS_TO_NATIVE[operator]
+        query_snippet = f"n.`{field}` {native} $param_{param_number}"
+        query_param = {f"param_{param_number}": filter_value}
+        return (query_snippet, query_param)
+    elif operator == "$between":
+        low, high = filter_value
+        query_snippet = (
+            f"$param_{param_number}_low <= n.`{field}` <= $param_{param_number}_high"
+        )
+        query_param = {
+            f"param_{param_number}_low": low,
+            f"param_{param_number}_high": high,
+        }
+        return (query_snippet, query_param)
+
+    elif operator in {"$in", "$nin", "$like", "$ilike"}:
+        # We'll do force coercion to text
+        if operator in {"$in", "$nin"}:
+            for val in filter_value:
+                if not isinstance(val, (str, int, float)):
+                    raise NotImplementedError(
+                        f"Unsupported type: {type(val)} for value: {val}"
+                    )
+        if operator in {"$in"}:
+            query_snippet = f"n.`{field}` IN $param_{param_number}"
+            query_param = {f"param_{param_number}": filter_value}
+            return (query_snippet, query_param)
+        elif operator in {"$nin"}:
+            query_snippet = f"n.`{field}` NOT IN $param_{param_number}"
+            query_param = {f"param_{param_number}": filter_value}
+            return (query_snippet, query_param)
+        elif operator in {"$like"}:
+            query_snippet = f"n.`{field}` CONTAINS $param_{param_number}"
+            query_param = {f"param_{param_number}": filter_value.rstrip("%")}
+            return (query_snippet, query_param)
+        elif operator in {"$ilike"}:
+            query_snippet = f"toLower(n.`{field}`) CONTAINS $param_{param_number}"
+            query_param = {f"param_{param_number}": filter_value.rstrip("%")}
+            return (query_snippet, query_param)
+        else:
+            raise NotImplementedError()
+    else:
+        raise NotImplementedError()
+
+
+def construct_metadata_filter(filter: Dict[str, Any]) -> Tuple[str, Dict]:
+    """Construct a metadata filter.
+
+    Args:
+        filter: A dictionary representing the filter condition.
+
+    Returns:
+        Tuple[str, Dict]
+    """
+
+    if isinstance(filter, dict):
+        if len(filter) == 1:
+            # The only operators allowed at the top level are $AND and $OR
+            # First check if an operator or a field
+            key, value = list(filter.items())[0]
+            if key.startswith("$"):
+                # Then it's an operator
+                if key.lower() not in ["$and", "$or"]:
+                    raise ValueError(
+                        f"Invalid filter condition. Expected $and or $or "
+                        f"but got: {key}"
+                    )
+            else:
+                # Then it's a field
+                return _handle_field_filter(key, filter[key])
+
+            # Here we handle the $and and $or operators
+            if not isinstance(value, list):
+                raise ValueError(
+                    f"Expected a list, but got {type(value)} for value: {value}"
+                )
+            if key.lower() == "$and":
+                and_ = combine_queries(
+                    [construct_metadata_filter(el) for el in value], "AND"
+                )
+                if len(and_) >= 1:
+                    return and_
+                else:
+                    raise ValueError(
+                        "Invalid filter condition. Expected a dictionary "
+                        "but got an empty dictionary"
+                    )
+            elif key.lower() == "$or":
+                or_ = combine_queries(
+                    [construct_metadata_filter(el) for el in value], "OR"
+                )
+                if len(or_) >= 1:
+                    return or_
+                else:
+                    raise ValueError(
+                        "Invalid filter condition. Expected a dictionary "
+                        "but got an empty dictionary"
+                    )
+            else:
+                raise ValueError(
+                    f"Invalid filter condition. Expected $and or $or " f"but got: {key}"
+                )
+        elif len(filter) > 1:
+            # Then all keys have to be fields (they cannot be operators)
+            for key in filter.keys():
+                if key.startswith("$"):
+                    raise ValueError(
+                        f"Invalid filter condition. Expected a field but got: {key}"
+                    )
+            # These should all be fields and combined using an $and operator
+            and_multiple = collect_params(
+                [
+                    _handle_field_filter(k, v, index)
+                    for index, (k, v) in enumerate(filter.items())
+                ]
+            )
+            if len(and_multiple) >= 1:
+                return " AND ".join(and_multiple[0]), and_multiple[1]
+            else:
+                raise ValueError(
+                    "Invalid filter condition. Expected a dictionary "
+                    "but got an empty dictionary"
+                )
+        else:
+            raise ValueError("Got an empty dictionary for filters.")
+
+
 class Neo4jVector(VectorStore):
     """`Neo4j` vector index.
 
     To use, you should have the ``neo4j`` python package installed.
 
     Args:
         url: Neo4j connection url
@@ -151,24 +469,26 @@
         embedding: Embeddings,
         *,
         search_type: SearchType = SearchType.VECTOR,
         username: Optional[str] = None,
         password: Optional[str] = None,
         url: Optional[str] = None,
         keyword_index_name: Optional[str] = "keyword",
-        database: str = "neo4j",
+        database: Optional[str] = None,
         index_name: str = "vector",
         node_label: str = "Chunk",
         embedding_node_property: str = "embedding",
         text_node_property: str = "text",
         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
         logger: Optional[logging.Logger] = None,
         pre_delete_collection: bool = False,
         retrieval_query: str = "",
         relevance_score_fn: Optional[Callable[[float], float]] = None,
+        index_type: IndexType = DEFAULT_INDEX_TYPE,
+        graph: Optional[Neo4jGraph] = None,
     ) -> None:
         try:
             import neo4j
         except ImportError:
             raise ImportError(
                 "Could not import neo4j python package. "
                 "Please install it with `pip install neo4j`."
@@ -179,41 +499,54 @@
             DistanceStrategy.EUCLIDEAN_DISTANCE,
             DistanceStrategy.COSINE,
         ]:
             raise ValueError(
                 "distance_strategy must be either 'EUCLIDEAN_DISTANCE' or 'COSINE'"
             )
 
-        # Handle if the credentials are environment variables
-
-        # Support URL for backwards compatibility
-        url = os.environ.get("NEO4J_URL", url)
-        url = get_from_env("url", "NEO4J_URI", url)
-        username = get_from_env("username", "NEO4J_USERNAME", username)
-        password = get_from_env("password", "NEO4J_PASSWORD", password)
-        database = get_from_env("database", "NEO4J_DATABASE", database)
-
-        self._driver = neo4j.GraphDatabase.driver(url, auth=(username, password))
-        self._database = database
-        self.schema = ""
-        # Verify connection
-        try:
-            self._driver.verify_connectivity()
-        except neo4j.exceptions.ServiceUnavailable:
-            raise ValueError(
-                "Could not connect to Neo4j database. "
-                "Please ensure that the url is correct"
+        # Graph object takes precedent over env or input params
+        if graph:
+            self._driver = graph._driver
+            self._database = graph._database
+        else:
+            # Handle if the credentials are environment variables
+            # Support URL for backwards compatibility
+            if not url:
+                url = os.environ.get("NEO4J_URL")
+
+            url = get_from_dict_or_env({"url": url}, "url", "NEO4J_URI")
+            username = get_from_dict_or_env(
+                {"username": username}, "username", "NEO4J_USERNAME"
             )
-        except neo4j.exceptions.AuthError:
-            raise ValueError(
-                "Could not connect to Neo4j database. "
-                "Please ensure that the username and password are correct"
+            password = get_from_dict_or_env(
+                {"password": password}, "password", "NEO4J_PASSWORD"
             )
+            database = get_from_dict_or_env(
+                {"database": database}, "database", "NEO4J_DATABASE", "neo4j"
+            )
+
+            self._driver = neo4j.GraphDatabase.driver(url, auth=(username, password))
+            self._database = database
+            # Verify connection
+            try:
+                self._driver.verify_connectivity()
+            except neo4j.exceptions.ServiceUnavailable:
+                raise ValueError(
+                    "Could not connect to Neo4j database. "
+                    "Please ensure that the url is correct"
+                )
+            except neo4j.exceptions.AuthError:
+                raise ValueError(
+                    "Could not connect to Neo4j database. "
+                    "Please ensure that the username and password are correct"
+                )
 
+        self.schema = ""
         # Verify if the version support vector index
+        self._is_enterprise = False
         self.verify_version()
 
         # Verify that required values are not null
         check_if_not_null(
             [
                 "index_name",
                 "node_label",
@@ -230,14 +563,15 @@
         self.node_label = node_label
         self.embedding_node_property = embedding_node_property
         self.text_node_property = text_node_property
         self.logger = logger or logging.getLogger(__name__)
         self.override_relevance_score_fn = relevance_score_fn
         self.retrieval_query = retrieval_query
         self.search_type = search_type
+        self._index_type = index_type
         # Calculate embedding dimension
         self.embedding_dimension = len(embedding.embed_query("foo"))
 
         # Delete existing data if flagged
         if pre_delete_collection:
             from neo4j.exceptions import DatabaseError
 
@@ -281,66 +615,77 @@
         Check if the connected Neo4j database version supports vector indexing.
 
         Queries the Neo4j database to retrieve its version and compares it
         against a target version (5.11.0) that is known to support vector
         indexing. Raises a ValueError if the connected Neo4j version is
         not supported.
         """
-        version = self.query("CALL dbms.components()")[0]["versions"][0]
+        db_data = self.query("CALL dbms.components()")
+        version = db_data[0]["versions"][0]
         if "aura" in version:
             version_tuple = tuple(map(int, version.split("-")[0].split("."))) + (0,)
         else:
             version_tuple = tuple(map(int, version.split(".")))
 
         target_version = (5, 11, 0)
 
         if version_tuple < target_version:
             raise ValueError(
                 "Version index is only supported in Neo4j version 5.11 or greater"
             )
 
-    def retrieve_existing_index(self) -> Optional[int]:
+        # Flag for metadata filtering
+        metadata_target_version = (5, 18, 0)
+        if version_tuple < metadata_target_version:
+            self.support_metadata_filter = False
+        else:
+            self.support_metadata_filter = True
+        # Flag for enterprise
+        self._is_enterprise = True if db_data[0]["edition"] == "enterprise" else False
+
+    def retrieve_existing_index(self) -> Tuple[Optional[int], Optional[str]]:
         """
         Check if the vector index exists in the Neo4j database
         and returns its embedding dimension.
 
         This method queries the Neo4j database for existing indexes
         and attempts to retrieve the dimension of the vector index
         with the specified name. If the index exists, its dimension is returned.
         If the index doesn't exist, `None` is returned.
 
         Returns:
             int or None: The embedding dimension of the existing index if found.
         """
 
         index_information = self.query(
-            "SHOW INDEXES YIELD name, type, labelsOrTypes, properties, options "
-            "WHERE type = 'VECTOR' AND (name = $index_name "
+            "SHOW INDEXES YIELD name, type, entityType, labelsOrTypes, "
+            "properties, options WHERE type = 'VECTOR' AND (name = $index_name "
             "OR (labelsOrTypes[0] = $node_label AND "
             "properties[0] = $embedding_node_property)) "
-            "RETURN name, labelsOrTypes, properties, options ",
+            "RETURN name, entityType, labelsOrTypes, properties, options ",
             params={
                 "index_name": self.index_name,
                 "node_label": self.node_label,
                 "embedding_node_property": self.embedding_node_property,
             },
         )
         # sort by index_name
         index_information = sort_by_index_name(index_information, self.index_name)
         try:
             self.index_name = index_information[0]["name"]
             self.node_label = index_information[0]["labelsOrTypes"][0]
             self.embedding_node_property = index_information[0]["properties"][0]
+            self._index_type = index_information[0]["entityType"]
             embedding_dimension = index_information[0]["options"]["indexConfig"][
                 "vector.dimensions"
             ]
 
-            return embedding_dimension
+            return embedding_dimension, index_information[0]["entityType"]
         except IndexError:
-            return None
+            return None, None
 
     def retrieve_existing_fts_index(
         self, text_node_properties: List[str] = []
     ) -> Optional[str]:
         """
         Check if the fulltext index exists in the Neo4j database
 
@@ -422,26 +767,32 @@
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         create_id_index: bool = True,
         search_type: SearchType = SearchType.VECTOR,
         **kwargs: Any,
     ) -> Neo4jVector:
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [md5(text.encode("utf-8")).hexdigest() for text in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         store = cls(
             embedding=embedding,
             search_type=search_type,
             **kwargs,
         )
         # Check if the vector index already exists
-        embedding_dimension = store.retrieve_existing_index()
+        embedding_dimension, index_type = store.retrieve_existing_index()
+
+        # Raise error if relationship index type
+        if index_type == "RELATIONSHIP":
+            raise ValueError(
+                "Data ingestion is not supported with relationship vector index."
+            )
 
         # If the vector index doesn't exist yet
         if not embedding_dimension:
             store.create_new_index()
         # If the index already exists, check if embedding dimensions match
         elif not store.embedding_dimension == embedding_dimension:
             raise ValueError(
@@ -489,15 +840,15 @@
         Args:
             texts: Iterable of strings to add to the vectorstore.
             embeddings: List of list of embedding vectors.
             metadatas: List of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [md5(text.encode("utf-8")).hexdigest() for text in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         import_query = (
             "UNWIND $data AS row "
             "CALL { WITH row "
@@ -545,14 +896,16 @@
             texts=texts, embeddings=embeddings, metadatas=metadatas, ids=ids, **kwargs
         )
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
+        params: Dict[str, Any] = {},
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Run similarity search with Neo4jVector.
 
         Args:
             query (str): Query text to search for.
             k (int): Number of results to return. Defaults to 4.
@@ -561,36 +914,54 @@
             List of Documents most similar to the query.
         """
         embedding = self.embedding.embed_query(text=query)
         return self.similarity_search_by_vector(
             embedding=embedding,
             k=k,
             query=query,
+            params=params,
+            filter=filter,
+            **kwargs,
         )
 
     def similarity_search_with_score(
-        self, query: str, k: int = 4
+        self,
+        query: str,
+        k: int = 4,
+        params: Dict[str, Any] = {},
+        filter: Optional[Dict[str, Any]] = None,
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs most similar to query.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
 
         Returns:
             List of Documents most similar to the query and score for each
         """
         embedding = self.embedding.embed_query(query)
         docs = self.similarity_search_with_score_by_vector(
-            embedding=embedding, k=k, query=query
+            embedding=embedding,
+            k=k,
+            query=query,
+            params=params,
+            filter=filter,
+            **kwargs,
         )
         return docs
 
     def similarity_search_with_score_by_vector(
-        self, embedding: List[float], k: int = 4, **kwargs: Any
+        self,
+        embedding: List[float],
+        k: int = 4,
+        filter: Optional[Dict[str, Any]] = None,
+        params: Dict[str, Any] = {},
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """
         Perform a similarity search in the Neo4j database using a
         given vector and return the top k similar documents with their scores.
 
         This method uses a Cypher query to find the top k documents that
         are most similar to a given embedding. The similarity is measured
@@ -602,66 +973,114 @@
             embedding (List[float]): The embedding vector to compare against.
             k (int, optional): The number of top similar documents to retrieve.
 
         Returns:
             List[Tuple[Document, float]]: A list of tuples, each containing
                                 a Document object and its similarity score.
         """
-        default_retrieval = (
-            f"RETURN node.`{self.text_node_property}` AS text, score, "
-            f"node {{.*, `{self.text_node_property}`: Null, "
-            f"`{self.embedding_node_property}`: Null, id: Null }} AS metadata"
-        )
+        if filter:
+            # Verify that 5.18 or later is used
+            if not self.support_metadata_filter:
+                raise ValueError(
+                    "Metadata filtering is only supported in "
+                    "Neo4j version 5.18 or greater"
+                )
+            # Metadata filtering and hybrid doesn't work
+            if self.search_type == SearchType.HYBRID:
+                raise ValueError(
+                    "Metadata filtering can't be use in combination with "
+                    "a hybrid search approach"
+                )
+            parallel_query = (
+                "CYPHER runtime = parallel parallelRuntimeSupport=all "
+                if self._is_enterprise
+                else ""
+            )
+            base_index_query = parallel_query + (
+                f"MATCH (n:`{self.node_label}`) WHERE "
+                f"n.`{self.embedding_node_property}` IS NOT NULL AND "
+                f"size(n.`{self.embedding_node_property}`) = "
+                f"toInteger({self.embedding_dimension}) AND "
+            )
+            base_cosine_query = (
+                " WITH n as node, vector.similarity.cosine("
+                f"n.`{self.embedding_node_property}`, "
+                "$embedding) AS score ORDER BY score DESC LIMIT toInteger($k) "
+            )
+            filter_snippets, filter_params = construct_metadata_filter(filter)
+            index_query = base_index_query + filter_snippets + base_cosine_query
+
+        else:
+            index_query = _get_search_index_query(self.search_type, self._index_type)
+            filter_params = {}
+
+        if self._index_type == IndexType.RELATIONSHIP:
+            default_retrieval = (
+                f"RETURN relationship.`{self.text_node_property}` AS text, score, "
+                f"relationship {{.*, `{self.text_node_property}`: Null, "
+                f"`{self.embedding_node_property}`: Null, id: Null }} AS metadata"
+            )
+        else:
+            default_retrieval = (
+                f"RETURN node.`{self.text_node_property}` AS text, score, "
+                f"node {{.*, `{self.text_node_property}`: Null, "
+                f"`{self.embedding_node_property}`: Null, id: Null }} AS metadata"
+            )
 
         retrieval_query = (
             self.retrieval_query if self.retrieval_query else default_retrieval
         )
 
-        read_query = _get_search_index_query(self.search_type) + retrieval_query
+        read_query = index_query + retrieval_query
         parameters = {
             "index": self.index_name,
             "k": k,
             "embedding": embedding,
             "keyword_index": self.keyword_index_name,
             "query": remove_lucene_chars(kwargs["query"]),
+            **params,
+            **filter_params,
         }
 
         results = self.query(read_query, params=parameters)
 
         docs = [
             (
                 Document(
-                    page_content=result["text"],
+                    page_content=dict_to_yaml_str(result["text"])
+                    if isinstance(result["text"], dict)
+                    else result["text"],
                     metadata={
                         k: v for k, v in result["metadata"].items() if v is not None
                     },
                 ),
                 result["score"],
             )
             for result in results
         ]
         return docs
 
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to embedding vector.
 
         Args:
             embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
 
         Returns:
             List of Documents most similar to the query vector.
         """
         docs_and_scores = self.similarity_search_with_score_by_vector(
-            embedding=embedding, k=k, **kwargs
+            embedding=embedding, k=k, filter=filter, **kwargs
         )
         return [doc for doc, _ in docs_and_scores]
 
     @classmethod
     def from_texts(
         cls: Type[Neo4jVector],
         texts: List[str],
@@ -759,15 +1178,23 @@
             embedding=embedding,
             index_name=index_name,
             keyword_index_name=keyword_index_name,
             search_type=search_type,
             **kwargs,
         )
 
-        embedding_dimension = store.retrieve_existing_index()
+        embedding_dimension, index_type = store.retrieve_existing_index()
+
+        # Raise error if relationship index type
+        if index_type == "RELATIONSHIP":
+            raise ValueError(
+                "Relationship vector index is not supported with "
+                "`from_existing_index` method. Please use the "
+                "`from_existing_relationship_index` method."
+            )
 
         if not embedding_dimension:
             raise ValueError(
                 "The specified vector index name does not exist. "
                 "Make sure to check if you spelled it correctly"
             )
 
@@ -793,14 +1220,69 @@
                     raise ValueError(
                         "Vector and keyword index don't index the same node label"
                     )
 
         return store
 
     @classmethod
+    def from_existing_relationship_index(
+        cls: Type[Neo4jVector],
+        embedding: Embeddings,
+        index_name: str,
+        search_type: SearchType = DEFAULT_SEARCH_TYPE,
+        **kwargs: Any,
+    ) -> Neo4jVector:
+        """
+        Get instance of an existing Neo4j relationship vector index.
+        This method will return the instance of the store without
+        inserting any new embeddings.
+        Neo4j credentials are required in the form of `url`, `username`,
+        and `password` and optional `database` parameters along with
+        the `index_name` definition.
+        """
+
+        if search_type == SearchType.HYBRID:
+            raise ValueError(
+                "Hybrid search is not supported in combination "
+                "with relationship vector index"
+            )
+
+        store = cls(
+            embedding=embedding,
+            index_name=index_name,
+            **kwargs,
+        )
+
+        embedding_dimension, index_type = store.retrieve_existing_index()
+
+        if not embedding_dimension:
+            raise ValueError(
+                "The specified vector index name does not exist. "
+                "Make sure to check if you spelled it correctly"
+            )
+        # Raise error if relationship index type
+        if index_type == "NODE":
+            raise ValueError(
+                "Node vector index is not supported with "
+                "`from_existing_relationship_index` method. Please use the "
+                "`from_existing_index` method."
+            )
+
+        # Check if embedding function and vector index dimensions match
+        if not store.embedding_dimension == embedding_dimension:
+            raise ValueError(
+                "The provided embedding function and vector index "
+                "dimensions do not match.\n"
+                f"Embedding function dimension: {store.embedding_dimension}\n"
+                f"Vector index dimension: {embedding_dimension}"
+            )
+
+        return store
+
+    @classmethod
     def from_documents(
         cls: Type[Neo4jVector],
         documents: List[Document],
         embedding: Embeddings,
         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
@@ -884,15 +1366,23 @@
             retrieval_query=retrieval_query,
             node_label=node_label,
             embedding_node_property=embedding_node_property,
             **kwargs,
         )
 
         # Check if the vector index already exists
-        embedding_dimension = store.retrieve_existing_index()
+        embedding_dimension, index_type = store.retrieve_existing_index()
+
+        # Raise error if relationship index type
+        if index_type == "RELATIONSHIP":
+            raise ValueError(
+                "`from_existing_graph` method does not support "
+                " existing relationship vector index. "
+                "Please use `from_existing_relationship_index` method"
+            )
 
         # If the vector index doesn't exist yet
         if not embedding_dimension:
             store.create_new_index()
         # If the index already exists, check if embedding dimensions match
         elif not store.embedding_dimension == embedding_dimension:
             raise ValueError(
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/nucliadb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/nucliadb.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,15 @@
             api_key: A contributor API key for the kb (needed when local is False)
             backend: The backend url to use when local is True, defaults to
             http://localhost:8080
         """
         try:
             from nuclia.sdk import NucliaAuth
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "nuclia python package not found. "
                 "Please install it with `pip install nuclia`."
             )
         self._config["LOCAL"] = local
         zone = os.environ.get("NUCLIA_ZONE", "europe-1")
         self._kb = knowledge_box
         if local:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/opensearch_vector_search.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/opensearch_vector_search.py`

 * *Files 27% similar despite different names*

```diff
@@ -11,37 +11,59 @@
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 IMPORT_OPENSEARCH_PY_ERROR = (
     "Could not import OpenSearch. Please install it with `pip install opensearch-py`."
 )
+IMPORT_ASYNC_OPENSEARCH_PY_ERROR = """
+Could not import AsyncOpenSearch. 
+Please install it with `pip install opensearch-py`."""
+
 SCRIPT_SCORING_SEARCH = "script_scoring"
 PAINLESS_SCRIPTING_SEARCH = "painless_scripting"
 MATCH_ALL_QUERY = {"match_all": {}}  # type: Dict
 
 
 def _import_opensearch() -> Any:
     """Import OpenSearch if available, otherwise raise error."""
     try:
         from opensearchpy import OpenSearch
     except ImportError:
         raise ImportError(IMPORT_OPENSEARCH_PY_ERROR)
     return OpenSearch
 
 
+def _import_async_opensearch() -> Any:
+    """Import AsyncOpenSearch if available, otherwise raise error."""
+    try:
+        from opensearchpy import AsyncOpenSearch
+    except ImportError:
+        raise ImportError(IMPORT_ASYNC_OPENSEARCH_PY_ERROR)
+    return AsyncOpenSearch
+
+
 def _import_bulk() -> Any:
     """Import bulk if available, otherwise raise error."""
     try:
         from opensearchpy.helpers import bulk
     except ImportError:
         raise ImportError(IMPORT_OPENSEARCH_PY_ERROR)
     return bulk
 
 
+def _import_async_bulk() -> Any:
+    """Import async_bulk if available, otherwise raise error."""
+    try:
+        from opensearchpy.helpers import async_bulk
+    except ImportError:
+        raise ImportError(IMPORT_ASYNC_OPENSEARCH_PY_ERROR)
+    return async_bulk
+
+
 def _import_not_found_error() -> Any:
     """Import not found error if available, otherwise raise error."""
     try:
         from opensearchpy.exceptions import NotFoundError
     except ImportError:
         raise ImportError(IMPORT_OPENSEARCH_PY_ERROR)
     return NotFoundError
@@ -56,14 +78,27 @@
         raise ImportError(
             f"OpenSearch client string provided is not in proper format. "
             f"Got error: {e} "
         )
     return client
 
 
+def _get_async_opensearch_client(opensearch_url: str, **kwargs: Any) -> Any:
+    """Get AsyncOpenSearch client from the opensearch_url, otherwise raise error."""
+    try:
+        async_opensearch = _import_async_opensearch()
+        client = async_opensearch(opensearch_url, **kwargs)
+    except ValueError as e:
+        raise ImportError(
+            f"AsyncOpenSearch client string provided is not in proper format. "
+            f"Got error: {e} "
+        )
+    return client
+
+
 def _validate_embeddings_and_bulk_size(embeddings_length: int, bulk_size: int) -> None:
     """Validate Embeddings Length and Bulk Size."""
     if embeddings_length == 0:
         raise RuntimeError("Embeddings size is zero")
     if bulk_size < embeddings_length:
         raise RuntimeError(
             f"The embeddings count, {embeddings_length} is more than the "
@@ -137,14 +172,65 @@
         return_ids.append(_id)
     bulk(client, requests, max_chunk_bytes=max_chunk_bytes)
     if not is_aoss:
         client.indices.refresh(index=index_name)
     return return_ids
 
 
+async def _abulk_ingest_embeddings(
+    client: Any,
+    index_name: str,
+    embeddings: List[List[float]],
+    texts: Iterable[str],
+    metadatas: Optional[List[dict]] = None,
+    ids: Optional[List[str]] = None,
+    vector_field: str = "vector_field",
+    text_field: str = "text",
+    mapping: Optional[Dict] = None,
+    max_chunk_bytes: Optional[int] = 1 * 1024 * 1024,
+    is_aoss: bool = False,
+) -> List[str]:
+    """Bulk Ingest Embeddings into given index asynchronously using AsyncOpenSearch."""
+    if not mapping:
+        mapping = dict()
+
+    async_bulk = _import_async_bulk()
+    not_found_error = _import_not_found_error()
+    requests = []
+    return_ids = []
+
+    try:
+        await client.indices.get(index=index_name)
+    except not_found_error:
+        await client.indices.create(index=index_name, body=mapping)
+
+    for i, text in enumerate(texts):
+        metadata = metadatas[i] if metadatas else {}
+        _id = ids[i] if ids else str(uuid.uuid4())
+        request = {
+            "_op_type": "index",
+            "_index": index_name,
+            vector_field: embeddings[i],
+            text_field: text,
+            "metadata": metadata,
+        }
+        if is_aoss:
+            request["id"] = _id
+        else:
+            request["_id"] = _id
+        requests.append(request)
+        return_ids.append(_id)
+
+    await async_bulk(client, requests, max_chunk_bytes=max_chunk_bytes)
+    if not is_aoss:
+        await client.indices.refresh(index=index_name)
+
+    return return_ids
+
+
 def _default_scripting_text_mapping(
     dim: int,
     vector_field: str = "vector_field",
 ) -> Dict:
     """For Painless Scripting or Script Scoring,the default mapping to create index."""
     return {
         "mappings": {
@@ -184,32 +270,36 @@
     }
 
 
 def _default_approximate_search_query(
     query_vector: List[float],
     k: int = 4,
     vector_field: str = "vector_field",
+    score_threshold: Optional[float] = 0.0,
 ) -> Dict:
     """For Approximate k-NN Search, this is the default query."""
     return {
         "size": k,
+        "min_score": score_threshold,
         "query": {"knn": {vector_field: {"vector": query_vector, "k": k}}},
     }
 
 
 def _approximate_search_query_with_boolean_filter(
     query_vector: List[float],
     boolean_filter: Dict,
     k: int = 4,
     vector_field: str = "vector_field",
     subquery_clause: str = "must",
+    score_threshold: Optional[float] = 0.0,
 ) -> Dict:
     """For Approximate k-NN Search, with Boolean Filter."""
     return {
         "size": k,
+        "min_score": score_threshold,
         "query": {
             "bool": {
                 "filter": boolean_filter,
                 subquery_clause: [
                     {"knn": {vector_field: {"vector": query_vector, "k": k}}}
                 ],
             }
@@ -218,38 +308,41 @@
 
 
 def _approximate_search_query_with_efficient_filter(
     query_vector: List[float],
     efficient_filter: Dict,
     k: int = 4,
     vector_field: str = "vector_field",
+    score_threshold: Optional[float] = 0.0,
 ) -> Dict:
     """For Approximate k-NN Search, with Efficient Filter for Lucene and
     Faiss Engines."""
     search_query = _default_approximate_search_query(
-        query_vector, k=k, vector_field=vector_field
+        query_vector, k=k, vector_field=vector_field, score_threshold=score_threshold
     )
     search_query["query"]["knn"][vector_field]["filter"] = efficient_filter
     return search_query
 
 
 def _default_script_query(
     query_vector: List[float],
     k: int = 4,
     space_type: str = "l2",
     pre_filter: Optional[Dict] = None,
     vector_field: str = "vector_field",
+    score_threshold: Optional[float] = 0.0,
 ) -> Dict:
     """For Script Scoring Search, this is the default query."""
 
     if not pre_filter:
         pre_filter = MATCH_ALL_QUERY
 
     return {
         "size": k,
+        "min_score": score_threshold,
         "query": {
             "script_score": {
                 "query": pre_filter,
                 "script": {
                     "source": "knn_score",
                     "lang": "knn",
                     "params": {
@@ -278,23 +371,25 @@
 
 def _default_painless_scripting_query(
     query_vector: List[float],
     k: int = 4,
     space_type: str = "l2Squared",
     pre_filter: Optional[Dict] = None,
     vector_field: str = "vector_field",
+    score_threshold: Optional[float] = 0.0,
 ) -> Dict:
     """For Painless Scripting Search, this is the default query."""
 
     if not pre_filter:
         pre_filter = MATCH_ALL_QUERY
 
     source = __get_painless_scripting_source(space_type, vector_field=vector_field)
     return {
         "size": k,
+        "min_score": score_threshold,
         "query": {
             "script_score": {
                 "query": pre_filter,
                 "script": {
                     "source": source,
                     "params": {
                         "field": vector_field,
@@ -330,14 +425,15 @@
     ):
         """Initialize with necessary components."""
         self.embedding_function = embedding_function
         self.index_name = index_name
         http_auth = kwargs.get("http_auth")
         self.is_aoss = _is_aoss_enabled(http_auth=http_auth)
         self.client = _get_opensearch_client(opensearch_url, **kwargs)
+        self.async_client = _get_async_opensearch_client(opensearch_url, **kwargs)
         self.engine = kwargs.get("engine")
 
     @property
     def embeddings(self) -> Embeddings:
         return self.embedding_function
 
     def __add(
@@ -377,14 +473,121 @@
             vector_field=vector_field,
             text_field=text_field,
             mapping=mapping,
             max_chunk_bytes=max_chunk_bytes,
             is_aoss=self.is_aoss,
         )
 
+    async def __aadd(
+        self,
+        texts: Iterable[str],
+        embeddings: List[List[float]],
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        bulk_size: int = 500,
+        **kwargs: Any,
+    ) -> List[str]:
+        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)
+        index_name = kwargs.get("index_name", self.index_name)
+        text_field = kwargs.get("text_field", "text")
+        dim = len(embeddings[0])
+        engine = kwargs.get("engine", "nmslib")
+        space_type = kwargs.get("space_type", "l2")
+        ef_search = kwargs.get("ef_search", 512)
+        ef_construction = kwargs.get("ef_construction", 512)
+        m = kwargs.get("m", 16)
+        vector_field = kwargs.get("vector_field", "vector_field")
+        max_chunk_bytes = kwargs.get("max_chunk_bytes", 1 * 1024 * 1024)
+
+        _validate_aoss_with_engines(self.is_aoss, engine)
+
+        mapping = _default_text_mapping(
+            dim, engine, space_type, ef_search, ef_construction, m, vector_field
+        )
+
+        return await _abulk_ingest_embeddings(
+            self.async_client,
+            index_name,
+            embeddings,
+            texts,
+            metadatas=metadatas,
+            ids=ids,
+            vector_field=vector_field,
+            text_field=text_field,
+            mapping=mapping,
+            max_chunk_bytes=max_chunk_bytes,
+            is_aoss=self.is_aoss,
+        )
+
+    def delete_index(self, index_name: Optional[str] = None) -> Optional[bool]:
+        """Deletes a given index from vectorstore."""
+        if index_name is None:
+            if self.index_name is None:
+                raise ValueError("index_name must be provided.")
+            index_name = self.index_name
+        try:
+            self.client.indices.delete(index=index_name)
+            return True
+        except Exception as e:
+            raise e
+
+    def index_exists(self, index_name: Optional[str] = None) -> Optional[bool]:
+        """If given index present in vectorstore, returns True else False."""
+        if index_name is None:
+            if self.index_name is None:
+                raise ValueError("index_name must be provided.")
+            index_name = self.index_name
+
+        return self.client.indices.exists(index=index_name)
+
+    def create_index(
+        self,
+        dimension: int,
+        index_name: Optional[str] = uuid.uuid4().hex,
+        **kwargs: Any,
+    ) -> Optional[str]:
+        """Create a new Index with given arguments"""
+        is_appx_search = kwargs.get("is_appx_search", True)
+        vector_field = kwargs.get("vector_field", "vector_field")
+        kwargs.get("text_field", "text")
+        http_auth = kwargs.get("http_auth")
+        is_aoss = _is_aoss_enabled(http_auth=http_auth)
+
+        if is_aoss and not is_appx_search:
+            raise ValueError(
+                "Amazon OpenSearch Service Serverless only "
+                "supports `approximate_search`"
+            )
+
+        if is_appx_search:
+            engine = kwargs.get("engine", "nmslib")
+            space_type = kwargs.get("space_type", "l2")
+            ef_search = kwargs.get("ef_search", 512)
+            ef_construction = kwargs.get("ef_construction", 512)
+            m = kwargs.get("m", 16)
+
+            _validate_aoss_with_engines(is_aoss, engine)
+
+            mapping = _default_text_mapping(
+                dimension,
+                engine,
+                space_type,
+                ef_search,
+                ef_construction,
+                m,
+                vector_field,
+            )
+        else:
+            mapping = _default_scripting_text_mapping(dimension)
+
+        if self.index_exists(index_name):
+            raise RuntimeError(f"The index, {index_name} already exists.")
+        self.client.indices.create(index=index_name, body=mapping)
+        return index_name
+
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         bulk_size: int = 500,
         **kwargs: Any,
@@ -413,14 +616,36 @@
             embeddings,
             metadatas=metadatas,
             ids=ids,
             bulk_size=bulk_size,
             **kwargs,
         )
 
+    async def aadd_texts(
+        self,
+        texts: Iterable[str],
+        metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        bulk_size: int = 500,
+        **kwargs: Any,
+    ) -> List[str]:
+        """
+        Asynchronously run more texts through the embeddings
+        and add to the vectorstore.
+        """
+        embeddings = await self.embedding_function.aembed_documents(list(texts))
+        return await self.__aadd(
+            texts,
+            embeddings,
+            metadatas=metadatas,
+            ids=ids,
+            bulk_size=bulk_size,
+            **kwargs,
+        )
+
     def add_embeddings(
         self,
         text_embeddings: Iterable[Tuple[str, List[float]]],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         bulk_size: int = 500,
         **kwargs: Any,
@@ -450,25 +675,85 @@
             list(embeddings),
             metadatas=metadatas,
             ids=ids,
             bulk_size=bulk_size,
             **kwargs,
         )
 
+    def delete(
+        self,
+        ids: Optional[List[str]] = None,
+        refresh_indices: Optional[bool] = True,
+        **kwargs: Any,
+    ) -> Optional[bool]:
+        """Delete documents from the Opensearch index.
+
+        Args:
+            ids: List of ids of documents to delete.
+            refresh_indices: Whether to refresh the index
+                            after deleting documents. Defaults to True.
+        """
+        bulk = _import_bulk()
+
+        body = []
+
+        if ids is None:
+            raise ValueError("ids must be provided.")
+
+        for _id in ids:
+            body.append({"_op_type": "delete", "_index": self.index_name, "_id": _id})
+
+        if len(body) > 0:
+            try:
+                bulk(self.client, body, refresh=refresh_indices, ignore_status=404)
+                return True
+            except Exception as e:
+                raise e
+        else:
+            return False
+
+    async def adelete(
+        self, ids: Optional[List[str]] = None, **kwargs: Any
+    ) -> Optional[bool]:
+        """Asynchronously delete by vector ID or other criteria.
+
+        Args:
+            ids: List of ids to delete.
+            **kwargs: Other keyword arguments that subclasses might use.
+
+        Returns:
+            Optional[bool]: True if deletion is successful,
+            False otherwise, None if not implemented.
+        """
+        if ids is None:
+            raise ValueError("No ids provided to delete.")
+
+        actions = [{"delete": {"_index": self.index_name, "_id": id_}} for id_ in ids]
+        response = await self.async_client.bulk(body=actions, **kwargs)
+        return not any(
+            item.get("delete", {}).get("error") for item in response["items"]
+        )
+
     def similarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
+        self,
+        query: str,
+        k: int = 4,
+        score_threshold: Optional[float] = 0.0,
+        **kwargs: Any,
     ) -> List[Document]:
         """Return docs most similar to query.
 
         By default, supports Approximate Search.
         Also supports Script Scoring and Painless Scripting.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
+            score_threshold: Specify a score threshold to return only documents
+            above the threshold. Defaults to 0.0.
 
         Returns:
             List of Documents most similar to the query.
 
         Optional Args:
             vector_field: Document field embeddings are stored in. Defaults to
             "vector_field".
@@ -509,75 +794,133 @@
             search_type: "painless_scripting"; default: "approximate_search"
 
             space_type: "l2Squared", "l1Norm", "cosineSimilarity"; default: "l2Squared"
 
             pre_filter: script_score query to pre-filter documents before identifying
             nearest neighbors; default: {"match_all": {}}
         """
-        docs_with_scores = self.similarity_search_with_score(query, k, **kwargs)
+        docs_with_scores = self.similarity_search_with_score(
+            query, k, score_threshold, **kwargs
+        )
+        return [doc[0] for doc in docs_with_scores]
+
+    def similarity_search_by_vector(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        score_threshold: Optional[float] = 0.0,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs most similar to the embedding vector."""
+        docs_with_scores = self.similarity_search_with_score_by_vector(
+            embedding, k, score_threshold, **kwargs
+        )
         return [doc[0] for doc in docs_with_scores]
 
     def similarity_search_with_score(
-        self, query: str, k: int = 4, **kwargs: Any
+        self,
+        query: str,
+        k: int = 4,
+        score_threshold: Optional[float] = 0.0,
+        **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         """Return docs and it's scores most similar to query.
 
         By default, supports Approximate Search.
         Also supports Script Scoring and Painless Scripting.
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
+            score_threshold: Specify a score threshold to return only documents
+            above the threshold. Defaults to 0.0.
 
         Returns:
             List of Documents along with its scores most similar to the query.
 
         Optional Args:
             same as `similarity_search`
         """
+        embedding = self.embedding_function.embed_query(query)
+        return self.similarity_search_with_score_by_vector(
+            embedding, k, score_threshold, **kwargs
+        )
+
+    def similarity_search_with_score_by_vector(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        score_threshold: Optional[float] = 0.0,
+        **kwargs: Any,
+    ) -> List[Tuple[Document, float]]:
+        """Return docs and it's scores most similar to the embedding vector.
+
+        By default, supports Approximate Search.
+        Also supports Script Scoring and Painless Scripting.
+
+        Args:
+            embedding: Embedding vector to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            score_threshold: Specify a score threshold to return only documents
+            above the threshold. Defaults to 0.0.
+
+        Returns:
+            List of Documents along with its scores most similar to the query.
 
+        Optional Args:
+            same as `similarity_search`
+        """
         text_field = kwargs.get("text_field", "text")
         metadata_field = kwargs.get("metadata_field", "metadata")
 
-        hits = self._raw_similarity_search_with_score(query=query, k=k, **kwargs)
+        hits = self._raw_similarity_search_with_score_by_vector(
+            embedding=embedding, k=k, score_threshold=score_threshold, **kwargs
+        )
 
         documents_with_scores = [
             (
                 Document(
                     page_content=hit["_source"][text_field],
-                    metadata=hit["_source"]
-                    if metadata_field == "*" or metadata_field not in hit["_source"]
-                    else hit["_source"][metadata_field],
+                    metadata=(
+                        hit["_source"]
+                        if metadata_field == "*" or metadata_field not in hit["_source"]
+                        else hit["_source"][metadata_field]
+                    ),
                 ),
                 hit["_score"],
             )
             for hit in hits
         ]
         return documents_with_scores
 
-    def _raw_similarity_search_with_score(
-        self, query: str, k: int = 4, **kwargs: Any
+    def _raw_similarity_search_with_score_by_vector(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        score_threshold: Optional[float] = 0.0,
+        **kwargs: Any,
     ) -> List[dict]:
         """Return raw opensearch documents (dict) including vectors,
-        scores most similar to query.
+        scores most similar to the embedding vector.
 
         By default, supports Approximate Search.
         Also supports Script Scoring and Painless Scripting.
 
         Args:
-            query: Text to look up documents similar to.
+            embedding: Embedding vector to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
+            score_threshold: Specify a score threshold to return only documents
+            above the threshold. Defaults to 0.0.
 
         Returns:
-            List of dict with its scores most similar to the query.
+            List of dict with its scores most similar to the embedding.
 
         Optional Args:
             same as `similarity_search`
         """
-        embedding = self.embedding_function.embed_query(query)
         search_type = kwargs.get("search_type", "approximate_search")
         vector_field = kwargs.get("vector_field", "vector_field")
         index_name = kwargs.get("index_name", self.index_name)
         filter = kwargs.get("filter", {})
 
         if (
             self.is_aoss
@@ -628,42 +971,64 @@
             if boolean_filter != {}:
                 search_query = _approximate_search_query_with_boolean_filter(
                     embedding,
                     boolean_filter,
                     k=k,
                     vector_field=vector_field,
                     subquery_clause=subquery_clause,
+                    score_threshold=score_threshold,
                 )
             elif efficient_filter != {}:
                 search_query = _approximate_search_query_with_efficient_filter(
-                    embedding, efficient_filter, k=k, vector_field=vector_field
+                    embedding,
+                    efficient_filter,
+                    k=k,
+                    vector_field=vector_field,
+                    score_threshold=score_threshold,
                 )
             elif lucene_filter != {}:
                 warnings.warn(
                     "`lucene_filter` is deprecated. Please use the keyword argument"
                     " `efficient_filter`"
                 )
                 search_query = _approximate_search_query_with_efficient_filter(
-                    embedding, lucene_filter, k=k, vector_field=vector_field
+                    embedding,
+                    lucene_filter,
+                    k=k,
+                    vector_field=vector_field,
+                    score_threshold=score_threshold,
                 )
             else:
                 search_query = _default_approximate_search_query(
-                    embedding, k=k, vector_field=vector_field
+                    embedding,
+                    k=k,
+                    vector_field=vector_field,
+                    score_threshold=score_threshold,
                 )
         elif search_type == SCRIPT_SCORING_SEARCH:
             space_type = kwargs.get("space_type", "l2")
             pre_filter = kwargs.get("pre_filter", MATCH_ALL_QUERY)
             search_query = _default_script_query(
-                embedding, k, space_type, pre_filter, vector_field
+                embedding,
+                k,
+                space_type,
+                pre_filter,
+                vector_field,
+                score_threshold=score_threshold,
             )
         elif search_type == PAINLESS_SCRIPTING_SEARCH:
             space_type = kwargs.get("space_type", "l2Squared")
             pre_filter = kwargs.get("pre_filter", MATCH_ALL_QUERY)
             search_query = _default_painless_scripting_query(
-                embedding, k, space_type, pre_filter, vector_field
+                embedding,
+                k,
+                space_type,
+                pre_filter,
+                vector_field,
+                score_threshold=score_threshold,
             )
         else:
             raise ValueError("Invalid `search_type` provided as an argument")
 
         response = self.client.search(index=index_name, body=search_query)
 
         return [hit for hit in response["hits"]["hits"]]
@@ -698,15 +1063,17 @@
         text_field = kwargs.get("text_field", "text")
         metadata_field = kwargs.get("metadata_field", "metadata")
 
         # Get embedding of the user query
         embedding = self.embedding_function.embed_query(query)
 
         # Do ANN/KNN search to get top fetch_k results where fetch_k >= k
-        results = self._raw_similarity_search_with_score(query, fetch_k, **kwargs)
+        results = self._raw_similarity_search_with_score_by_vector(
+            embedding, fetch_k, **kwargs
+        )
 
         embeddings = [result["_source"][vector_field] for result in results]
 
         # Rerank top k results using MMR, (mmr_selected is a list of indices)
         mmr_selected = maximal_marginal_relevance(
             np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult
         )
@@ -781,14 +1148,79 @@
             metadatas=metadatas,
             bulk_size=bulk_size,
             ids=ids,
             **kwargs,
         )
 
     @classmethod
+    async def afrom_texts(
+        cls,
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        bulk_size: int = 500,
+        ids: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> OpenSearchVectorSearch:
+        """Asynchronously construct OpenSearchVectorSearch wrapper from raw texts.
+
+        Example:
+            .. code-block:: python
+
+                from langchain_community.vectorstores import OpenSearchVectorSearch
+                from langchain_community.embeddings import OpenAIEmbeddings
+                embeddings = OpenAIEmbeddings()
+                opensearch_vector_search = await OpenSearchVectorSearch.afrom_texts(
+                    texts,
+                    embeddings,
+                    opensearch_url="http://localhost:9200"
+                )
+
+        OpenSearch by default supports Approximate Search powered by nmslib, faiss
+        and lucene engines recommended for large datasets. Also supports brute force
+        search through Script Scoring and Painless Scripting.
+
+        Optional Args:
+            vector_field: Document field embeddings are stored in. Defaults to
+            "vector_field".
+
+            text_field: Document field the text of the document is stored in. Defaults
+            to "text".
+
+        Optional Keyword Args for Approximate Search:
+            engine: "nmslib", "faiss", "lucene"; default: "nmslib"
+
+            space_type: "l2", "l1", "cosinesimil", "linf", "innerproduct"; default: "l2"
+
+            ef_search: Size of the dynamic list used during k-NN searches. Higher values
+            lead to more accurate but slower searches; default: 512
+
+            ef_construction: Size of the dynamic list used during k-NN graph creation.
+            Higher values lead to more accurate graph but slower indexing speed;
+            default: 512
+
+            m: Number of bidirectional links created for each new element. Large impact
+            on memory consumption. Between 2 and 100; default: 16
+
+        Keyword Args for Script Scoring or Painless Scripting:
+            is_appx_search: False
+
+        """
+        embeddings = await embedding.aembed_documents(texts)
+        return await cls.afrom_embeddings(
+            embeddings,
+            texts,
+            embedding,
+            metadatas=metadatas,
+            bulk_size=bulk_size,
+            ids=ids,
+            **kwargs,
+        )
+
+    @classmethod
     def from_embeddings(
         cls,
         embeddings: List[List[float]],
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         bulk_size: int = 500,
@@ -902,14 +1334,146 @@
             client,
             index_name,
             embeddings,
             texts,
             ids=ids,
             metadatas=metadatas,
             vector_field=vector_field,
+            text_field=text_field,
+            mapping=mapping,
+            max_chunk_bytes=max_chunk_bytes,
+            is_aoss=is_aoss,
+        )
+        kwargs["engine"] = engine
+        return cls(opensearch_url, index_name, embedding, **kwargs)
+
+    @classmethod
+    async def afrom_embeddings(
+        cls,
+        embeddings: List[List[float]],
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        bulk_size: int = 500,
+        ids: Optional[List[str]] = None,
+        **kwargs: Any,
+    ) -> OpenSearchVectorSearch:
+        """Asynchronously construct OpenSearchVectorSearch wrapper from pre-vectorized
+        embeddings.
+
+        Example:
+            .. code-block:: python
+
+                from langchain_community.vectorstores import OpenSearchVectorSearch
+                from langchain_community.embeddings import OpenAIEmbeddings
+                embedder = OpenAIEmbeddings()
+                embeddings = await embedder.aembed_documents(["foo", "bar"])
+                opensearch_vector_search =
+                    await OpenSearchVectorSearch.afrom_embeddings(
+                        embeddings,
+                        texts,
+                        embedder,
+                        opensearch_url="http://localhost:9200"
+                )
+
+        OpenSearch by default supports Approximate Search powered by nmslib, faiss
+        and lucene engines recommended for large datasets. Also supports brute force
+        search through Script Scoring and Painless Scripting.
+
+        Optional Args:
+            vector_field: Document field embeddings are stored in. Defaults to
+            "vector_field".
+
+            text_field: Document field the text of the document is stored in. Defaults
+            to "text".
+
+        Optional Keyword Args for Approximate Search:
+            engine: "nmslib", "faiss", "lucene"; default: "nmslib"
+
+            space_type: "l2", "l1", "cosinesimil", "linf", "innerproduct"; default: "l2"
+
+            ef_search: Size of the dynamic list used during k-NN searches. Higher values
+            lead to more accurate but slower searches; default: 512
+
+            ef_construction: Size of the dynamic list used during k-NN graph creation.
+            Higher values lead to more accurate graph but slower indexing speed;
+            default: 512
+
+            m: Number of bidirectional links created for each new element. Large impact
+            on memory consumption. Between 2 and 100; default: 16
+
+        Keyword Args for Script Scoring or Painless Scripting:
+            is_appx_search: False
+
+        """
+        opensearch_url = get_from_dict_or_env(
+            kwargs, "opensearch_url", "OPENSEARCH_URL"
+        )
+        # List of arguments that needs to be removed from kwargs
+        # before passing kwargs to get opensearch client
+        keys_list = [
+            "opensearch_url",
+            "index_name",
+            "is_appx_search",
+            "vector_field",
+            "text_field",
+            "engine",
+            "space_type",
+            "ef_search",
+            "ef_construction",
+            "m",
+            "max_chunk_bytes",
+            "is_aoss",
+        ]
+        _validate_embeddings_and_bulk_size(len(embeddings), bulk_size)
+        dim = len(embeddings[0])
+        # Get the index name from either from kwargs or ENV Variable
+        # before falling back to random generation
+        index_name = get_from_dict_or_env(
+            kwargs, "index_name", "OPENSEARCH_INDEX_NAME", default=uuid.uuid4().hex
+        )
+        is_appx_search = kwargs.get("is_appx_search", True)
+        vector_field = kwargs.get("vector_field", "vector_field")
+        text_field = kwargs.get("text_field", "text")
+        max_chunk_bytes = kwargs.get("max_chunk_bytes", 1 * 1024 * 1024)
+        http_auth = kwargs.get("http_auth")
+        is_aoss = _is_aoss_enabled(http_auth=http_auth)
+        engine = None
+
+        if is_aoss and not is_appx_search:
+            raise ValueError(
+                "Amazon OpenSearch Service Serverless only "
+                "supports `approximate_search`"
+            )
+
+        if is_appx_search:
+            engine = kwargs.get("engine", "nmslib")
+            space_type = kwargs.get("space_type", "l2")
+            ef_search = kwargs.get("ef_search", 512)
+            ef_construction = kwargs.get("ef_construction", 512)
+            m = kwargs.get("m", 16)
+
+            _validate_aoss_with_engines(is_aoss, engine)
+
+            mapping = _default_text_mapping(
+                dim, engine, space_type, ef_search, ef_construction, m, vector_field
+            )
+        else:
+            mapping = _default_scripting_text_mapping(dim)
+
+        [kwargs.pop(key, None) for key in keys_list]
+        client = _get_async_opensearch_client(opensearch_url, **kwargs)
+        await _abulk_ingest_embeddings(
+            client,
+            index_name,
+            embeddings,
+            texts,
+            ids=ids,
+            metadatas=metadatas,
+            vector_field=vector_field,
             text_field=text_field,
             mapping=mapping,
             max_chunk_bytes=max_chunk_bytes,
             is_aoss=is_aoss,
         )
         kwargs["engine"] = engine
         return cls(opensearch_url, index_name, embedding, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/pgembedding.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/pgembedding.py`

 * *Files 2% similar despite different names*

```diff
@@ -203,17 +203,17 @@
 
         # Execute the queries
         try:
             with Session(self._conn) as session:
                 # Create the HNSW index
                 session.execute(create_index_query)
                 session.commit()
-            print("HNSW extension and index created successfully.")
+            print("HNSW extension and index created successfully.")  # noqa: T201
         except Exception as e:
-            print(f"Failed to create HNSW extension or index: {e}")
+            print(f"Failed to create HNSW extension or index: {e}")  # noqa: T201
 
     def delete_collection(self) -> None:
         self.logger.debug("Trying to delete collection")
         with Session(self._conn) as session:
             collection = self.get_collection(session)
             if not collection:
                 self.logger.warning("Collection not found")
@@ -233,15 +233,15 @@
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
         pre_delete_collection: bool = False,
         **kwargs: Any,
     ) -> PGEmbedding:
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         connection_string = cls.get_connection_string(kwargs)
 
         store = cls(
@@ -284,15 +284,15 @@
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[str]:
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         embeddings = self.embedding_function.embed_documents(list(texts))
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         with Session(self._conn) as session:
@@ -394,15 +394,15 @@
                 .limit(k)
                 .all()
             )
 
         docs = [
             (
                 Document(
-                    page_content=result.EmbeddingStore.document,
+                    page_content=result.EmbeddingStore.document,  # type: ignore[arg-type]
                     metadata=result.EmbeddingStore.cmetadata,
                 ),
                 result.distance if self.embedding_function is not None else 0.0,
             )
             for result in results
         ]
         return docs
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/pgvecto_rs.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/kdbai.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,260 +1,271 @@
 from __future__ import annotations
 
+import logging
 import uuid
-from typing import Any, Iterable, List, Literal, Optional, Tuple, Type
+from typing import Any, Iterable, List, Optional, Tuple
 
-import numpy as np
-import sqlalchemy
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
-from sqlalchemy import insert, select
-from sqlalchemy.dialects import postgresql
-from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
-from sqlalchemy.orm.session import Session
-
-
-class _ORMBase(DeclarativeBase):
-    __tablename__: str
-    id: Mapped[uuid.UUID]
-    text: Mapped[str]
-    meta: Mapped[dict]
-    embedding: Mapped[np.ndarray]
-
-
-class PGVecto_rs(VectorStore):
-    """VectorStore backed by pgvecto_rs."""
-
-    _engine: sqlalchemy.engine.Engine
-    _table: Type[_ORMBase]
-    _embedding: Embeddings
+
+from langchain_community.vectorstores.utils import DistanceStrategy
+
+logger = logging.getLogger(__name__)
+
+
+class KDBAI(VectorStore):
+    """`KDB.AI` vector store.
+
+     See [https://kdb.ai](https://kdb.ai)
+
+    To use, you should have the `kdbai_client` python package installed.
+
+    Args:
+        table: kdbai_client.Table object to use as storage,
+        embedding: Any embedding function implementing
+            `langchain.embeddings.base.Embeddings` interface,
+        distance_strategy: One option from DistanceStrategy.EUCLIDEAN_DISTANCE,
+            DistanceStrategy.DOT_PRODUCT or DistanceStrategy.COSINE.
+
+    See the example [notebook](https://github.com/KxSystems/langchain/blob/KDB.AI/docs/docs/integrations/vectorstores/kdbai.ipynb).
+    """
 
     def __init__(
         self,
+        table: Any,
         embedding: Embeddings,
-        dimension: int,
-        db_url: str,
-        collection_name: str,
-        new_table: bool = False,
-    ) -> None:
-        """Initialize a PGVecto_rs vectorstore.
-
-        Args:
-            embedding: Embeddings to use.
-            dimension: Dimension of the embeddings.
-            db_url: Database URL.
-            collection_name: Name of the collection.
-            new_table: Whether to create a new table or connect to an existing one.
-              Defaults to False.
-        """
+        distance_strategy: Optional[
+            DistanceStrategy
+        ] = DistanceStrategy.EUCLIDEAN_DISTANCE,
+    ):
         try:
-            from pgvecto_rs.sqlalchemy import Vector
-        except ImportError as e:
+            import kdbai_client  # noqa
+        except ImportError:
             raise ImportError(
-                "Unable to import pgvector_rs, please install with "
-                "`pip install pgvector_rs`."
-            ) from e
-
-        class _Table(_ORMBase):
-            __tablename__ = f"collection_{collection_name}"
-            id: Mapped[uuid.UUID] = mapped_column(
-                postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4
+                "Could not import kdbai_client python package. "
+                "Please install it with `pip install kdbai_client`."
             )
-            text: Mapped[str] = mapped_column(sqlalchemy.String)
-            meta: Mapped[dict] = mapped_column(postgresql.JSONB)
-            embedding: Mapped[np.ndarray] = mapped_column(Vector(dimension))
-
-        self._engine = sqlalchemy.create_engine(db_url)
-        self._table = _Table
-        self._table.__table__.create(self._engine, checkfirst=not new_table)  # type: ignore
+        self._table = table
         self._embedding = embedding
+        self.distance_strategy = distance_strategy
 
-    # ================ Create interface =================
-    @classmethod
-    def from_texts(
-        cls,
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        db_url: str = "",
-        collection_name: str = str(uuid.uuid4().hex),
-        **kwargs: Any,
-    ) -> PGVecto_rs:
-        """Return VectorStore initialized from texts and optional metadatas."""
-        sample_embedding = embedding.embed_query("Hello pgvecto_rs!")
-        dimension = len(sample_embedding)
-        if db_url is None:
-            raise ValueError("db_url must be provided")
-        _self: PGVecto_rs = cls(
-            embedding=embedding,
-            dimension=dimension,
-            db_url=db_url,
-            collection_name=collection_name,
-            new_table=True,
-        )
-        _self.add_texts(texts, metadatas, **kwargs)
-        return _self
+    @property
+    def embeddings(self) -> Optional[Embeddings]:
+        if isinstance(self._embedding, Embeddings):
+            return self._embedding
+        return None
+
+    def _embed_documents(self, texts: Iterable[str]) -> List[List[float]]:
+        if isinstance(self._embedding, Embeddings):
+            return self._embedding.embed_documents(list(texts))
+        return [self._embedding(t) for t in texts]
+
+    def _embed_query(self, text: str) -> List[float]:
+        if isinstance(self._embedding, Embeddings):
+            return self._embedding.embed_query(text)
+        return self._embedding(text)
 
-    @classmethod
-    def from_documents(
-        cls,
-        documents: List[Document],
-        embedding: Embeddings,
-        db_url: str = "",
-        collection_name: str = str(uuid.uuid4().hex),
-        **kwargs: Any,
-    ) -> PGVecto_rs:
-        """Return VectorStore initialized from documents."""
-        texts = [document.page_content for document in documents]
-        metadatas = [document.metadata for document in documents]
-        return cls.from_texts(
-            texts, embedding, metadatas, db_url, collection_name, **kwargs
-        )
+    def _insert(
+        self,
+        texts: List[str],
+        ids: Optional[List[str]],
+        metadata: Optional[Any] = None,
+    ) -> None:
+        try:
+            import numpy as np
+        except ImportError:
+            raise ImportError(
+                "Could not import numpy python package. "
+                "Please install it with `pip install numpy`."
+            )
 
-    @classmethod
-    def from_collection_name(
-        cls,
-        embedding: Embeddings,
-        db_url: str,
-        collection_name: str,
-    ) -> PGVecto_rs:
-        """Create new empty vectorstore with collection_name.
-        Or connect to an existing vectorstore in database if exists.
-        Arguments should be the same as when the vectorstore was created."""
-        sample_embedding = embedding.embed_query("Hello pgvecto_rs!")
-        return cls(
-            embedding=embedding,
-            dimension=len(sample_embedding),
-            db_url=db_url,
-            collection_name=collection_name,
-        )
+        try:
+            import pandas as pd
+        except ImportError:
+            raise ImportError(
+                "Could not import pandas python package. "
+                "Please install it with `pip install pandas`."
+            )
 
-    # ================ Insert interface =================
+        embeds = self._embedding.embed_documents(texts)
+        df = pd.DataFrame()
+        df["id"] = ids
+        df["text"] = [t.encode("utf-8") for t in texts]
+        df["embeddings"] = [np.array(e, dtype="float32") for e in embeds]
+        if metadata is not None:
+            df = pd.concat([df, metadata], axis=1)
+        self._table.insert(df, warn=False)
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
+        ids: Optional[List[str]] = None,
+        batch_size: int = 32,
         **kwargs: Any,
     ) -> List[str]:
         """Run more texts through the embeddings and add to the vectorstore.
 
         Args:
-            texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
-            kwargs: vectorstore specific parameters
+            texts (Iterable[str]): Texts to add to the vectorstore.
+            metadatas (Optional[List[dict]]): List of metadata corresponding to each
+                chunk of text.
+            ids (Optional[List[str]]): List of IDs corresponding to each chunk of text.
+            batch_size (Optional[int]): Size of batch of chunks of text to insert at
+                once.
 
         Returns:
-            List of ids of the added texts.
-
+            List[str]: List of IDs of the added texts.
         """
-        embeddings = self._embedding.embed_documents(list(texts))
-        with Session(self._engine) as _session:
-            results: List[str] = []
-            for text, embedding, metadata in zip(
-                texts, embeddings, metadatas or [dict()] * len(list(texts))
-            ):
-                t = insert(self._table).values(
-                    text=text, meta=metadata, embedding=embedding
-                )
-                id = _session.execute(t).inserted_primary_key[0]  # type: ignore
-                results.append(str(id))
-            _session.commit()
-            return results
 
-    def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:
+        try:
+            import pandas as pd
+        except ImportError:
+            raise ImportError(
+                "Could not import pandas python package. "
+                "Please install it with `pip install pandas`."
+            )
+
+        texts = list(texts)
+        metadf: pd.DataFrame = None
+        if metadatas is not None:
+            if isinstance(metadatas, pd.DataFrame):
+                metadf = metadatas
+            else:
+                metadf = pd.DataFrame(metadatas)
+        out_ids: List[str] = []
+        nbatches = (len(texts) - 1) // batch_size + 1
+        for i in range(nbatches):
+            istart = i * batch_size
+            iend = (i + 1) * batch_size
+            batch = texts[istart:iend]
+            if ids:
+                batch_ids = ids[istart:iend]
+            else:
+                batch_ids = [str(uuid.uuid4()) for _ in range(len(batch))]
+            if metadf is not None:
+                batch_meta = metadf.iloc[istart:iend].reset_index(drop=True)
+            else:
+                batch_meta = None
+            self._insert(batch, batch_ids, batch_meta)
+            out_ids = out_ids + batch_ids
+        return out_ids
+
+    def add_documents(
+        self, documents: List[Document], batch_size: int = 32, **kwargs: Any
+    ) -> List[str]:
         """Run more documents through the embeddings and add to the vectorstore.
 
         Args:
-            documents (List[Document]): List of documents to add to the vectorstore.
+            documents (List[Document]: Documents to add to the vectorstore.
+            batch_size (Optional[int]): Size of batch of documents to insert at once.
 
         Returns:
-            List of ids of the added documents.
+            List[str]: List of IDs of the added texts.
         """
-        return self.add_texts(
-            [document.page_content for document in documents],
-            [document.metadata for document in documents],
-            **kwargs,
-        )
 
-    # ================ Query interface =================
-    def similarity_search_with_score_by_vector(
+        try:
+            import pandas as pd
+        except ImportError:
+            raise ImportError(
+                "Could not import pandas python package. "
+                "Please install it with `pip install pandas`."
+            )
+
+        texts = [x.page_content for x in documents]
+        metadata = pd.DataFrame([x.metadata for x in documents])
+        return self.add_texts(texts, metadata=metadata, batch_size=batch_size)
+
+    def similarity_search_with_score(
         self,
-        query_vector: List[float],
-        k: int = 4,
-        distance_func: Literal[
-            "sqrt_euclid", "neg_dot_prod", "ned_cos"
-        ] = "sqrt_euclid",
+        query: str,
+        k: int = 1,
+        filter: Optional[List] = [],
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Return docs most similar to query vector, with its score."""
-        with Session(self._engine) as _session:
-            real_distance_func = (
-                self._table.embedding.squared_euclidean_distance
-                if distance_func == "sqrt_euclid"
-                else self._table.embedding.negative_dot_product_distance
-                if distance_func == "neg_dot_prod"
-                else self._table.embedding.negative_cosine_distance
-                if distance_func == "ned_cos"
-                else None
-            )
-            if real_distance_func is None:
-                raise ValueError("Invalid distance function")
+        """Run similarity search with distance from a query string.
 
-            t = (
-                select(self._table, real_distance_func(query_vector).label("score"))
-                .order_by("score")
-                .limit(k)  # type: ignore
-            )
-            return [
-                (Document(page_content=row[0].text, metadata=row[0].meta), row[1])
-                for row in _session.execute(t)
-            ]
+        Args:
+            query (str): Query string.
+            k (Optional[int]): number of neighbors to retrieve.
+            filter (Optional[List]): KDB.AI metadata filter clause: https://code.kx.com/kdbai/use/filter.html
 
-    def similarity_search_by_vector(
+        Returns:
+            List[Document]: List of similar documents.
+        """
+        return self.similarity_search_by_vector_with_score(
+            self._embed_query(query), k=k, filter=filter, **kwargs
+        )
+
+    def similarity_search_by_vector_with_score(
         self,
         embedding: List[float],
-        k: int = 4,
-        distance_func: Literal[
-            "sqrt_euclid", "neg_dot_prod", "ned_cos"
-        ] = "sqrt_euclid",
+        *,
+        k: int = 1,
+        filter: Optional[List] = [],
         **kwargs: Any,
-    ) -> List[Document]:
-        return [
-            doc
-            for doc, score in self.similarity_search_with_score_by_vector(
-                embedding, k, distance_func, **kwargs
+    ) -> List[Tuple[Document, float]]:
+        """Return documents most similar to embedding, along with scores.
+
+        Args:
+            embedding (List[float]): query vector.
+            k (Optional[int]): number of neighbors to retrieve.
+            filter (Optional[List]): KDB.AI metadata filter clause: https://code.kx.com/kdbai/use/filter.html
+
+        Returns:
+            List[Document]: List of similar documents.
+        """
+        if "n" in kwargs:
+            k = kwargs.pop("n")
+        matches = self._table.search(vectors=[embedding], n=k, filter=filter, **kwargs)
+        docs: list = []
+        if isinstance(matches, list):
+            matches = matches[0]
+        else:
+            return docs
+        for row in matches.to_dict(orient="records"):
+            text = row.pop("text")
+            score = row.pop("__nn_distance")
+            docs.append(
+                (
+                    Document(
+                        page_content=text,
+                        metadata={k: v for k, v in row.items() if k != "text"},
+                    ),
+                    score,
+                )
             )
-        ]
+        return docs
 
-    def similarity_search_with_score(
+    def similarity_search(
         self,
         query: str,
-        k: int = 4,
-        distance_func: Literal[
-            "sqrt_euclid", "neg_dot_prod", "ned_cos"
-        ] = "sqrt_euclid",
+        k: int = 1,
+        filter: Optional[List] = [],
         **kwargs: Any,
-    ) -> List[Tuple[Document, float]]:
-        query_vector = self._embedding.embed_query(query)
-        return self.similarity_search_with_score_by_vector(
-            query_vector, k, distance_func, **kwargs
+    ) -> List[Document]:
+        """Run similarity search from a query string.
+
+        Args:
+            query (str): Query string.
+            k (Optional[int]): number of neighbors to retrieve.
+            filter (Optional[List]): KDB.AI metadata filter clause: https://code.kx.com/kdbai/use/filter.html
+
+        Returns:
+            List[Document]: List of similar documents.
+        """
+        docs_and_scores = self.similarity_search_with_score(
+            query, k=k, filter=filter, **kwargs
         )
+        return [doc for doc, _ in docs_and_scores]
 
-    def similarity_search(
-        self,
-        query: str,
-        k: int = 4,
-        distance_func: Literal[
-            "sqrt_euclid", "neg_dot_prod", "ned_cos"
-        ] = "sqrt_euclid",
+    @classmethod
+    def from_texts(
+        cls: Any,
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
-    ) -> List[Document]:
-        """Return docs most similar to query."""
-        query_vector = self._embedding.embed_query(query)
-        return [
-            doc
-            for doc, score in self.similarity_search_with_score_by_vector(
-                query_vector, k, distance_func, **kwargs
-            )
-        ]
+    ) -> Any:
+        """Not implemented."""
+        raise Exception("Not implemented.")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/pgvector.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/kinetica.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,422 +1,387 @@
 from __future__ import annotations
 
 import asyncio
-import contextlib
 import enum
+import json
 import logging
+import struct
 import uuid
+from collections import OrderedDict
+from enum import Enum
 from functools import partial
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Generator,
-    Iterable,
-    List,
-    Optional,
-    Tuple,
-    Type,
-)
+from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type
 
 import numpy as np
-import sqlalchemy
-from sqlalchemy import delete
-from sqlalchemy.dialects.postgresql import JSON, UUID
-from sqlalchemy.orm import Session, relationship
-
-try:
-    from sqlalchemy.orm import declarative_base
-except ImportError:
-    from sqlalchemy.ext.declarative import declarative_base
-
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
-from langchain_core.utils import get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseSettings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 
 class DistanceStrategy(str, enum.Enum):
     """Enumerator of the Distance strategies."""
 
     EUCLIDEAN = "l2"
     COSINE = "cosine"
     MAX_INNER_PRODUCT = "inner"
 
 
-DEFAULT_DISTANCE_STRATEGY = DistanceStrategy.COSINE
-
-Base = declarative_base()  # type: Any
-
-
-_LANGCHAIN_DEFAULT_COLLECTION_NAME = "langchain"
-
-
-class BaseModel(Base):
-    """Base model for the SQL stores."""
-
-    __abstract__ = True
-    uuid = sqlalchemy.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-
-
-_classes: Any = None
-
-
-def _get_embedding_collection_store() -> Any:
-    global _classes
-    if _classes is not None:
-        return _classes
+def _results_to_docs(docs_and_scores: Any) -> List[Document]:
+    """Return docs from docs and scores."""
+    return [doc for doc, _ in docs_and_scores]
 
-    from pgvector.sqlalchemy import Vector
 
-    class CollectionStore(BaseModel):
-        """Collection store."""
+class Dimension(int, Enum):
+    """Some default dimensions for known embeddings."""
 
-        __tablename__ = "langchain_pg_collection"
+    OPENAI = 1536
 
-        name = sqlalchemy.Column(sqlalchemy.String)
-        cmetadata = sqlalchemy.Column(JSON)
 
-        embeddings = relationship(
-            "EmbeddingStore",
-            back_populates="collection",
-            passive_deletes=True,
-        )
+DEFAULT_DISTANCE_STRATEGY = DistanceStrategy.EUCLIDEAN
 
-        @classmethod
-        def get_by_name(
-            cls, session: Session, name: str
-        ) -> Optional["CollectionStore"]:
-            return session.query(cls).filter(cls.name == name).first()  # type: ignore
+_LANGCHAIN_DEFAULT_SCHEMA_NAME = "langchain"  ## Default Kinetica schema name
+_LANGCHAIN_DEFAULT_COLLECTION_NAME = (
+    "langchain_kinetica_embeddings"  ## Default Kinetica table name
+)
 
-        @classmethod
-        def get_or_create(
-            cls,
-            session: Session,
-            name: str,
-            cmetadata: Optional[dict] = None,
-        ) -> Tuple["CollectionStore", bool]:
-            """
-            Get or create a collection.
-            Returns [Collection, bool] where the bool is True if the collection was created.
-            """  # noqa: E501
-            created = False
-            collection = cls.get_by_name(session, name)
-            if collection:
-                return collection, created
 
-            collection = cls(name=name, cmetadata=cmetadata)
-            session.add(collection)
-            session.commit()
-            created = True
-            return collection, created
+class KineticaSettings(BaseSettings):
+    """`Kinetica` client configuration.
 
-    class EmbeddingStore(BaseModel):
-        """Embedding store."""
+    Attribute:
+        host (str) : An URL to connect to MyScale backend.
+                             Defaults to 'localhost'.
+        port (int) : URL port to connect with HTTP. Defaults to 8443.
+        username (str) : Username to login. Defaults to None.
+        password (str) : Password to login. Defaults to None.
+        database (str) : Database name to find the table. Defaults to 'default'.
+        table (str) : Table name to operate on.
+                      Defaults to 'vector_table'.
+        metric (str) : Metric to compute distance,
+                       supported are ('angular', 'euclidean', 'manhattan', 'hamming',
+                       'dot'). Defaults to 'angular'.
+                       https://github.com/spotify/annoy/blob/main/src/annoymodule.cc#L149-L169
 
-        __tablename__ = "langchain_pg_embedding"
+    """
 
-        collection_id = sqlalchemy.Column(
-            UUID(as_uuid=True),
-            sqlalchemy.ForeignKey(
-                f"{CollectionStore.__tablename__}.uuid",
-                ondelete="CASCADE",
-            ),
-        )
-        collection = relationship(CollectionStore, back_populates="embeddings")
+    host: str = "http://127.0.0.1"
+    port: int = 9191
 
-        embedding: Vector = sqlalchemy.Column(Vector(None))
-        document = sqlalchemy.Column(sqlalchemy.String, nullable=True)
-        cmetadata = sqlalchemy.Column(JSON, nullable=True)
+    username: Optional[str] = None
+    password: Optional[str] = None
 
-        # custom_id : any user defined id
-        custom_id = sqlalchemy.Column(sqlalchemy.String, nullable=True)
+    database: str = _LANGCHAIN_DEFAULT_SCHEMA_NAME
+    table: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME
+    metric: str = DEFAULT_DISTANCE_STRATEGY.value
 
-    _classes = (EmbeddingStore, CollectionStore)
+    def __getitem__(self, item: str) -> Any:
+        return getattr(self, item)
 
-    return _classes
+    class Config:
+        env_file = ".env"
+        env_prefix = "kinetica_"
+        env_file_encoding = "utf-8"
 
 
-def _results_to_docs(docs_and_scores: Any) -> List[Document]:
-    """Return docs from docs and scores."""
-    return [doc for doc, _ in docs_and_scores]
+class Kinetica(VectorStore):
+    """`Kinetica` vector store.
 
-
-class PGVector(VectorStore):
-    """`Postgres`/`PGVector` vector store.
-
-    To use, you should have the ``pgvector`` python package installed.
+    To use, you should have the ``gpudb`` python package installed.
 
     Args:
-        connection_string: Postgres connection string.
+        kinetica_settings: Kinetica connection settings class.
         embedding_function: Any embedding function implementing
             `langchain.embeddings.base.Embeddings` interface.
         collection_name: The name of the collection to use. (default: langchain)
             NOTE: This is not the name of the table, but the name of the collection.
             The tables will be created when initializing the store (if not exists)
             So, make sure the user has the right permissions to create tables.
         distance_strategy: The distance strategy to use. (default: COSINE)
         pre_delete_collection: If True, will delete the collection if it exists.
             (default: False). Useful for testing.
         engine_args: SQLAlchemy's create engine arguments.
 
     Example:
         .. code-block:: python
 
-            from langchain_community.vectorstores import PGVector
+            from langchain_community.vectorstores import Kinetica, KineticaSettings
             from langchain_community.embeddings.openai import OpenAIEmbeddings
 
-            CONNECTION_STRING = "postgresql+psycopg2://hwc@localhost:5432/test3"
-            COLLECTION_NAME = "state_of_the_union_test"
+            kinetica_settings = KineticaSettings(
+                host="http://127.0.0.1", username="", password=""
+                )
+            COLLECTION_NAME = "kinetica_store"
             embeddings = OpenAIEmbeddings()
-            vectorestore = PGVector.from_documents(
-                embedding=embeddings,
+            vectorstore = Kinetica.from_documents(
                 documents=docs,
+                embedding=embeddings,
                 collection_name=COLLECTION_NAME,
-                connection_string=CONNECTION_STRING,
+                config=kinetica_settings,
             )
-
-
     """
 
     def __init__(
         self,
-        connection_string: str,
+        config: KineticaSettings,
         embedding_function: Embeddings,
         collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
-        collection_metadata: Optional[dict] = None,
+        schema_name: str = _LANGCHAIN_DEFAULT_SCHEMA_NAME,
         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
         pre_delete_collection: bool = False,
         logger: Optional[logging.Logger] = None,
         relevance_score_fn: Optional[Callable[[float], float]] = None,
-        *,
-        connection: Optional[sqlalchemy.engine.Connection] = None,
-        engine_args: Optional[dict[str, Any]] = None,
     ) -> None:
-        self.connection_string = connection_string
+        """Constructor for the Kinetica class
+
+        Args:
+            config (KineticaSettings): a `KineticaSettings` instance
+            embedding_function (Embeddings): embedding function to use
+            collection_name (str, optional): the Kinetica table name.
+                            Defaults to _LANGCHAIN_DEFAULT_COLLECTION_NAME.
+            schema_name (str, optional): the Kinetica table name.
+                            Defaults to _LANGCHAIN_DEFAULT_SCHEMA_NAME.
+            distance_strategy (DistanceStrategy, optional): _description_.
+                            Defaults to DEFAULT_DISTANCE_STRATEGY.
+            pre_delete_collection (bool, optional): _description_. Defaults to False.
+            logger (Optional[logging.Logger], optional): _description_.
+                            Defaults to None.
+        """
+
+        self._config = config
         self.embedding_function = embedding_function
         self.collection_name = collection_name
-        self.collection_metadata = collection_metadata
+        self.schema_name = schema_name
         self._distance_strategy = distance_strategy
         self.pre_delete_collection = pre_delete_collection
         self.logger = logger or logging.getLogger(__name__)
         self.override_relevance_score_fn = relevance_score_fn
-        self.engine_args = engine_args or {}
-        # Create a connection if not provided, otherwise use the provided connection
-        self._conn = connection if connection else self.connect()
-        self.__post_init__()
+        self._db = self.__get_db(self._config)
 
-    def __post_init__(
-        self,
-    ) -> None:
-        """Initialize the store."""
-        self.create_vector_extension()
-
-        EmbeddingStore, CollectionStore = _get_embedding_collection_store()
-        self.CollectionStore = CollectionStore
-        self.EmbeddingStore = EmbeddingStore
-        self.create_tables_if_not_exists()
-        self.create_collection()
-
-    def __del__(self) -> None:
-        if self._conn:
-            self._conn.close()
-
-    @property
-    def embeddings(self) -> Embeddings:
-        return self.embedding_function
-
-    def connect(self) -> sqlalchemy.engine.Connection:
-        engine = sqlalchemy.create_engine(self.connection_string, **self.engine_args)
-        conn = engine.connect()
-        return conn
-
-    def create_vector_extension(self) -> None:
+    def __post_init__(self, dimensions: int) -> None:
+        """
+        Initialize the store.
+        """
         try:
-            with Session(self._conn) as session:
-                # The advisor lock fixes issue arising from concurrent
-                # creation of the vector extension.
-                # https://github.com/langchain-ai/langchain/issues/12933
-                # For more information see:
-                # https://www.postgresql.org/docs/16/explicit-locking.html#ADVISORY-LOCKS
-                statement = sqlalchemy.text(
-                    "BEGIN;"
-                    "SELECT pg_advisory_xact_lock(1573678846307946496);"
-                    "CREATE EXTENSION IF NOT EXISTS vector;"
-                    "COMMIT;"
-                )
-                session.execute(statement)
-                session.commit()
-        except Exception as e:
-            raise Exception(f"Failed to create vector extension: {e}") from e
-
-    def create_tables_if_not_exists(self) -> None:
-        with self._conn.begin():
-            Base.metadata.create_all(self._conn)
+            from gpudb import GPUdbTable
+        except ImportError:
+            raise ImportError(
+                "Could not import Kinetica python API. "
+                "Please install it with `pip install gpudb==7.2.0.1`."
+            )
 
-    def drop_tables(self) -> None:
-        with self._conn.begin():
-            Base.metadata.drop_all(self._conn)
+        self.dimensions = dimensions
+        dimension_field = f"vector({dimensions})"
 
-    def create_collection(self) -> None:
         if self.pre_delete_collection:
-            self.delete_collection()
-        with Session(self._conn) as session:
-            self.CollectionStore.get_or_create(
-                session, self.collection_name, cmetadata=self.collection_metadata
-            )
+            self.delete_schema()
 
-    def delete_collection(self) -> None:
-        self.logger.debug("Trying to delete collection")
-        with Session(self._conn) as session:
-            collection = self.get_collection(session)
-            if not collection:
-                self.logger.warning("Collection not found")
-                return
-            session.delete(collection)
-            session.commit()
-
-    @contextlib.contextmanager
-    def _make_session(self) -> Generator[Session, None, None]:
-        """Create a context manager for the session, bind to _conn string."""
-        yield Session(self._conn)
+        self.table_name = self.collection_name
+        if self.schema_name is not None and len(self.schema_name) > 0:
+            self.table_name = f"{self.schema_name}.{self.collection_name}"
+
+        self.table_schema = [
+            ["text", "string"],
+            ["embedding", "bytes", dimension_field],
+            ["metadata", "string", "json"],
+            ["id", "string", "uuid"],
+        ]
 
-    def delete(
-        self,
-        ids: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Delete vectors by ids or uuids.
+        self.create_schema()
+        self.EmbeddingStore: GPUdbTable = self.create_tables_if_not_exists()
 
-        Args:
-            ids: List of ids to delete.
-        """
-        with Session(self._conn) as session:
-            if ids is not None:
-                self.logger.debug(
-                    "Trying to delete vectors by ids (represented by the model "
-                    "using the custom ids field)"
-                )
-                stmt = delete(self.EmbeddingStore).where(
-                    self.EmbeddingStore.custom_id.in_(ids)
-                )
-                session.execute(stmt)
-            session.commit()
+    def __get_db(self, config: KineticaSettings) -> Any:
+        try:
+            from gpudb import GPUdb
+        except ImportError:
+            raise ImportError(
+                "Could not import Kinetica python API. "
+                "Please install it with `pip install gpudb==7.2.0.1`."
+            )
 
-    def get_collection(self, session: Session) -> Any:
-        return self.CollectionStore.get_by_name(session, self.collection_name)
+        options = GPUdb.Options()
+        options.username = config.username
+        options.password = config.password
+        options.skip_ssl_cert_verification = True
+        return GPUdb(host=config.host, options=options)
+
+    @property
+    def embeddings(self) -> Embeddings:
+        return self.embedding_function
 
     @classmethod
     def __from(
         cls,
+        config: KineticaSettings,
         texts: List[str],
         embeddings: List[List[float]],
         embedding: Embeddings,
+        dimensions: int,
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        connection_string: Optional[str] = None,
         pre_delete_collection: bool = False,
+        logger: Optional[logging.Logger] = None,
         **kwargs: Any,
-    ) -> PGVector:
+    ) -> Kinetica:
+        """Class method to assist in constructing the `Kinetica` store instance
+            using different combinations of parameters
+
+        Args:
+            config (KineticaSettings): a `KineticaSettings` instance
+            texts (List[str]): The list of texts to generate embeddings for and store
+            embeddings (List[List[float]]): List of embeddings
+            embedding (Embeddings): the Embedding function
+            dimensions (int): The number of dimensions the embeddings have
+            metadatas (Optional[List[dict]], optional): List of JSON data associated
+                        with each text. Defaults to None.
+            ids (Optional[List[str]], optional): List of unique IDs (UUID by default)
+                        associated with each text. Defaults to None.
+            collection_name (str, optional): Kinetica schema name.
+                        Defaults to _LANGCHAIN_DEFAULT_COLLECTION_NAME.
+            distance_strategy (DistanceStrategy, optional): Not used for now.
+                        Defaults to DEFAULT_DISTANCE_STRATEGY.
+            pre_delete_collection (bool, optional): Whether to delete the Kinetica
+                        schema or not. Defaults to False.
+            logger (Optional[logging.Logger], optional): Logger to use for logging at
+                        different levels. Defaults to None.
+
+        Returns:
+            Kinetica: An instance of Kinetica class
+        """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
-        if connection_string is None:
-            connection_string = cls.get_connection_string(kwargs)
 
         store = cls(
-            connection_string=connection_string,
+            config=config,
             collection_name=collection_name,
             embedding_function=embedding,
+            # dimensions=dimensions,
             distance_strategy=distance_strategy,
             pre_delete_collection=pre_delete_collection,
+            logger=logger,
             **kwargs,
         )
 
+        store.__post_init__(dimensions)
+
         store.add_embeddings(
             texts=texts, embeddings=embeddings, metadatas=metadatas, ids=ids, **kwargs
         )
 
         return store
 
+    def create_tables_if_not_exists(self) -> Any:
+        """Create the table to store the texts and embeddings"""
+
+        try:
+            from gpudb import GPUdbTable
+        except ImportError:
+            raise ImportError(
+                "Could not import Kinetica python API. "
+                "Please install it with `pip install gpudb==7.2.0.1`."
+            )
+        return GPUdbTable(
+            _type=self.table_schema,
+            name=self.table_name,
+            db=self._db,
+            options={"is_replicated": "true"},
+        )
+
+    def drop_tables(self) -> None:
+        """Delete the table"""
+        self._db.clear_table(
+            f"{self.table_name}", options={"no_error_if_not_exists": "true"}
+        )
+
+    def create_schema(self) -> None:
+        """Create a new Kinetica schema"""
+        self._db.create_schema(self.schema_name)
+
+    def delete_schema(self) -> None:
+        """Delete a Kinetica schema with cascade set to `true`
+        This method will delete a schema with all tables in it.
+        """
+        self.logger.debug("Trying to delete collection")
+        self._db.drop_schema(
+            self.schema_name, {"no_error_if_not_exists": "true", "cascade": "true"}
+        )
+
     def add_embeddings(
         self,
         texts: Iterable[str],
         embeddings: List[List[float]],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """Add embeddings to the vectorstore.
 
         Args:
             texts: Iterable of strings to add to the vectorstore.
             embeddings: List of list of embedding vectors.
             metadatas: List of metadatas associated with the texts.
+            ids: List of ids for the text embedding pairs
             kwargs: vectorstore specific parameters
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
-        with Session(self._conn) as session:
-            collection = self.get_collection(session)
-            if not collection:
-                raise ValueError("Collection not found")
-            for text, metadata, embedding, id in zip(texts, metadatas, embeddings, ids):
-                embedding_store = self.EmbeddingStore(
-                    embedding=embedding,
-                    document=text,
-                    cmetadata=metadata,
-                    custom_id=id,
-                    collection_id=collection.uuid,
-                )
-                session.add(embedding_store)
-            session.commit()
+        records = []
+        for text, embedding, metadata, id in zip(texts, embeddings, metadatas, ids):
+            buf = struct.pack("%sf" % self.dimensions, *embedding)
+            records.append([text, buf, json.dumps(metadata), id])
+
+        self.EmbeddingStore.insert_records(records)
 
         return ids
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """Run more texts through the embeddings and add to the vectorstore.
 
         Args:
             texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
+            metadatas: Optional list of metadatas (JSON data) associated with the texts.
+            ids: List of IDs (UUID) for the texts supplied; will be generated if None
             kwargs: vectorstore specific parameters
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
         embeddings = self.embedding_function.embed_documents(list(texts))
+        self.dimensions = len(embeddings[0])
+        if not hasattr(self, "EmbeddingStore"):
+            self.__post_init__(self.dimensions)
         return self.add_embeddings(
             texts=texts, embeddings=embeddings, metadatas=metadatas, ids=ids, **kwargs
         )
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
         filter: Optional[dict] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        """Run similarity search with PGVector with distance.
+        """Run similarity search with Kinetica with distance.
 
         Args:
             query (str): Query text to search for.
             k (int): Number of results to return. Defaults to 4.
             filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
 
         Returns:
@@ -439,119 +404,34 @@
 
         Args:
             query: Text to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
             filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
 
         Returns:
-            List of Documents most similar to the query and score for each.
+            List of Documents most similar to the query and score for each
         """
         embedding = self.embedding_function.embed_query(query)
         docs = self.similarity_search_with_score_by_vector(
             embedding=embedding, k=k, filter=filter
         )
         return docs
 
-    @property
-    def distance_strategy(self) -> Any:
-        if self._distance_strategy == DistanceStrategy.EUCLIDEAN:
-            return self.EmbeddingStore.embedding.l2_distance
-        elif self._distance_strategy == DistanceStrategy.COSINE:
-            return self.EmbeddingStore.embedding.cosine_distance
-        elif self._distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
-            return self.EmbeddingStore.embedding.max_inner_product
-        else:
-            raise ValueError(
-                f"Got unexpected value for distance: {self._distance_strategy}. "
-                f"Should be one of {', '.join([ds.value for ds in DistanceStrategy])}."
-            )
-
     def similarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[dict] = None,
     ) -> List[Tuple[Document, float]]:
-        results = self.__query_collection(embedding=embedding, k=k, filter=filter)
-
-        return self._results_to_docs_and_scores(results)
+        resp: Dict = self.__query_collection(embedding, k, filter)
 
-    def _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:
-        """Return docs and scores from results."""
-        docs = [
-            (
-                Document(
-                    page_content=result.EmbeddingStore.document,
-                    metadata=result.EmbeddingStore.cmetadata,
-                ),
-                result.distance if self.embedding_function is not None else None,
-            )
-            for result in results
-        ]
-        return docs
+        records: OrderedDict = resp["records"]
+        results = list(zip(*list(records.values())))
 
-    def __query_collection(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        filter: Optional[Dict[str, str]] = None,
-    ) -> List[Any]:
-        """Query the collection."""
-        with Session(self._conn) as session:
-            collection = self.get_collection(session)
-            if not collection:
-                raise ValueError("Collection not found")
-
-            filter_by = self.EmbeddingStore.collection_id == collection.uuid
-
-            if filter is not None:
-                filter_clauses = []
-                IN, NIN = "in", "nin"
-                for key, value in filter.items():
-                    if isinstance(value, dict):
-                        value_case_insensitive = {
-                            k.lower(): v for k, v in value.items()
-                        }
-                        if IN in map(str.lower, value):
-                            filter_by_metadata = self.EmbeddingStore.cmetadata[
-                                key
-                            ].astext.in_(value_case_insensitive[IN])
-                        elif NIN in map(str.lower, value):
-                            filter_by_metadata = self.EmbeddingStore.cmetadata[
-                                key
-                            ].astext.not_in(value_case_insensitive[NIN])
-                        else:
-                            filter_by_metadata = None
-                        if filter_by_metadata is not None:
-                            filter_clauses.append(filter_by_metadata)
-                    else:
-                        filter_by_metadata = self.EmbeddingStore.cmetadata[
-                            key
-                        ].astext == str(value)
-                        filter_clauses.append(filter_by_metadata)
-
-                filter_by = sqlalchemy.and_(filter_by, *filter_clauses)
-
-            _type = self.EmbeddingStore
-
-            results: List[Any] = (
-                session.query(
-                    self.EmbeddingStore,
-                    self.distance_strategy(embedding).label("distance"),  # type: ignore
-                )
-                .filter(filter_by)
-                .order_by(sqlalchemy.asc("distance"))
-                .join(
-                    self.CollectionStore,
-                    self.EmbeddingStore.collection_id == self.CollectionStore.uuid,
-                )
-                .limit(k)
-                .all()
-            )
-        return results
+        return self._results_to_docs_and_scores(results)
 
     def similarity_search_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[dict] = None,
         **kwargs: Any,
@@ -565,184 +445,29 @@
 
         Returns:
             List of Documents most similar to the query vector.
         """
         docs_and_scores = self.similarity_search_with_score_by_vector(
             embedding=embedding, k=k, filter=filter
         )
-        return _results_to_docs(docs_and_scores)
+        return [doc for doc, _ in docs_and_scores]
 
-    @classmethod
-    def from_texts(
-        cls: Type[PGVector],
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        ids: Optional[List[str]] = None,
-        pre_delete_collection: bool = False,
-        **kwargs: Any,
-    ) -> PGVector:
-        """
-        Return VectorStore initialized from texts and embeddings.
-        Postgres connection string is required
-        "Either pass it as a parameter
-        or set the PGVECTOR_CONNECTION_STRING environment variable.
-        """
-        embeddings = embedding.embed_documents(list(texts))
-
-        return cls.__from(
-            texts,
-            embeddings,
-            embedding,
-            metadatas=metadatas,
-            ids=ids,
-            collection_name=collection_name,
-            distance_strategy=distance_strategy,
-            pre_delete_collection=pre_delete_collection,
-            **kwargs,
-        )
-
-    @classmethod
-    def from_embeddings(
-        cls,
-        text_embeddings: List[Tuple[str, List[float]]],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        ids: Optional[List[str]] = None,
-        pre_delete_collection: bool = False,
-        **kwargs: Any,
-    ) -> PGVector:
-        """Construct PGVector wrapper from raw documents and pre-
-        generated embeddings.
-
-        Return VectorStore initialized from documents and embeddings.
-        Postgres connection string is required
-        "Either pass it as a parameter
-        or set the PGVECTOR_CONNECTION_STRING environment variable.
-
-        Example:
-            .. code-block:: python
-
-                from langchain_community.vectorstores import PGVector
-                from langchain_community.embeddings import OpenAIEmbeddings
-                embeddings = OpenAIEmbeddings()
-                text_embeddings = embeddings.embed_documents(texts)
-                text_embedding_pairs = list(zip(texts, text_embeddings))
-                faiss = PGVector.from_embeddings(text_embedding_pairs, embeddings)
-        """
-        texts = [t[0] for t in text_embeddings]
-        embeddings = [t[1] for t in text_embeddings]
-
-        return cls.__from(
-            texts,
-            embeddings,
-            embedding,
-            metadatas=metadatas,
-            ids=ids,
-            collection_name=collection_name,
-            distance_strategy=distance_strategy,
-            pre_delete_collection=pre_delete_collection,
-            **kwargs,
-        )
-
-    @classmethod
-    def from_existing_index(
-        cls: Type[PGVector],
-        embedding: Embeddings,
-        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        pre_delete_collection: bool = False,
-        **kwargs: Any,
-    ) -> PGVector:
-        """
-        Get instance of an existing PGVector store.This method will
-        return the instance of the store without inserting any new
-        embeddings
-        """
-
-        connection_string = cls.get_connection_string(kwargs)
-
-        store = cls(
-            connection_string=connection_string,
-            collection_name=collection_name,
-            embedding_function=embedding,
-            distance_strategy=distance_strategy,
-            pre_delete_collection=pre_delete_collection,
-        )
-
-        return store
-
-    @classmethod
-    def get_connection_string(cls, kwargs: Dict[str, Any]) -> str:
-        connection_string: str = get_from_dict_or_env(
-            data=kwargs,
-            key="connection_string",
-            env_key="PGVECTOR_CONNECTION_STRING",
-        )
-
-        if not connection_string:
-            raise ValueError(
-                "Postgres connection string is required"
-                "Either pass it as a parameter"
-                "or set the PGVECTOR_CONNECTION_STRING environment variable."
+    def _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:
+        """Return docs and scores from results."""
+        docs = [
+            (
+                Document(
+                    page_content=result[0],
+                    metadata=json.loads(result[1]),
+                ),
+                result[2] if self.embedding_function is not None else None,
             )
-
-        return connection_string
-
-    @classmethod
-    def from_documents(
-        cls: Type[PGVector],
-        documents: List[Document],
-        embedding: Embeddings,
-        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        ids: Optional[List[str]] = None,
-        pre_delete_collection: bool = False,
-        **kwargs: Any,
-    ) -> PGVector:
-        """
-        Return VectorStore initialized from documents and embeddings.
-        Postgres connection string is required
-        "Either pass it as a parameter
-        or set the PGVECTOR_CONNECTION_STRING environment variable.
-        """
-
-        texts = [d.page_content for d in documents]
-        metadatas = [d.metadata for d in documents]
-        connection_string = cls.get_connection_string(kwargs)
-
-        kwargs["connection_string"] = connection_string
-
-        return cls.from_texts(
-            texts=texts,
-            pre_delete_collection=pre_delete_collection,
-            embedding=embedding,
-            distance_strategy=distance_strategy,
-            metadatas=metadatas,
-            ids=ids,
-            collection_name=collection_name,
-            **kwargs,
-        )
-
-    @classmethod
-    def connection_string_from_db_params(
-        cls,
-        driver: str,
-        host: str,
-        port: int,
-        database: str,
-        user: str,
-        password: str,
-    ) -> str:
-        """Return connection string from database parameters."""
-        return f"postgresql+{driver}://{user}:{password}@{host}:{port}/{database}"
+            for result in results
+        ]
+        return docs
 
     def _select_relevance_score_fn(self) -> Callable[[float], float]:
         """
         The 'correct' relevance function
         may differ depending on a few things, including:
         - the distance / similarity metric used by the VectorStore
         - the scale of your embeddings (OpenAI's are unit normed. Many others are not!)
@@ -760,17 +485,81 @@
             return self._euclidean_relevance_score_fn
         elif self._distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
             return self._max_inner_product_relevance_score_fn
         else:
             raise ValueError(
                 "No supported normalization function"
                 f" for distance_strategy of {self._distance_strategy}."
-                "Consider providing relevance_score_fn to PGVector constructor."
+                "Consider providing relevance_score_fn to Kinetica constructor."
+            )
+
+    @property
+    def distance_strategy(self) -> str:
+        if self._distance_strategy == DistanceStrategy.EUCLIDEAN:
+            return "l2_distance"
+        elif self._distance_strategy == DistanceStrategy.COSINE:
+            return "cosine_distance"
+        elif self._distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
+            return "dot_product"
+        else:
+            raise ValueError(
+                f"Got unexpected value for distance: {self._distance_strategy}. "
+                f"Should be one of {', '.join([ds.value for ds in DistanceStrategy])}."
             )
 
+    def __query_collection(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        filter: Optional[Dict[str, str]] = None,
+    ) -> Dict:
+        """Query the collection."""
+        # if filter is not None:
+        #     filter_clauses = []
+        #     for key, value in filter.items():
+        #         IN = "in"
+        #         if isinstance(value, dict) and IN in map(str.lower, value):
+        #             value_case_insensitive = {
+        #                 k.lower(): v for k, v in value.items()
+        #             }
+        #             filter_by_metadata = self.EmbeddingStore.cmetadata[
+        #                 key
+        #             ].astext.in_(value_case_insensitive[IN])
+        #             filter_clauses.append(filter_by_metadata)
+        #         else:
+        #             filter_by_metadata = self.EmbeddingStore.cmetadata[
+        #                 key
+        #             ].astext == str(value)
+        #             filter_clauses.append(filter_by_metadata)
+
+        json_filter = json.dumps(filter) if filter is not None else None
+        where_clause = (
+            f" where '{json_filter}' = JSON(metadata) "
+            if json_filter is not None
+            else ""
+        )
+
+        embedding_str = "[" + ",".join([str(x) for x in embedding]) + "]"
+
+        dist_strategy = self.distance_strategy
+
+        query_string = f"""
+                SELECT text, metadata, {dist_strategy}(embedding, '{embedding_str}') 
+                as distance, embedding
+                FROM {self.table_name}
+                {where_clause}
+                ORDER BY distance asc NULLS LAST
+                LIMIT {k}
+        """
+
+        self.logger.debug(query_string)
+        resp = self._db.execute_sql_and_decode(query_string)
+        self.logger.debug(resp)
+        return resp
+
     def max_marginal_relevance_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         fetch_k: int = 20,
         lambda_mult: float = 0.5,
         filter: Optional[Dict[str, str]] = None,
@@ -793,17 +582,22 @@
                 Defaults to 0.5.
             filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.
 
         Returns:
             List[Tuple[Document, float]]: List of Documents selected by maximal marginal
                 relevance to the query and score for each.
         """
-        results = self.__query_collection(embedding=embedding, k=fetch_k, filter=filter)
-
-        embedding_list = [result.EmbeddingStore.embedding for result in results]
+        resp = self.__query_collection(embedding=embedding, k=fetch_k, filter=filter)
+        records: OrderedDict = resp["records"]
+        results = list(zip(*list(records.values())))
+
+        embedding_list = [
+            struct.unpack("%sf" % self.dimensions, embedding)
+            for embedding in records["embedding"]
+        ]
 
         mmr_selected = maximal_marginal_relevance(
             np.array(embedding, dtype=np.float32),
             embedding_list,
             k=k,
             lambda_mult=lambda_mult,
         )
@@ -950,7 +744,176 @@
             k=k,
             fetch_k=fetch_k,
             lambda_mult=lambda_mult,
             filter=filter,
             **kwargs,
         )
         return await asyncio.get_event_loop().run_in_executor(None, func)
+
+    @classmethod
+    def from_texts(
+        cls: Type[Kinetica],
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        config: KineticaSettings = KineticaSettings(),
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
+        ids: Optional[List[str]] = None,
+        pre_delete_collection: bool = False,
+        **kwargs: Any,
+    ) -> Kinetica:
+        """Adds the texts passed in to the vector store and returns it
+
+        Args:
+            cls (Type[Kinetica]): Kinetica class
+            texts (List[str]): A list of texts for which the embeddings are generated
+            embedding (Embeddings): List of embeddings
+            metadatas (Optional[List[dict]], optional): List of dicts, JSON
+                        describing the texts/documents. Defaults to None.
+            config (KineticaSettings): a `KineticaSettings` instance
+            collection_name (str, optional): Kinetica schema name.
+                        Defaults to _LANGCHAIN_DEFAULT_COLLECTION_NAME.
+            distance_strategy (DistanceStrategy, optional): Distance strategy
+                        e.g., l2, cosine etc.. Defaults to DEFAULT_DISTANCE_STRATEGY.
+            ids (Optional[List[str]], optional): A list of UUIDs for each
+                        text/document. Defaults to None.
+            pre_delete_collection (bool, optional): Indicates whether the Kinetica
+                        schema is to be deleted or not. Defaults to False.
+
+        Returns:
+            Kinetica: a `Kinetica` instance
+        """
+
+        if len(texts) == 0:
+            raise ValueError("texts is empty")
+
+        try:
+            first_embedding = embedding.embed_documents(texts[0:1])
+        except NotImplementedError:
+            first_embedding = [embedding.embed_query(texts[0])]
+
+        dimensions = len(first_embedding[0])
+        embeddings = embedding.embed_documents(list(texts))
+
+        kinetica_store = cls.__from(
+            texts=texts,
+            embeddings=embeddings,
+            embedding=embedding,
+            dimensions=dimensions,
+            config=config,
+            metadatas=metadatas,
+            ids=ids,
+            collection_name=collection_name,
+            distance_strategy=distance_strategy,
+            pre_delete_collection=pre_delete_collection,
+            **kwargs,
+        )
+
+        return kinetica_store
+
+    @classmethod
+    def from_embeddings(
+        cls: Type[Kinetica],
+        text_embeddings: List[Tuple[str, List[float]]],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        config: KineticaSettings = KineticaSettings(),
+        dimensions: int = Dimension.OPENAI,
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
+        ids: Optional[List[str]] = None,
+        pre_delete_collection: bool = False,
+        **kwargs: Any,
+    ) -> Kinetica:
+        """Adds the embeddings passed in to the vector store and returns it
+
+        Args:
+            cls (Type[Kinetica]): Kinetica class
+            text_embeddings (List[Tuple[str, List[float]]]): A list of texts
+                            and the embeddings
+            embedding (Embeddings): List of embeddings
+            metadatas (Optional[List[dict]], optional): List of dicts, JSON describing
+                        the texts/documents. Defaults to None.
+            config (KineticaSettings): a `KineticaSettings` instance
+            dimensions (int, optional): Dimension for the vector data, if not passed a
+                        default will be used. Defaults to Dimension.OPENAI.
+            collection_name (str, optional): Kinetica schema name.
+                        Defaults to _LANGCHAIN_DEFAULT_COLLECTION_NAME.
+            distance_strategy (DistanceStrategy, optional): Distance strategy
+                        e.g., l2, cosine etc.. Defaults to DEFAULT_DISTANCE_STRATEGY.
+            ids (Optional[List[str]], optional): A list of UUIDs for each text/document.
+                        Defaults to None.
+            pre_delete_collection (bool, optional): Indicates whether the
+                        Kinetica schema is to be deleted or not. Defaults to False.
+
+        Returns:
+            Kinetica: a `Kinetica` instance
+        """
+
+        texts = [t[0] for t in text_embeddings]
+        embeddings = [t[1] for t in text_embeddings]
+        dimensions = len(embeddings[0])
+
+        return cls.__from(
+            texts=texts,
+            embeddings=embeddings,
+            embedding=embedding,
+            dimensions=dimensions,
+            config=config,
+            metadatas=metadatas,
+            ids=ids,
+            collection_name=collection_name,
+            distance_strategy=distance_strategy,
+            pre_delete_collection=pre_delete_collection,
+            **kwargs,
+        )
+
+    @classmethod
+    def from_documents(
+        cls: Type[Kinetica],
+        documents: List[Document],
+        embedding: Embeddings,
+        config: KineticaSettings = KineticaSettings(),
+        metadatas: Optional[List[dict]] = None,
+        collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
+        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
+        ids: Optional[List[str]] = None,
+        pre_delete_collection: bool = False,
+        **kwargs: Any,
+    ) -> Kinetica:
+        """Adds the list of `Document` passed in to the vector store and returns it
+
+        Args:
+            cls (Type[Kinetica]): Kinetica class
+            texts (List[str]): A list of texts for which the embeddings are generated
+            embedding (Embeddings): List of embeddings
+            config (KineticaSettings): a `KineticaSettings` instance
+            metadatas (Optional[List[dict]], optional): List of dicts, JSON describing
+                        the texts/documents. Defaults to None.
+            collection_name (str, optional): Kinetica schema name.
+                        Defaults to _LANGCHAIN_DEFAULT_COLLECTION_NAME.
+            distance_strategy (DistanceStrategy, optional): Distance strategy
+                        e.g., l2, cosine etc.. Defaults to DEFAULT_DISTANCE_STRATEGY.
+            ids (Optional[List[str]], optional): A list of UUIDs for each text/document.
+                        Defaults to None.
+            pre_delete_collection (bool, optional): Indicates whether the Kinetica
+                        schema is to be deleted or not. Defaults to False.
+
+        Returns:
+            Kinetica: a `Kinetica` instance
+        """
+
+        texts = [d.page_content for d in documents]
+        metadatas = [d.metadata for d in documents]
+
+        return cls.from_texts(
+            texts=texts,
+            embedding=embedding,
+            metadatas=metadatas,
+            config=config,
+            collection_name=collection_name,
+            distance_strategy=distance_strategy,
+            ids=ids,
+            pre_delete_collection=pre_delete_collection,
+            **kwargs,
+        )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/pinecone.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/pinecone.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,76 +1,81 @@
 from __future__ import annotations
 
 import logging
+import os
 import uuid
 import warnings
 from typing import TYPE_CHECKING, Any, Callable, Iterable, List, Optional, Tuple, Union
 
 import numpy as np
+from langchain_core._api.deprecation import deprecated
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.utils.iter import batch_iterate
 from langchain_core.vectorstores import VectorStore
+from packaging import version
 
 from langchain_community.vectorstores.utils import (
     DistanceStrategy,
     maximal_marginal_relevance,
 )
 
 if TYPE_CHECKING:
     from pinecone import Index
 
 logger = logging.getLogger(__name__)
 
 
+def _import_pinecone() -> Any:
+    try:
+        import pinecone
+    except ImportError as e:
+        raise ImportError(
+            "Could not import pinecone python package. "
+            "Please install it with `pip install pinecone-client`."
+        ) from e
+    return pinecone
+
+
+def _is_pinecone_v3() -> bool:
+    pinecone = _import_pinecone()
+    pinecone_client_version = pinecone.__version__
+    return version.parse(pinecone_client_version) >= version.parse("3.0.0.dev")
+
+
+@deprecated(
+    since="0.0.18", removal="0.3.0", alternative_import="langchain_pinecone.Pinecone"
+)
 class Pinecone(VectorStore):
     """`Pinecone` vector store.
 
     To use, you should have the ``pinecone-client`` python package installed.
 
-    Example:
-        .. code-block:: python
-
-            from langchain_community.vectorstores import Pinecone
-            from langchain_community.embeddings.openai import OpenAIEmbeddings
-            import pinecone
-
-            # The environment should be the one specified next to the API key
-            # in your Pinecone console
-            pinecone.init(api_key="***", environment="...")
-            index = pinecone.Index("langchain-demo")
-            embeddings = OpenAIEmbeddings()
-            vectorstore = Pinecone(index, embeddings.embed_query, "text")
+    This version of Pinecone is deprecated. Please use `langchain_pinecone.Pinecone`
+    instead.
     """
 
     def __init__(
         self,
         index: Any,
         embedding: Union[Embeddings, Callable],
         text_key: str,
         namespace: Optional[str] = None,
         distance_strategy: Optional[DistanceStrategy] = DistanceStrategy.COSINE,
     ):
         """Initialize with Pinecone client."""
-        try:
-            import pinecone
-        except ImportError:
-            raise ImportError(
-                "Could not import pinecone python package. "
-                "Please install it with `pip install pinecone-client`."
-            )
+        pinecone = _import_pinecone()
         if not isinstance(embedding, Embeddings):
             warnings.warn(
                 "Passing in `embedding` as a Callable is deprecated. Please pass in an"
                 " Embeddings object instead."
             )
-        if not isinstance(index, pinecone.index.Index):
+        if not isinstance(index, pinecone.Index):
             raise ValueError(
-                f"client should be an instance of pinecone.index.Index, "
-                f"got {type(index)}"
+                f"client should be an instance of pinecone.Index, " f"got {type(index)}"
             )
         self._index = index
         self._embedding = embedding
         self._text_key = text_key
         self._namespace = namespace
         self.distance_strategy = distance_strategy
 
@@ -185,15 +190,15 @@
     ) -> List[Tuple[Document, float]]:
         """Return pinecone documents most similar to embedding, along with scores."""
 
         if namespace is None:
             namespace = self._namespace
         docs = []
         results = self._index.query(
-            [embedding],
+            vector=[embedding],
             top_k=k,
             include_metadata=True,
             namespace=namespace,
             filter=filter,
         )
         for res in results["matches"]:
             metadata = res["metadata"]
@@ -283,15 +288,15 @@
                         Defaults to 0.5.
         Returns:
             List of Documents selected by maximal marginal relevance.
         """
         if namespace is None:
             namespace = self._namespace
         results = self._index.query(
-            [embedding],
+            vector=[embedding],
             top_k=fetch_k,
             include_values=True,
             include_metadata=True,
             namespace=namespace,
             filter=filter,
         )
         mmr_selected = maximal_marginal_relevance(
@@ -347,36 +352,41 @@
 
         Args:
             index_name: Name of the index to use.
             pool_threads: Number of threads to use for index upsert.
         Returns:
             Pinecone Index instance."""
 
-        try:
-            import pinecone
-        except ImportError:
-            raise ValueError(
-                "Could not import pinecone python package. "
-                "Please install it with `pip install pinecone-client`."
-            )
+        pinecone = _import_pinecone()
 
-        indexes = pinecone.list_indexes()  # checks if provided index exists
+        if _is_pinecone_v3():
+            pinecone_instance = pinecone.Pinecone(
+                api_key=os.environ.get("PINECONE_API_KEY"), pool_threads=pool_threads
+            )
+            indexes = pinecone_instance.list_indexes()
+            index_names = [i.name for i in indexes.index_list["indexes"]]
+        else:
+            index_names = pinecone.list_indexes()
 
-        if index_name in indexes:
-            index = pinecone.Index(index_name, pool_threads=pool_threads)
-        elif len(indexes) == 0:
+        if index_name in index_names:
+            index = (
+                pinecone_instance.Index(index_name)
+                if _is_pinecone_v3()
+                else pinecone.Index(index_name, pool_threads=pool_threads)
+            )
+        elif len(index_names) == 0:
             raise ValueError(
                 "No active indexes found in your Pinecone project, "
                 "are you sure you're using the right Pinecone API key and Environment? "
                 "Please double check your Pinecone dashboard."
             )
         else:
             raise ValueError(
                 f"Index '{index_name}' not found in your Pinecone project. "
-                f"Did you mean one of the following indexes: {', '.join(indexes)}"
+                f"Did you mean one of the following indexes: {', '.join(index_names)}"
             )
         return index
 
     @classmethod
     def from_texts(
         cls,
         texts: List[str],
@@ -388,38 +398,39 @@
         namespace: Optional[str] = None,
         index_name: Optional[str] = None,
         upsert_kwargs: Optional[dict] = None,
         pool_threads: int = 4,
         embeddings_chunk_size: int = 1000,
         **kwargs: Any,
     ) -> Pinecone:
-        """Construct Pinecone wrapper from raw documents.
+        """
+        DEPRECATED: use langchain_pinecone.PineconeVectorStore.from_texts instead:
+        Construct Pinecone wrapper from raw documents.
 
         This is a user friendly interface that:
             1. Embeds documents.
             2. Adds the documents to a provided Pinecone index
 
         This is intended to be a quick way to get started.
 
         The `pool_threads` affects the speed of the upsert operations.
+
         Example:
             .. code-block:: python
 
-                from langchain_community.vectorstores import Pinecone
-                from langchain_community.embeddings import OpenAIEmbeddings
-                import pinecone
-
-                # The environment should be the one specified next to the API key
-                # in your Pinecone console
-                pinecone.init(api_key="***", environment="...")
+                from langchain_pinecone import PineconeVectorStore
+                from langchain_openai import OpenAIEmbeddings
+
                 embeddings = OpenAIEmbeddings()
-                pinecone = Pinecone.from_texts(
-                    texts,
-                    embeddings,
-                    index_name="langchain-demo"
+                index_name = "my-index"
+                namespace = "my-namespace"
+                vectorstore = Pinecone(
+                    index_name=index_name,
+                    embedding=embedding,
+                    namespace=namespace,
                 )
         """
         pinecone_index = cls.get_pinecone_index(index_name, pool_threads)
         pinecone = cls(pinecone_index, embedding, text_key, namespace, **kwargs)
 
         pinecone.add_texts(
             texts,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/qdrant.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/qdrant.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 from __future__ import annotations
 
-import asyncio
 import functools
 import uuid
 import warnings
 from itertools import islice
 from operator import itemgetter
 from typing import (
     TYPE_CHECKING,
@@ -19,18 +18,20 @@
     Sequence,
     Tuple,
     Type,
     Union,
 )
 
 import numpy as np
-from langchain_core.documents import Document
+from langchain_core._api.deprecation import deprecated
 from langchain_core.embeddings import Embeddings
+from langchain_core.runnables.config import run_in_executor
 from langchain_core.vectorstores import VectorStore
 
+from langchain_community.docstore.document import Document
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 if TYPE_CHECKING:
     from qdrant_client import grpc  # noqa
     from qdrant_client.conversions import common_types
     from qdrant_client.http import models as rest
 
@@ -54,22 +55,24 @@
         try:
             return await method(self, *args, **kwargs)
         except NotImplementedError:
             # If the async method is not implemented, call the synchronous method
             # by removing the first letter from the method name. For example,
             # if the async method is called ``aaad_texts``, the synchronous method
             # will be called ``aad_texts``.
-            sync_method = functools.partial(
-                getattr(self, method.__name__[1:]), *args, **kwargs
+            return await run_in_executor(
+                None, getattr(self, method.__name__[1:]), *args, **kwargs
             )
-            return await asyncio.get_event_loop().run_in_executor(None, sync_method)
 
     return wrapper
 
 
+@deprecated(
+    since="0.0.37", removal="0.3.0", alternative_import="langchain_qdrant.Qdrant"
+)
 class Qdrant(VectorStore):
     """`Qdrant` vector store.
 
     To use you should have the ``qdrant-client`` package installed.
 
     Example:
         .. code-block:: python
@@ -91,14 +94,15 @@
         client: Any,
         collection_name: str,
         embeddings: Optional[Embeddings] = None,
         content_payload_key: str = CONTENT_KEY,
         metadata_payload_key: str = METADATA_KEY,
         distance_strategy: str = "COSINE",
         vector_name: Optional[str] = VECTOR_NAME,
+        async_client: Optional[Any] = None,
         embedding_function: Optional[Callable] = None,  # deprecated
     ):
         """Initialize with necessary components."""
         try:
             import qdrant_client
         except ImportError:
             raise ImportError(
@@ -108,28 +112,37 @@
 
         if not isinstance(client, qdrant_client.QdrantClient):
             raise ValueError(
                 f"client should be an instance of qdrant_client.QdrantClient, "
                 f"got {type(client)}"
             )
 
+        if async_client is not None and not isinstance(
+            async_client, qdrant_client.AsyncQdrantClient
+        ):
+            raise ValueError(
+                f"async_client should be an instance of qdrant_client.AsyncQdrantClient"
+                f"got {type(async_client)}"
+            )
+
         if embeddings is None and embedding_function is None:
             raise ValueError(
                 "`embeddings` value can't be None. Pass `Embeddings` instance."
             )
 
         if embeddings is not None and embedding_function is not None:
             raise ValueError(
                 "Both `embeddings` and `embedding_function` are passed. "
                 "Use `embeddings` only."
             )
 
         self._embeddings = embeddings
         self._embeddings_function = embedding_function
         self.client: qdrant_client.QdrantClient = client
+        self.async_client: Optional[qdrant_client.AsyncQdrantClient] = async_client
         self.collection_name = collection_name
         self.content_payload_key = content_payload_key or self.CONTENT_KEY
         self.metadata_payload_key = metadata_payload_key or self.METADATA_KEY
         self.vector_name = vector_name or self.VECTOR_NAME
 
         if embedding_function is not None:
             warnings.warn(
@@ -205,26 +218,29 @@
             batch_size:
                 How many vectors upload per-request.
                 Default: 64
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
-        from qdrant_client import grpc  # noqa
-        from qdrant_client.conversions.conversion import RestToGrpc
+        from qdrant_client.local.async_qdrant_local import AsyncQdrantLocal
+
+        if self.async_client is None or isinstance(
+            self.async_client._client, AsyncQdrantLocal
+        ):
+            raise NotImplementedError(
+                "QdrantLocal cannot interoperate with sync and async clients"
+            )
 
         added_ids = []
         async for batch_ids, points in self._agenerate_rest_batches(
             texts, metadatas, ids, batch_size
         ):
-            await self.client.async_grpc_points.Upsert(
-                grpc.UpsertPoints(
-                    collection_name=self.collection_name,
-                    points=[RestToGrpc.convert_point_struct(point) for point in points],
-                )
+            await self.async_client.upsert(
+                collection_name=self.collection_name, points=points, **kwargs
             )
             added_ids.extend(batch_ids)
 
         return added_ids
 
     def similarity_search(
         self,
@@ -396,21 +412,22 @@
                 - 'majority' - query all replicas, but return values present in the
                                majority of replicas
                 - 'quorum' - query the majority of replicas, return values present in
                              all of them
                 - 'all' - query all replicas, and return values present in all replicas
             **kwargs:
                 Any other named arguments to pass through to
-                QdrantClient.async_grpc_points.Search().
+                AsyncQdrantClient.Search().
 
         Returns:
             List of documents most similar to the query text and distance for each.
         """
+        query_embedding = await self._aembed_query(query)
         return await self.asimilarity_search_with_score_by_vector(
-            self._embed_query(query),
+            query_embedding,
             k,
             filter=filter,
             search_params=search_params,
             offset=offset,
             score_threshold=score_threshold,
             consistency=consistency,
             **kwargs,
@@ -511,15 +528,15 @@
                 - 'majority' - query all replicas, but return values present in the
                                majority of replicas
                 - 'quorum' - query the majority of replicas, return values present in
                              all of them
                 - 'all' - query all replicas, and return values present in all replicas
             **kwargs:
                 Any other named arguments to pass through to
-                QdrantClient.async_grpc_points.Search().
+                AsyncQdrantClient.Search().
 
         Returns:
             List of Documents most similar to the query.
         """
         results = await self.asimilarity_search_with_score_by_vector(
             embedding,
             k,
@@ -604,71 +621,24 @@
             score_threshold=score_threshold,
             consistency=consistency,
             **kwargs,
         )
         return [
             (
                 self._document_from_scored_point(
-                    result, self.content_payload_key, self.metadata_payload_key
+                    result,
+                    self.collection_name,
+                    self.content_payload_key,
+                    self.metadata_payload_key,
                 ),
                 result.score,
             )
             for result in results
         ]
 
-    async def _asearch_with_score_by_vector(
-        self,
-        embedding: List[float],
-        *,
-        k: int = 4,
-        filter: Optional[MetadataFilter] = None,
-        search_params: Optional[common_types.SearchParams] = None,
-        offset: int = 0,
-        score_threshold: Optional[float] = None,
-        consistency: Optional[common_types.ReadConsistency] = None,
-        with_vectors: bool = False,
-        **kwargs: Any,
-    ) -> Any:
-        """Return results most similar to embedding vector."""
-        from qdrant_client import grpc  # noqa
-        from qdrant_client.conversions.conversion import RestToGrpc
-        from qdrant_client.http import models as rest
-
-        if filter is not None and isinstance(filter, dict):
-            warnings.warn(
-                "Using dict as a `filter` is deprecated. Please use qdrant-client "
-                "filters directly: "
-                "https://qdrant.tech/documentation/concepts/filtering/",
-                DeprecationWarning,
-            )
-            qdrant_filter = self._qdrant_filter_from_dict(filter)
-        else:
-            qdrant_filter = filter
-
-        if qdrant_filter is not None and isinstance(qdrant_filter, rest.Filter):
-            qdrant_filter = RestToGrpc.convert_filter(qdrant_filter)
-
-        response = await self.client.async_grpc_points.Search(
-            grpc.SearchPoints(
-                collection_name=self.collection_name,
-                vector_name=self.vector_name,
-                vector=embedding,
-                filter=qdrant_filter,
-                params=search_params,
-                limit=k,
-                offset=offset,
-                with_payload=grpc.WithPayloadSelector(enable=True),
-                with_vectors=grpc.WithVectorsSelector(enable=with_vectors),
-                score_threshold=score_threshold,
-                read_consistency=consistency,
-                **kwargs,
-            )
-        )
-        return response
-
     @sync_call_fallback
     async def asimilarity_search_with_score_by_vector(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[MetadataFilter] = None,
         search_params: Optional[common_types.SearchParams] = None,
@@ -703,38 +673,66 @@
                 - 'majority' - query all replicas, but return values present in the
                                majority of replicas
                 - 'quorum' - query the majority of replicas, return values present in
                              all of them
                 - 'all' - query all replicas, and return values present in all replicas
             **kwargs:
                 Any other named arguments to pass through to
-                QdrantClient.async_grpc_points.Search().
+                AsyncQdrantClient.Search().
 
         Returns:
             List of documents most similar to the query text and distance for each.
         """
-        response = await self._asearch_with_score_by_vector(
-            embedding,
-            k=k,
-            filter=filter,
+        from qdrant_client.local.async_qdrant_local import AsyncQdrantLocal
+
+        if self.async_client is None or isinstance(
+            self.async_client._client, AsyncQdrantLocal
+        ):
+            raise NotImplementedError(
+                "QdrantLocal cannot interoperate with sync and async clients"
+            )
+        if filter is not None and isinstance(filter, dict):
+            warnings.warn(
+                "Using dict as a `filter` is deprecated. Please use qdrant-client "
+                "filters directly: "
+                "https://qdrant.tech/documentation/concepts/filtering/",
+                DeprecationWarning,
+            )
+            qdrant_filter = self._qdrant_filter_from_dict(filter)
+        else:
+            qdrant_filter = filter
+
+        query_vector = embedding
+        if self.vector_name is not None:
+            query_vector = (self.vector_name, embedding)  # type: ignore[assignment]
+
+        results = await self.async_client.search(
+            collection_name=self.collection_name,
+            query_vector=query_vector,
+            query_filter=qdrant_filter,
             search_params=search_params,
+            limit=k,
             offset=offset,
+            with_payload=True,
+            with_vectors=False,  # Langchain does not expect vectors to be returned
             score_threshold=score_threshold,
             consistency=consistency,
             **kwargs,
         )
-
         return [
             (
-                self._document_from_scored_point_grpc(
-                    result, self.content_payload_key, self.metadata_payload_key
+                self._document_from_scored_point(
+                    result,
+                    self.collection_name,
+                    self.content_payload_key,
+                    self.metadata_payload_key,
                 ),
                 result.score,
             )
-            for result in response.result
+            for result in results
         ]
 
     def max_marginal_relevance_search(
         self,
         query: str,
         k: int = 4,
         fetch_k: int = 20,
@@ -840,19 +838,19 @@
                 - 'majority' - query all replicas, but return values present in the
                                majority of replicas
                 - 'quorum' - query the majority of replicas, return values present in
                              all of them
                 - 'all' - query all replicas, and return values present in all replicas
             **kwargs:
                 Any other named arguments to pass through to
-                QdrantClient.async_grpc_points.Search().
+                AsyncQdrantClient.Search().
         Returns:
             List of Documents selected by maximal marginal relevance.
         """
-        query_embedding = self._embed_query(query)
+        query_embedding = await self._aembed_query(query)
         return await self.amax_marginal_relevance_search_by_vector(
             query_embedding,
             k=k,
             fetch_k=fetch_k,
             lambda_mult=lambda_mult,
             filter=filter,
             search_params=search_params,
@@ -965,15 +963,15 @@
                 - 'majority' - query all replicas, but return values present in the
                                majority of replicas
                 - 'quorum' - query the majority of replicas, return values present in
                              all of them
                 - 'all' - query all replicas, and return values present in all replicas
             **kwargs:
                 Any other named arguments to pass through to
-                QdrantClient.async_grpc_points.Search().
+                AsyncQdrantClient.Search().
         Returns:
             List of Documents selected by maximal marginal relevance and distance for
             each.
         """
         results = await self.amax_marginal_relevance_search_with_score_by_vector(
             embedding,
             k=k,
@@ -1060,15 +1058,18 @@
         ]
         mmr_selected = maximal_marginal_relevance(
             np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult
         )
         return [
             (
                 self._document_from_scored_point(
-                    results[i], self.content_payload_key, self.metadata_payload_key
+                    results[i],
+                    self.collection_name,
+                    self.content_payload_key,
+                    self.metadata_payload_key,
                 ),
                 results[i].score,
             )
             for i in mmr_selected
         ]
 
     @sync_call_fallback
@@ -1096,72 +1097,109 @@
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5.
         Returns:
             List of Documents selected by maximal marginal relevance and distance for
             each.
         """
-        from qdrant_client.conversions.conversion import GrpcToRest
+        from qdrant_client.local.async_qdrant_local import AsyncQdrantLocal
 
-        response = await self._asearch_with_score_by_vector(
-            embedding,
-            k=fetch_k,
-            filter=filter,
+        if self.async_client is None or isinstance(
+            self.async_client._client, AsyncQdrantLocal
+        ):
+            raise NotImplementedError(
+                "QdrantLocal cannot interoperate with sync and async clients"
+            )
+        query_vector = embedding
+        if self.vector_name is not None:
+            query_vector = (self.vector_name, query_vector)  # type: ignore[assignment]
+
+        results = await self.async_client.search(
+            collection_name=self.collection_name,
+            query_vector=query_vector,
+            query_filter=filter,
             search_params=search_params,
+            limit=fetch_k,
+            with_payload=True,
+            with_vectors=True,
             score_threshold=score_threshold,
             consistency=consistency,
-            with_vectors=True,
             **kwargs,
         )
-        results = [
-            GrpcToRest.convert_vectors(result.vectors) for result in response.result
-        ]
-        embeddings: List[List[float]] = [
-            result.get(self.vector_name)  # type: ignore
-            if isinstance(result, dict)
-            else result
+        embeddings = [
+            result.vector.get(self.vector_name)  # type: ignore[index, union-attr]
+            if self.vector_name is not None
+            else result.vector
             for result in results
         ]
-        mmr_selected: List[int] = maximal_marginal_relevance(
-            np.array(embedding),
-            embeddings,
-            k=k,
-            lambda_mult=lambda_mult,
+        mmr_selected = maximal_marginal_relevance(
+            np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult
         )
         return [
             (
-                self._document_from_scored_point_grpc(
-                    response.result[i],
+                self._document_from_scored_point(
+                    results[i],
+                    self.collection_name,
                     self.content_payload_key,
                     self.metadata_payload_key,
                 ),
-                response.result[i].score,
+                results[i].score,
             )
             for i in mmr_selected
         ]
 
     def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> Optional[bool]:
         """Delete by vector ID or other criteria.
 
         Args:
             ids: List of ids to delete.
             **kwargs: Other keyword arguments that subclasses might use.
 
         Returns:
-            Optional[bool]: True if deletion is successful,
-            False otherwise, None if not implemented.
+            True if deletion is successful, False otherwise.
         """
         from qdrant_client.http import models as rest
 
         result = self.client.delete(
             collection_name=self.collection_name,
             points_selector=ids,
         )
         return result.status == rest.UpdateStatus.COMPLETED
 
+    @sync_call_fallback
+    async def adelete(
+        self, ids: Optional[List[str]] = None, **kwargs: Any
+    ) -> Optional[bool]:
+        """Delete by vector ID or other criteria.
+
+        Args:
+            ids: List of ids to delete.
+            **kwargs: Other keyword arguments that subclasses might use.
+
+        Returns:
+            True if deletion is successful, False otherwise.
+        """
+        from qdrant_client.local.async_qdrant_local import AsyncQdrantLocal
+
+        if self.async_client is None or isinstance(
+            self.async_client._client, AsyncQdrantLocal
+        ):
+            raise NotImplementedError(
+                "QdrantLocal cannot interoperate with sync and async clients"
+            )
+
+        from qdrant_client.http import models as rest
+
+        result = await self.async_client.delete(
+            collection_name=self.collection_name,
+            points_selector=ids,
+        )
+
+        return result.status == rest.UpdateStatus.COMPLETED
+
     @classmethod
     def from_texts(
         cls: Type[Qdrant],
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         ids: Optional[Sequence[str]] = None,
@@ -1330,14 +1368,59 @@
             force_recreate,
             **kwargs,
         )
         qdrant.add_texts(texts, metadatas, ids, batch_size)
         return qdrant
 
     @classmethod
+    def from_existing_collection(
+        cls: Type[Qdrant],
+        embedding: Embeddings,
+        path: str,
+        collection_name: str,
+        location: Optional[str] = None,
+        url: Optional[str] = None,
+        port: Optional[int] = 6333,
+        grpc_port: int = 6334,
+        prefer_grpc: bool = False,
+        https: Optional[bool] = None,
+        api_key: Optional[str] = None,
+        prefix: Optional[str] = None,
+        timeout: Optional[float] = None,
+        host: Optional[str] = None,
+        **kwargs: Any,
+    ) -> Qdrant:
+        """
+        Get instance of an existing Qdrant collection.
+        This method will return the instance of the store without inserting any new
+        embeddings
+        """
+        client, async_client = cls._generate_clients(
+            location=location,
+            url=url,
+            port=port,
+            grpc_port=grpc_port,
+            prefer_grpc=prefer_grpc,
+            https=https,
+            api_key=api_key,
+            prefix=prefix,
+            timeout=timeout,
+            host=host,
+            path=path,
+            **kwargs,
+        )
+        return cls(
+            client=client,
+            async_client=async_client,
+            collection_name=collection_name,
+            embeddings=embedding,
+            **kwargs,
+        )
+
+    @classmethod
     @sync_call_fallback
     async def afrom_texts(
         cls: Type[Qdrant],
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         ids: Optional[Sequence[str]] = None,
@@ -1540,30 +1623,30 @@
         quantization_config: Optional[common_types.QuantizationConfig] = None,
         init_from: Optional[common_types.InitFrom] = None,
         on_disk: Optional[bool] = None,
         force_recreate: bool = False,
         **kwargs: Any,
     ) -> Qdrant:
         try:
-            import qdrant_client
+            import qdrant_client  # noqa
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import qdrant-client python package. "
                 "Please install it with `pip install qdrant-client`."
             )
         from grpc import RpcError
         from qdrant_client.http import models as rest
         from qdrant_client.http.exceptions import UnexpectedResponse
 
         # Just do a single quick embedding to get vector size
         partial_embeddings = embedding.embed_documents(texts[:1])
         vector_size = len(partial_embeddings[0])
         collection_name = collection_name or uuid.uuid4().hex
         distance_func = distance_func.upper()
-        client = qdrant_client.QdrantClient(
+        client, async_client = cls._generate_clients(
             location=location,
             url=url,
             port=port,
             grpc_port=grpc_port,
             prefer_grpc=prefer_grpc,
             https=https,
             api_key=api_key,
@@ -1666,14 +1749,15 @@
             client=client,
             collection_name=collection_name,
             embeddings=embedding,
             content_payload_key=content_payload_key,
             metadata_payload_key=metadata_payload_key,
             distance_strategy=distance_func,
             vector_name=vector_name,
+            async_client=async_client,
         )
         return qdrant
 
     @classmethod
     async def aconstruct_instance(
         cls: Type[Qdrant],
         texts: List[str],
@@ -1704,30 +1788,30 @@
         quantization_config: Optional[common_types.QuantizationConfig] = None,
         init_from: Optional[common_types.InitFrom] = None,
         on_disk: Optional[bool] = None,
         force_recreate: bool = False,
         **kwargs: Any,
     ) -> Qdrant:
         try:
-            import qdrant_client
+            import qdrant_client  # noqa
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import qdrant-client python package. "
                 "Please install it with `pip install qdrant-client`."
             )
         from grpc import RpcError
         from qdrant_client.http import models as rest
         from qdrant_client.http.exceptions import UnexpectedResponse
 
         # Just do a single quick embedding to get vector size
         partial_embeddings = await embedding.aembed_documents(texts[:1])
         vector_size = len(partial_embeddings[0])
         collection_name = collection_name or uuid.uuid4().hex
         distance_func = distance_func.upper()
-        client = qdrant_client.QdrantClient(
+        client, async_client = cls._generate_clients(
             location=location,
             url=url,
             port=port,
             grpc_port=grpc_port,
             prefer_grpc=prefer_grpc,
             https=https,
             api_key=api_key,
@@ -1830,14 +1914,15 @@
             client=client,
             collection_name=collection_name,
             embeddings=embedding,
             content_payload_key=content_payload_key,
             metadata_payload_key=metadata_payload_key,
             distance_strategy=distance_func,
             vector_name=vector_name,
+            async_client=async_client,
         )
         return qdrant
 
     @staticmethod
     def _cosine_relevance_score_fn(distance: float) -> float:
         """Normalize the distance to a score on a scale [0, 1]."""
         return (distance + 1.0) / 2.0
@@ -1911,35 +1996,24 @@
 
         return payloads
 
     @classmethod
     def _document_from_scored_point(
         cls,
         scored_point: Any,
+        collection_name: str,
         content_payload_key: str,
         metadata_payload_key: str,
     ) -> Document:
+        metadata = scored_point.payload.get(metadata_payload_key) or {}
+        metadata["_id"] = scored_point.id
+        metadata["_collection_name"] = collection_name
         return Document(
             page_content=scored_point.payload.get(content_payload_key),
-            metadata=scored_point.payload.get(metadata_payload_key) or {},
-        )
-
-    @classmethod
-    def _document_from_scored_point_grpc(
-        cls,
-        scored_point: Any,
-        content_payload_key: str,
-        metadata_payload_key: str,
-    ) -> Document:
-        from qdrant_client.conversions.conversion import grpc_to_payload
-
-        payload = grpc_to_payload(scored_point.payload)
-        return Document(
-            page_content=payload[content_payload_key],
-            metadata=payload.get(metadata_payload_key) or {},
+            metadata=metadata,
         )
 
     def _build_condition(self, key: str, value: Any) -> List[rest.FieldCondition]:
         from qdrant_client.http import models as rest
 
         out = []
 
@@ -1994,14 +2068,34 @@
         else:
             if self._embeddings_function is not None:
                 embedding = self._embeddings_function(query)
             else:
                 raise ValueError("Neither of embeddings or embedding_function is set")
         return embedding.tolist() if hasattr(embedding, "tolist") else embedding
 
+    async def _aembed_query(self, query: str) -> List[float]:
+        """Embed query text asynchronously.
+
+        Used to provide backward compatibility with `embedding_function` argument.
+
+        Args:
+            query: Query text.
+
+        Returns:
+            List of floats representing the query embedding.
+        """
+        if self.embeddings is not None:
+            embedding = await self.embeddings.aembed_query(query)
+        else:
+            if self._embeddings_function is not None:
+                embedding = self._embeddings_function(query)
+            else:
+                raise ValueError("Neither of embeddings or embedding_function is set")
+        return embedding.tolist() if hasattr(embedding, "tolist") else embedding
+
     def _embed_texts(self, texts: Iterable[str]) -> List[List[float]]:
         """Embed search texts.
 
         Used to provide backward compatibility with `embedding_function` argument.
 
         Args:
             texts: Iterable of texts to embed.
@@ -2131,7 +2225,61 @@
                         self.content_payload_key,
                         self.metadata_payload_key,
                     ),
                 )
             ]
 
             yield batch_ids, points
+
+    @staticmethod
+    def _generate_clients(
+        location: Optional[str] = None,
+        url: Optional[str] = None,
+        port: Optional[int] = 6333,
+        grpc_port: int = 6334,
+        prefer_grpc: bool = False,
+        https: Optional[bool] = None,
+        api_key: Optional[str] = None,
+        prefix: Optional[str] = None,
+        timeout: Optional[float] = None,
+        host: Optional[str] = None,
+        path: Optional[str] = None,
+        **kwargs: Any,
+    ) -> Tuple[Any, Any]:
+        from qdrant_client import AsyncQdrantClient, QdrantClient
+
+        sync_client = QdrantClient(
+            location=location,
+            url=url,
+            port=port,
+            grpc_port=grpc_port,
+            prefer_grpc=prefer_grpc,
+            https=https,
+            api_key=api_key,
+            prefix=prefix,
+            timeout=timeout,
+            host=host,
+            path=path,
+            **kwargs,
+        )
+
+        if location == ":memory:" or path is not None:
+            # Local Qdrant cannot co-exist with Sync and Async clients
+            # We fallback to sync operations in this case
+            async_client = None
+        else:
+            async_client = AsyncQdrantClient(
+                location=location,
+                url=url,
+                port=port,
+                grpc_port=grpc_port,
+                prefer_grpc=prefer_grpc,
+                https=https,
+                api_key=api_key,
+                prefix=prefix,
+                timeout=timeout,
+                host=host,
+                path=path,
+                **kwargs,
+            )
+
+        return sync_client, async_client
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/base.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/redis/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,15 +19,18 @@
     Union,
     cast,
 )
 
 import numpy as np
 import yaml
 from langchain_core._api import deprecated
-from langchain_core.callbacks import CallbackManagerForRetrieverRun
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForRetrieverRun,
+    CallbackManagerForRetrieverRun,
+)
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.utils import get_from_dict_or_env
 from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
 
 from langchain_community.utilities.redis import (
     _array_to_buffer,
@@ -38,14 +41,15 @@
 from langchain_community.vectorstores.redis.constants import (
     REDIS_REQUIRED_MODULES,
     REDIS_TAG_SEPARATOR,
 )
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 logger = logging.getLogger(__name__)
+ListOfDict = List[Dict[str, str]]
 
 if TYPE_CHECKING:
     from redis.client import Redis as RedisType
     from redis.commands.search.query import Query
 
     from langchain_community.vectorstores.redis.filters import RedisFilterExpression
     from langchain_community.vectorstores.redis.schema import RedisModel
@@ -192,15 +196,15 @@
     hybrid querying (filtering) capability of Redis.
 
     By default, this implementation will automatically generate the index
     schema according to the following rules:
         - All strings are indexed as text fields
         - All numbers are indexed as numeric fields
         - All lists of strings are indexed as tag fields (joined by
-            langchain.vectorstores.redis.constants.REDIS_TAG_SEPARATOR)
+            langchain_community.vectorstores.redis.constants.REDIS_TAG_SEPARATOR)
         - All None values are not indexed but still stored in Redis these are
             not retrievable through the interface here, but the raw Redis client
             can be used to retrieve them.
         - All other types are not indexed
 
     To override these rules, you can pass in a custom index schema like the following
 
@@ -242,15 +246,15 @@
     }
 
     def __init__(
         self,
         redis_url: str,
         index_name: str,
         embedding: Embeddings,
-        index_schema: Optional[Union[Dict[str, str], str, os.PathLike]] = None,
+        index_schema: Optional[Union[Dict[str, ListOfDict], str, os.PathLike]] = None,
         vector_schema: Optional[Dict[str, Union[str, int]]] = None,
         relevance_score_fn: Optional[Callable[[float], float]] = None,
         key_prefix: Optional[str] = None,
         **kwargs: Any,
     ):
         """Initialize Redis vector store with necessary components."""
         self._check_deprecated_kwargs(kwargs)
@@ -286,15 +290,15 @@
     @classmethod
     def from_texts_return_keys(
         cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         index_name: Optional[str] = None,
-        index_schema: Optional[Union[Dict[str, str], str, os.PathLike]] = None,
+        index_schema: Optional[Union[Dict[str, ListOfDict], str, os.PathLike]] = None,
         vector_schema: Optional[Dict[str, Union[str, int]]] = None,
         **kwargs: Any,
     ) -> Tuple[Redis, List[str]]:
         """Create a Redis vectorstore from raw documents.
 
         This is a user-friendly interface that:
             1. Embeds documents.
@@ -328,15 +332,16 @@
         Args:
             texts (List[str]): List of texts to add to the vectorstore.
             embedding (Embeddings): Embeddings to use for the vectorstore.
             metadatas (Optional[List[dict]], optional): Optional list of metadata
                 dicts to add to the vectorstore. Defaults to None.
             index_name (Optional[str], optional): Optional name of the index to
                 create or add to. Defaults to None.
-            index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional):
+            index_schema (Optional[Union[Dict[str, ListOfDict], str, os.PathLike]],
+                optional):
                 Optional fields to index within the metadata. Overrides generated
                 schema. Defaults to None.
             vector_schema (Optional[Dict[str, Union[str, int]]], optional): Optional
                 vector schema to use. Defaults to None.
             **kwargs (Any): Additional keyword arguments to pass to the Redis client.
 
         Returns:
@@ -421,15 +426,15 @@
     @classmethod
     def from_texts(
         cls: Type[Redis],
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         index_name: Optional[str] = None,
-        index_schema: Optional[Union[Dict[str, str], str, os.PathLike]] = None,
+        index_schema: Optional[Union[Dict[str, ListOfDict], str, os.PathLike]] = None,
         vector_schema: Optional[Dict[str, Union[str, int]]] = None,
         **kwargs: Any,
     ) -> Redis:
         """Create a Redis vectorstore from a list of texts.
 
         This is a user-friendly interface that:
             1. Embeds documents.
@@ -464,15 +469,16 @@
             texts (List[str]): List of texts to add to the vectorstore.
             embedding (Embeddings): Embedding model class (i.e. OpenAIEmbeddings)
                 for embedding queries.
             metadatas (Optional[List[dict]], optional): Optional list of metadata dicts
                 to add to the vectorstore. Defaults to None.
             index_name (Optional[str], optional): Optional name of the index to create
                 or add to. Defaults to None.
-            index_schema (Optional[Union[Dict[str, str], str, os.PathLike]], optional):
+            index_schema (Optional[Union[Dict[str, ListOfDict], str, os.PathLike]],
+                optional):
                 Optional fields to index within the metadata. Overrides generated
                 schema. Defaults to None.
             vector_schema (Optional[Dict[str, Union[str, int]]], optional): Optional
                 vector schema to use. Defaults to None.
             **kwargs (Any): Additional keyword arguments to pass to the Redis client.
 
         Returns:
@@ -494,15 +500,15 @@
         return instance
 
     @classmethod
     def from_existing_index(
         cls,
         embedding: Embeddings,
         index_name: str,
-        schema: Union[Dict[str, str], str, os.PathLike],
+        schema: Union[Dict[str, ListOfDict], str, os.PathLike, Dict[str, ListOfDict]],
         key_prefix: Optional[str] = None,
         **kwargs: Any,
     ) -> Redis:
         """Connect to an existing Redis index.
 
         Example:
             .. code-block:: python
@@ -521,16 +527,17 @@
                     redis_url="redis://username:password@localhost:6379",
                 )
 
         Args:
             embedding (Embeddings): Embedding model class (i.e. OpenAIEmbeddings)
                 for embedding queries.
             index_name (str): Name of the index to connect to.
-            schema (Union[Dict[str, str], str, os.PathLike]): Schema of the index
-                and the vector schema. Can be a dict, or path to yaml file.
+            schema (Union[Dict[str, str], str, os.PathLike, Dict[str, ListOfDict]]):
+                Schema of the index and the vector schema. Can be a dict, or path to
+                yaml file.
             key_prefix (Optional[str]): Prefix to use for all keys in Redis associated
                 with this index.
             **kwargs (Any): Additional keyword arguments to pass to the Redis client.
 
         Returns:
             Redis: Redis VectorStore instance.
 
@@ -599,15 +606,15 @@
 
         if ids is None:
             raise ValueError("'ids' (keys)() were not provided.")
 
         try:
             import redis  # noqa: F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import redis python package. "
                 "Please install it with `pip install redis`."
             )
         try:
             # We need to first remove redis_url from kwargs,
             # otherwise passing it to Redis will result in an error.
             if "redis_url" in kwargs:
@@ -640,15 +647,15 @@
         Returns:
             bool: Whether or not the drop was successful.
         """
         redis_url = get_from_dict_or_env(kwargs, "redis_url", "REDIS_URL")
         try:
             import redis  # noqa: F401
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import redis python package. "
                 "Please install it with `pip install redis`."
             )
         try:
             # We need to first remove redis_url from kwargs,
             # otherwise passing it to Redis will result in an error.
             if "redis_url" in kwargs:
@@ -735,15 +742,15 @@
         return ids
 
     def as_retriever(self, **kwargs: Any) -> RedisVectorStoreRetriever:
         tags = kwargs.pop("tags", None) or []
         tags.extend(self._get_retriever_tags())
         return RedisVectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)
 
-    @deprecated("0.0.272", alternative="similarity_search(distance_threshold=0.1)")
+    @deprecated("0.0.1", alternative="similarity_search(distance_threshold=0.1)")
     def similarity_search_limit_score(
         self, query: str, k: int = 4, score_threshold: float = 0.2, **kwargs: Any
     ) -> List[Document]:
         """
         Returns the most similar indexed documents to the query text within the
         score_threshold range.
 
@@ -1111,15 +1118,15 @@
                 "Please install it with `pip install redis`."
             ) from e
         return_fields = return_fields or []
         vector_key = self._schema.content_vector_key
         base_query = f"@{vector_key}:[VECTOR_RANGE $distance_threshold $vector]"
 
         if filter:
-            base_query = "(" + base_query + " " + str(filter) + ")"
+            base_query = str(filter) + " " + base_query
 
         query_string = base_query + "=>{$yield_distance_as: distance}"
 
         return (
             Query(query_string)
             .return_fields(*return_fields)
             .sort_by("distance")
@@ -1163,15 +1170,15 @@
             .paging(0, k)
             .dialect(2)
         )
         return query
 
     def _get_schema_with_defaults(
         self,
-        index_schema: Optional[Union[Dict[str, str], str, os.PathLike]] = None,
+        index_schema: Optional[Union[Dict[str, ListOfDict], str, os.PathLike]] = None,
         vector_schema: Optional[Dict[str, Union[str, int]]] = None,
     ) -> "RedisModel":
         # should only be called after init of Redis (so Import handled)
         from langchain_community.vectorstores.redis.schema import (
             RedisModel,
             read_schema,
         )
@@ -1460,14 +1467,45 @@
             docs = self.vectorstore.max_marginal_relevance_search(
                 query, **self.search_kwargs
             )
         else:
             raise ValueError(f"search_type of {self.search_type} not allowed.")
         return docs
 
+    async def _aget_relevant_documents(
+        self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
+    ) -> List[Document]:
+        if self.search_type == "similarity":
+            docs = await self.vectorstore.asimilarity_search(
+                query, **self.search_kwargs
+            )
+        elif self.search_type == "similarity_distance_threshold":
+            if self.search_kwargs["distance_threshold"] is None:
+                raise ValueError(
+                    "distance_threshold must be provided for "
+                    + "similarity_distance_threshold retriever"
+                )
+            docs = await self.vectorstore.asimilarity_search(
+                query, **self.search_kwargs
+            )
+        elif self.search_type == "similarity_score_threshold":
+            docs_and_similarities = (
+                await self.vectorstore.asimilarity_search_with_relevance_scores(
+                    query, **self.search_kwargs
+                )
+            )
+            docs = [doc for doc, _ in docs_and_similarities]
+        elif self.search_type == "mmr":
+            docs = await self.vectorstore.amax_marginal_relevance_search(
+                query, **self.search_kwargs
+            )
+        else:
+            raise ValueError(f"search_type of {self.search_type} not allowed.")
+        return docs
+
     def add_documents(self, documents: List[Document], **kwargs: Any) -> List[str]:
         """Add documents to vectorstore."""
         return self.vectorstore.add_documents(documents, **kwargs)
 
     async def aadd_documents(
         self, documents: List[Document], **kwargs: Any
     ) -> List[str]:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/filters.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/redis/filters.py`

 * *Files 1% similar despite different names*

```diff
@@ -96,15 +96,15 @@
             )
         return func(instance, *args, **kwargs)
 
     return wrapper
 
 
 class RedisTag(RedisFilterField):
-    """A RedisFilterField representing a tag in a Redis index."""
+    """RedisFilterField representing a tag in a Redis index."""
 
     OPERATORS: Dict[RedisFilterOperator, str] = {
         RedisFilterOperator.EQ: "==",
         RedisFilterOperator.NE: "!=",
         RedisFilterOperator.IN: "==",
     }
     OPERATOR_MAP: Dict[RedisFilterOperator, str] = {
@@ -188,15 +188,15 @@
         return self.OPERATOR_MAP[self._operator] % (
             self._field,
             self._formatted_tag_value,
         )
 
 
 class RedisNum(RedisFilterField):
-    """A RedisFilterField representing a numeric field in a Redis index."""
+    """RedisFilterField representing a numeric field in a Redis index."""
 
     OPERATORS: Dict[RedisFilterOperator, str] = {
         RedisFilterOperator.EQ: "==",
         RedisFilterOperator.NE: "!=",
         RedisFilterOperator.LT: "<",
         RedisFilterOperator.GT: ">",
         RedisFilterOperator.LE: "<=",
@@ -307,15 +307,15 @@
             >>> filter = RedisNum("age") <= 18
         """
         self._set_value(other, self.SUPPORTED_VAL_TYPES, RedisFilterOperator.LE)  # type: ignore
         return RedisFilterExpression(str(self))
 
 
 class RedisText(RedisFilterField):
-    """A RedisFilterField representing a text field in a Redis index."""
+    """RedisFilterField representing a text field in a Redis index."""
 
     OPERATORS: Dict[RedisFilterOperator, str] = {
         RedisFilterOperator.EQ: "==",
         RedisFilterOperator.NE: "!=",
         RedisFilterOperator.LIKE: "%",
     }
     OPERATOR_MAP: Dict[RedisFilterOperator, str] = {
@@ -377,15 +377,15 @@
         return self.OPERATOR_MAP[self._operator] % (
             self._field,
             self._value,
         )
 
 
 class RedisFilterExpression:
-    """A logical expression of RedisFilterFields.
+    """Logical expression of RedisFilterFields.
 
     RedisFilterExpressions can be combined using the & and | operators to create
     complex logical expressions that evaluate to the Redis Query language.
 
     This presents an interface by which users can create complex queries
     without having to know the Redis Query language.
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/redis/schema.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/redis/schema.py`

 * *Files 0% similar despite different names*

```diff
@@ -281,15 +281,15 @@
                         keys.append(field.name)
         return keys
 
 
 def read_schema(
     index_schema: Optional[Union[Dict[str, List[Any]], str, os.PathLike]],
 ) -> Dict[str, Any]:
-    """Reads in the index schema from a dict or yaml file.
+    """Read in the index schema from a dict or yaml file.
 
     Check if it is a dict and return RedisModel otherwise, check if it's a path and
     read in the file assuming it's a yaml file and return a RedisModel
     """
     if isinstance(index_schema, dict):
         return index_schema
     elif isinstance(index_schema, Path):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/rocksetdb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/tidb_vector.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,334 +1,364 @@
-from __future__ import annotations
-
-import logging
-from enum import Enum
-from typing import Any, Iterable, List, Optional, Tuple
+import uuid
+from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
-logger = logging.getLogger(__name__)
+DEFAULT_DISTANCE_STRATEGY = "cosine"  # or "l2", "inner_product"
+DEFAULT_TiDB_VECTOR_TABLE_NAME = "langchain_vector"
 
 
-class Rockset(VectorStore):
-    """`Rockset` vector store.
+class TiDBVectorStore(VectorStore):
+    """TiDB Vector Store."""
 
-    To use, you should have the `rockset` python package installed. Note that to use
-    this, the collection being used must already exist in your Rockset instance.
-    You must also ensure you use a Rockset ingest transformation to apply
-    `VECTOR_ENFORCE` on the column being used to store `embedding_key` in the
-    collection.
-    See: https://rockset.com/blog/introducing-vector-search-on-rockset/ for more details
+    def __init__(
+        self,
+        connection_string: str,
+        embedding_function: Embeddings,
+        table_name: str = DEFAULT_TiDB_VECTOR_TABLE_NAME,
+        distance_strategy: str = DEFAULT_DISTANCE_STRATEGY,
+        *,
+        engine_args: Optional[Dict[str, Any]] = None,
+        drop_existing_table: bool = False,
+        **kwargs: Any,
+    ) -> None:
+        """
+        Initialize a TiDB Vector Store in Langchain with a flexible
+        and standardized table structure for storing vector data
+        which remains fixed regardless of the dynamic table name setting.
+
+        The vector table schema includes:
+        - 'id': a UUID for each entry.
+        - 'embedding': stores vector data in a VectorType column.
+        - 'document': a Text column for the original data or additional information.
+        - 'meta': a JSON column for flexible metadata storage.
+        - 'create_time' and 'update_time': timestamp columns for tracking data changes.
+
+        This table structure caters to general use cases and
+        complex scenarios where the table serves as a semantic layer for advanced
+        data integration and analysis, leveraging SQL for join queries.
 
-    Everything below assumes `commons` Rockset workspace.
+        Args:
+            connection_string (str): The connection string for the TiDB database,
+                format: "mysql+pymysql://root@34.212.137.91:4000/test".
+            embedding_function: The embedding function used to generate embeddings.
+            table_name (str, optional): The name of the table that will be used to
+                store vector data. If you do not provide a table name,
+                a default table named `langchain_vector` will be created automatically.
+            distance_strategy: The strategy used for similarity search,
+                defaults to "cosine", valid values: "l2", "cosine", "inner_product".
+            engine_args (Optional[Dict]): Additional arguments for the database engine,
+                defaults to None.
+            drop_existing_table: Drop the existing TiDB table before initializing,
+                defaults to False.
+            **kwargs (Any): Additional keyword arguments.
+
+        Examples:
+            .. code-block:: python
+
+            from langchain_community.vectorstores import TiDBVectorStore
+            from langchain_openai import OpenAIEmbeddings
+
+            embeddingFunc = OpenAIEmbeddings()
+            CONNECTION_STRING = "mysql+pymysql://root@34.212.137.91:4000/test"
+
+            vs = TiDBVector.from_texts(
+                embedding=embeddingFunc,
+                texts = [..., ...],
+                connection_string=CONNECTION_STRING,
+                distance_strategy="l2",
+                table_name="tidb_vector_langchain",
+            )
 
-    Example:
-        .. code-block:: python
+            query = "What did the president say about Ketanji Brown Jackson"
+            docs = db.similarity_search_with_score(query)
 
-            from langchain_community.vectorstores import Rockset
-            from langchain_community.embeddings.openai import OpenAIEmbeddings
-            import rockset
+        """
 
-            # Make sure you use the right host (region) for your Rockset instance
-            # and APIKEY has both read-write access to your collection.
+        super().__init__(**kwargs)
+        self._connection_string = connection_string
+        self._embedding_function = embedding_function
+        self._distance_strategy = distance_strategy
+        self._vector_dimension = self._get_dimension()
 
-            rs = rockset.RocksetClient(host=rockset.Regions.use1a1, api_key="***")
-            collection_name = "langchain_demo"
-            embeddings = OpenAIEmbeddings()
-            vectorstore = Rockset(rs, collection_name, embeddings,
-                "description", "description_embedding")
+        try:
+            from tidb_vector.integrations import TiDBVectorClient
+        except ImportError:
+            raise ImportError(
+                "Could not import tidbvec python package. "
+                "Please install it with `pip install tidb-vector`."
+            )
 
-    """
+        self._tidb = TiDBVectorClient(
+            connection_string=connection_string,
+            table_name=table_name,
+            distance_strategy=distance_strategy,
+            vector_dimension=self._vector_dimension,
+            engine_args=engine_args,
+            drop_existing_table=drop_existing_table,
+            **kwargs,
+        )
+
+    @property
+    def embeddings(self) -> Embeddings:
+        """Return the function used to generate embeddings."""
+        return self._embedding_function
+
+    @property
+    def tidb_vector_client(self) -> Any:
+        """Return the TiDB Vector Client."""
+        return self._tidb
+
+    @property
+    def distance_strategy(self) -> Any:
+        """
+        Returns the current distance strategy.
+        """
+        return self._distance_strategy
+
+    def _get_dimension(self) -> int:
+        """
+        Get the dimension of the vector using embedding functions.
+        """
+        return len(self._embedding_function.embed_query("test embedding length"))
+
+    @classmethod
+    def from_texts(
+        cls,
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        **kwargs: Any,
+    ) -> "TiDBVectorStore":
+        """
+        Create a VectorStore from a list of texts.
 
-    def __init__(
-        self,
-        client: Any,
-        embeddings: Embeddings,
-        collection_name: str,
-        text_key: str,
-        embedding_key: str,
-        workspace: str = "commons",
-    ):
-        """Initialize with Rockset client.
         Args:
-            client: Rockset client object
-            collection: Rockset collection to insert docs / query
-            embeddings: Langchain Embeddings object to use to generate
-                        embedding for given text.
-            text_key: column in Rockset collection to use to store the text
-            embedding_key: column in Rockset collection to use to store the embedding.
-                           Note: We must apply `VECTOR_ENFORCE()` on this column via
-                           Rockset ingest transformation.
+            texts (List[str]): The list of texts to be added to the TiDB Vector.
+            embedding (Embeddings): The function to use for generating embeddings.
+            metadatas: The list of metadata dictionaries corresponding to each text,
+                defaults to None.
+            **kwargs (Any): Additional keyword arguments.
+                connection_string (str): The connection string for the TiDB database,
+                    format: "mysql+pymysql://root@34.212.137.91:4000/test".
+                table_name (str, optional): The name of table used to store vector data,
+                    defaults to "langchain_vector".
+                distance_strategy: The distance strategy used for similarity search,
+                    defaults to "cosine", allowed: "l2", "cosine", "inner_product".
+                ids (Optional[List[str]]): The list of IDs corresponding to each text,
+                    defaults to None.
+                engine_args: Additional arguments for the underlying database engine,
+                    defaults to None.
+                drop_existing_table: Drop the existing TiDB table before initializing,
+                    defaults to False.
+
+        Returns:
+            VectorStore: The created TiDB Vector Store.
 
         """
+
+        # Extract arguments from kwargs with default values
+        connection_string = kwargs.pop("connection_string", None)
+        if connection_string is None:
+            raise ValueError("please provide your tidb connection_url")
+        table_name = kwargs.pop("table_name", "langchain_vector")
+        distance_strategy = kwargs.pop("distance_strategy", "cosine")
+        ids = kwargs.pop("ids", None)
+        engine_args = kwargs.pop("engine_args", None)
+        drop_existing_table = kwargs.pop("drop_existing_table", False)
+
+        embeddings = embedding.embed_documents(list(texts))
+
+        vs = cls(
+            connection_string=connection_string,
+            table_name=table_name,
+            embedding_function=embedding,
+            distance_strategy=distance_strategy,
+            engine_args=engine_args,
+            drop_existing_table=drop_existing_table,
+            **kwargs,
+        )
+
+        vs._tidb.insert(
+            texts=texts, embeddings=embeddings, metadatas=metadatas, ids=ids, **kwargs
+        )
+
+        return vs
+
+    @classmethod
+    def from_existing_vector_table(
+        cls,
+        embedding: Embeddings,
+        connection_string: str,
+        table_name: str,
+        distance_strategy: str = DEFAULT_DISTANCE_STRATEGY,
+        *,
+        engine_args: Optional[Dict[str, Any]] = None,
+        **kwargs: Any,
+    ) -> VectorStore:
+        """
+        Create a VectorStore instance from an existing TiDB Vector Store in TiDB.
+
+        Args:
+            embedding (Embeddings): The function to use for generating embeddings.
+            connection_string (str): The connection string for the TiDB database,
+                format: "mysql+pymysql://root@34.212.137.91:4000/test".
+            table_name (str, optional): The name of table used to store vector data,
+                defaults to "langchain_vector".
+            distance_strategy: The distance strategy used for similarity search,
+                defaults to "cosine", allowed: "l2", "cosine", 'inner_product'.
+            engine_args: Additional arguments for the underlying database engine,
+                defaults to None.
+            **kwargs (Any): Additional keyword arguments.
+        Returns:
+            VectorStore: The VectorStore instance.
+
+        Raises:
+            NoSuchTableError: If the specified table does not exist in the TiDB.
+        """
+
         try:
-            from rockset import RocksetClient
+            from tidb_vector.integrations import check_table_existence
         except ImportError:
             raise ImportError(
-                "Could not import rockset client python package. "
-                "Please install it with `pip install rockset`."
+                "Could not import tidbvec python package. "
+                "Please install it with `pip install tidb-vector`."
             )
 
-        if not isinstance(client, RocksetClient):
-            raise ValueError(
-                f"client should be an instance of rockset.RocksetClient, "
-                f"got {type(client)}"
+        if check_table_existence(connection_string, table_name):
+            return cls(
+                connection_string=connection_string,
+                table_name=table_name,
+                embedding_function=embedding,
+                distance_strategy=distance_strategy,
+                engine_args=engine_args,
+                **kwargs,
             )
-        # TODO: check that `collection_name` exists in rockset. Create if not.
-        self._client = client
-        self._collection_name = collection_name
-        self._embeddings = embeddings
-        self._text_key = text_key
-        self._embedding_key = embedding_key
-        self._workspace = workspace
-
-        try:
-            self._client.set_application("langchain")
-        except AttributeError:
-            # ignore
-            pass
+        else:
+            raise ValueError(f"Table {table_name} does not exist in the TiDB database.")
 
-    @property
-    def embeddings(self) -> Embeddings:
-        return self._embeddings
+    def drop_vectorstore(self) -> None:
+        """
+        Drop the Vector Store from the TiDB database.
+        """
+        self._tidb.drop_table()
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
-        batch_size: int = 32,
         **kwargs: Any,
     ) -> List[str]:
-        """Run more texts through the embeddings and add to the vectorstore
+        """
+        Add texts to TiDB Vector Store.
 
-                Args:
-            texts: Iterable of strings to add to the vectorstore.
-            metadatas: Optional list of metadatas associated with the texts.
-            ids: Optional list of ids to associate with the texts.
-            batch_size: Send documents in batches to rockset.
+        Args:
+            texts (Iterable[str]): The texts to be added.
+            metadatas (Optional[List[dict]]): The metadata associated with each text,
+                Defaults to None.
+            ids (Optional[List[str]]): The IDs to be assigned to each text,
+                Defaults to None, will be generated if not provided.
 
         Returns:
-            List of ids from adding the texts into the vectorstore.
-
+            List[str]: The IDs assigned to the added texts.
         """
-        batch: list[dict] = []
-        stored_ids = []
 
-        for i, text in enumerate(texts):
-            if len(batch) == batch_size:
-                stored_ids += self._write_documents_to_rockset(batch)
-                batch = []
-            doc = {}
-            if metadatas and len(metadatas) > i:
-                doc = metadatas[i]
-            if ids and len(ids) > i:
-                doc["_id"] = ids[i]
-            doc[self._text_key] = text
-            doc[self._embedding_key] = self._embeddings.embed_query(text)
-            batch.append(doc)
-        if len(batch) > 0:
-            stored_ids += self._write_documents_to_rockset(batch)
-            batch = []
-        return stored_ids
+        embeddings = self._embedding_function.embed_documents(list(texts))
+        if ids is None:
+            ids = [str(uuid.uuid4()) for _ in texts]
+        if not metadatas:
+            metadatas = [{} for _ in texts]
 
-    @classmethod
-    def from_texts(
-        cls,
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        client: Any = None,
-        collection_name: str = "",
-        text_key: str = "",
-        embedding_key: str = "",
-        ids: Optional[List[str]] = None,
-        batch_size: int = 32,
-        **kwargs: Any,
-    ) -> Rockset:
-        """Create Rockset wrapper with existing texts.
-        This is intended as a quicker way to get started.
-        """
-
-        # Sanitize inputs
-        assert client is not None, "Rockset Client cannot be None"
-        assert collection_name, "Collection name cannot be empty"
-        assert text_key, "Text key name cannot be empty"
-        assert embedding_key, "Embedding key cannot be empty"
-
-        rockset = cls(client, embedding, collection_name, text_key, embedding_key)
-        rockset.add_texts(texts, metadatas, ids, batch_size)
-        return rockset
-
-    # Rockset supports these vector distance functions.
-    class DistanceFunction(Enum):
-        COSINE_SIM = "COSINE_SIM"
-        EUCLIDEAN_DIST = "EUCLIDEAN_DIST"
-        DOT_PRODUCT = "DOT_PRODUCT"
-
-        # how to sort results for "similarity"
-        def order_by(self) -> str:
-            if self.value == "EUCLIDEAN_DIST":
-                return "ASC"
-            return "DESC"
+        return self._tidb.insert(
+            texts=texts, embeddings=embeddings, metadatas=metadatas, ids=ids, **kwargs
+        )
 
-    def similarity_search_with_relevance_scores(
+    def delete(
         self,
-        query: str,
-        k: int = 4,
-        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
-        where_str: Optional[str] = None,
+        ids: Optional[List[str]] = None,
         **kwargs: Any,
-    ) -> List[Tuple[Document, float]]:
-        """Perform a similarity search with Rockset
+    ) -> None:
+        """
+        Delete vector data from the TiDB Vector Store.
 
         Args:
-            query (str): Text to look up documents similar to.
-            distance_func (DistanceFunction): how to compute distance between two
-                vectors in Rockset.
-            k (int, optional): Top K neighbors to retrieve. Defaults to 4.
-            where_str (Optional[str], optional): Metadata filters supplied as a
-                SQL `where` condition string. Defaults to None.
-                eg. "price<=70.0 AND brand='Nintendo'"
-
-            NOTE: Please do not let end-user to fill this and always be aware
-                  of SQL injection.
-
-        Returns:
-            List[Tuple[Document, float]]: List of documents with their relevance score
+            ids (Optional[List[str]]): A list of vector IDs to delete.
+            **kwargs: Additional keyword arguments.
         """
-        return self.similarity_search_by_vector_with_relevance_scores(
-            self._embeddings.embed_query(query),
-            k,
-            distance_func,
-            where_str,
-            **kwargs,
-        )
+
+        self._tidb.delete(ids=ids, **kwargs)
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
-        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
-        where_str: Optional[str] = None,
+        filter: Optional[dict] = None,
         **kwargs: Any,
     ) -> List[Document]:
-        """Same as `similarity_search_with_relevance_scores` but
-        doesn't return the scores.
         """
-        return self.similarity_search_by_vector(
-            self._embeddings.embed_query(query),
-            k,
-            distance_func,
-            where_str,
-            **kwargs,
-        )
+        Perform a similarity search using the given query.
 
-    def similarity_search_by_vector(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
-        where_str: Optional[str] = None,
-        **kwargs: Any,
-    ) -> List[Document]:
-        """Accepts a query_embedding (vector), and returns documents with
-        similar embeddings."""
+        Args:
+            query (str): The query string.
+            k (int, optional): The number of results to retrieve. Defaults to 4.
+            filter (dict, optional): A filter to apply to the search results.
+                Defaults to None.
+            **kwargs: Additional keyword arguments.
 
-        docs_and_scores = self.similarity_search_by_vector_with_relevance_scores(
-            embedding, k, distance_func, where_str, **kwargs
-        )
-        return [doc for doc, _ in docs_and_scores]
+        Returns:
+            List[Document]: A list of Document objects representing the search results.
+        """
+        result = self.similarity_search_with_score(query, k, filter, **kwargs)
+        return [doc for doc, _ in result]
 
-    def similarity_search_by_vector_with_relevance_scores(
+    def similarity_search_with_score(
         self,
-        embedding: List[float],
-        k: int = 4,
-        distance_func: DistanceFunction = DistanceFunction.COSINE_SIM,
-        where_str: Optional[str] = None,
+        query: str,
+        k: int = 5,
+        filter: Optional[dict] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Accepts a query_embedding (vector), and returns documents with
-        similar embeddings along with their relevance scores."""
-
-        q_str = self._build_query_sql(embedding, distance_func, k, where_str)
-        try:
-            query_response = self._client.Queries.query(sql={"query": q_str})
-        except Exception as e:
-            logger.error("Exception when querying Rockset: %s\n", e)
-            return []
-        finalResult: list[Tuple[Document, float]] = []
-        for document in query_response.results:
-            metadata = {}
-            assert isinstance(
-                document, dict
-            ), "document should be of type `dict[str,Any]`. But found: `{}`".format(
-                type(document)
-            )
-            for k, v in document.items():
-                if k == self._text_key:
-                    assert isinstance(v, str), (
-                        "page content stored in column `{}` must be of type `str`. "
-                        "But found: `{}`"
-                    ).format(self._text_key, type(v))
-                    page_content = v
-                elif k == "dist":
-                    assert isinstance(v, float), (
-                        "Computed distance between vectors must of type `float`. "
-                        "But found {}"
-                    ).format(type(v))
-                    score = v
-                elif k not in ["_id", "_event_time", "_meta"]:
-                    # These columns are populated by Rockset when documents are
-                    # inserted. No need to return them in metadata dict.
-                    metadata[k] = v
-            finalResult.append(
-                (Document(page_content=page_content, metadata=metadata), score)
-            )
-        return finalResult
+        """
+        Perform a similarity search with score based on the given query.
 
-    # Helper functions
+        Args:
+            query (str): The query string.
+            k (int, optional): The number of results to return. Defaults to 5.
+            filter (dict, optional): A filter to apply to the search results.
+                Defaults to None.
+            **kwargs: Additional keyword arguments.
 
-    def _build_query_sql(
-        self,
-        query_embedding: List[float],
-        distance_func: DistanceFunction,
-        k: int = 4,
-        where_str: Optional[str] = None,
-    ) -> str:
-        """Builds Rockset SQL query to query similar vectors to query_vector"""
-
-        q_embedding_str = ",".join(map(str, query_embedding))
-        distance_str = f"""{distance_func.value}({self._embedding_key}, \
-[{q_embedding_str}]) as dist"""
-        where_str = f"WHERE {where_str}\n" if where_str else ""
-        return f"""\
-SELECT * EXCEPT({self._embedding_key}), {distance_str}
-FROM {self._workspace}.{self._collection_name}
-{where_str}\
-ORDER BY dist {distance_func.order_by()}
-LIMIT {str(k)}
-"""
-
-    def _write_documents_to_rockset(self, batch: List[dict]) -> List[str]:
-        add_doc_res = self._client.Documents.add_documents(
-            collection=self._collection_name, data=batch, workspace=self._workspace
+        Returns:
+            A list of tuples containing relevant documents and their similarity scores.
+        """
+        query_vector = self._embedding_function.embed_query(query)
+        relevant_docs = self._tidb.query(
+            query_vector=query_vector, k=k, filter=filter, **kwargs
         )
-        return [doc_status._id for doc_status in add_doc_res.data]
-
-    def delete_texts(self, ids: List[str]) -> None:
-        """Delete a list of docs from the Rockset collection"""
-        try:
-            from rockset.models import DeleteDocumentsRequestData
-        except ImportError:
-            raise ImportError(
-                "Could not import rockset client python package. "
-                "Please install it with `pip install rockset`."
+        return [
+            (
+                Document(
+                    page_content=doc.document,
+                    metadata=doc.metadata,
+                ),
+                doc.distance,
             )
+            for doc in relevant_docs
+        ]
 
-        self._client.Documents.delete_documents(
-            collection=self._collection_name,
-            data=[DeleteDocumentsRequestData(id=i) for i in ids],
-            workspace=self._workspace,
-        )
+    def _select_relevance_score_fn(self) -> Callable[[float], float]:
+        """
+        Select the relevance score function based on the distance strategy.
+        """
+        if self._distance_strategy == "cosine":
+            return self._cosine_relevance_score_fn
+        elif self._distance_strategy == "l2":
+            return self._euclidean_relevance_score_fn
+        else:
+            raise ValueError(
+                "No supported normalization function"
+                f" for distance_strategy of {self._distance_strategy}."
+                "Consider providing relevance_score_fn to PGVector constructor."
+            )
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/scann.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/scann.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 import uuid
 from pathlib import Path
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
+from langchain_core.utils import guard_import
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.docstore.base import AddableMixin, Docstore
 from langchain_community.docstore.in_memory import InMemoryDocstore
 from langchain_community.vectorstores.utils import DistanceStrategy
 
 
@@ -22,22 +23,15 @@
     return x
 
 
 def dependable_scann_import() -> Any:
     """
     Import `scann` if available, otherwise raise error.
     """
-    try:
-        import scann
-    except ImportError:
-        raise ImportError(
-            "Could not import scann python package. "
-            "Please install it with `pip install scann` "
-        )
-    return scann
+    return guard_import("scann")
 
 
 class ScaNN(VectorStore):
     """`ScaNN` vector store.
 
     To use, you should have the ``scann`` python package installed.
 
@@ -308,15 +302,15 @@
         embeddings: List[List[float]],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         ids: Optional[List[str]] = None,
         normalize_L2: bool = False,
         **kwargs: Any,
     ) -> ScaNN:
-        scann = dependable_scann_import()
+        scann = guard_import("scann")
         distance_strategy = kwargs.get(
             "distance_strategy", DistanceStrategy.EUCLIDEAN_DISTANCE
         )
         scann_config = kwargs.get("scann_config", None)
 
         vector = np.array(embeddings, dtype=np.float32)
         if normalize_L2:
@@ -456,29 +450,49 @@
 
     @classmethod
     def load_local(
         cls,
         folder_path: str,
         embedding: Embeddings,
         index_name: str = "index",
+        *,
+        allow_dangerous_deserialization: bool = False,
         **kwargs: Any,
     ) -> ScaNN:
         """Load ScaNN index, docstore, and index_to_docstore_id from disk.
 
         Args:
             folder_path: folder path to load index, docstore,
                 and index_to_docstore_id from.
-            embeddings: Embeddings to use when generating queries
+            embedding: Embeddings to use when generating queries
             index_name: for saving with a specific index file name
+            allow_dangerous_deserialization: whether to allow deserialization
+                of the data which involves loading a pickle file.
+                Pickle files can be modified by malicious actors to deliver a
+                malicious payload that results in execution of
+                arbitrary code on your machine.
         """
+        if not allow_dangerous_deserialization:
+            raise ValueError(
+                "The de-serialization relies loading a pickle file. "
+                "Pickle files can be modified to deliver a malicious payload that "
+                "results in execution of arbitrary code on your machine."
+                "You will need to set `allow_dangerous_deserialization` to `True` to "
+                "enable deserialization. If you do this, make sure that you "
+                "trust the source of the data. For example, if you are loading a "
+                "file that you created, and know that no one else has modified the "
+                "file, then this is safe to do. Do not set this to `True` if you are "
+                "loading a file from an untrusted source (e.g., some random site on "
+                "the internet.)."
+            )
         path = Path(folder_path)
         scann_path = path / "{index_name}.scann".format(index_name=index_name)
         scann_path.mkdir(exist_ok=True, parents=True)
         # load index separately since it is not picklable
-        scann = dependable_scann_import()
+        scann = guard_import("scann")
         index = scann.scann_ops_pybind.load_searcher(str(scann_path))
 
         # load docstore and index_to_docstore_id
         with open(path / "{index_name}.pkl".format(index_name=index_name), "rb") as f:
             docstore, index_to_docstore_id = pickle.load(f)
         return cls(embedding, index, docstore, index_to_docstore_id, **kwargs)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/semadb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/semadb.py`

 * *Files 1% similar despite different names*

```diff
@@ -142,15 +142,15 @@
             batch = points[i : i + batch_size]
             response = requests.post(
                 SemaDB.BASE_URL + f"/collections/{self.collection_name}/points",
                 json={"points": batch},
                 headers=self.headers,
             )
             if response.status_code != 200:
-                print("HERE--", batch)
+                print("HERE--", batch)  # noqa: T201
                 raise ValueError(f"Error adding points: {response.text}")
             failed_ranges = response.json()["failedRanges"]
             if len(failed_ranges) > 0:
                 raise ValueError(f"Error adding points: {failed_ranges}")
         # Return ids
         return ids
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/singlestoredb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/weaviate.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,448 +1,528 @@
 from __future__ import annotations
 
-import json
-import re
+import datetime
+import os
 from typing import (
+    TYPE_CHECKING,
     Any,
     Callable,
+    Dict,
     Iterable,
     List,
     Optional,
     Tuple,
-    Type,
 )
+from uuid import uuid4
 
+import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
-from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
-from sqlalchemy.pool import QueuePool
+from langchain_core.vectorstores import VectorStore
 
-from langchain_community.vectorstores.utils import DistanceStrategy
+from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
-DEFAULT_DISTANCE_STRATEGY = DistanceStrategy.DOT_PRODUCT
+if TYPE_CHECKING:
+    import weaviate
 
-ORDERING_DIRECTIVE: dict = {
-    DistanceStrategy.EUCLIDEAN_DISTANCE: "",
-    DistanceStrategy.DOT_PRODUCT: "DESC",
-}
 
+def _default_schema(index_name: str, text_key: str) -> Dict:
+    return {
+        "class": index_name,
+        "properties": [
+            {
+                "name": text_key,
+                "dataType": ["text"],
+            }
+        ],
+    }
+
+
+def _create_weaviate_client(
+    url: Optional[str] = None,
+    api_key: Optional[str] = None,
+    **kwargs: Any,
+) -> weaviate.Client:
+    try:
+        import weaviate
+    except ImportError:
+        raise ImportError(
+            "Could not import weaviate python  package. "
+            "Please install it with `pip install weaviate-client`"
+        )
+    url = url or os.environ.get("WEAVIATE_URL")
+    api_key = api_key or os.environ.get("WEAVIATE_API_KEY")
+    auth = weaviate.auth.AuthApiKey(api_key=api_key) if api_key else None
+    return weaviate.Client(url=url, auth_client_secret=auth, **kwargs)
 
-class SingleStoreDB(VectorStore):
-    """`SingleStore DB` vector store.
-
-    The prerequisite for using this class is the installation of the ``singlestoredb``
-    Python package.
-
-    The SingleStoreDB vectorstore can be created by providing an embedding function and
-    the relevant parameters for the database connection, connection pool, and
-    optionally, the names of the table and the fields to use.
-    """
-
-    def _get_connection(self: SingleStoreDB) -> Any:
-        try:
-            import singlestoredb as s2
-        except ImportError:
-            raise ImportError(
-                "Could not import singlestoredb python package. "
-                "Please install it with `pip install singlestoredb`."
-            )
-        return s2.connect(**self.connection_kwargs)
-
-    def __init__(
-        self,
-        embedding: Embeddings,
-        *,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        table_name: str = "embeddings",
-        content_field: str = "content",
-        metadata_field: str = "metadata",
-        vector_field: str = "vector",
-        pool_size: int = 5,
-        max_overflow: int = 10,
-        timeout: float = 30,
-        **kwargs: Any,
-    ):
-        """Initialize with necessary components.
-
-        Args:
-            embedding (Embeddings): A text embedding model.
-
-            distance_strategy (DistanceStrategy, optional):
-                Determines the strategy employed for calculating
-                the distance between vectors in the embedding space.
-                Defaults to DOT_PRODUCT.
-                Available options are:
-                - DOT_PRODUCT: Computes the scalar product of two vectors.
-                    This is the default behavior
-                - EUCLIDEAN_DISTANCE: Computes the Euclidean distance between
-                    two vectors. This metric considers the geometric distance in
-                    the vector space, and might be more suitable for embeddings
-                    that rely on spatial relationships.
-
-            table_name (str, optional): Specifies the name of the table in use.
-                Defaults to "embeddings".
-            content_field (str, optional): Specifies the field to store the content.
-                Defaults to "content".
-            metadata_field (str, optional): Specifies the field to store metadata.
-                Defaults to "metadata".
-            vector_field (str, optional): Specifies the field to store the vector.
-                Defaults to "vector".
-
-            Following arguments pertain to the connection pool:
-
-            pool_size (int, optional): Determines the number of active connections in
-                the pool. Defaults to 5.
-            max_overflow (int, optional): Determines the maximum number of connections
-                allowed beyond the pool_size. Defaults to 10.
-            timeout (float, optional): Specifies the maximum wait time in seconds for
-                establishing a connection. Defaults to 30.
-
-            Following arguments pertain to the database connection:
-
-            host (str, optional): Specifies the hostname, IP address, or URL for the
-                database connection. The default scheme is "mysql".
-            user (str, optional): Database username.
-            password (str, optional): Database password.
-            port (int, optional): Database port. Defaults to 3306 for non-HTTP
-                connections, 80 for HTTP connections, and 443 for HTTPS connections.
-            database (str, optional): Database name.
-
-            Additional optional arguments provide further customization over the
-            database connection:
-
-            pure_python (bool, optional): Toggles the connector mode. If True,
-                operates in pure Python mode.
-            local_infile (bool, optional): Allows local file uploads.
-            charset (str, optional): Specifies the character set for string values.
-            ssl_key (str, optional): Specifies the path of the file containing the SSL
-                key.
-            ssl_cert (str, optional): Specifies the path of the file containing the SSL
-                certificate.
-            ssl_ca (str, optional): Specifies the path of the file containing the SSL
-                certificate authority.
-            ssl_cipher (str, optional): Sets the SSL cipher list.
-            ssl_disabled (bool, optional): Disables SSL usage.
-            ssl_verify_cert (bool, optional): Verifies the server's certificate.
-                Automatically enabled if ``ssl_ca`` is specified.
-            ssl_verify_identity (bool, optional): Verifies the server's identity.
-            conv (dict[int, Callable], optional): A dictionary of data conversion
-                functions.
-            credential_type (str, optional): Specifies the type of authentication to
-                use: auth.PASSWORD, auth.JWT, or auth.BROWSER_SSO.
-            autocommit (bool, optional): Enables autocommits.
-            results_type (str, optional): Determines the structure of the query results:
-                tuples, namedtuples, dicts.
-            results_format (str, optional): Deprecated. This option has been renamed to
-                results_type.
-
-        Examples:
-            Basic Usage:
-
-            .. code-block:: python
 
-                from langchain_community.embeddings import OpenAIEmbeddings
-                from langchain_community.vectorstores import SingleStoreDB
+def _default_score_normalizer(val: float) -> float:
+    return 1 - 1 / (1 + np.exp(val))
 
-                vectorstore = SingleStoreDB(
-                    OpenAIEmbeddings(),
-                    host="https://user:password@127.0.0.1:3306/database"
-                )
 
-            Advanced Usage:
+def _json_serializable(value: Any) -> Any:
+    if isinstance(value, datetime.datetime):
+        return value.isoformat()
+    return value
 
-            .. code-block:: python
 
-                from langchain_community.embeddings import OpenAIEmbeddings
-                from langchain_community.vectorstores import SingleStoreDB
+class Weaviate(VectorStore):
+    """`Weaviate` vector store.
 
-                vectorstore = SingleStoreDB(
-                    OpenAIEmbeddings(),
-                    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,
-                    host="127.0.0.1",
-                    port=3306,
-                    user="user",
-                    password="password",
-                    database="db",
-                    table_name="my_custom_table",
-                    pool_size=10,
-                    timeout=60,
-                )
+    To use, you should have the ``weaviate-client`` python package installed.
 
-            Using environment variables:
+    Example:
+        .. code-block:: python
 
-            .. code-block:: python
+            import weaviate
+            from langchain_community.vectorstores import Weaviate
 
-                from langchain_community.embeddings import OpenAIEmbeddings
-                from langchain_community.vectorstores import SingleStoreDB
+            client = weaviate.Client(url=os.environ["WEAVIATE_URL"], ...)
+            weaviate = Weaviate(client, index_name, text_key)
 
-                os.environ['SINGLESTOREDB_URL'] = 'me:p455w0rd@s2-host.com/my_db'
-                vectorstore = SingleStoreDB(OpenAIEmbeddings())
-        """
+    """
 
-        self.embedding = embedding
-        self.distance_strategy = distance_strategy
-        self.table_name = self._sanitize_input(table_name)
-        self.content_field = self._sanitize_input(content_field)
-        self.metadata_field = self._sanitize_input(metadata_field)
-        self.vector_field = self._sanitize_input(vector_field)
-
-        # Pass the rest of the kwargs to the connection.
-        self.connection_kwargs = kwargs
-
-        # Add program name and version to connection attributes.
-        if "conn_attrs" not in self.connection_kwargs:
-            self.connection_kwargs["conn_attrs"] = dict()
-
-        self.connection_kwargs["conn_attrs"]["_connector_name"] = "langchain python sdk"
-        self.connection_kwargs["conn_attrs"]["_connector_version"] = "1.0.1"
-
-        # Create connection pool.
-        self.connection_pool = QueuePool(
-            self._get_connection,
-            max_overflow=max_overflow,
-            pool_size=pool_size,
-            timeout=timeout,
-        )
-        self._create_table()
+    def __init__(
+        self,
+        client: Any,
+        index_name: str,
+        text_key: str,
+        embedding: Optional[Embeddings] = None,
+        attributes: Optional[List[str]] = None,
+        relevance_score_fn: Optional[
+            Callable[[float], float]
+        ] = _default_score_normalizer,
+        by_text: bool = True,
+    ):
+        """Initialize with Weaviate client."""
+        try:
+            import weaviate
+        except ImportError:
+            raise ImportError(
+                "Could not import weaviate python package. "
+                "Please install it with `pip install weaviate-client`."
+            )
+        if not isinstance(client, weaviate.Client):
+            raise ValueError(
+                f"client should be an instance of weaviate.Client, got {type(client)}"
+            )
+        self._client = client
+        self._index_name = index_name
+        self._embedding = embedding
+        self._text_key = text_key
+        self._query_attrs = [self._text_key]
+        self.relevance_score_fn = relevance_score_fn
+        self._by_text = by_text
+        if attributes is not None:
+            self._query_attrs.extend(attributes)
 
     @property
-    def embeddings(self) -> Embeddings:
-        return self.embedding
-
-    def _sanitize_input(self, input_str: str) -> str:
-        # Remove characters that are not alphanumeric or underscores
-        return re.sub(r"[^a-zA-Z0-9_]", "", input_str)
+    def embeddings(self) -> Optional[Embeddings]:
+        return self._embedding
 
     def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        return self._max_inner_product_relevance_score_fn
-
-    def _create_table(self: SingleStoreDB) -> None:
-        """Create table if it doesn't exist."""
-        conn = self.connection_pool.connect()
-        try:
-            cur = conn.cursor()
-            try:
-                cur.execute(
-                    """CREATE TABLE IF NOT EXISTS {}
-                    ({} TEXT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci,
-                    {} BLOB, {} JSON);""".format(
-                        self.table_name,
-                        self.content_field,
-                        self.vector_field,
-                        self.metadata_field,
-                    ),
-                )
-            finally:
-                cur.close()
-        finally:
-            conn.close()
+        return (
+            self.relevance_score_fn
+            if self.relevance_score_fn
+            else _default_score_normalizer
+        )
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
-        embeddings: Optional[List[List[float]]] = None,
         **kwargs: Any,
     ) -> List[str]:
-        """Add more texts to the vectorstore.
+        """Upload texts with metadata (properties) to Weaviate."""
+        from weaviate.util import get_valid_uuid
+
+        ids = []
+        embeddings: Optional[List[List[float]]] = None
+        if self._embedding:
+            if not isinstance(texts, list):
+                texts = list(texts)
+            embeddings = self._embedding.embed_documents(texts)
+
+        with self._client.batch as batch:
+            for i, text in enumerate(texts):
+                data_properties = {self._text_key: text}
+                if metadatas is not None:
+                    for key, val in metadatas[i].items():
+                        data_properties[key] = _json_serializable(val)
+
+                # Allow for ids (consistent w/ other methods)
+                # # Or uuids (backwards compatible w/ existing arg)
+                # If the UUID of one of the objects already exists
+                # then the existing object will be replaced by the new object.
+                _id = get_valid_uuid(uuid4())
+                if "uuids" in kwargs:
+                    _id = kwargs["uuids"][i]
+                elif "ids" in kwargs:
+                    _id = kwargs["ids"][i]
+
+                batch.add_data_object(
+                    data_object=data_properties,
+                    class_name=self._index_name,
+                    uuid=_id,
+                    vector=embeddings[i] if embeddings else None,
+                    tenant=kwargs.get("tenant"),
+                )
+                ids.append(_id)
+        return ids
+
+    def similarity_search(
+        self, query: str, k: int = 4, **kwargs: Any
+    ) -> List[Document]:
+        """Return docs most similar to query.
 
         Args:
-            texts (Iterable[str]): Iterable of strings/text to add to the vectorstore.
-            metadatas (Optional[List[dict]], optional): Optional list of metadatas.
-                Defaults to None.
-            embeddings (Optional[List[List[float]]], optional): Optional pre-generated
-                embeddings. Defaults to None.
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
 
         Returns:
-            List[str]: empty list
+            List of Documents most similar to the query.
         """
-        conn = self.connection_pool.connect()
-        try:
-            cur = conn.cursor()
-            try:
-                # Write data to singlestore db
-                for i, text in enumerate(texts):
-                    # Use provided values by default or fallback
-                    metadata = metadatas[i] if metadatas else {}
-                    embedding = (
-                        embeddings[i]
-                        if embeddings
-                        else self.embedding.embed_documents([text])[0]
-                    )
-                    cur.execute(
-                        "INSERT INTO {} VALUES (%s, JSON_ARRAY_PACK(%s), %s)".format(
-                            self.table_name
-                        ),
-                        (
-                            text,
-                            "[{}]".format(",".join(map(str, embedding))),
-                            json.dumps(metadata),
-                        ),
-                    )
-            finally:
-                cur.close()
-        finally:
-            conn.close()
-        return []
+        if self._by_text:
+            return self.similarity_search_by_text(query, k, **kwargs)
+        else:
+            if self._embedding is None:
+                raise ValueError(
+                    "_embedding cannot be None for similarity_search when "
+                    "_by_text=False"
+                )
+            embedding = self._embedding.embed_query(query)
+            return self.similarity_search_by_vector(embedding, k, **kwargs)
 
-    def similarity_search(
-        self, query: str, k: int = 4, filter: Optional[dict] = None, **kwargs: Any
+    def similarity_search_by_text(
+        self, query: str, k: int = 4, **kwargs: Any
     ) -> List[Document]:
-        """Returns the most similar indexed documents to the query text.
-
-        Uses cosine similarity.
+        """Return docs most similar to query.
 
         Args:
-            query (str): The query text for which to find similar documents.
-            k (int): The number of documents to return. Default is 4.
-            filter (dict): A dictionary of metadata fields and values to filter by.
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
 
         Returns:
-            List[Document]: A list of documents that are most similar to the query text.
+            List of Documents most similar to the query.
+        """
+        content: Dict[str, Any] = {"concepts": [query]}
+        if kwargs.get("search_distance"):
+            content["certainty"] = kwargs.get("search_distance")
+        query_obj = self._client.query.get(self._index_name, self._query_attrs)
+        if kwargs.get("where_filter"):
+            query_obj = query_obj.with_where(kwargs.get("where_filter"))
+        if kwargs.get("tenant"):
+            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
+        if kwargs.get("additional"):
+            query_obj = query_obj.with_additional(kwargs.get("additional"))
+        result = query_obj.with_near_text(content).with_limit(k).do()
+        if "errors" in result:
+            raise ValueError(f"Error during query: {result['errors']}")
+        docs = []
+        for res in result["data"]["Get"][self._index_name]:
+            text = res.pop(self._text_key)
+            docs.append(Document(page_content=text, metadata=res))
+        return docs
 
-        Examples:
-            .. code-block:: python
-                from langchain_community.vectorstores import SingleStoreDB
-                from langchain_community.embeddings import OpenAIEmbeddings
-                s2 = SingleStoreDB.from_documents(
-                    docs,
-                    OpenAIEmbeddings(),
-                    host="username:password@localhost:3306/database"
-                )
-                s2.similarity_search("query text", 1,
-                    {"metadata_field": "metadata_value"})
+    def similarity_search_by_vector(
+        self, embedding: List[float], k: int = 4, **kwargs: Any
+    ) -> List[Document]:
+        """Look up similar documents by embedding vector in Weaviate."""
+        vector = {"vector": embedding}
+        query_obj = self._client.query.get(self._index_name, self._query_attrs)
+        if kwargs.get("where_filter"):
+            query_obj = query_obj.with_where(kwargs.get("where_filter"))
+        if kwargs.get("tenant"):
+            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
+        if kwargs.get("additional"):
+            query_obj = query_obj.with_additional(kwargs.get("additional"))
+        result = query_obj.with_near_vector(vector).with_limit(k).do()
+        if "errors" in result:
+            raise ValueError(f"Error during query: {result['errors']}")
+        docs = []
+        for res in result["data"]["Get"][self._index_name]:
+            text = res.pop(self._text_key)
+            docs.append(Document(page_content=text, metadata=res))
+        return docs
+
+    def max_marginal_relevance_search(
+        self,
+        query: str,
+        k: int = 4,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
+
+        Args:
+            query: Text to look up documents similar to.
+            k: Number of Documents to return. Defaults to 4.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
+
+        Returns:
+            List of Documents selected by maximal marginal relevance.
         """
-        docs_and_scores = self.similarity_search_with_score(
-            query=query, k=k, filter=filter
+        if self._embedding is not None:
+            embedding = self._embedding.embed_query(query)
+        else:
+            raise ValueError(
+                "max_marginal_relevance_search requires a suitable Embeddings object"
+            )
+
+        return self.max_marginal_relevance_search_by_vector(
+            embedding, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs
         )
-        return [doc for doc, _ in docs_and_scores]
 
-    def similarity_search_with_score(
-        self, query: str, k: int = 4, filter: Optional[dict] = None
-    ) -> List[Tuple[Document, float]]:
-        """Return docs most similar to query. Uses cosine similarity.
+    def max_marginal_relevance_search_by_vector(
+        self,
+        embedding: List[float],
+        k: int = 4,
+        fetch_k: int = 20,
+        lambda_mult: float = 0.5,
+        **kwargs: Any,
+    ) -> List[Document]:
+        """Return docs selected using the maximal marginal relevance.
+
+        Maximal marginal relevance optimizes for similarity to query AND diversity
+        among selected documents.
 
         Args:
-            query: Text to look up documents similar to.
+            embedding: Embedding to look up documents similar to.
             k: Number of Documents to return. Defaults to 4.
-            filter: A dictionary of metadata fields and values to filter by.
-                    Defaults to None.
+            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
+            lambda_mult: Number between 0 and 1 that determines the degree
+                        of diversity among the results with 0 corresponding
+                        to maximum diversity and 1 to minimum diversity.
+                        Defaults to 0.5.
 
         Returns:
-            List of Documents most similar to the query and score for each
+            List of Documents selected by maximal marginal relevance.
         """
-        # Creates embedding vector from user query
-        embedding = self.embedding.embed_query(query)
-        conn = self.connection_pool.connect()
-        result = []
-        where_clause: str = ""
-        where_clause_values: List[Any] = []
-        if filter:
-            where_clause = "WHERE "
-            arguments = []
-
-            def build_where_clause(
-                where_clause_values: List[Any],
-                sub_filter: dict,
-                prefix_args: Optional[List[str]] = None,
-            ) -> None:
-                prefix_args = prefix_args or []
-                for key in sub_filter.keys():
-                    if isinstance(sub_filter[key], dict):
-                        build_where_clause(
-                            where_clause_values, sub_filter[key], prefix_args + [key]
-                        )
-                    else:
-                        arguments.append(
-                            "JSON_EXTRACT_JSON({}, {}) = %s".format(
-                                self.metadata_field,
-                                ", ".join(["%s"] * (len(prefix_args) + 1)),
-                            )
-                        )
-                        where_clause_values += prefix_args + [key]
-                        where_clause_values.append(json.dumps(sub_filter[key]))
+        vector = {"vector": embedding}
+        query_obj = self._client.query.get(self._index_name, self._query_attrs)
+        if kwargs.get("where_filter"):
+            query_obj = query_obj.with_where(kwargs.get("where_filter"))
+        if kwargs.get("tenant"):
+            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
+        results = (
+            query_obj.with_additional("vector")
+            .with_near_vector(vector)
+            .with_limit(fetch_k)
+            .do()
+        )
 
-            build_where_clause(where_clause_values, filter)
-            where_clause += " AND ".join(arguments)
+        payload = results["data"]["Get"][self._index_name]
+        embeddings = [result["_additional"]["vector"] for result in payload]
+        mmr_selected = maximal_marginal_relevance(
+            np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult
+        )
 
-        try:
-            cur = conn.cursor()
-            try:
-                cur.execute(
-                    """SELECT {}, {}, {}({}, JSON_ARRAY_PACK(%s)) as __score
-                    FROM {} {} ORDER BY __score {} LIMIT %s""".format(
-                        self.content_field,
-                        self.metadata_field,
-                        self.distance_strategy.name
-                        if isinstance(self.distance_strategy, DistanceStrategy)
-                        else self.distance_strategy,
-                        self.vector_field,
-                        self.table_name,
-                        where_clause,
-                        ORDERING_DIRECTIVE[self.distance_strategy],
-                    ),
-                    ("[{}]".format(",".join(map(str, embedding))),)
-                    + tuple(where_clause_values)
-                    + (k,),
-                )
+        docs = []
+        for idx in mmr_selected:
+            text = payload[idx].pop(self._text_key)
+            payload[idx].pop("_additional")
+            meta = payload[idx]
+            docs.append(Document(page_content=text, metadata=meta))
+        return docs
+
+    def similarity_search_with_score(
+        self, query: str, k: int = 4, **kwargs: Any
+    ) -> List[Tuple[Document, float]]:
+        """
+        Return list of documents most similar to the query
+        text and cosine distance in float for each.
+        Lower score represents more similarity.
+        """
+        if self._embedding is None:
+            raise ValueError(
+                "_embedding cannot be None for similarity_search_with_score"
+            )
+        content: Dict[str, Any] = {"concepts": [query]}
+        if kwargs.get("search_distance"):
+            content["certainty"] = kwargs.get("search_distance")
+        query_obj = self._client.query.get(self._index_name, self._query_attrs)
+        if kwargs.get("where_filter"):
+            query_obj = query_obj.with_where(kwargs.get("where_filter"))
+        if kwargs.get("tenant"):
+            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
+
+        embedded_query = self._embedding.embed_query(query)
+        if not self._by_text:
+            vector = {"vector": embedded_query}
+            result = (
+                query_obj.with_near_vector(vector)
+                .with_limit(k)
+                .with_additional("vector")
+                .do()
+            )
+        else:
+            result = (
+                query_obj.with_near_text(content)
+                .with_limit(k)
+                .with_additional("vector")
+                .do()
+            )
 
-                for row in cur.fetchall():
-                    doc = Document(page_content=row[0], metadata=row[1])
-                    result.append((doc, float(row[2])))
-            finally:
-                cur.close()
-        finally:
-            conn.close()
-        return result
+        if "errors" in result:
+            raise ValueError(f"Error during query: {result['errors']}")
+
+        docs_and_scores = []
+        for res in result["data"]["Get"][self._index_name]:
+            text = res.pop(self._text_key)
+            score = np.dot(res["_additional"]["vector"], embedded_query)
+            docs_and_scores.append((Document(page_content=text, metadata=res), score))
+        return docs_and_scores
 
     @classmethod
     def from_texts(
-        cls: Type[SingleStoreDB],
+        cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
-        distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
-        table_name: str = "embeddings",
-        content_field: str = "content",
-        metadata_field: str = "metadata",
-        vector_field: str = "vector",
-        pool_size: int = 5,
-        max_overflow: int = 10,
-        timeout: float = 30,
+        *,
+        client: Optional[weaviate.Client] = None,
+        weaviate_url: Optional[str] = None,
+        weaviate_api_key: Optional[str] = None,
+        batch_size: Optional[int] = None,
+        index_name: Optional[str] = None,
+        text_key: str = "text",
+        by_text: bool = False,
+        relevance_score_fn: Optional[
+            Callable[[float], float]
+        ] = _default_score_normalizer,
         **kwargs: Any,
-    ) -> SingleStoreDB:
-        """Create a SingleStoreDB vectorstore from raw documents.
+    ) -> Weaviate:
+        """Construct Weaviate wrapper from raw documents.
+
         This is a user-friendly interface that:
             1. Embeds documents.
-            2. Creates a new table for the embeddings in SingleStoreDB.
-            3. Adds the documents to the newly created table.
+            2. Creates a new index for the embeddings in the Weaviate instance.
+            3. Adds the documents to the newly created Weaviate index.
+
         This is intended to be a quick way to get started.
+
+        Args:
+            texts: Texts to add to vector store.
+            embedding: Text embedding model to use.
+            metadatas: Metadata associated with each text.
+            client: weaviate.Client to use.
+            weaviate_url: The Weaviate URL. If using Weaviate Cloud Services get it
+                from the ``Details`` tab. Can be passed in as a named param or by
+                setting the environment variable ``WEAVIATE_URL``. Should not be
+                specified if client is provided.
+            weaviate_api_key: The Weaviate API key. If enabled and using Weaviate Cloud
+                Services, get it from ``Details`` tab. Can be passed in as a named param
+                or by setting the environment variable ``WEAVIATE_API_KEY``. Should
+                not be specified if client is provided.
+            batch_size: Size of batch operations.
+            index_name: Index name.
+            text_key: Key to use for uploading/retrieving text to/from vectorstore.
+            by_text: Whether to search by text or by embedding.
+            relevance_score_fn: Function for converting whatever distance function the
+                vector store uses to a relevance score, which is a normalized similarity
+                score (0 means dissimilar, 1 means similar).
+            **kwargs: Additional named parameters to pass to ``Weaviate.__init__()``.
+
         Example:
             .. code-block:: python
-                from langchain_community.vectorstores import SingleStoreDB
+
                 from langchain_community.embeddings import OpenAIEmbeddings
-                s2 = SingleStoreDB.from_texts(
+                from langchain_community.vectorstores import Weaviate
+
+                embeddings = OpenAIEmbeddings()
+                weaviate = Weaviate.from_texts(
                     texts,
-                    OpenAIEmbeddings(),
-                    host="username:password@localhost:3306/database"
+                    embeddings,
+                    weaviate_url="http://localhost:8080"
                 )
         """
 
-        instance = cls(
-            embedding,
-            distance_strategy=distance_strategy,
-            table_name=table_name,
-            content_field=content_field,
-            metadata_field=metadata_field,
-            vector_field=vector_field,
-            pool_size=pool_size,
-            max_overflow=max_overflow,
-            timeout=timeout,
+        try:
+            from weaviate.util import get_valid_uuid
+        except ImportError as e:
+            raise ImportError(
+                "Could not import weaviate python  package. "
+                "Please install it with `pip install weaviate-client`"
+            ) from e
+
+        client = client or _create_weaviate_client(
+            url=weaviate_url,
+            api_key=weaviate_api_key,
+        )
+        if batch_size:
+            client.batch.configure(batch_size=batch_size)
+
+        index_name = index_name or f"LangChain_{uuid4().hex}"
+        schema = _default_schema(index_name, text_key)
+        # check whether the index already exists
+        if not client.schema.exists(index_name):
+            client.schema.create_class(schema)
+
+        embeddings = embedding.embed_documents(texts) if embedding else None
+        attributes = list(metadatas[0].keys()) if metadatas else None
+
+        # If the UUID of one of the objects already exists
+        # then the existing object will be replaced by the new object.
+        if "uuids" in kwargs:
+            uuids = kwargs.pop("uuids")
+        else:
+            uuids = [get_valid_uuid(uuid4()) for _ in range(len(texts))]
+
+        with client.batch as batch:
+            for i, text in enumerate(texts):
+                data_properties = {
+                    text_key: text,
+                }
+                if metadatas is not None:
+                    for key in metadatas[i].keys():
+                        data_properties[key] = metadatas[i][key]
+
+                _id = uuids[i]
+
+                # if an embedding strategy is not provided, we let
+                # weaviate create the embedding. Note that this will only
+                # work if weaviate has been installed with a vectorizer module
+                # like text2vec-contextionary for example
+                params = {
+                    "uuid": _id,
+                    "data_object": data_properties,
+                    "class_name": index_name,
+                }
+                if embeddings is not None:
+                    params["vector"] = embeddings[i]
+
+                batch.add_data_object(**params)
+
+            batch.flush()
+
+        return cls(
+            client,
+            index_name,
+            text_key,
+            embedding=embedding,
+            attributes=attributes,
+            relevance_score_fn=relevance_score_fn,
+            by_text=by_text,
             **kwargs,
         )
-        instance.add_texts(texts, metadatas, embedding.embed_documents(texts), **kwargs)
-        return instance
 
+    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:
+        """Delete by vector IDs.
+
+        Args:
+            ids: List of ids to delete.
+        """
+
+        if ids is None:
+            raise ValueError("No ids provided to delete.")
 
-# SingleStoreDBRetriever is not needed, but we keep it for backwards compatibility
-SingleStoreDBRetriever = VectorStoreRetriever
+        # TODO: Check if this can be done in bulk
+        for id in ids:
+            self._client.data_object.delete(uuid=id)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/sklearn.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/sklearn.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-""" Wrapper around scikit-learn NearestNeighbors implementation.
+"""Wrapper around scikit-learn NearestNeighbors implementation.
 
 The vector store can be persisted in json, bson or parquet format.
 """
 
 import json
 import math
 import os
@@ -38,15 +38,15 @@
 
     @abstractmethod
     def load(self) -> Any:
         """Loads the data from the persist_path"""
 
 
 class JsonSerializer(BaseSerializer):
-    """Serializes data in json using the json package from python standard library."""
+    """Serialize data in JSON using the json package from python standard library."""
 
     @classmethod
     def extension(cls) -> str:
         return "json"
 
     def save(self, data: Any) -> None:
         with open(self.persist_path, "w") as fp:
@@ -54,15 +54,15 @@
 
     def load(self) -> Any:
         with open(self.persist_path, "r") as fp:
             return json.load(fp)
 
 
 class BsonSerializer(BaseSerializer):
-    """Serializes data in binary json using the `bson` python package."""
+    """Serialize data in Binary JSON using the `bson` python package."""
 
     def __init__(self, persist_path: str) -> None:
         super().__init__(persist_path)
         self.bson = guard_import("bson")
 
     @classmethod
     def extension(cls) -> str:
@@ -74,15 +74,15 @@
 
     def load(self) -> Any:
         with open(self.persist_path, "rb") as fp:
             return self.bson.loads(fp.read())
 
 
 class ParquetSerializer(BaseSerializer):
-    """Serializes data in `Apache Parquet` format using the `pyarrow` package."""
+    """Serialize data in `Apache Parquet` format using the `pyarrow` package."""
 
     def __init__(self, persist_path: str) -> None:
         super().__init__(persist_path)
         self.pd = guard_import("pandas")
         self.pa = guard_import("pyarrow")
         self.pq = guard_import("pyarrow.parquet")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/sqlitevss.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/sqlitevss.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,15 +20,16 @@
 if TYPE_CHECKING:
     import sqlite3
 
 logger = logging.getLogger(__name__)
 
 
 class SQLiteVSS(VectorStore):
-    """Wrapper around SQLite with vss extension as a vector database.
+    """SQLite with VSS extension as a vector database.
+
     To use, you should have the ``sqlite-vss`` python package installed.
     Example:
         .. code-block:: python
             from langchain_community.vectorstores import SQLiteVSS
             from langchain_community.embeddings.openai import OpenAIEmbeddings
             ...
     """
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/starrocks.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/starrocks.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,15 +34,15 @@
 def debug_output(s: Any) -> None:
     """
     Print a debug message if DEBUG is True.
     Args:
         s: The message to print
     """
     if DEBUG:
-        print(s)
+        print(s)  # noqa: T201
 
 
 def get_named_result(connection: Any, query: str) -> List[dict[str, Any]]:
     """
     Get a named result from a query.
     Args:
         connection: The connection to the database
@@ -213,17 +213,19 @@
         embed_tuple_index = tuple(column_names).index(
             self.config.column_map["embedding"]
         )
         _data = []
         for n in transac:
             n = ",".join(
                 [
-                    f"'{self.escape_str(str(_n))}'"
-                    if idx != embed_tuple_index
-                    else f"array<float>{str(_n)}"
+                    (
+                        f"'{self.escape_str(str(_n))}'"
+                        if idx != embed_tuple_index
+                        else f"array<float>{str(_n)}"
+                    )
                     for (idx, _n) in enumerate(n)
                 ]
             )
             _data.append(f"({n})")
         i_str = f"""
                 INSERT INTO
                     {self.config.database}.{self.config.table}({ks})
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/supabase.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/supabase.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from __future__ import annotations
 
 import uuid
+import warnings
 from itertools import repeat
 from typing import (
     TYPE_CHECKING,
     Any,
     Dict,
     Iterable,
     List,
@@ -200,15 +201,15 @@
         query: str,
         k: int = 4,
         filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         vector = self._embedding.embed_query(query)
         return self.similarity_search_by_vector_with_relevance_scores(
-            vector, k=k, filter=filter
+            vector, k=k, filter=filter, **kwargs
         )
 
     def match_args(
         self, query: List[float], filter: Optional[Dict[str, Any]]
     ) -> Dict[str, Any]:
         ret: Dict[str, Any] = dict(query_embedding=query)
         if filter:
@@ -217,14 +218,15 @@
 
     def similarity_search_by_vector_with_relevance_scores(
         self,
         query: List[float],
         k: int,
         filter: Optional[Dict[str, Any]] = None,
         postgrest_filter: Optional[str] = None,
+        score_threshold: Optional[float] = None,
     ) -> List[Tuple[Document, float]]:
         match_documents_params = self.match_args(query, filter)
         query_builder = self._client.rpc(self.query_name, match_documents_params)
 
         if postgrest_filter:
             query_builder.params = query_builder.params.set(
                 "and", f"({postgrest_filter})"
@@ -242,14 +244,26 @@
                 ),
                 search.get("similarity", 0.0),
             )
             for search in res.data
             if search.get("content")
         ]
 
+        if score_threshold is not None:
+            match_result = [
+                (doc, similarity)
+                for doc, similarity in match_result
+                if similarity >= score_threshold
+            ]
+            if len(match_result) == 0:
+                warnings.warn(
+                    "No relevant docs were retrieved using the relevance score"
+                    f" threshold {score_threshold}"
+                )
+
         return match_result
 
     def similarity_search_by_vector_returning_embeddings(
         self,
         query: List[float],
         k: int,
         filter: Optional[Dict[str, Any]] = None,
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/surrealdb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/surrealdb.py`

 * *Files 8% similar despite different names*

```diff
@@ -27,16 +27,16 @@
             (default: "documents")
 
         (optional) db_user and db_pass: surrealdb credentials
 
     Example:
         .. code-block:: python
 
-            from langchain.vectorstores.surrealdb import SurrealDBStore
-            from langchain.embeddings import HuggingFaceEmbeddings
+            from langchain_community.vectorstores.surrealdb import SurrealDBStore
+            from langchain_community.embeddings import HuggingFaceEmbeddings
 
             embedding_function = HuggingFaceEmbeddings()
             dburl = "ws://localhost:8000/rpc"
             ns = "langchain"
             db = "docstore"
             collection = "documents"
             db_user = "root"
@@ -51,35 +51,45 @@
     """
 
     def __init__(
         self,
         embedding_function: Embeddings,
         **kwargs: Any,
     ) -> None:
-        from surrealdb import Surreal
+        try:
+            from surrealdb import Surreal
+        except ImportError as e:
+            raise ImportError(
+                """Cannot import from surrealdb.
+                please install with `pip install surrealdb`."""
+            ) from e
+
+        self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
+
+        if self.dburl[0:2] == "ws":
+            self.sdb = Surreal(self.dburl)
+        else:
+            raise ValueError("Only websocket connections are supported at this time.")
 
-        self.collection = kwargs.pop("collection", "documents")
         self.ns = kwargs.pop("ns", "langchain")
         self.db = kwargs.pop("db", "database")
-        self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
+        self.collection = kwargs.pop("collection", "documents")
         self.embedding_function = embedding_function
-        self.sdb = Surreal(self.dburl)
         self.kwargs = kwargs
 
     async def initialize(self) -> None:
         """
         Initialize connection to surrealdb database
         and authenticate if credentials are provided
         """
-        await self.sdb.connect(self.dburl)
+        await self.sdb.connect()
         if "db_user" in self.kwargs and "db_pass" in self.kwargs:
             user = self.kwargs.get("db_user")
             password = self.kwargs.get("db_pass")
             await self.sdb.signin({"user": user, "pass": password})
-
         await self.sdb.use(self.ns, self.db)
 
     @property
     def embeddings(self) -> Optional[Embeddings]:
         return (
             self.embedding_function
             if isinstance(self.embedding_function, Embeddings)
@@ -101,15 +111,17 @@
             List of ids for the newly inserted documents
         """
         embeddings = self.embedding_function.embed_documents(list(texts))
         ids = []
         for idx, text in enumerate(texts):
             data = {"text": text, "embedding": embeddings[idx]}
             if metadatas is not None and idx < len(metadatas):
-                data["metadata"] = metadatas[idx]
+                data["metadata"] = metadatas[idx]  # type: ignore[assignment]
+            else:
+                data["metadata"] = []
             record = await self.sdb.create(
                 self.collection,
                 data,
             )
             ids.append(record[0]["id"])
         return ids
 
@@ -204,34 +216,46 @@
         """
         args = {
             "collection": self.collection,
             "embedding": embedding,
             "k": k,
             "score_threshold": kwargs.get("score_threshold", 0),
         }
-        query = """select id, text, metadata,
-        vector::similarity::cosine(embedding,{embedding}) as similarity
-        from {collection}
-        where vector::similarity::cosine(embedding,{embedding}) >= {score_threshold}
-        order by similarity desc LIMIT {k}
-        """.format(**args)
-        results = await self.sdb.query(query)
+        query = f"""
+        select
+            id,
+            text,
+            metadata,
+            vector::similarity::cosine(embedding, $embedding) as similarity
+        from {args["collection"]}
+        where vector::similarity::cosine(embedding, $embedding) >= $score_threshold
+        order by similarity desc LIMIT $k;
+        """
+        results = await self.sdb.query(query, args)
 
         if len(results) == 0:
             return []
 
+        result = results[0]
+
+        if result["status"] != "OK":
+            from surrealdb.ws import SurrealException
+
+            err = result.get("result", "Unknown Error")
+            raise SurrealException(err)
+
         return [
             (
                 Document(
-                    page_content=result["text"],
-                    metadata={"id": result["id"], **result["metadata"]},
+                    page_content=doc["text"],
+                    metadata={"id": doc["id"], **(doc.get("metadata", None) or {})},
                 ),
-                result["similarity"],
+                doc["similarity"],
             )
-            for result in results[0]["result"]
+            for doc in result["result"]
         ]
 
     async def asimilarity_search_with_relevance_scores(
         self, query: str, k: int = 4, **kwargs: Any
     ) -> List[Tuple[Document, float]]:
         """Run similarity search asynchronously and return relevance scores
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/tair.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/tair.py`

 * *Files 2% similar despite different names*

```diff
@@ -169,15 +169,15 @@
         content_key: str = "content",
         metadata_key: str = "metadata",
         **kwargs: Any,
     ) -> Tair:
         try:
             from tair import tairvector
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import tair python package. "
                 "Please install it with `pip install tair`."
             )
         url = get_from_dict_or_env(kwargs, "tair_url", "TAIR_URL")
         if "tair_url" in kwargs:
             kwargs.pop("tair_url")
 
@@ -258,15 +258,15 @@
 
         Returns:
             bool: True if the index is dropped successfully.
         """
         try:
             from tair import Tair as TairClient
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import tair python package. "
                 "Please install it with `pip install tair`."
             )
         url = get_from_dict_or_env(kwargs, "tair_url", "TAIR_URL")
 
         try:
             if "tair_url" in kwargs:
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/tencentvectordb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/meilisearch.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,392 +1,349 @@
-"""Wrapper around the Tencent vector database."""
 from __future__ import annotations
 
-import json
-import logging
-import time
-from typing import Any, Dict, Iterable, List, Optional, Tuple
+import uuid
+from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple, Type
 
-import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
-from langchain_core.utils import guard_import
+from langchain_core.utils import get_from_env
 from langchain_core.vectorstores import VectorStore
 
-from langchain_community.vectorstores.utils import maximal_marginal_relevance
+if TYPE_CHECKING:
+    from meilisearch import Client
 
-logger = logging.getLogger(__name__)
 
-
-class ConnectionParams:
-    """Tencent vector DB Connection params.
-
-    See the following documentation for details:
-    https://cloud.tencent.com/document/product/1709/95820
-
-    Attribute:
-        url (str) : The access address of the vector database server
-            that the client needs to connect to.
-        key (str): API key for client to access the vector database server,
-            which is used for authentication.
-        username (str) : Account for client to access the vector database server.
-        timeout (int) : Request Timeout.
-    """
-
-    def __init__(self, url: str, key: str, username: str = "root", timeout: int = 10):
-        self.url = url
-        self.key = key
-        self.username = username
-        self.timeout = timeout
-
-
-class IndexParams:
-    """Tencent vector DB Index params.
-
-    See the following documentation for details:
-    https://cloud.tencent.com/document/product/1709/95826
-    """
-
-    def __init__(
-        self,
-        dimension: int,
-        shard: int = 1,
-        replicas: int = 2,
-        index_type: str = "HNSW",
-        metric_type: str = "L2",
-        params: Optional[Dict] = None,
-    ):
-        self.dimension = dimension
-        self.shard = shard
-        self.replicas = replicas
-        self.index_type = index_type
-        self.metric_type = metric_type
-        self.params = params
-
-
-class TencentVectorDB(VectorStore):
-    """Tencent VectorDB as a vector store.
-
-    In order to use this you need to have a database instance.
-    See the following documentation for details:
-    https://cloud.tencent.com/document/product/1709/94951
+def _create_client(
+    client: Optional[Client] = None,
+    url: Optional[str] = None,
+    api_key: Optional[str] = None,
+) -> Client:
+    try:
+        import meilisearch
+    except ImportError:
+        raise ImportError(
+            "Could not import meilisearch python package. "
+            "Please install it with `pip install meilisearch`."
+        )
+    if not client:
+        url = url or get_from_env("url", "MEILI_HTTP_ADDR")
+        try:
+            api_key = api_key or get_from_env("api_key", "MEILI_MASTER_KEY")
+        except Exception:
+            pass
+        client = meilisearch.Client(url=url, api_key=api_key)
+    elif not isinstance(client, meilisearch.Client):
+        raise ValueError(
+            f"client should be an instance of meilisearch.Client, "
+            f"got {type(client)}"
+        )
+    try:
+        client.version()
+    except ValueError as e:
+        raise ValueError(f"Failed to connect to Meilisearch: {e}")
+    return client
+
+
+class Meilisearch(VectorStore):
+    """`Meilisearch` vector store.
+
+    To use this, you need to have `meilisearch` python package installed,
+    and a running Meilisearch instance.
+
+    To learn more about Meilisearch Python, refer to the in-depth
+    Meilisearch Python documentation: https://meilisearch.github.io/meilisearch-python/.
+
+    See the following documentation for how to run a Meilisearch instance:
+    https://www.meilisearch.com/docs/learn/getting_started/quick_start.
+
+    Example:
+        .. code-block:: python
+
+            from langchain_community.vectorstores import Meilisearch
+            from langchain_community.embeddings.openai import OpenAIEmbeddings
+            import meilisearch
+
+            # api_key is optional; provide it if your meilisearch instance requires it
+            client = meilisearch.Client(url='http://127.0.0.1:7700', api_key='***')
+            embeddings = OpenAIEmbeddings()
+            embedders = {
+                "theEmbedderName": {
+                    "source": "userProvided",
+                    "dimensions": "1536"
+                }
+            }
+            vectorstore = Meilisearch(
+                embedding=embeddings,
+                embedders=embedders,
+                client=client,
+                index_name='langchain_demo',
+                text_key='text')
     """
 
-    field_id: str = "id"
-    field_vector: str = "vector"
-    field_text: str = "text"
-    field_metadata: str = "metadata"
-
     def __init__(
         self,
         embedding: Embeddings,
-        connection_params: ConnectionParams,
-        index_params: IndexParams = IndexParams(128),
-        database_name: str = "LangChainDatabase",
-        collection_name: str = "LangChainCollection",
-        drop_old: Optional[bool] = False,
+        client: Optional[Client] = None,
+        url: Optional[str] = None,
+        api_key: Optional[str] = None,
+        index_name: str = "langchain-demo",
+        text_key: str = "text",
+        metadata_key: str = "metadata",
+        *,
+        embedders: Optional[Dict[str, Any]] = None,
     ):
-        self.document = guard_import("tcvectordb.model.document")
-        tcvectordb = guard_import("tcvectordb")
-        self.embedding_func = embedding
-        self.index_params = index_params
-        self.vdb_client = tcvectordb.VectorDBClient(
-            url=connection_params.url,
-            username=connection_params.username,
-            key=connection_params.key,
-            timeout=connection_params.timeout,
-        )
-        db_list = self.vdb_client.list_databases()
-        db_exist: bool = False
-        for db in db_list:
-            if database_name == db.database_name:
-                db_exist = True
-                break
-        if db_exist:
-            self.database = self.vdb_client.database(database_name)
-        else:
-            self.database = self.vdb_client.create_database(database_name)
-        try:
-            self.collection = self.database.describe_collection(collection_name)
-            if drop_old:
-                self.database.drop_collection(collection_name)
-                self._create_collection(collection_name)
-        except tcvectordb.exceptions.VectorDBException:
-            self._create_collection(collection_name)
-
-    def _create_collection(self, collection_name: str) -> None:
-        enum = guard_import("tcvectordb.model.enum")
-        vdb_index = guard_import("tcvectordb.model.index")
-        index_type = None
-        for k, v in enum.IndexType.__members__.items():
-            if k == self.index_params.index_type:
-                index_type = v
-        if index_type is None:
-            raise ValueError("unsupported index_type")
-        metric_type = None
-        for k, v in enum.MetricType.__members__.items():
-            if k == self.index_params.metric_type:
-                metric_type = v
-        if metric_type is None:
-            raise ValueError("unsupported metric_type")
-        if self.index_params.params is None:
-            params = vdb_index.HNSWParams(m=16, efconstruction=200)
-        else:
-            params = vdb_index.HNSWParams(
-                m=self.index_params.params.get("M", 16),
-                efconstruction=self.index_params.params.get("efConstruction", 200),
-            )
-        index = vdb_index.Index(
-            vdb_index.FilterIndex(
-                self.field_id, enum.FieldType.String, enum.IndexType.PRIMARY_KEY
-            ),
-            vdb_index.VectorIndex(
-                self.field_vector,
-                self.index_params.dimension,
-                index_type,
-                metric_type,
-                params,
-            ),
-            vdb_index.FilterIndex(
-                self.field_text, enum.FieldType.String, enum.IndexType.FILTER
-            ),
-            vdb_index.FilterIndex(
-                self.field_metadata, enum.FieldType.String, enum.IndexType.FILTER
-            ),
-        )
-        self.collection = self.database.create_collection(
-            name=collection_name,
-            shard=self.index_params.shard,
-            replicas=self.index_params.replicas,
-            description="Collection for LangChain",
-            index=index,
-        )
+        """Initialize with Meilisearch client."""
+        client = _create_client(client=client, url=url, api_key=api_key)
 
-    @property
-    def embeddings(self) -> Embeddings:
-        return self.embedding_func
-
-    @classmethod
-    def from_texts(
-        cls,
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        connection_params: Optional[ConnectionParams] = None,
-        index_params: Optional[IndexParams] = None,
-        database_name: str = "LangChainDatabase",
-        collection_name: str = "LangChainCollection",
-        drop_old: Optional[bool] = False,
-        **kwargs: Any,
-    ) -> TencentVectorDB:
-        """Create a collection, indexes it with HNSW, and insert data."""
-        if len(texts) == 0:
-            raise ValueError("texts is empty")
-        if connection_params is None:
-            raise ValueError("connection_params is empty")
-        try:
-            embeddings = embedding.embed_documents(texts[0:1])
-        except NotImplementedError:
-            embeddings = [embedding.embed_query(texts[0])]
-        dimension = len(embeddings[0])
-        if index_params is None:
-            index_params = IndexParams(dimension=dimension)
-        else:
-            index_params.dimension = dimension
-        vector_db = cls(
-            embedding=embedding,
-            connection_params=connection_params,
-            index_params=index_params,
-            database_name=database_name,
-            collection_name=collection_name,
-            drop_old=drop_old,
-        )
-        vector_db.add_texts(texts=texts, metadatas=metadatas)
-        return vector_db
+        self._client = client
+        self._index_name = index_name
+        self._embedding = embedding
+        self._text_key = text_key
+        self._metadata_key = metadata_key
+        self._embedders = embedders
+        self._embedders_settings = self._client.index(
+            str(self._index_name)
+        ).update_embedders(embedders)
 
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
-        timeout: Optional[int] = None,
-        batch_size: int = 1000,
+        ids: Optional[List[str]] = None,
+        embedder_name: Optional[str] = "default",
         **kwargs: Any,
     ) -> List[str]:
-        """Insert text data into TencentVectorDB."""
+        """Run more texts through the embedding and add them to the vector store.
+
+        Args:
+            texts (Iterable[str]): Iterable of strings/text to add to the vectorstore.
+            embedder_name: Name of the embedder. Defaults to "default".
+            metadatas (Optional[List[dict]]): Optional list of metadata.
+                Defaults to None.
+            ids Optional[List[str]]: Optional list of IDs.
+                Defaults to None.
+
+        Returns:
+            List[str]: List of IDs of the texts added to the vectorstore.
+        """
         texts = list(texts)
-        try:
-            embeddings = self.embedding_func.embed_documents(texts)
-        except NotImplementedError:
-            embeddings = [self.embedding_func.embed_query(x) for x in texts]
-        if len(embeddings) == 0:
-            logger.debug("Nothing to insert, skipping.")
-            return []
-        pks: list[str] = []
-        total_count = len(embeddings)
-        for start in range(0, total_count, batch_size):
-            # Grab end index
-            docs = []
-            end = min(start + batch_size, total_count)
-            for id in range(start, end, 1):
-                metadata = "{}"
-                if metadatas is not None:
-                    metadata = json.dumps(metadatas[id])
-                doc = self.document.Document(
-                    id="{}-{}-{}".format(time.time_ns(), hash(texts[id]), id),
-                    vector=embeddings[id],
-                    text=texts[id],
-                    metadata=metadata,
-                )
-                docs.append(doc)
-                pks.append(str(id))
-            self.collection.upsert(docs, timeout)
-        return pks
+
+        # Embed and create the documents
+        docs = []
+        if ids is None:
+            ids = [uuid.uuid4().hex for _ in texts]
+        if metadatas is None:
+            metadatas = [{} for _ in texts]
+        embedding_vectors = self._embedding.embed_documents(texts)
+
+        for i, text in enumerate(texts):
+            id = ids[i]
+            metadata = metadatas[i]
+            metadata[self._text_key] = text
+            embedding = embedding_vectors[i]
+            docs.append(
+                {
+                    "id": id,
+                    "_vectors": {f"{embedder_name}": embedding},
+                    f"{self._metadata_key}": metadata,
+                }
+            )
+
+        # Send to Meilisearch
+        self._client.index(str(self._index_name)).add_documents(docs)
+        return ids
 
     def similarity_search(
         self,
         query: str,
         k: int = 4,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        filter: Optional[Dict[str, str]] = None,
+        embedder_name: Optional[str] = "default",
         **kwargs: Any,
     ) -> List[Document]:
-        """Perform a similarity search against the query string."""
-        res = self.similarity_search_with_score(
-            query=query, k=k, param=param, expr=expr, timeout=timeout, **kwargs
+        """Return meilisearch documents most similar to the query.
+
+        Args:
+            query (str): Query text for which to find similar documents.
+            embedder_name: Name of the embedder to be used. Defaults to "default".
+            k (int): Number of documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None.
+
+        Returns:
+            List[Document]: List of Documents most similar to the query
+            text and score for each.
+        """
+        docs_and_scores = self.similarity_search_with_score(
+            query=query,
+            embedder_name=embedder_name,
+            k=k,
+            filter=filter,
+            kwargs=kwargs,
         )
-        return [doc for doc, _ in res]
+        return [doc for doc, _ in docs_and_scores]
 
     def similarity_search_with_score(
         self,
         query: str,
         k: int = 4,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        filter: Optional[Dict[str, str]] = None,
+        embedder_name: Optional[str] = "default",
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Perform a search on a query string and return results with score."""
-        # Embed the query text.
-        embedding = self.embedding_func.embed_query(query)
-        res = self.similarity_search_with_score_by_vector(
-            embedding=embedding, k=k, param=param, expr=expr, timeout=timeout, **kwargs
-        )
-        return res
+        """Return meilisearch documents most similar to the query, along with scores.
 
-    def similarity_search_by_vector(
-        self,
-        embedding: List[float],
-        k: int = 4,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
-        **kwargs: Any,
-    ) -> List[Document]:
-        """Perform a similarity search against the query string."""
-        res = self.similarity_search_with_score_by_vector(
-            embedding=embedding, k=k, param=param, expr=expr, timeout=timeout, **kwargs
+        Args:
+            query (str): Query text for which to find similar documents.
+            embedder_name: Name of the embedder to be used. Defaults to "default".
+            k (int): Number of documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None.
+
+        Returns:
+            List[Document]: List of Documents most similar to the query
+            text and score for each.
+        """
+        _query = self._embedding.embed_query(query)
+
+        docs = self.similarity_search_by_vector_with_scores(
+            embedding=_query,
+            embedder_name=embedder_name,
+            k=k,
+            filter=filter,
+            kwargs=kwargs,
         )
-        return [doc for doc, _ in res]
+        return docs
 
-    def similarity_search_with_score_by_vector(
+    def similarity_search_by_vector_with_scores(
         self,
         embedding: List[float],
+        embedder_name: Optional[str] = "default",
         k: int = 4,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        filter: Optional[Dict[str, Any]] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Perform a search on a query string and return results with score."""
-        filter = None if expr is None else self.document.Filter(expr)
-        ef = 10 if param is None else param.get("ef", 10)
-        res: List[List[Dict]] = self.collection.search(
-            vectors=[embedding],
-            filter=filter,
-            params=self.document.HNSWSearchParams(ef=ef),
-            retrieve_vector=False,
-            limit=k,
-            timeout=timeout,
-        )
-        # Organize results.
-        ret: List[Tuple[Document, float]] = []
-        if res is None or len(res) == 0:
-            return ret
-        for result in res[0]:
-            meta = result.get(self.field_metadata)
-            if meta is not None:
-                meta = json.loads(meta)
-            doc = Document(page_content=result.get(self.field_text), metadata=meta)
-            pair = (doc, result.get("score", 0.0))
-            ret.append(pair)
-        return ret
+        """Return meilisearch documents most similar to embedding vector.
 
-    def max_marginal_relevance_search(
+        Args:
+            embedding (List[float]): Embedding to look up similar documents.
+            embedder_name: Name of the embedder to be used. Defaults to "default".
+            k (int): Number of documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None.
+
+        Returns:
+            List[Document]: List of Documents most similar to the query
+                vector and score for each.
+        """
+        docs = []
+        results = self._client.index(str(self._index_name)).search(
+            "",
+            {
+                "vector": embedding,
+                "hybrid": {"semanticRatio": 1.0, "embedder": embedder_name},
+                "limit": k,
+                "filter": filter,
+            },
+        )
+
+        for result in results["hits"]:
+            metadata = result[self._metadata_key]
+            if self._text_key in metadata:
+                text = metadata.pop(self._text_key)
+                semantic_score = result["_semanticScore"]
+                docs.append(
+                    (Document(page_content=text, metadata=metadata), semantic_score)
+                )
+
+        return docs
+
+    def similarity_search_by_vector(
         self,
-        query: str,
+        embedding: List[float],
         k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+        filter: Optional[Dict[str, str]] = None,
+        embedder_name: Optional[str] = "default",
         **kwargs: Any,
     ) -> List[Document]:
-        """Perform a search and return results that are reordered by MMR."""
-        embedding = self.embedding_func.embed_query(query)
-        return self.max_marginal_relevance_search_by_vector(
+        """Return meilisearch documents most similar to embedding vector.
+
+        Args:
+            embedding (List[float]): Embedding to look up similar documents.
+            embedder_name: Name of the embedder to be used. Defaults to "default".
+            k (int): Number of documents to return. Defaults to 4.
+            filter (Optional[Dict[str, str]]): Filter by metadata.
+                Defaults to None.
+
+        Returns:
+            List[Document]: List of Documents most similar to the query
+                vector and score for each.
+        """
+        docs = self.similarity_search_by_vector_with_scores(
             embedding=embedding,
+            embedder_name=embedder_name,
             k=k,
-            fetch_k=fetch_k,
-            lambda_mult=lambda_mult,
-            param=param,
-            expr=expr,
-            timeout=timeout,
-            **kwargs,
+            filter=filter,
+            kwargs=kwargs,
         )
+        return [doc for doc, _ in docs]
 
-    def max_marginal_relevance_search_by_vector(
-        self,
-        embedding: list[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        param: Optional[dict] = None,
-        expr: Optional[str] = None,
-        timeout: Optional[int] = None,
+    @classmethod
+    def from_texts(
+        cls: Type[Meilisearch],
+        texts: List[str],
+        embedding: Embeddings,
+        metadatas: Optional[List[dict]] = None,
+        client: Optional[Client] = None,
+        url: Optional[str] = None,
+        api_key: Optional[str] = None,
+        index_name: str = "langchain-demo",
+        ids: Optional[List[str]] = None,
+        text_key: Optional[str] = "text",
+        metadata_key: Optional[str] = "metadata",
+        embedders: Dict[str, Any] = {},
+        embedder_name: Optional[str] = "default",
         **kwargs: Any,
-    ) -> List[Document]:
-        """Perform a search and return results that are reordered by MMR."""
-        filter = None if expr is None else self.document.Filter(expr)
-        ef = 10 if param is None else param.get("ef", 10)
-        res: List[List[Dict]] = self.collection.search(
-            vectors=[embedding],
-            filter=filter,
-            params=self.document.HNSWSearchParams(ef=ef),
-            retrieve_vector=True,
-            limit=fetch_k,
-            timeout=timeout,
-        )
-        # Organize results.
-        documents = []
-        ordered_result_embeddings = []
-        for result in res[0]:
-            meta = result.get(self.field_metadata)
-            if meta is not None:
-                meta = json.loads(meta)
-            doc = Document(page_content=result.get(self.field_text), metadata=meta)
-            documents.append(doc)
-            ordered_result_embeddings.append(result.get(self.field_vector))
-        # Get the new order of results.
-        new_ordering = maximal_marginal_relevance(
-            np.array(embedding), ordered_result_embeddings, k=k, lambda_mult=lambda_mult
+    ) -> Meilisearch:
+        """Construct Meilisearch wrapper from raw documents.
+
+        This is a user-friendly interface that:
+            1. Embeds documents.
+            2. Adds the documents to a provided Meilisearch index.
+
+        This is intended to be a quick way to get started.
+
+        Example:
+            .. code-block:: python
+
+                from langchain_community.vectorstores import Meilisearch
+                from langchain_community.embeddings import OpenAIEmbeddings
+                import meilisearch
+
+                # The environment should be the one specified next to the API key
+                # in your Meilisearch console
+                client = meilisearch.Client(url='http://127.0.0.1:7700', api_key='***')
+                embedding = OpenAIEmbeddings()
+                embedders: Embedders index setting.
+                embedder_name: Name of the embedder. Defaults to "default".
+                docsearch = Meilisearch.from_texts(
+                    client=client,
+                    embedding=embedding,
+                )
+        """
+        client = _create_client(client=client, url=url, api_key=api_key)
+
+        vectorstore = cls(
+            embedding=embedding,
+            embedders=embedders,
+            client=client,
+            index_name=index_name,
+        )
+        vectorstore.add_texts(
+            texts=texts,
+            embedder_name=embedder_name,
+            metadatas=metadatas,
+            ids=ids,
+            text_key=text_key,
+            metadata_key=metadata_key,
         )
-        # Reorder the values and return.
-        ret = []
-        for x in new_ordering:
-            # Function can return -1 index
-            if x == -1:
-                break
-            else:
-                ret.append(documents[x])
-        return ret
+        return vectorstore
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/tigris.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/tigris.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/tiledb.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/tiledb.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """Wrapper around TileDB vector database."""
+
 from __future__ import annotations
 
 import pickle
 import random
 import sys
 from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
+from langchain_core.utils import guard_import
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 INDEX_METRICS = frozenset(["euclidean"])
 DEFAULT_METRIC = "euclidean"
 DOCUMENTS_ARRAY_NAME = "documents"
@@ -20,24 +22,18 @@
 MAX_UINT64 = np.iinfo(np.dtype("uint64")).max
 MAX_FLOAT_32 = np.finfo(np.dtype("float32")).max
 MAX_FLOAT = sys.float_info.max
 
 
 def dependable_tiledb_import() -> Any:
     """Import tiledb-vector-search if available, otherwise raise error."""
-    try:
-        import tiledb as tiledb
-        import tiledb.vector_search as tiledb_vs
-    except ImportError:
-        raise ValueError(
-            "Could not import tiledb-vector-search python package. "
-            "Please install it with `conda install -c tiledb tiledb-vector-search` "
-            "or `pip install tiledb-vector-search`"
-        )
-    return tiledb_vs, tiledb
+    return (
+        guard_import("tiledb.vector_search"),
+        guard_import("tiledb"),
+    )
 
 
 def get_vector_index_uri_from_group(group: Any) -> str:
     """Get the URI of the vector index."""
     return group[VECTOR_INDEX_NAME].uri
 
 
@@ -83,24 +79,46 @@
         index_uri: str,
         metric: str,
         *,
         vector_index_uri: str = "",
         docs_array_uri: str = "",
         config: Optional[Mapping[str, Any]] = None,
         timestamp: Any = None,
+        allow_dangerous_deserialization: bool = False,
         **kwargs: Any,
     ):
-        """Initialize with necessary components."""
+        """Initialize with necessary components.
+
+        Args:
+            allow_dangerous_deserialization: whether to allow deserialization
+                of the data which involves loading data using pickle.
+                data can be modified by malicious actors to deliver a
+                malicious payload that results in execution of
+                arbitrary code on your machine.
+        """
+        if not allow_dangerous_deserialization:
+            raise ValueError(
+                "TileDB relies on pickle for serialization and deserialization. "
+                "This can be dangerous if the data is intercepted and/or modified "
+                "by malicious actors prior to being de-serialized. "
+                "If you are sure that the data is safe from modification, you can "
+                " set allow_dangerous_deserialization=True to proceed. "
+                "Loading of compromised data using pickle can result in execution of "
+                "arbitrary code on your machine."
+            )
         self.embedding = embedding
         self.embedding_function = embedding.embed_query
         self.index_uri = index_uri
         self.metric = metric
         self.config = config
 
-        tiledb_vs, tiledb = dependable_tiledb_import()
+        tiledb_vs, tiledb = (
+            guard_import("tiledb.vector_search"),
+            guard_import("tiledb"),
+        )
         with tiledb.scope_ctx(ctx_or_config=config):
             index_group = tiledb.Group(self.index_uri, "r")
             self.vector_index_uri = (
                 vector_index_uri
                 if vector_index_uri != ""
                 else get_vector_index_uri_from_group(index_group)
             )
@@ -150,15 +168,15 @@
             k: Number of Documents to return. Defaults to 4.
             filter (Optional[Dict[str, Any]]): Filter by metadata. Defaults to None.
             score_threshold: Optional, a floating point value to filter the
                 resulting set of retrieved docs
         Returns:
             List of Documents and scores.
         """
-        tiledb_vs, tiledb = dependable_tiledb_import()
+        tiledb = guard_import("tiledb")
         docs = []
         docs_array = tiledb.open(
             self.docs_array_uri, "r", timestamp=self.timestamp, config=self.config
         )
         for idx, score in zip(ids, scores):
             if idx == 0 and score == 0:
                 continue
@@ -454,15 +472,18 @@
         index_type: str,
         dimensions: int,
         vector_type: np.dtype,
         *,
         metadatas: bool = True,
         config: Optional[Mapping[str, Any]] = None,
     ) -> None:
-        tiledb_vs, tiledb = dependable_tiledb_import()
+        tiledb_vs, tiledb = (
+            guard_import("tiledb.vector_search"),
+            guard_import("tiledb"),
+        )
         with tiledb.scope_ctx(ctx_or_config=config):
             try:
                 tiledb.group_create(index_uri)
             except tiledb.TileDBError as err:
                 raise err
             group = tiledb.Group(index_uri, "w")
             vector_index_uri = get_vector_index_uri(group.uri)
@@ -527,15 +548,18 @@
         if metric not in INDEX_METRICS:
             raise ValueError(
                 (
                     f"Unsupported distance metric: {metric}. "
                     f"Expected one of {list(INDEX_METRICS)}"
                 )
             )
-        tiledb_vs, tiledb = dependable_tiledb_import()
+        tiledb_vs, tiledb = (
+            guard_import("tiledb.vector_search"),
+            guard_import("tiledb"),
+        )
         input_vectors = np.array(embeddings).astype(np.float32)
         cls.create(
             index_uri=index_uri,
             index_type=index_type,
             dimensions=input_vectors.shape[1],
             vector_type=input_vectors.dtype,
             metadatas=metadatas is not None,
@@ -623,15 +647,15 @@
             ids: Optional ids of each text object.
             timestamp: Optional timestamp to write new texts with.
             kwargs: vectorstore specific parameters
 
         Returns:
             List of ids from adding the texts into the vectorstore.
         """
-        tiledb_vs, tiledb = dependable_tiledb_import()
+        tiledb = guard_import("tiledb")
         embeddings = self.embedding.embed_documents(list(texts))
         if ids is None:
             ids = [str(random.randint(0, MAX_UINT64 - 1)) for _ in texts]
 
         external_ids = np.array(ids).astype(np.uint64)
         vectors = np.empty((len(embeddings)), dtype="O")
         for i in range(len(embeddings)):
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/timescalevector.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/timescalevector.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """VectorStore wrapper around a Postgres-TimescaleVector database."""
+
 from __future__ import annotations
 
 import enum
 import logging
 import uuid
 from datetime import timedelta
 from typing import (
@@ -146,15 +147,15 @@
         service_url: Optional[str] = None,
         pre_delete_collection: bool = False,
         **kwargs: Any,
     ) -> TimescaleVector:
         num_dimensions = len(embeddings[0])
 
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         if service_url is None:
             service_url = cls.get_service_url(kwargs)
 
@@ -187,15 +188,15 @@
         service_url: Optional[str] = None,
         pre_delete_collection: bool = False,
         **kwargs: Any,
     ) -> TimescaleVector:
         num_dimensions = len(embeddings[0])
 
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         if service_url is None:
             service_url = cls.get_service_url(kwargs)
 
@@ -228,15 +229,15 @@
         Args:
             texts: Iterable of strings to add to the vectorstore.
             embeddings: List of list of embedding vectors.
             metadatas: List of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         records = list(zip(ids, metadatas, texts, embeddings))
         self.sync_client.upsert(records)
 
@@ -255,15 +256,15 @@
         Args:
             texts: Iterable of strings to add to the vectorstore.
             embeddings: List of list of embedding vectors.
             metadatas: List of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
         """
         if ids is None:
-            ids = [str(uuid.uuid1()) for _ in texts]
+            ids = [str(uuid.uuid4()) for _ in texts]
 
         if not metadatas:
             metadatas = [{} for _ in texts]
 
         records = list(zip(ids, metadatas, texts, embeddings))
         await self.async_client.upsert(records)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/typesense.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/typesense.py`

 * *Files 1% similar despite different names*

```diff
@@ -223,15 +223,15 @@
                     protocol="http",
                     typesense_collection_name="langchain-memory",
                 )
         """
         try:
             from typesense import Client
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import typesense python package. "
                 "Please install it with `pip install typesense`."
             )
 
         node = {
             "host": host,
             "port": str(port),
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/usearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/usearch.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,32 +1,26 @@
 from __future__ import annotations
 
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
+from langchain_core.utils import guard_import
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.docstore.base import AddableMixin, Docstore
 from langchain_community.docstore.in_memory import InMemoryDocstore
 
 
 def dependable_usearch_import() -> Any:
     """
     Import usearch if available, otherwise raise error.
     """
-    try:
-        import usearch.index
-    except ImportError:
-        raise ImportError(
-            "Could not import usearch python package. "
-            "Please install it with `pip install usearch` "
-        )
-    return usearch.index
+    return guard_import("usearch.index")
 
 
 class USearch(VectorStore):
     """`USearch` vector store.
 
     To use, you should have the ``usearch`` python package installed.
     """
@@ -166,11 +160,11 @@
         if ids is None:
             ids = np.array([str(id) for id, _ in enumerate(texts)])
         for i, text in enumerate(texts):
             metadata = metadatas[i] if metadatas else {}
             documents.append(Document(page_content=text, metadata=metadata))
 
         docstore = InMemoryDocstore(dict(zip(ids, documents)))
-        usearch = dependable_usearch_import()
+        usearch = guard_import("usearch.index")
         index = usearch.Index(ndim=len(embeddings[0]), metric=metric)
         index.add(np.array(ids), np.array(embeddings))
         return cls(embedding, index, docstore, ids.tolist())
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/utils.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/utils.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/vald.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/vald.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 """Wrapper around Vald vector database."""
+
 from __future__ import annotations
 
 from typing import Any, Iterable, List, Optional, Tuple, Type
 
 import numpy as np
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 
 from langchain_community.vectorstores.utils import maximal_marginal_relevance
 
 
 class Vald(VectorStore):
-    """Wrapper around Vald vector database.
+    """Vald vector database.
 
     To use, you should have the ``vald-client-python`` python package installed.
 
     Example:
         .. code-block:: python
 
             from langchain_community.embeddings import HuggingFaceEmbeddings
@@ -54,15 +55,15 @@
     def embeddings(self) -> Optional[Embeddings]:
         return self._embedding
 
     def _get_channel(self) -> Any:
         try:
             import grpc
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import grpcio python package. "
                 "Please install it with `pip install grpcio`."
             )
         return (
             grpc.secure_channel(
                 self.target, self.grpc_credentials, options=self.grpc_options
             )
@@ -82,15 +83,15 @@
         Args:
             skip_strict_exist_check: Deprecated. This is not used basically.
         """
         try:
             from vald.v1.payload import payload_pb2
             from vald.v1.vald import upsert_pb2_grpc
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import vald-client-python python package. "
                 "Please install it with `pip install vald-client-python`."
             )
 
         channel = self._get_channel()
         # Depending on the network quality,
         # it is necessary to wait for ChannelConnectivity.READY.
@@ -122,15 +123,15 @@
         Args:
             skip_strict_exist_check: Deprecated. This is not used basically.
         """
         try:
             from vald.v1.payload import payload_pb2
             from vald.v1.vald import remove_pb2_grpc
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import vald-client-python python package. "
                 "Please install it with `pip install vald-client-python`."
             )
 
         if ids is None:
             raise ValueError("No ids provided to delete")
 
@@ -217,15 +218,15 @@
         grpc_metadata: Optional[Any] = None,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
         try:
             from vald.v1.payload import payload_pb2
             from vald.v1.vald import search_pb2_grpc
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import vald-client-python python package. "
                 "Please install it with `pip install vald-client-python`."
             )
 
         channel = self._get_channel()
         # Depending on the network quality,
         # it is necessary to wait for ChannelConnectivity.READY.
@@ -285,15 +286,15 @@
         grpc_metadata: Optional[Any] = None,
         **kwargs: Any,
     ) -> List[Document]:
         try:
             from vald.v1.payload import payload_pb2
             from vald.v1.vald import object_pb2_grpc
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import vald-client-python python package. "
                 "Please install it with `pip install vald-client-python`."
             )
         channel = self._get_channel()
         # Depending on the network quality,
         # it is necessary to wait for ChannelConnectivity.READY.
         # _ = grpc.channel_ready_future(channel).result(timeout=10)
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/vearch.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/vearch.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,15 +35,15 @@
         """
         try:
             if flag:
                 import vearch_cluster
             else:
                 import vearch
         except ImportError:
-            raise ValueError(
+            raise ImportError(
                 "Could not import suitable python package. "
                 "Please install it with `pip install vearch or vearch_cluster`."
             )
 
         if flag:
             if path_or_url is None:
                 raise ValueError("Please input url of cluster")
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/vectara.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/vectara.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,28 +14,33 @@
 from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class SummaryConfig:
-    """
+    """Configuration for summary generation.
+
     is_enabled: True if summary is enabled, False otherwise
     max_results: maximum number of results to summarize
     response_lang: requested language for the summary
+    prompt_name: name of the prompt to use for summarization
+      (see https://docs.vectara.com/docs/learn/grounded-generation/select-a-summarizer)
     """
 
     is_enabled: bool = False
     max_results: int = 7
     response_lang: str = "eng"
+    prompt_name: str = "vectara-summary-ext-v1.2.0"
 
 
 @dataclass
 class MMRConfig:
-    """
+    """Configuration for Maximal Marginal Relevance (MMR) search.
+
     is_enabled: True if MMR is enabled, False otherwise
     mmr_k: number of results to fetch for MMR, defaults to 50
     diversity_bias: number between 0 and 1 that determines the degree
         of diversity among the results with 0 corresponding
         to minimum diversity and 1 to maximum diversity.
         Defaults to 0.3.
         Note: diversity_bias is equivalent 1-lambda_mult
@@ -46,15 +51,16 @@
     is_enabled: bool = False
     mmr_k: int = 50
     diversity_bias: float = 0.3
 
 
 @dataclass
 class VectaraQueryConfig:
-    """
+    """Configuration for Vectara query.
+
     k: Number of Documents to return. Defaults to 10.
     lambda_val: lexical match parameter for hybrid search.
     filter Dictionary of argument(s) to filter on metadata. For example a
         filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
         https://docs.vectara.com/docs/search-apis/sql/filter-overview
         for more details.
     score_threshold: minimal score threshold for the result.
@@ -79,15 +85,15 @@
     """`Vectara API` vector store.
 
      See (https://vectara.com).
 
     Example:
         .. code-block:: python
 
-            from langchain.vectorstores import Vectara
+            from langchain_community.vectorstores import Vectara
 
             vectorstore = Vectara(
                 vectara_customer_id=vectara_customer_id,
                 vectara_corpus_id=vectara_corpus_id,
                 vectara_api_key=vectara_api_key
             )
     """
@@ -301,15 +307,15 @@
 
         success_str = self._index_doc(doc, use_core_api=use_core_api)
 
         if success_str == "E_ALREADY_EXISTS":
             self._delete_doc(doc_id)
             self._index_doc(doc)
         elif success_str == "E_NO_PERMISSIONS":
-            print(
+            print(  # noqa: T201
                 """No permissions to add document to Vectara. 
                 Check your corpus ID, customer ID and API key"""
             )
         return [doc_id]
 
     def vectara_query(
         self,
@@ -332,17 +338,19 @@
             config.summary_config = SummaryConfig(**config.summary_config)
 
         data = {
             "query": [
                 {
                     "query": query,
                     "start": 0,
-                    "numResults": config.mmr_config.mmr_k
-                    if config.mmr_config.is_enabled
-                    else config.k,
+                    "numResults": (
+                        config.mmr_config.mmr_k
+                        if config.mmr_config.is_enabled
+                        else config.k
+                    ),
                     "contextConfig": {
                         "sentencesBefore": config.n_sentence_context,
                         "sentencesAfter": config.n_sentence_context,
                     },
                     "corpusKey": [
                         {
                             "customerId": self._vectara_customer_id,
@@ -360,14 +368,15 @@
                 "mmrConfig": {"diversityBias": config.mmr_config.diversity_bias},
             }
         if config.summary_config.is_enabled:
             data["query"][0]["summary"] = [
                 {
                     "maxSummarizedResults": config.summary_config.max_results,
                     "responseLang": config.summary_config.response_lang,
+                    "summarizerPromptName": config.summary_config.prompt_name,
                 }
             ]
 
         response = self._session.post(
             headers=self._get_post_headers(),
             url="https://api.vectara.io/v1/query",
             data=json.dumps(data),
@@ -376,15 +385,15 @@
 
         if response.status_code != 200:
             logger.error(
                 "Query failed %s",
                 f"(code {response.status_code}, reason {response.reason}, details "
                 f"{response.text})",
             )
-            return [], ""
+            return [], ""  # type: ignore[return-value]
 
         result = response.json()
 
         if config.score_threshold:
             responses = [
                 r
                 for r in result["responseSet"][0]["response"]
@@ -446,15 +455,15 @@
         Returns:
             List of Documents most similar to the query and score for each.
         """
         config = VectaraQueryConfig(**kwargs)
         docs = self.vectara_query(query, config)
         return docs
 
-    def similarity_search(
+    def similarity_search(  # type: ignore[override]
         self,
         query: str,
         **kwargs: Any,
     ) -> List[Document]:
         """Return Vectara documents most similar to query, along with scores.
 
         Args:
@@ -466,15 +475,15 @@
         """
         docs_and_scores = self.similarity_search_with_score(
             query,
             **kwargs,
         )
         return [doc for doc, _ in docs_and_scores]
 
-    def max_marginal_relevance_search(
+    def max_marginal_relevance_search(  # type: ignore[override]
         self,
         query: str,
         fetch_k: int = 50,
         lambda_mult: float = 0.5,
         **kwargs: Any,
     ) -> List[Document]:
         """Return docs selected using the maximal marginal relevance.
@@ -508,15 +517,15 @@
         **kwargs: Any,
     ) -> Vectara:
         """Construct Vectara wrapper from raw documents.
         This is intended to be a quick way to get started.
         Example:
             .. code-block:: python
 
-                from langchain.vectorstores import Vectara
+                from langchain_community.vectorstores import Vectara
                 vectara = Vectara.from_texts(
                     texts,
                     vectara_customer_id=customer_id,
                     vectara_corpus_id=corpus_id,
                     vectara_api_key=api_key,
                 )
         """
@@ -540,15 +549,15 @@
         **kwargs: Any,
     ) -> Vectara:
         """Construct Vectara wrapper from raw documents.
         This is intended to be a quick way to get started.
         Example:
             .. code-block:: python
 
-                from langchain.vectorstores import Vectara
+                from langchain_community.vectorstores import Vectara
                 vectara = Vectara.from_files(
                     files_list,
                     vectara_customer_id=customer_id,
                     vectara_corpus_id=corpus_id,
                     vectara_api_key=api_key,
                 )
         """
@@ -556,24 +565,25 @@
         # embeddings (required by interface)
         vectara = cls(**kwargs)
         vectara.add_files(files, metadatas)
         return vectara
 
 
 class VectaraRetriever(VectorStoreRetriever):
-    """Retriever class for `Vectara`."""
+    """Retriever for `Vectara`."""
 
     vectorstore: Vectara
     """Vectara vectorstore."""
     search_kwargs: dict = Field(
         default_factory=lambda: {
             "lambda_val": 0.0,
             "k": 5,
             "filter": "",
             "n_sentence_context": "2",
+            "summary_config": SummaryConfig(),
         }
     )
 
     """Search params.
         k: Number of Documents to return. Defaults to 5.
         lambda_val: lexical match parameter for hybrid search.
         filter: Dictionary of argument(s) to filter on metadata. For example a
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/vespa.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/vespa.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/weaviate.py` & `gigachain_community-0.2.0/langchain_community/chat_models/yuan2.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,528 +1,486 @@
+"""ChatYuan2 wrapper."""
+
 from __future__ import annotations
 
-import datetime
-import os
+import logging
 from typing import (
-    TYPE_CHECKING,
     Any,
+    AsyncIterator,
     Callable,
     Dict,
-    Iterable,
+    Iterator,
     List,
+    Mapping,
     Optional,
     Tuple,
+    Type,
+    Union,
 )
-from uuid import uuid4
-
-import numpy as np
-from langchain_core.documents import Document
-from langchain_core.embeddings import Embeddings
-from langchain_core.vectorstores import VectorStore
-
-from langchain_community.vectorstores.utils import maximal_marginal_relevance
-
-if TYPE_CHECKING:
-    import weaviate
-
-
-def _default_schema(index_name: str) -> Dict:
-    return {
-        "class": index_name,
-        "properties": [
-            {
-                "name": "text",
-                "dataType": ["text"],
-            }
-        ],
-    }
-
-
-def _create_weaviate_client(
-    url: Optional[str] = None,
-    api_key: Optional[str] = None,
-    **kwargs: Any,
-) -> weaviate.Client:
-    try:
-        import weaviate
-    except ImportError:
-        raise ImportError(
-            "Could not import weaviate python  package. "
-            "Please install it with `pip install weaviate-client`"
-        )
-    url = url or os.environ.get("WEAVIATE_URL")
-    api_key = api_key or os.environ.get("WEAVIATE_API_KEY")
-    auth = weaviate.auth.AuthApiKey(api_key=api_key) if api_key else None
-    return weaviate.Client(url=url, auth_client_secret=auth, **kwargs)
 
+from langchain_core.callbacks import (
+    AsyncCallbackManagerForLLMRun,
+    CallbackManagerForLLMRun,
+)
+from langchain_core.language_models.chat_models import (
+    BaseChatModel,
+    agenerate_from_stream,
+    generate_from_stream,
+)
+from langchain_core.messages import (
+    AIMessage,
+    AIMessageChunk,
+    BaseMessage,
+    BaseMessageChunk,
+    ChatMessage,
+    ChatMessageChunk,
+    FunctionMessage,
+    HumanMessage,
+    HumanMessageChunk,
+    SystemMessage,
+    SystemMessageChunk,
+)
+from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
+from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
+from langchain_core.utils import (
+    get_from_dict_or_env,
+    get_pydantic_field_names,
+)
+from tenacity import (
+    before_sleep_log,
+    retry,
+    retry_if_exception_type,
+    stop_after_attempt,
+    wait_exponential,
+)
 
-def _default_score_normalizer(val: float) -> float:
-    return 1 - 1 / (1 + np.exp(val))
-
+logger = logging.getLogger(__name__)
 
-def _json_serializable(value: Any) -> Any:
-    if isinstance(value, datetime.datetime):
-        return value.isoformat()
-    return value
 
+class ChatYuan2(BaseChatModel):
+    """`Yuan2.0` Chat models API.
 
-class Weaviate(VectorStore):
-    """`Weaviate` vector store.
+    To use, you should have the ``openai-python`` package installed, if package
+    not installed, using ```pip install openai``` to install it. The
+    environment variable ``YUAN2_API_KEY`` set to your API key, if not set,
+    everyone can access apis.
 
-    To use, you should have the ``weaviate-client`` python package installed.
+    Any parameters that are valid to be passed to the openai.create call can be passed
+    in, even if not explicitly saved on this class.
 
     Example:
         .. code-block:: python
 
-            import weaviate
-            from langchain_community.vectorstores import Weaviate
-
-            client = weaviate.Client(url=os.environ["WEAVIATE_URL"], ...)
-            weaviate = Weaviate(client, index_name, text_key)
+            from langchain_community.chat_models import ChatYuan2
 
+            chat = ChatYuan2()
     """
 
-    def __init__(
-        self,
-        client: Any,
-        index_name: str,
-        text_key: str,
-        embedding: Optional[Embeddings] = None,
-        attributes: Optional[List[str]] = None,
-        relevance_score_fn: Optional[
-            Callable[[float], float]
-        ] = _default_score_normalizer,
-        by_text: bool = True,
-    ):
-        """Initialize with Weaviate client."""
-        try:
-            import weaviate
-        except ImportError:
-            raise ImportError(
-                "Could not import weaviate python package. "
-                "Please install it with `pip install weaviate-client`."
-            )
-        if not isinstance(client, weaviate.Client):
-            raise ValueError(
-                f"client should be an instance of weaviate.Client, got {type(client)}"
-            )
-        self._client = client
-        self._index_name = index_name
-        self._embedding = embedding
-        self._text_key = text_key
-        self._query_attrs = [self._text_key]
-        self.relevance_score_fn = relevance_score_fn
-        self._by_text = by_text
-        if attributes is not None:
-            self._query_attrs.extend(attributes)
+    client: Any  #: :meta private:
+    async_client: Any = Field(default=None, exclude=True)  #: :meta private:
+
+    model_name: str = Field(default="yuan2", alias="model")
+    """Model name to use."""
+
+    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
+    """Holds any model parameters valid for `create` call not explicitly specified."""
+
+    yuan2_api_key: Optional[str] = Field(default="EMPTY", alias="api_key")
+    """Automatically inferred from env var `YUAN2_API_KEY` if not provided."""
+
+    yuan2_api_base: Optional[str] = Field(
+        default="http://127.0.0.1:8000/v1", alias="base_url"
+    )
+    """Base URL path for API requests, an OpenAI compatible API server."""
+
+    request_timeout: Optional[Union[float, Tuple[float, float]]] = None
+    """Timeout for requests to yuan2 completion API. Default is 600 seconds."""
+
+    max_retries: int = 6
+    """Maximum number of retries to make when generating."""
+
+    streaming: bool = False
+    """Whether to stream the results or not."""
+
+    max_tokens: Optional[int] = None
+    """Maximum number of tokens to generate."""
+
+    temperature: float = 1.0
+    """What sampling temperature to use."""
+
+    top_p: Optional[float] = 0.9
+    """The top-p value to use for sampling."""
+
+    stop: Optional[List[str]] = ["<eod>"]
+    """A list of strings to stop generation when encountered."""
+
+    repeat_last_n: Optional[int] = 64
+    "Last n tokens to penalize"
+
+    repeat_penalty: Optional[float] = 1.18
+    """The penalty to apply to repeated tokens."""
+
+    class Config:
+        """Configuration for this pydantic object."""
+
+        allow_population_by_field_name = True
 
     @property
-    def embeddings(self) -> Optional[Embeddings]:
-        return self._embedding
+    def lc_secrets(self) -> Dict[str, str]:
+        return {"yuan2_api_key": "YUAN2_API_KEY"}
 
-    def _select_relevance_score_fn(self) -> Callable[[float], float]:
-        return (
-            self.relevance_score_fn
-            if self.relevance_score_fn
-            else _default_score_normalizer
-        )
+    @property
+    def lc_attributes(self) -> Dict[str, Any]:
+        attributes: Dict[str, Any] = {}
 
-    def add_texts(
-        self,
-        texts: Iterable[str],
-        metadatas: Optional[List[dict]] = None,
-        **kwargs: Any,
-    ) -> List[str]:
-        """Upload texts with metadata (properties) to Weaviate."""
-        from weaviate.util import get_valid_uuid
-
-        ids = []
-        embeddings: Optional[List[List[float]]] = None
-        if self._embedding:
-            if not isinstance(texts, list):
-                texts = list(texts)
-            embeddings = self._embedding.embed_documents(texts)
-
-        with self._client.batch as batch:
-            for i, text in enumerate(texts):
-                data_properties = {self._text_key: text}
-                if metadatas is not None:
-                    for key, val in metadatas[i].items():
-                        data_properties[key] = _json_serializable(val)
-
-                # Allow for ids (consistent w/ other methods)
-                # # Or uuids (backwards compatible w/ existing arg)
-                # If the UUID of one of the objects already exists
-                # then the existing object will be replaced by the new object.
-                _id = get_valid_uuid(uuid4())
-                if "uuids" in kwargs:
-                    _id = kwargs["uuids"][i]
-                elif "ids" in kwargs:
-                    _id = kwargs["ids"][i]
-
-                batch.add_data_object(
-                    data_object=data_properties,
-                    class_name=self._index_name,
-                    uuid=_id,
-                    vector=embeddings[i] if embeddings else None,
-                    tenant=kwargs.get("tenant"),
-                )
-                ids.append(_id)
-        return ids
+        if self.yuan2_api_base:
+            attributes["yuan2_api_base"] = self.yuan2_api_base
 
-    def similarity_search(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> List[Document]:
-        """Return docs most similar to query.
-
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-
-        Returns:
-            List of Documents most similar to the query.
-        """
-        if self._by_text:
-            return self.similarity_search_by_text(query, k, **kwargs)
-        else:
-            if self._embedding is None:
-                raise ValueError(
-                    "_embedding cannot be None for similarity_search when "
-                    "_by_text=False"
+        if self.yuan2_api_key:
+            attributes["yuan2_api_key"] = self.yuan2_api_key
+
+        return attributes
+
+    @root_validator(pre=True)
+    def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
+        """Build extra kwargs from additional params that were passed in."""
+        all_required_field_names = get_pydantic_field_names(cls)
+        extra = values.get("model_kwargs", {})
+        for field_name in list(values):
+            if field_name in extra:
+                raise ValueError(f"Found {field_name} supplied twice.")
+            if field_name not in all_required_field_names:
+                logger.warning(
+                    f"""WARNING! {field_name} is not default parameter.
+                    {field_name} was transferred to model_kwargs.
+                    Please confirm that {field_name} is what you intended."""
                 )
-            embedding = self._embedding.embed_query(query)
-            return self.similarity_search_by_vector(embedding, k, **kwargs)
+                extra[field_name] = values.pop(field_name)
+
+        invalid_model_kwargs = all_required_field_names.intersection(extra.keys())
+        if invalid_model_kwargs:
+            raise ValueError(
+                f"Parameters {invalid_model_kwargs} should be specified explicitly. "
+                f"Instead they were passed in as part of `model_kwargs` parameter."
+            )
 
-    def similarity_search_by_text(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> List[Document]:
-        """Return docs most similar to query.
-
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-
-        Returns:
-            List of Documents most similar to the query.
-        """
-        content: Dict[str, Any] = {"concepts": [query]}
-        if kwargs.get("search_distance"):
-            content["certainty"] = kwargs.get("search_distance")
-        query_obj = self._client.query.get(self._index_name, self._query_attrs)
-        if kwargs.get("where_filter"):
-            query_obj = query_obj.with_where(kwargs.get("where_filter"))
-        if kwargs.get("tenant"):
-            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
-        if kwargs.get("additional"):
-            query_obj = query_obj.with_additional(kwargs.get("additional"))
-        result = query_obj.with_near_text(content).with_limit(k).do()
-        if "errors" in result:
-            raise ValueError(f"Error during query: {result['errors']}")
-        docs = []
-        for res in result["data"]["Get"][self._index_name]:
-            text = res.pop(self._text_key)
-            docs.append(Document(page_content=text, metadata=res))
-        return docs
-
-    def similarity_search_by_vector(
-        self, embedding: List[float], k: int = 4, **kwargs: Any
-    ) -> List[Document]:
-        """Look up similar documents by embedding vector in Weaviate."""
-        vector = {"vector": embedding}
-        query_obj = self._client.query.get(self._index_name, self._query_attrs)
-        if kwargs.get("where_filter"):
-            query_obj = query_obj.with_where(kwargs.get("where_filter"))
-        if kwargs.get("tenant"):
-            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
-        if kwargs.get("additional"):
-            query_obj = query_obj.with_additional(kwargs.get("additional"))
-        result = query_obj.with_near_vector(vector).with_limit(k).do()
-        if "errors" in result:
-            raise ValueError(f"Error during query: {result['errors']}")
-        docs = []
-        for res in result["data"]["Get"][self._index_name]:
-            text = res.pop(self._text_key)
-            docs.append(Document(page_content=text, metadata=res))
-        return docs
+        values["model_kwargs"] = extra
+        return values
 
-    def max_marginal_relevance_search(
-        self,
-        query: str,
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> List[Document]:
-        """Return docs selected using the maximal marginal relevance.
+    @root_validator()
+    def validate_environment(cls, values: Dict) -> Dict:
+        """Validate that api key and python package exists in environment."""
+        values["yuan2_api_key"] = get_from_dict_or_env(
+            values, "yuan2_api_key", "YUAN2_API_KEY"
+        )
 
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
+        try:
+            import openai
 
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
-                        Defaults to 0.5.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        if self._embedding is not None:
-            embedding = self._embedding.embed_query(query)
-        else:
-            raise ValueError(
-                "max_marginal_relevance_search requires a suitable Embeddings object"
+        except ImportError:
+            raise ImportError(
+                "Could not import openai python package. "
+                "Please install it with `pip install openai`."
             )
+        client_params = {
+            "api_key": values["yuan2_api_key"],
+            "base_url": values["yuan2_api_base"],
+            "timeout": values["request_timeout"],
+            "max_retries": values["max_retries"],
+        }
+
+        # generate client and async_client
+        if not values.get("client"):
+            values["client"] = openai.OpenAI(**client_params).chat.completions
+        if not values.get("async_client"):
+            values["async_client"] = openai.AsyncOpenAI(
+                **client_params
+            ).chat.completions
+
+        return values
 
-        return self.max_marginal_relevance_search_by_vector(
-            embedding, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs
+    @property
+    def _default_params(self) -> Dict[str, Any]:
+        """Get the default parameters for calling yuan2 API."""
+        params = {
+            "model": self.model_name,
+            "stream": self.streaming,
+            "temperature": self.temperature,
+            "top_p": self.top_p,
+            **self.model_kwargs,
+        }
+        if self.max_tokens is not None:
+            params["max_tokens"] = self.max_tokens
+        if self.request_timeout is not None:
+            params["request_timeout"] = self.request_timeout
+        return params
+
+    def completion_with_retry(self, **kwargs: Any) -> Any:
+        """Use tenacity to retry the completion call."""
+        retry_decorator = _create_retry_decorator(self)
+
+        @retry_decorator
+        def _completion_with_retry(**kwargs: Any) -> Any:
+            return self.client.create(**kwargs)
+
+        return _completion_with_retry(**kwargs)
+
+    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:
+        overall_token_usage: dict = {}
+        logger.debug(
+            f"type(llm_outputs): {type(llm_outputs)}; llm_outputs: {llm_outputs}"
         )
+        for output in llm_outputs:
+            if output is None:
+                # Happens in streaming
+                continue
+            token_usage = output["token_usage"]
+            for k, v in token_usage.items():
+                if k in overall_token_usage:
+                    overall_token_usage[k] += v
+                else:
+                    overall_token_usage[k] = v
+        return {"token_usage": overall_token_usage, "model_name": self.model_name}
 
-    def max_marginal_relevance_search_by_vector(
+    def _stream(
         self,
-        embedding: List[float],
-        k: int = 4,
-        fetch_k: int = 20,
-        lambda_mult: float = 0.5,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> List[Document]:
-        """Return docs selected using the maximal marginal relevance.
-
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
+    ) -> Iterator[ChatGenerationChunk]:
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs, "stream": True}
+
+        default_chunk_class = AIMessageChunk
+        for chunk in self.completion_with_retry(messages=message_dicts, **params):
+            if not isinstance(chunk, dict):
+                chunk = chunk.model_dump()
+            if len(chunk["choices"]) == 0:
+                continue
+            choice = chunk["choices"][0]
+            chunk = _convert_delta_to_message_chunk(
+                choice["delta"], default_chunk_class
+            )
+            finish_reason = choice.get("finish_reason")
+            generation_info = (
+                dict(finish_reason=finish_reason) if finish_reason is not None else None
+            )
+            default_chunk_class = chunk.__class__
+            cg_chunk = ChatGenerationChunk(
+                message=chunk,
+                generation_info=generation_info,
+            )
+            if run_manager:
+                run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
 
-        Args:
-            embedding: Embedding to look up documents similar to.
-            k: Number of Documents to return. Defaults to 4.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
-                        Defaults to 0.5.
-
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        vector = {"vector": embedding}
-        query_obj = self._client.query.get(self._index_name, self._query_attrs)
-        if kwargs.get("where_filter"):
-            query_obj = query_obj.with_where(kwargs.get("where_filter"))
-        if kwargs.get("tenant"):
-            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
-        results = (
-            query_obj.with_additional("vector")
-            .with_near_vector(vector)
-            .with_limit(fetch_k)
-            .do()
-        )
+    def _generate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[CallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> ChatResult:
+        if self.streaming:
+            stream_iter = self._stream(
+                messages=messages, stop=stop, run_manager=run_manager, **kwargs
+            )
+            return generate_from_stream(stream_iter)
 
-        payload = results["data"]["Get"][self._index_name]
-        embeddings = [result["_additional"]["vector"] for result in payload]
-        mmr_selected = maximal_marginal_relevance(
-            np.array(embedding), embeddings, k=k, lambda_mult=lambda_mult
-        )
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs}
+        response = self.completion_with_retry(messages=message_dicts, **params)
+        return self._create_chat_result(response)
+
+    def _create_message_dicts(
+        self, messages: List[BaseMessage], stop: Optional[List[str]]
+    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
+        params = dict(self._invocation_params)
+        if stop is not None:
+            if "stop" in params:
+                raise ValueError("`stop` found in both the input and default params.")
+            params["stop"] = stop
+        message_dicts = [_convert_message_to_dict(m) for m in messages]
+        return message_dicts, params
+
+    def _create_chat_result(self, response: Union[dict, BaseModel]) -> ChatResult:
+        generations = []
+        logger.debug(f"type(response): {type(response)}; response: {response}")
+        if not isinstance(response, dict):
+            response = response.dict()
+        for res in response["choices"]:
+            message = _convert_dict_to_message(res["message"])
+            generation_info = dict(finish_reason=res["finish_reason"])
+            if "logprobs" in res:
+                generation_info["logprobs"] = res["logprobs"]
+            gen = ChatGeneration(
+                message=message,
+                generation_info=generation_info,
+            )
+            generations.append(gen)
+        llm_output = {
+            "token_usage": response.get("usage", {}),
+            "model_name": self.model_name,
+        }
+        return ChatResult(generations=generations, llm_output=llm_output)
 
-        docs = []
-        for idx in mmr_selected:
-            text = payload[idx].pop(self._text_key)
-            payload[idx].pop("_additional")
-            meta = payload[idx]
-            docs.append(Document(page_content=text, metadata=meta))
-        return docs
-
-    def similarity_search_with_score(
-        self, query: str, k: int = 4, **kwargs: Any
-    ) -> List[Tuple[Document, float]]:
-        """
-        Return list of documents most similar to the query
-        text and cosine distance in float for each.
-        Lower score represents more similarity.
-        """
-        if self._embedding is None:
-            raise ValueError(
-                "_embedding cannot be None for similarity_search_with_score"
+    async def _astream(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
+        **kwargs: Any,
+    ) -> AsyncIterator[ChatGenerationChunk]:
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs, "stream": True}
+
+        default_chunk_class = AIMessageChunk
+        async for chunk in await acompletion_with_retry(
+            self, messages=message_dicts, **params
+        ):
+            if not isinstance(chunk, dict):
+                chunk = chunk.model_dump()
+            if len(chunk["choices"]) == 0:
+                continue
+            choice = chunk["choices"][0]
+            chunk = _convert_delta_to_message_chunk(
+                choice["delta"], default_chunk_class
+            )
+            finish_reason = choice.get("finish_reason")
+            generation_info = (
+                dict(finish_reason=finish_reason) if finish_reason is not None else None
             )
-        content: Dict[str, Any] = {"concepts": [query]}
-        if kwargs.get("search_distance"):
-            content["certainty"] = kwargs.get("search_distance")
-        query_obj = self._client.query.get(self._index_name, self._query_attrs)
-        if kwargs.get("where_filter"):
-            query_obj = query_obj.with_where(kwargs.get("where_filter"))
-        if kwargs.get("tenant"):
-            query_obj = query_obj.with_tenant(kwargs.get("tenant"))
-
-        embedded_query = self._embedding.embed_query(query)
-        if not self._by_text:
-            vector = {"vector": embedded_query}
-            result = (
-                query_obj.with_near_vector(vector)
-                .with_limit(k)
-                .with_additional("vector")
-                .do()
-            )
-        else:
-            result = (
-                query_obj.with_near_text(content)
-                .with_limit(k)
-                .with_additional("vector")
-                .do()
-            )
-
-        if "errors" in result:
-            raise ValueError(f"Error during query: {result['errors']}")
-
-        docs_and_scores = []
-        for res in result["data"]["Get"][self._index_name]:
-            text = res.pop(self._text_key)
-            score = np.dot(res["_additional"]["vector"], embedded_query)
-            docs_and_scores.append((Document(page_content=text, metadata=res), score))
-        return docs_and_scores
-
-    @classmethod
-    def from_texts(
-        cls,
-        texts: List[str],
-        embedding: Embeddings,
-        metadatas: Optional[List[dict]] = None,
-        *,
-        client: Optional[weaviate.Client] = None,
-        weaviate_url: Optional[str] = None,
-        weaviate_api_key: Optional[str] = None,
-        batch_size: Optional[int] = None,
-        index_name: Optional[str] = None,
-        text_key: str = "text",
-        by_text: bool = False,
-        relevance_score_fn: Optional[
-            Callable[[float], float]
-        ] = _default_score_normalizer,
+            default_chunk_class = chunk.__class__
+            cg_chunk = ChatGenerationChunk(
+                message=chunk,
+                generation_info=generation_info,
+            )
+            if run_manager:
+                await run_manager.on_llm_new_token(chunk.content, chunk=cg_chunk)
+            yield cg_chunk
+
+    async def _agenerate(
+        self,
+        messages: List[BaseMessage],
+        stop: Optional[List[str]] = None,
+        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
         **kwargs: Any,
-    ) -> Weaviate:
-        """Construct Weaviate wrapper from raw documents.
+    ) -> ChatResult:
+        if self.streaming:
+            stream_iter = self._astream(
+                messages=messages, stop=stop, run_manager=run_manager, **kwargs
+            )
+            return await agenerate_from_stream(stream_iter)
 
-        This is a user-friendly interface that:
-            1. Embeds documents.
-            2. Creates a new index for the embeddings in the Weaviate instance.
-            3. Adds the documents to the newly created Weaviate index.
-
-        This is intended to be a quick way to get started.
-
-        Args:
-            texts: Texts to add to vector store.
-            embedding: Text embedding model to use.
-            metadatas: Metadata associated with each text.
-            client: weaviate.Client to use.
-            weaviate_url: The Weaviate URL. If using Weaviate Cloud Services get it
-                from the ``Details`` tab. Can be passed in as a named param or by
-                setting the environment variable ``WEAVIATE_URL``. Should not be
-                specified if client is provided.
-            weaviate_api_key: The Weaviate API key. If enabled and using Weaviate Cloud
-                Services, get it from ``Details`` tab. Can be passed in as a named param
-                or by setting the environment variable ``WEAVIATE_API_KEY``. Should
-                not be specified if client is provided.
-            batch_size: Size of batch operations.
-            index_name: Index name.
-            text_key: Key to use for uploading/retrieving text to/from vectorstore.
-            by_text: Whether to search by text or by embedding.
-            relevance_score_fn: Function for converting whatever distance function the
-                vector store uses to a relevance score, which is a normalized similarity
-                score (0 means dissimilar, 1 means similar).
-            **kwargs: Additional named parameters to pass to ``Weaviate.__init__()``.
-
-        Example:
-            .. code-block:: python
-
-                from langchain_community.embeddings import OpenAIEmbeddings
-                from langchain_community.vectorstores import Weaviate
-
-                embeddings = OpenAIEmbeddings()
-                weaviate = Weaviate.from_texts(
-                    texts,
-                    embeddings,
-                    weaviate_url="http://localhost:8080"
-                )
-        """
+        message_dicts, params = self._create_message_dicts(messages, stop)
+        params = {**params, **kwargs}
+        response = await acompletion_with_retry(self, messages=message_dicts, **params)
+        return self._create_chat_result(response)
 
-        try:
-            from weaviate.util import get_valid_uuid
-        except ImportError as e:
-            raise ImportError(
-                "Could not import weaviate python  package. "
-                "Please install it with `pip install weaviate-client`"
-            ) from e
-
-        client = client or _create_weaviate_client(
-            url=weaviate_url,
-            api_key=weaviate_api_key,
-        )
-        if batch_size:
-            client.batch.configure(batch_size=batch_size)
+    @property
+    def _invocation_params(self) -> Mapping[str, Any]:
+        """Get the parameters used to invoke the model."""
+        yuan2_creds: Dict[str, Any] = {
+            "model": self.model_name,
+        }
+        return {**yuan2_creds, **self._default_params}
+
+    @property
+    def _llm_type(self) -> str:
+        """Return type of chat model."""
+        return "chat-yuan2"
+
+
+def _create_retry_decorator(llm: ChatYuan2) -> Callable[[Any], Any]:
+    import openai
+
+    min_seconds = 1
+    max_seconds = 60
+    # Wait 2^x * 1 second between each retry starting with
+    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards
+    return retry(
+        reraise=True,
+        stop=stop_after_attempt(llm.max_retries),
+        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),
+        retry=(
+            retry_if_exception_type(openai.APITimeoutError)
+            | retry_if_exception_type(openai.APIError)
+            | retry_if_exception_type(openai.APIConnectionError)
+            | retry_if_exception_type(openai.RateLimitError)
+            | retry_if_exception_type(openai.InternalServerError)
+        ),
+        before_sleep=before_sleep_log(logger, logging.WARNING),
+    )
+
+
+async def acompletion_with_retry(llm: ChatYuan2, **kwargs: Any) -> Any:
+    """Use tenacity to retry the async completion call."""
+    retry_decorator = _create_retry_decorator(llm)
+
+    @retry_decorator
+    async def _completion_with_retry(**kwargs: Any) -> Any:
+        # Use OpenAI's async api https://github.com/openai/openai-python#async-api
+        return await llm.async_client.create(**kwargs)
+
+    return await _completion_with_retry(**kwargs)
+
+
+def _convert_delta_to_message_chunk(
+    _dict: Mapping[str, Any], default_class: Type[BaseMessageChunk]
+) -> BaseMessageChunk:
+    role = _dict.get("role")
+    content = _dict.get("content") or ""
+
+    if role == "user" or default_class == HumanMessageChunk:
+        return HumanMessageChunk(content=content)
+    elif role == "assistant" or default_class == AIMessageChunk:
+        return AIMessageChunk(content=content)
+    elif role == "system" or default_class == SystemMessageChunk:
+        return SystemMessageChunk(content=content)
+    elif role or default_class == ChatMessageChunk:
+        return ChatMessageChunk(content=content, role=role)  # type: ignore[arg-type]
+    else:
+        return default_class(content=content)  # type: ignore[call-arg]
+
+
+def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:
+    role = _dict.get("role")
+    if role == "user":
+        return HumanMessage(content=_dict.get("content", ""))
+    elif role == "assistant":
+        return AIMessage(content=_dict.get("content", ""))
+    elif role == "system":
+        return SystemMessage(content=_dict.get("content", ""))
+    else:
+        return ChatMessage(content=_dict.get("content", ""), role=role)  # type: ignore[arg-type]
 
-        index_name = index_name or f"LangChain_{uuid4().hex}"
-        schema = _default_schema(index_name)
-        # check whether the index already exists
-        if not client.schema.exists(index_name):
-            client.schema.create_class(schema)
-
-        embeddings = embedding.embed_documents(texts) if embedding else None
-        attributes = list(metadatas[0].keys()) if metadatas else None
-
-        # If the UUID of one of the objects already exists
-        # then the existing object will be replaced by the new object.
-        if "uuids" in kwargs:
-            uuids = kwargs.pop("uuids")
-        else:
-            uuids = [get_valid_uuid(uuid4()) for _ in range(len(texts))]
-
-        with client.batch as batch:
-            for i, text in enumerate(texts):
-                data_properties = {
-                    text_key: text,
-                }
-                if metadatas is not None:
-                    for key in metadatas[i].keys():
-                        data_properties[key] = metadatas[i][key]
-
-                _id = uuids[i]
-
-                # if an embedding strategy is not provided, we let
-                # weaviate create the embedding. Note that this will only
-                # work if weaviate has been installed with a vectorizer module
-                # like text2vec-contextionary for example
-                params = {
-                    "uuid": _id,
-                    "data_object": data_properties,
-                    "class_name": index_name,
-                }
-                if embeddings is not None:
-                    params["vector"] = embeddings[i]
-
-                batch.add_data_object(**params)
-
-            batch.flush()
-
-        return cls(
-            client,
-            index_name,
-            text_key,
-            embedding=embedding,
-            attributes=attributes,
-            relevance_score_fn=relevance_score_fn,
-            by_text=by_text,
-            **kwargs,
-        )
 
-    def delete(self, ids: Optional[List[str]] = None, **kwargs: Any) -> None:
-        """Delete by vector IDs.
+def _convert_message_to_dict(message: BaseMessage) -> dict:
+    """Convert a LangChain message to a dictionary.
 
-        Args:
-            ids: List of ids to delete.
-        """
-
-        if ids is None:
-            raise ValueError("No ids provided to delete.")
-
-        # TODO: Check if this can be done in bulk
-        for id in ids:
-            self._client.data_object.delete(uuid=id)
+    Args:
+        message: The LangChain message.
+
+    Returns:
+        The dictionary.
+    """
+    message_dict: Dict[str, Any]
+    if isinstance(message, ChatMessage):
+        message_dict = {"role": message.role, "content": message.content}
+    elif isinstance(message, HumanMessage):
+        message_dict = {"role": "user", "content": message.content}
+    elif isinstance(message, AIMessage):
+        message_dict = {"role": "assistant", "content": message.content}
+    elif isinstance(message, SystemMessage):
+        message_dict = {"role": "system", "content": message.content}
+    elif isinstance(message, FunctionMessage):
+        message_dict = {
+            "role": "function",
+            "name": message.name,
+            "content": message.content,
+        }
+    else:
+        raise ValueError(f"Got unknown type {message}")
+    if "name" in message.additional_kwargs:
+        message_dict["name"] = message.additional_kwargs["name"]
+    return message_dict
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/xata.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/xata.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,15 +23,15 @@
         api_key: str,
         db_url: str,
         embedding: Embeddings,
         table_name: str,
     ) -> None:
         """Initialize with Xata client."""
         try:
-            from xata.client import XataClient  # noqa: F401
+            from xata.client import XataClient
         except ImportError:
             raise ImportError(
                 "Could not import xata python package. "
                 "Please install it with `pip install xata`."
             )
         self._client = XataClient(api_key=api_key, db_url=db_url)
         self._embedding: Embeddings = embedding
```

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/zep.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/zep.py`

 * *Files identical despite different names*

### Comparing `gigachain_community-0.0.6.1/langchain_community/vectorstores/zilliz.py` & `gigachain_community-0.2.0/langchain_community/vectorstores/zilliz.py`

 * *Files 5% similar despite different names*

```diff
@@ -32,14 +32,17 @@
             Defaults to "Session".
         index_params (Optional[dict]): Which index params to use. Defaults to
             HNSW/AUTOINDEX depending on service.
         search_params (Optional[dict]): Which search params to use. Defaults to
             default of index.
         drop_old (Optional[bool]): Whether to drop the current collection. Defaults
             to False.
+        auto_id (bool): Whether to enable auto id for primary key. Defaults to False.
+            If False, you needs to provide text ids (string less than 65535 bytes).
+            If True, Milvus will generate unique integers as primary keys.
 
     The connection args used for this class comes in the form of a dict,
     here are a few of the options:
         address (str): The actual address of Zilliz
             instance. Example address: "localhost:19530"
         uri (str): The uri of Zilliz instance. Example uri:
             "https://in03-ba4234asae.api.gcp-us-west1.zillizcloud.com",
@@ -142,14 +145,17 @@
         metadatas: Optional[List[dict]] = None,
         collection_name: str = "LangChainCollection",
         connection_args: Optional[Dict[str, Any]] = None,
         consistency_level: str = "Session",
         index_params: Optional[dict] = None,
         search_params: Optional[dict] = None,
         drop_old: bool = False,
+        *,
+        ids: Optional[List[str]] = None,
+        auto_id: bool = False,
         **kwargs: Any,
     ) -> Zilliz:
         """Create a Zilliz collection, indexes it with HNSW, and insert data.
 
         Args:
             texts (List[str]): Text data.
             embedding (Embeddings): Embedding function.
@@ -163,23 +169,28 @@
                 to "Session".
             index_params (Optional[dict], optional): Which index_params to use.
                 Defaults to None.
             search_params (Optional[dict], optional): Which search params to use.
                 Defaults to None.
             drop_old (Optional[bool], optional): Whether to drop the collection with
                 that name if it exists. Defaults to False.
+            ids (Optional[List[str]]): List of text ids.
+            auto_id (bool): Whether to enable auto id for primary key. Defaults to
+                False. If False, you needs to provide text ids (string less than 65535
+                bytes). If True, Milvus will generate unique integers as primary keys.
 
         Returns:
             Zilliz: Zilliz Vector Store
         """
         vector_db = cls(
             embedding_function=embedding,
             collection_name=collection_name,
             connection_args=connection_args or {},
             consistency_level=consistency_level,
             index_params=index_params,
             search_params=search_params,
             drop_old=drop_old,
+            auto_id=auto_id,
             **kwargs,
         )
-        vector_db.add_texts(texts=texts, metadatas=metadatas)
+        vector_db.add_texts(texts=texts, metadatas=metadatas, ids=ids)
         return vector_db
```

### Comparing `gigachain_community-0.0.6.1/pyproject.toml` & `gigachain_community-0.2.0/pyproject.toml`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 [tool.poetry]
 name = "gigachain-community"
-version = "0.0.6.1"
-description = "Community contributed LangChain integrations."
+version = "0.2.0"
+description = "Community contributed gigachain integrations."
 authors = []
 license = "MIT"
 readme = "README.md"
 repository = "https://github.com/langchain-ai/langchain"
 packages = [
     {include = "langchain_community"}
 ]
 
 [tool.poetry.dependencies]
 python = ">=3.8.1,<4.0"
-gigachain-core = "^0.1"
+gigachain-core = "^0.2.0"
+gigachain = "^0.2.0"
 SQLAlchemy = ">=1.4,<3"
 requests = "^2"
 PyYAML = ">=5.3"
 numpy = "^1"
 aiohttp = "^3.8.3"
 tenacity = "^8.1.0"
 dataclasses-json = ">= 0.5.7, < 0.7"
-langsmith = "~0.0.63"
+langsmith = "^0.1.0"
+gigachat = "^0.1.27"
 tqdm = {version = ">=4.48.0", optional = true}
 openapi-pydantic = {version = "^0.3.2", optional = true}
 faiss-cpu = {version = "^1", optional = true}
 beautifulsoup4 = {version = "^4", optional = true}
 jinja2 = {version = "^3", optional = true}
 cohere = {version = "^4", optional = true}
 openai = {version = "<2", optional = true}
@@ -34,15 +36,15 @@
 gradientai = {version="^1.4.0", optional = true}
 pgvector = {version = "^0.1.6", optional = true}
 atlassian-python-api = {version = "^3.36.0", optional=true}
 html2text = {version="^2020.1.16", optional=true}
 numexpr = {version="^2.8.6", optional=true}
 jq = {version = "^1.4.1", optional = true}
 pdfminer-six = {version = "^20221105", optional = true}
-lxml = {version = "^4.9.2", optional = true}
+lxml = {version = ">=4.9.3,<6.0", optional = true}
 pymupdf = {version = "^1.22.3", optional = true}
 rapidocr-onnxruntime = {version = "^1.3.2", optional = true, python = ">=3.8.1,<3.12"}
 pypdfium2 = {version = "^4.10.0", optional = true}
 gql = {version = "^3.4.1", optional = true}
 pandas = {version = "^2.0.1", optional = true}
 telethon = {version = "^1.28.5", optional = true}
 chardet = {version="^5.1.0", optional=true}
@@ -52,28 +54,27 @@
 bibtexparser = {version = "^1.4.0", optional = true}
 pyspark = {version = "^3.4.0", optional = true}
 mwparserfromhell = {version = "^0.6.4", optional = true}
 mwxml = {version = "^0.3.3", optional = true}
 esprima = {version = "^4.0.1", optional = true}
 streamlit = {version = "^1.18.0", optional = true, python = ">=3.8.1,<3.9.7 || >3.9.7,<4.0"}
 psychicapi = {version = "^0.8.0", optional = true}
-cassio = {version = "^0.1.0", optional = true}
+cassio = {version = "^0.1.6", optional = true}
 sympy = {version = "^1.12", optional = true}
 rapidfuzz = {version = "^3.1.1", optional = true}
 jsonschema = {version = ">1", optional = true}
 rank-bm25 = {version = "^0.2.2", optional = true}
 geopandas = {version = "^0.13.1", optional = true}
 gitpython = {version = "^3.1.32", optional = true}
 feedparser = {version = "^6.0.10", optional = true}
 newspaper3k = {version = "^0.2.8", optional = true}
 xata = {version = "^1.0.0a7", optional = true}
 xmltodict = {version = "^0.13.0", optional = true}
 markdownify = {version = "^0.11.6", optional = true}
 assemblyai = {version = "^0.17.0", optional = true}
-dashvector = {version = "^1.0.1", optional = true}
 sqlite-vss = {version = "^0.1.2", optional = true}
 motor = {version = "^3.3.1", optional = true}
 timescale-vector = {version = "^0.0.1", optional = true}
 typer = {version= "^0.9.0", optional = true}
 anthropic = {version = "^0.3.11", optional = true}
 aiosqlite = {version = "^0.19.0", optional = true}
 rspace_client = {version = "^2.5.0", optional = true}
@@ -81,41 +82,62 @@
 google-cloud-documentai = {version = "^2.20.1", optional = true}
 fireworks-ai = {version = "^0.9.0", optional = true}
 javelin-sdk = {version = "^0.1.8", optional = true}
 hologres-vector = {version = "^0.0.6", optional = true}
 praw = {version = "^7.7.1", optional = true}
 msal = {version = "^1.25.0", optional = true}
 databricks-vectorsearch = {version = "^0.21", optional = true}
+cloudpickle = {version = ">=2.0.0", optional = true}
 dgml-utils = {version = "^0.3.0", optional = true}
 datasets = {version = "^2.15.0", optional = true}
+tree-sitter = {version = "^0.20.2", optional = true}
+tree-sitter-languages = {version = "^1.8.0", optional = true}
 azure-ai-documentintelligence = {version = "^1.0.0b1", optional = true}
 oracle-ads = {version = "^2.9.1", optional = true}
+httpx = {version = "^0.24.1", optional = true}
+elasticsearch = {version = "^8.12.0", optional = true}
+hdbcli = {version = "^2.19.21", optional = true}
+oci = {version = "^2.119.1", optional = true}
+rdflib = {version = "7.0.0", optional = true}
+nvidia-riva-client = {version = "^2.14.0", optional = true}
+azure-search-documents = {version = "11.4.0", optional = true}
+azure-identity = {version = "^1.15.0", optional = true}
+tidb-vector = {version = ">=0.0.3,<1.0.0", optional = true}
+friendli-client = {version = "^1.2.4", optional = true}
+premai = {version = "^0.3.25", optional = true}
+vdms = {version = "^0.0.20", optional = true}
+httpx-sse = {version = "^0.4.0", optional = true}
+pyjwt = {version = "^2.8.0", optional = true}
+oracledb = {version = "^2.2.0", optional = true}
+httplib2 = {version = "^0.22.0"}
+google-auth-httplib2 = {version = "^0.2.0"}
 
 [tool.poetry.group.test]
 optional = true
 
 [tool.poetry.group.test.dependencies]
 # The only dependencies that should be added are
 # dependencies used for running tests (e.g., pytest, freezegun, response).
 # Any dependencies that do not meet that criteria will be removed.
 pytest = "^7.3.0"
-pytest-cov = "^4.0.0"
+pytest-cov = "^4.1.0"
 pytest-dotenv = "^0.5.2"
-duckdb-engine = "^0.9.2"
+duckdb-engine = "^0.11.0"
 pytest-watcher = "^0.2.6"
 freezegun = "^1.2.2"
 responses = "^0.22.0"
 pytest-asyncio = "^0.20.3"
 lark = "^1.1.5"
 pandas = "^2.0.0"
 pytest-mock  = "^3.10.0"
 pytest-socket = "^0.6.0"
 syrupy = "^4.0.2"
 requests-mock = "^1.11.0"
 gigachain-core = {path = "../core", develop = true}
+gigachain = {path = "../langchain", develop = true}
 
 [tool.poetry.group.codespell]
 optional = true
 
 [tool.poetry.group.codespell.dependencies]
 codespell = "^2.2.0"
 
@@ -138,49 +160,51 @@
 #    fixtures. Keep the fixtures minimal.
 # See Contributing Guide for more instructions on working with optional dependencies.
 # https://python.langchain.com/docs/contributing/code#working-with-optional-dependencies
 pytest-vcr = "^1.0.2"
 wrapt = "^1.15.0"
 openai = "^1"
 python-dotenv = "^1.0.0"
-cassio = "^0.1.0"
-tiktoken = "^0.3.2"
+cassio = "^0.1.6"
+tiktoken = ">=0.3.2,<0.6.0"
 anthropic = "^0.3.11"
 gigachain-core = { path = "../core", develop = true }
+gigachain = {path = "../langchain", develop = true}
 fireworks-ai = "^0.9.0"
-boto3 = ">=1.28.57,<2"
-google-cloud-aiplatform = ">=1.37.0,<2"
+vdms = "^0.0.20"
+exllamav2 = "^0.0.18"
 
 [tool.poetry.group.lint]
 optional = true
 
 [tool.poetry.group.lint.dependencies]
 ruff = "^0.1.5"
 
 [tool.poetry.group.typing.dependencies]
-mypy = "^0.991"
+mypy = "^1"
 types-pyyaml = "^6.0.12.2"
 types-requests = "^2.28.11.5"
 types-toml = "^0.10.8.1"
 types-pytz = "^2023.3.0.0"
 types-chardet = "^5.0.4.6"
 types-redis = "^4.3.21.6"
 mypy-protobuf = "^3.0.0"
 gigachain-core = {path = "../core", develop = true}
+gigachain-text-splitters = {path = "../text-splitters", develop = true}
+gigachain = {path = "../langchain", develop = true}
 
 [tool.poetry.group.dev]
 optional = true
 
 [tool.poetry.group.dev.dependencies]
 jupyter = "^1.0.0"
 setuptools = "^67.6.1"
 gigachain-core = {path = "../core", develop = true}
 
 [tool.poetry.extras]
-
 cli = ["typer"]
 
 # An extra used to be able to add extended testing.
 # Please use new-line on formatting to make it easier to add new packages without
 # merge-conflicts
 extended_testing = [
  "aleph-alpha-client",
@@ -222,49 +246,70 @@
  "rapidfuzz",
  "jsonschema",
  "rank-bm25",
  "geopandas",
  "jinja2",
  "gitpython",
  "newspaper3k",
+ "nvidia-riva-client",
  "feedparser",
  "xata",
  "xmltodict",
  "faiss-cpu",
  "openapi-pydantic",
  "markdownify",
  "arxiv",
- "dashvector",
  "sqlite-vss",
  "rapidocr-onnxruntime",
  "motor",
  "timescale-vector",
  "anthropic",
  "upstash-redis",
  "rspace_client",
  "fireworks-ai",
  "javelin-sdk",
  "hologres-vector",
  "praw",
  "databricks-vectorsearch",
+ "cloudpickle",
  "dgml-utils",
  "cohere",
+ "tree-sitter",
+ "tree-sitter-languages",
  "azure-ai-documentintelligence",
  "oracle-ads",
+ "httpx",
+ "elasticsearch",
+ "hdbcli",
+ "oci",
+ "rdflib",
+ "azure-search-documents",
+ "azure-identity",
+ "tidb-vector",
+ "cloudpickle",
+ "friendli-client",
+ "premai",
+ "vdms",
+ "httpx-sse",
+ "pyjwt",
+ "oracledb"
 ]
 
 [tool.ruff]
+exclude = [
+  "tests/examples/non-utf8-encoding.py",
+  "tests/integration_tests/examples/non-utf8-encoding.py",
+]
+
+[tool.ruff.lint]
 select = [
   "E",  # pycodestyle
   "F",  # pyflakes
   "I",  # isort
-]
-exclude = [
-  "tests/examples/non-utf8-encoding.py",
-  "tests/integration_tests/examples/non-utf8-encoding.py",
+  "T201", # print
 ]
 
 [tool.mypy]
 ignore_missing_imports = "True"
 disallow_untyped_defs = "True"
 exclude = ["notebooks", "examples", "example_data"]
 
@@ -294,14 +339,14 @@
   "requires: mark tests as requiring a specific library",
   "scheduled: mark tests to run in scheduled testing",
   "compile: mark placeholder test used to compile integration tests without running them"
 ]
 asyncio_mode = "auto"
 
 [tool.codespell]
-skip = '.git,*.pdf,*.svg,*.pdf,*.yaml,*.ipynb,poetry.lock,*.min.js,*.css,package-lock.json,example_data,_dist,examples'
+skip = '.git,*.pdf,*.svg,*.pdf,*.yaml,*.ipynb,poetry.lock,*.min.js,*.css,package-lock.json,example_data,_dist,examples,*.trig'
 # Ignore latin etc
 ignore-regex = '.*(Stati Uniti|Tense=Pres).*'
 # whats is a typo but used frequently in queries so kept as is
 # aapply - async apply
 # unsecure - typo but part of API, decided to not bother for now
 ignore-words-list = 'momento,collison,ned,foor,reworkd,parth,whats,aapply,mysogyny,unsecure,damon,crate,aadd,symbl,precesses,accademia,nin'
```

### Comparing `gigachain_community-0.0.6.1/PKG-INFO` & `gigachain_community-0.2.0/PKG-INFO`

 * *Files 17% similar despite different names*

```diff
@@ -1,119 +1,141 @@
 Metadata-Version: 2.1
 Name: gigachain-community
-Version: 0.0.6.1
-Summary: Community contributed LangChain integrations.
+Version: 0.2.0
+Summary: Community contributed gigachain integrations.
 Home-page: https://github.com/langchain-ai/langchain
 License: MIT
 Requires-Python: >=3.8.1,<4.0
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
+Classifier: Programming Language :: Python :: 3.12
 Provides-Extra: cli
 Provides-Extra: extended-testing
 Requires-Dist: PyYAML (>=5.3)
 Requires-Dist: SQLAlchemy (>=1.4,<3)
 Requires-Dist: aiohttp (>=3.8.3,<4.0.0)
 Requires-Dist: aiosqlite (>=0.19.0,<0.20.0) ; extra == "extended-testing"
 Requires-Dist: aleph-alpha-client (>=2.15.0,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: anthropic (>=0.3.11,<0.4.0) ; extra == "extended-testing"
 Requires-Dist: arxiv (>=1.4,<2.0) ; extra == "extended-testing"
 Requires-Dist: assemblyai (>=0.17.0,<0.18.0) ; extra == "extended-testing"
 Requires-Dist: atlassian-python-api (>=3.36.0,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: azure-ai-documentintelligence (>=1.0.0b1,<2.0.0) ; extra == "extended-testing"
+Requires-Dist: azure-identity (>=1.15.0,<2.0.0) ; extra == "extended-testing"
+Requires-Dist: azure-search-documents (==11.4.0) ; extra == "extended-testing"
 Requires-Dist: beautifulsoup4 (>=4,<5) ; extra == "extended-testing"
 Requires-Dist: bibtexparser (>=1.4.0,<2.0.0) ; extra == "extended-testing"
-Requires-Dist: cassio (>=0.1.0,<0.2.0) ; extra == "extended-testing"
+Requires-Dist: cassio (>=0.1.6,<0.2.0) ; extra == "extended-testing"
 Requires-Dist: chardet (>=5.1.0,<6.0.0) ; extra == "extended-testing"
+Requires-Dist: cloudpickle (>=2.0.0) ; extra == "extended-testing" or extra == "extended-testing"
 Requires-Dist: cohere (>=4,<5) ; extra == "extended-testing"
-Requires-Dist: dashvector (>=1.0.1,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: databricks-vectorsearch (>=0.21,<0.22) ; extra == "extended-testing"
 Requires-Dist: dataclasses-json (>=0.5.7,<0.7)
 Requires-Dist: datasets (>=2.15.0,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: dgml-utils (>=0.3.0,<0.4.0) ; extra == "extended-testing"
+Requires-Dist: elasticsearch (>=8.12.0,<9.0.0) ; extra == "extended-testing"
 Requires-Dist: esprima (>=4.0.1,<5.0.0) ; extra == "extended-testing"
 Requires-Dist: faiss-cpu (>=1,<2) ; extra == "extended-testing"
 Requires-Dist: feedparser (>=6.0.10,<7.0.0) ; extra == "extended-testing"
 Requires-Dist: fireworks-ai (>=0.9.0,<0.10.0) ; extra == "extended-testing"
+Requires-Dist: friendli-client (>=1.2.4,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: geopandas (>=0.13.1,<0.14.0) ; extra == "extended-testing"
-Requires-Dist: gigachain-core (>=0.1,<0.2)
+Requires-Dist: gigachain (>=0.2.0,<0.3.0)
+Requires-Dist: gigachain-core (>=0.2.0,<0.3.0)
+Requires-Dist: gigachat (>=0.1.27,<0.2.0)
 Requires-Dist: gitpython (>=3.1.32,<4.0.0) ; extra == "extended-testing"
+Requires-Dist: google-auth-httplib2 (>=0.2.0,<0.3.0)
 Requires-Dist: google-cloud-documentai (>=2.20.1,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: gql (>=3.4.1,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: gradientai (>=1.4.0,<2.0.0) ; extra == "extended-testing"
+Requires-Dist: hdbcli (>=2.19.21,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: hologres-vector (>=0.0.6,<0.0.7) ; extra == "extended-testing"
 Requires-Dist: html2text (>=2020.1.16,<2021.0.0) ; extra == "extended-testing"
+Requires-Dist: httplib2 (>=0.22.0,<0.23.0)
+Requires-Dist: httpx (>=0.24.1,<0.25.0) ; extra == "extended-testing"
+Requires-Dist: httpx-sse (>=0.4.0,<0.5.0) ; extra == "extended-testing"
 Requires-Dist: javelin-sdk (>=0.1.8,<0.2.0) ; extra == "extended-testing"
 Requires-Dist: jinja2 (>=3,<4) ; extra == "extended-testing"
 Requires-Dist: jq (>=1.4.1,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: jsonschema (>1) ; extra == "extended-testing"
-Requires-Dist: langsmith (>=0.0.63,<0.1.0)
-Requires-Dist: lxml (>=4.9.2,<5.0.0) ; extra == "extended-testing"
+Requires-Dist: langsmith (>=0.1.0,<0.2.0)
+Requires-Dist: lxml (>=4.9.3,<6.0) ; extra == "extended-testing"
 Requires-Dist: markdownify (>=0.11.6,<0.12.0) ; extra == "extended-testing"
 Requires-Dist: motor (>=3.3.1,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: msal (>=1.25.0,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: mwparserfromhell (>=0.6.4,<0.7.0) ; extra == "extended-testing"
 Requires-Dist: mwxml (>=0.3.3,<0.4.0) ; extra == "extended-testing"
 Requires-Dist: newspaper3k (>=0.2.8,<0.3.0) ; extra == "extended-testing"
 Requires-Dist: numexpr (>=2.8.6,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: numpy (>=1,<2)
+Requires-Dist: nvidia-riva-client (>=2.14.0,<3.0.0) ; extra == "extended-testing"
+Requires-Dist: oci (>=2.119.1,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: openai (<2) ; extra == "extended-testing"
 Requires-Dist: openapi-pydantic (>=0.3.2,<0.4.0) ; extra == "extended-testing"
 Requires-Dist: oracle-ads (>=2.9.1,<3.0.0) ; extra == "extended-testing"
+Requires-Dist: oracledb (>=2.2.0,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: pandas (>=2.0.1,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: pdfminer-six (>=20221105,<20221106) ; extra == "extended-testing"
 Requires-Dist: pgvector (>=0.1.6,<0.2.0) ; extra == "extended-testing"
 Requires-Dist: praw (>=7.7.1,<8.0.0) ; extra == "extended-testing"
+Requires-Dist: premai (>=0.3.25,<0.4.0) ; extra == "extended-testing"
 Requires-Dist: psychicapi (>=0.8.0,<0.9.0) ; extra == "extended-testing"
 Requires-Dist: py-trello (>=0.19.0,<0.20.0) ; extra == "extended-testing"
+Requires-Dist: pyjwt (>=2.8.0,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: pymupdf (>=1.22.3,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: pypdf (>=3.4.0,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: pypdfium2 (>=4.10.0,<5.0.0) ; extra == "extended-testing"
 Requires-Dist: pyspark (>=3.4.0,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: rank-bm25 (>=0.2.2,<0.3.0) ; extra == "extended-testing"
 Requires-Dist: rapidfuzz (>=3.1.1,<4.0.0) ; extra == "extended-testing"
 Requires-Dist: rapidocr-onnxruntime (>=1.3.2,<2.0.0) ; (python_full_version >= "3.8.1" and python_version < "3.12") and (extra == "extended-testing")
+Requires-Dist: rdflib (==7.0.0) ; extra == "extended-testing"
 Requires-Dist: requests (>=2,<3)
 Requires-Dist: requests-toolbelt (>=1.0.0,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: rspace_client (>=2.5.0,<3.0.0) ; extra == "extended-testing"
 Requires-Dist: scikit-learn (>=1.2.2,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: sqlite-vss (>=0.1.2,<0.2.0) ; extra == "extended-testing"
 Requires-Dist: streamlit (>=1.18.0,<2.0.0) ; (python_full_version >= "3.8.1" and python_full_version != "3.9.7" and python_version < "4.0") and (extra == "extended-testing")
 Requires-Dist: sympy (>=1.12,<2.0) ; extra == "extended-testing"
 Requires-Dist: telethon (>=1.28.5,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: tenacity (>=8.1.0,<9.0.0)
+Requires-Dist: tidb-vector (>=0.0.3,<1.0.0) ; extra == "extended-testing"
 Requires-Dist: timescale-vector (>=0.0.1,<0.0.2) ; extra == "extended-testing"
 Requires-Dist: tqdm (>=4.48.0) ; extra == "extended-testing"
+Requires-Dist: tree-sitter (>=0.20.2,<0.21.0) ; extra == "extended-testing"
+Requires-Dist: tree-sitter-languages (>=1.8.0,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: typer (>=0.9.0,<0.10.0) ; extra == "cli"
 Requires-Dist: upstash-redis (>=0.15.0,<0.16.0) ; extra == "extended-testing"
+Requires-Dist: vdms (>=0.0.20,<0.0.21) ; extra == "extended-testing"
 Requires-Dist: xata (>=1.0.0a7,<2.0.0) ; extra == "extended-testing"
 Requires-Dist: xmltodict (>=0.13.0,<0.14.0) ; extra == "extended-testing"
 Project-URL: Repository, https://github.com/langchain-ai/langchain
 Description-Content-Type: text/markdown
 
 #  LangChain Community
 
 [![Downloads](https://static.pepy.tech/badge/langchain_community/month)](https://pepy.tech/project/langchain_community)
 [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
 
 ## Quick Install
 
 ```bash
-pip install langchain-community
+pip install gigachain-community
 ```
 
 ## What is it?
 
 LangChain Community contains third-party integrations that implement the base interfaces defined in LangChain Core, making them ready-to-use in any LangChain application.
 
 For full documentation see the [API reference](https://api.python.langchain.com/en/stable/community_api_reference.html).
 
-![LangChain Stack](../../docs/static/img/langchain_stack.png)
+![Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.](../../docs/static/img/langchain_stack.png "LangChain Framework Overview")
 
 ##  Releases & Versioning
 
 `langchain-community` is currently on version `0.0.x`
 
 All changes will be accompanied by a patch version increase.
```

